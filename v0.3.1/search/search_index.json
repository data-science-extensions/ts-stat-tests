{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"code/","title":"Details about the tests","text":""},{"location":"code/#intro","title":"Intro","text":"<p>TL;DR</p> <p>There are a number of other really good libraries which implements these tests individually:</p> <ul> <li><code>pmdarima</code></li> <li><code>statsmodels</code></li> <li><code>arch</code></li> <li><code>tsfeatures</code></li> <li><code>antropy</code></li> <li><code>scipy</code></li> </ul> <p>These packages all implement the statistical tests in a slightly different way.  However, no one library contains all of the required tests, all in one place.</p>"},{"location":"code/#implementation-progress","title":"Implementation Progress","text":"module algorithms tests unit-tests Correlation <p>6/6   =  100%</p> <p>2/2  = 100%</p> <p>19/19   = 100%</p> Regularity <p>5/5   =  100%</p> <p>2/2  = 100%</p> <p>34/34   = 100%</p> Seasonality <p>0/6   =    0%</p> <p>0/2  =   0%</p> <p>0/10    =   0%</p> Stability <p>0/2   =    0%</p> <p>0/2  =   0%</p> <p>0/4     =   0%</p> Stationarity <p>0/7   =    0%</p> <p>0/2  =   0%</p> <p>0/44    =   0%</p> Normality <p>5/5   =  100%</p> <p>2/2  = 100%</p> <p>12/12   = 100%</p> Linearity <p>0/4   =    0%</p> <p>0/2  =   0%</p> <p>0/0     =   0%</p> Heteroscedasticity <p>0/4   =    0%</p> <p>0/2  =   0%</p> <p>0/0     =   0%</p> Overall <p>16/39 =   41%</p> <p>6/16 =  38%</p> <p>65/119 =  55%</p>"},{"location":"code/#tests","title":"Tests","text":"<p>Details</p> <p>Legend:</p> icon description \u2705 Already implemented in this package \ud83d\udd32 To be developed and implemented \u274e Will not be implemented as it is covered by a function from a different package Test InfoPython Import category algorithm library:test Correlation Auto-Correlation function (ACF) \u2705<code>statsmodels</code>:<code>acf()</code>\u274e<code>pmdarima</code>:<code>acf()</code> Correlation Partial Auto-Correlation function (PACF) \u2705<code>statsmodels</code>:<code>pacf()</code>\u274e<code>pmdarima</code>:<code>pacf()</code> Correlation Cross-Correlation function (CCF) \u2705<code>statsmodels</code>:<code>ccf()</code> Correlation Ljung-Box test of autocorrelation in residuals (LB) \u2705<code>statsmodels</code>:<code>acorr_ljungbox()</code> Correlation Lagrange Multiplier tests for autocorrelation (LM) \u2705<code>statsmodels</code>:<code>acorr_lm()</code> Correlation Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation (BGLM) \u2705<code>statsmodels</code>:<code>acorr_breusch_godfrey()</code> Regularity Approximate Entropy \u2705<code>antropy</code>:<code>app_entropy()</code> Regularity Sample Entropy \u2705<code>antropy</code>:<code>sample_entropy()</code> Regularity Permutation Entropy \u2705<code>antropy</code>:<code>perm_entropy()</code> Regularity Spectral Entropy \u2705<code>antropy</code>:<code>spectral_entropy()</code> Regularity SVD Entropy \u2705<code>antropy</code>:<code>svd_entropy()</code> Seasonality QS \u2705<code>seastests</code>:<code>qs()</code> Seasonality Osborn-Chui-Smith-Birchenhall test of seasonality (OCSB) \u2705<code>pmdarima</code>:<code>OCSBTest()</code> Seasonality Canova-Hansen test for seasonal differences (CH) \u2705<code>pmdarima</code>:<code>CHTest()</code> Seasonality Seasonal Strength \u2705<code>tsfeatures</code>:<code>stl_features()</code> Seasonality Trend Strength \u2705<code>tsfeatures</code>:<code>stl_features()</code> Seasonality Spikiness \u2705<code>tsfeatures</code>:<code>stl_features()</code> Stability Stability \u2705<code>tsfeatures</code>:<code>stability()</code> Stability Lumpiness \u2705<code>tsfeatures</code>:<code>lumpiness()</code> Stationarity Augmented Dickey-Fuller test for stationarity (ADF) \u2705<code>statsmodels</code>:<code>adfuller()</code>\u274e<code>pmdarima</code>:<code>ADFTest()</code>\u274e<code>arch</code>:<code>ADF()</code> Stationarity Kwiatkowski-Phillips-Schmidt-Shin test for stationarity (KPSS) \u2705<code>statsmodels</code>:<code>kpss()</code>\u274e<code>pmdarima</code>:<code>KPSSTest()</code>\u274e<code>arch</code>:<code>KPSS()</code> Stationarity Range unit-root test for stationarity (RUR) \u2705<code>statsmodels</code>:<code>range_unit_root_test()</code> Stationarity Zivot-Andrews structural-break unit-root test (ZA) \u2705<code>statsmodels</code>:<code>zivot_andrews()</code>\u274e<code>arch</code>:<code>ZivotAndrews()</code> Stationarity Phillips-Peron test for stationarity (PP) \u2705<code>pmdarima</code>:<code>PPTest()</code>\u274e<code>arch</code>:<code>PhillipsPerron()</code> Stationarity Elliott-Rothenberg-Stock (ERS) de-trended Dickey-Fuller test \u2705<code>arch</code>:<code>DFGLS()</code> Stationarity Variance Ratio (VR) test for a random walk \u2705<code>arch</code>:<code>VarianceRatio()</code> Normality Jarque-Bera test of normality (JB) \u2705<code>statsmodels</code>:<code>jarque_bera()</code> Normality Omnibus test for normality (OB) \u2705<code>statsmodels</code>:<code>omni_normtest()</code> Normality Shapiro-Wilk test for normality (SW) \u2705<code>scipy</code>:<code>shapiro()</code> Normality D'Agostino &amp; Pearson's test for normality (DP) \u2705<code>scipy</code>:<code>normaltest()</code> Normality Anderson-Darling test for normality (AD) \u2705<code>scipy</code>:<code>anderson()</code> Linearity Harvey Collier test for linearity (HC) \ud83d\udd32<code>statsmodels</code>:<code>linear_harvey_collier()</code> Linearity Lagrange Multiplier test for linearity (LM) \ud83d\udd32<code>statsmodels</code>:<code>linear_lm()</code> Linearity Rainbow test for linearity (RB) \ud83d\udd32<code>statsmodels</code>:<code>linear_rainbow()</code> Linearity Ramsey's RESET test for neglected nonlinearity (RR) \ud83d\udd32<code>statsmodels</code>:<code>linear_reset()</code> Heteroscedasticity Engle's Test for Autoregressive Conditional Heteroscedasticity (ARCH) \ud83d\udd32<code>statsmodels</code>:<code>het_arch()</code> Heteroscedasticity Breusch-Pagan Lagrange Multiplier test for heteroscedasticity (BPL) \ud83d\udd32<code>statsmodels</code>:<code>het_breuschpagan()</code> Heteroscedasticity Goldfeld-Quandt test for homoskedasticity (GQ) \ud83d\udd32<code>statsmodels</code>:<code>het_goldfeldquandt()</code> Heteroscedasticity White's Lagrange Multiplier Test for Heteroscedasticity (WLM) \ud83d\udd32<code>statsmodels</code>:<code>het_white()</code> Covariance ... test library:import ADF pmdarima: <code>from pmdarima.arima import ADFTest</code>statsmodels: <code>from statsmodels.tsa.stattools import adfuller</code>arch: <code>from arch.unitroot import ADF</code> KPSS pmdarima: <code>from pmdarima.arima import KPSSTest</code>statsmodels: <code>from statsmodels.tsa.stattools import kpss</code>arch: <code>from arch.unitroot import KPSS</code> PP pmdarima: <code>from pmdarima.arima import PPTest</code>arch: <code>from arch.unitroot import PhillipsPerron</code> RUR pmdarima: <code>from statsmodels.tsa.stattools import range_unit_root_test</code> ZA pmdarima: <code>from statsmodels.tsa.stattools import zivot_andrews</code> arch: <code>from arch.unitroot import ZivotAndrews</code> OCSB pmdarima: <code>from pmdarima.arima import OCSBTest</code> CH pmdarima: <code>from pmdarima.arima import CHTest</code> ACF pmdarima: <code>from pmdarima.utils import acf</code>statsmodels: <code>from statsmodels.tsa.stattools import acf</code> PACF pmdarima: <code>from pmdarima.utils import pacf</code>statsmodels: <code>from statsmodels.tsa.stattools import pacf</code> CCF statsmodels: <code>from statsmodels.tsa.stattools import ccf</code> ALB statsmodels: <code>from statsmodels.stats.diagnostic import acorr_ljungbox</code> ALM statsmodels: <code>from statsmodels.stats.diagnostic import acorr_lm</code> ABG statsmodels: <code>from statsmodels.stats.diagnostic import acorr_breusch_godfrey</code> JB statsmodels: <code>from statsmodels.stats.stattools import jarque_bera</code> OB statsmodels: <code>from statsmodels.stats.stattools import omni_normtest</code> HC statsmodels: <code>from statsmodels.stats.diagnostic import linear_harvey_collier</code> LM statsmodels: <code>from statsmodels.stats.diagnostic import linear_lm</code> RB statsmodels: <code>from statsmodels.stats.diagnostic import linear_rainbow</code> RR statsmodels: <code>from statsmodels.stats.diagnostic import linear_reset</code> ARCH statsmodels: <code>from statsmodels.stats.diagnostic import het_arch</code> BPL statsmodels: <code>from statsmodels.stats.diagnostic import het_breuschpagan</code> GQ statsmodels: <code>from statsmodels.stats.diagnostic import het_goldfeldquandt</code> WLM statsmodels: <code>from statsmodels.stats.diagnostic import het_white</code>"},{"location":"code/correlation/","title":"Test the <code>correlation</code> of a given Time-Series Dataset","text":""},{"location":"code/correlation/#introduction","title":"Introduction","text":"<p>Summary</p> <p>As stated by Anais Dotis-Georgiou:</p> <p>The term autocorrelation refers to the degree of similarity between A) a given time series, and B) a lagged version of itself, over C) successive time intervals. In other words, autocorrelation is intended to measure the relationship between a variable's present value and any past values that you may have access to.</p> <p>Therefore, a time series autocorrelation attempts to measure the current values of a variable against the historical data of that variable. It ultimately plots one series over the other, and determines the degree of similarity between the two.</p> <p>For the sake of comparison, autocorrelation is essentially the exact same process that you would go through when calculating the correlation between two different sets of time series values on your own. The major difference here is that autocorrelation uses the same time series two times: once in its original values, and then again once a few different time periods have occurred.</p> <p>Autocorrelation is also known as serial correlation, time series correlation and lagged correlation. Regardless of how it's being used, autocorrelation is an ideal method for uncovering trends and patterns in time series data that would have otherwise gone undiscovered.</p> <p> For more info, see: InfluxData: Autocorrelation in Time Series Data.</p> <p>Info</p> <p>An important test to do on Time-Series data is to measure it's level of Auto-Correlation (McMurry &amp; Politis, 2010; Hyndman, nd.(b)). While 'correlation' refers to how two variables change based on the other's value, 'auto-correlation' is how a variable changes based on it's own value over time (the phrase \"auto\" refers to \"self\"). For the Auto-Correlation Function, it uses a '<code>lag</code>' function. For example, a lag value of <code>0</code> is 100% correlated, which is logical, because that is it's own value; whereas a lag value of <code>1</code> or greater, the level of auto-correlation decreases as it gets further away from <code>lag0</code>.</p> <p>For well-structured time-series data sets, it would be expected to see a conical-shaped Auto-Correlation plot. If it were not a well-structured time-series data set, then this Auto-Correlation plot would look more like white noise, and there would not be any logical shape. The blue dotted lines are included as a reference point for determining if any of the observations are significantly different from zero.</p> <p>Moreover, analysis of the data's Auto-Correlation (ACF) should be combined with analysis of its Partial Auto-Correlation (PACF). While the ACF is the \"direct\" relationship between an observation and it's relevant lag observation, the PACF removes the \"indirect\" relationship between these observations. Effectively, the Partial Auto-Correlation between <code>lag1</code> and <code>lag5</code> is the \"actual\" correlation between these two observations, after removing the influence that <code>lag2</code>, <code>lag3</code>, and <code>lag4</code> has on <code>lag5</code>.</p> <p>What this means is that the Partial Auto-Correlation plot would have a very high value at <code>lag0</code>, which will drop very quickly at <code>lag1</code>, and should remain below the blue reference lines for the remainder of the Correlogram. The observations of <code>lag&gt;0</code> should resemble white noise data points. If it does not resemble white noise, and there is a distinct pattern occurring, then the data is not suitable for time-series forecasting.</p> <p> For more info, see: Time Series Analysis in Python: A Comprehensive Guide with Examples.</p> <p>Source Library</p> <p>The <code>statsmodels</code> package was chosen because it provides mature, well-tested implementations of core time-series tools (such as ACF, PACF, and correlograms), integrates seamlessly with NumPy and pandas data structures, and offers a comprehensive suite of statistical tests that align closely with the methods demonstrated in this project.</p> <p>Source Module</p> <p>All of the source code can be found within this modules:</p> <ul> <li><code>ts_stat_tests.algorithms.correlation</code>.</li> <li><code>ts_stat_tests.tests.correlation</code>.</li> </ul>"},{"location":"code/correlation/#correlation-tests","title":"Correlation Tests","text":""},{"location":"code/correlation/#ts_stat_tests.tests.correlation","title":"ts_stat_tests.tests.correlation","text":"<p>Summary</p> <p>This module contains tests for the correlation functions defined in the <code>ts_stat_tests.algorithms.correlation</code> module.</p>"},{"location":"code/correlation/#ts_stat_tests.tests.correlation.correlation","title":"correlation","text":"<pre><code>correlation(\n    x: ArrayLike,\n    algorithm: Literal[\"acf\", \"auto\", \"ac\"],\n    **kwargs: Any\n) -&gt; Union[np.ndarray, tuple[np.ndarray, ...]]\n</code></pre><pre><code>correlation(\n    x: ArrayLike1D,\n    algorithm: Literal[\"pacf\", \"partial\", \"pc\"],\n    **kwargs: Any\n) -&gt; Union[np.ndarray, tuple[np.ndarray, ...]]\n</code></pre><pre><code>correlation(\n    x: ArrayLike,\n    algorithm: Literal[\n        \"ccf\", \"cross\", \"cross-correlation\", \"cc\"\n    ],\n    **kwargs: Any\n) -&gt; Union[np.ndarray, tuple[np.ndarray, ...]]\n</code></pre><pre><code>correlation(\n    x: ArrayLike,\n    algorithm: Literal[\n        \"lb\",\n        \"alb\",\n        \"acorr_ljungbox\",\n        \"acor_lb\",\n        \"a_lb\",\n        \"ljungbox\",\n    ],\n    **kwargs: Any\n) -&gt; pd.DataFrame\n</code></pre><pre><code>correlation(\n    x: ArrayLike,\n    algorithm: Literal[\"lm\", \"alm\", \"acorr_lm\", \"a_lm\"],\n    **kwargs: Any\n) -&gt; Union[\n    tuple[float, float, float, float],\n    tuple[float, float, float, float, ResultsStore],\n]\n</code></pre><pre><code>correlation(\n    x: Union[RegressionResults, RegressionResultsWrapper],\n    algorithm: Literal[\"bglm\", \"breusch_godfrey\", \"bg\"],\n    **kwargs: Any\n) -&gt; Union[\n    tuple[float, float, float, float],\n    tuple[float, float, float, float, ResultsStore],\n]\n</code></pre> <pre><code>correlation(\n    x: Union[\n        ArrayLike,\n        ArrayLike1D,\n        RegressionResults,\n        RegressionResultsWrapper,\n    ],\n    algorithm: str = \"acf\",\n    **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Summary</p> <p>A unified interface for various correlation tests.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[ArrayLike, ArrayLike1D, RegressionResults, RegressionResultsWrapper]</code> <p>The input time series data or regression results.</p> required <code>algorithm</code> <code>str</code> <p>The correlation algorithm to use. Options include: - \"acf\", \"auto\", \"ac\": Autocorrelation Function - \"pacf\", \"partial\", \"pc\": Partial Autocorrelation Function - \"ccf\", \"cross\", \"cross-correlation\", \"cc\": Cross-Correlation Function - \"lb\", \"alb\", \"acorr_ljungbox\", \"acor_lb\", \"a_lb\", \"ljungbox\": Ljung-Box Test - \"lm\", \"alm\", \"acorr_lm\", \"a_lm\": Lagrange Multiplier Test - \"bglm\", \"breusch_godfrey\", \"bg\": Breusch-Godfrey Test</p> <code>'acf'</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments specific to the chosen algorithm.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the specified correlation test.</p> Source code in <code>src/ts_stat_tests/tests/correlation.py</code> <pre><code>@typechecked\ndef correlation(\n    x: Union[ArrayLike, ArrayLike1D, RegressionResults, RegressionResultsWrapper],\n    algorithm: str = \"acf\",\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"\n    !!! note \"Summary\"\n        A unified interface for various correlation tests.\n\n    Params:\n        x (Union[ArrayLike, ArrayLike1D, RegressionResults, RegressionResultsWrapper]):\n            The input time series data or regression results.\n        algorithm (str):\n            The correlation algorithm to use. Options include:\n            - \"acf\", \"auto\", \"ac\": Autocorrelation Function\n            - \"pacf\", \"partial\", \"pc\": Partial Autocorrelation Function\n            - \"ccf\", \"cross\", \"cross-correlation\", \"cc\": Cross-Correlation Function\n            - \"lb\", \"alb\", \"acorr_ljungbox\", \"acor_lb\", \"a_lb\", \"ljungbox\": Ljung-Box Test\n            - \"lm\", \"alm\", \"acorr_lm\", \"a_lm\": Lagrange Multiplier Test\n            - \"bglm\", \"breusch_godfrey\", \"bg\": Breusch-Godfrey Test\n        kwargs (Any):\n            Additional keyword arguments specific to the chosen algorithm.\n\n    Returns:\n        (Any):\n            The result of the specified correlation test.\n    \"\"\"\n\n    options: dict[str, tuple[str, ...]] = {\n        \"acf\": (\"acf\", \"auto\", \"ac\"),\n        \"pacf\": (\"pacf\", \"partial\", \"pc\"),\n        \"ccf\": (\"ccf\", \"cross\", \"cross-correlation\", \"cc\"),\n        \"lb\": (\"alb\", \"acorr_ljungbox\", \"acor_lb\", \"a_lb\", \"lb\", \"ljungbox\"),\n        \"lm\": (\"alm\", \"acorr_lm\", \"a_lm\", \"lm\"),\n        \"bglm\": (\"bglm\", \"breusch_godfrey\", \"bg\"),\n    }\n\n    if algorithm in options[\"acf\"]:\n        return _acf(x=x, **kwargs)  # type: ignore\n\n    if algorithm in options[\"pacf\"]:\n        return _pacf(x=x, **kwargs)  # type: ignore\n\n    if algorithm in options[\"lb\"]:\n        return _lb(x=x, **kwargs)  # type: ignore\n\n    if algorithm in options[\"lm\"]:\n        return _lm(resid=x, **kwargs)  # type: ignore\n\n    if algorithm in options[\"ccf\"]:\n        if \"y\" not in kwargs or kwargs[\"y\"] is None:\n            raise ValueError(\"The 'ccf' algorithm requires a 'y' parameter.\")\n        return _ccf(x=x, **kwargs)  # type: ignore\n\n    if algorithm in options[\"bglm\"]:\n        return _bglm(res=x, **kwargs)  # type: ignore\n\n    raise ValueError(\n        generate_error_message(\n            parameter_name=\"algorithm\",\n            value_parsed=algorithm,\n            options=options,\n        )\n    )\n</code></pre>"},{"location":"code/correlation/#ts_stat_tests.tests.correlation.is_correlated","title":"is_correlated","text":"<pre><code>is_correlated() -&gt; None\n</code></pre> <p>Summary</p> <p>A placeholder function for checking if a time series is correlated.</p> Source code in <code>src/ts_stat_tests/tests/correlation.py</code> <pre><code>@typechecked\ndef is_correlated() -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        A placeholder function for checking if a time series is correlated.\n    \"\"\"\n    raise NotImplementedError(\"is_correlated is a placeholder and has not been implemented yet.\")\n</code></pre>"},{"location":"code/correlation/#correlation-algorithms","title":"Correlation Algorithms","text":""},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation","title":"ts_stat_tests.algorithms.correlation","text":"<p>Summary</p> <p>The correlation algorithms module provides functions to compute correlation measures for time series data, including the autocorrelation function (ACF), partial autocorrelation function (PACF), and cross-correlation function (CCF). These measures help identify relationships and dependencies between time series variables, which are essential for time series analysis and forecasting.</p> <p>This module leverages the <code>statsmodels</code> library to implement these correlation measures, ensuring robust and efficient computations. The functions are designed to handle various input scenarios and provide options for customization, such as specifying the number of lags, confidence intervals, and handling missing data.</p> <p>By using these correlation algorithms, users can gain insights into the temporal dependencies within their time series data, aiding in model selection and improving forecasting accuracy.</p>"},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation.acf","title":"acf","text":"<pre><code>acf(\n    x: ArrayLike,\n    adjusted: bool = False,\n    nlags: Optional[int] = None,\n    fft: bool = True,\n    bartlett_confint: bool = True,\n    missing: VALID_ACF_MISSING_OPTIONS = \"none\",\n    *,\n    qstat: Literal[False] = False,\n    alpha: None = None\n) -&gt; np.ndarray\n</code></pre><pre><code>acf(\n    x: ArrayLike,\n    adjusted: bool = False,\n    nlags: Optional[int] = None,\n    fft: bool = True,\n    bartlett_confint: bool = True,\n    missing: VALID_ACF_MISSING_OPTIONS = \"none\",\n    *,\n    qstat: Literal[False] = False,\n    alpha: float\n) -&gt; tuple[np.ndarray, np.ndarray]\n</code></pre><pre><code>acf(\n    x: ArrayLike,\n    adjusted: bool = False,\n    nlags: Optional[int] = None,\n    fft: bool = True,\n    bartlett_confint: bool = True,\n    missing: VALID_ACF_MISSING_OPTIONS = \"none\",\n    *,\n    qstat: Literal[True],\n    alpha: None = None\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]\n</code></pre><pre><code>acf(\n    x: ArrayLike,\n    adjusted: bool = False,\n    nlags: Optional[int] = None,\n    fft: bool = True,\n    bartlett_confint: bool = True,\n    missing: VALID_ACF_MISSING_OPTIONS = \"none\",\n    *,\n    qstat: Literal[True],\n    alpha: float\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n</code></pre> <pre><code>acf(\n    x: ArrayLike,\n    adjusted: bool = False,\n    nlags: Optional[int] = None,\n    fft: bool = True,\n    bartlett_confint: bool = True,\n    missing: VALID_ACF_MISSING_OPTIONS = \"none\",\n    *,\n    qstat: bool = False,\n    alpha: Optional[float] = None\n) -&gt; Union[\n    np.ndarray,\n    tuple[np.ndarray, np.ndarray],\n    tuple[np.ndarray, np.ndarray, np.ndarray],\n    tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n]\n</code></pre> <p>Summary</p> <p>The autocorrelation function (ACF) is a statistical tool used to study the correlation between a time series and its lagged values. In time series forecasting, the ACF is used to identify patterns and relationships between values in a time series at different lags, which can then be used to make predictions about future values.</p> <p>This function will implement the <code>acf()</code> function from the <code>statsmodels</code> library.</p> Details <p>The acf at lag <code>0</code> (ie., <code>1</code>) is returned.</p> <p>For very long time series it is recommended to use <code>fft</code> convolution instead. When <code>fft</code> is <code>False</code> uses a simple, direct estimator of the autocovariances that only computes the first \\(nlag + 1\\) values. This can be much faster when the time series is long and only a small number of autocovariances are needed.</p> <p>If <code>adjusted</code> is <code>True</code>, the denominator for the autocovariance is adjusted for the loss of data.</p> <p>The ACF measures the correlation between a time series and its lagged values at different lags. The correlation is calculated as the ratio of the covariance between the series and its lagged values to the product of their standard deviations. The ACF is typically plotted as a graph, with the lag on the <code>x</code>-axis and the correlation coefficient on the <code>y</code>-axis.</p> <p>The ACF at lag \\(k\\) is defined as:</p> \\[ ACF(k) = \\frac{ Cov(Y_t, Y_{t-k}) } { Var(Y_t) \\times Var(Y_{t-k}) } \\] <p>where:</p> <ul> <li>\\(Y_t\\) and \\(Y_{t-k}\\) are the values of the time series at time \\(t\\) and time \\(t-k\\), respectively,</li> <li>\\(Cov(Y_t, Y_{t-k})\\) is the covariance between the two values, and</li> <li>\\(Var(Y_t)\\) and \\(Var(Y_{t-k})\\) are the variances of the two values.</li> </ul> <pre><code>ACF(k) = Cov(Y_t, Y_{t-k}) / (sqrt(Var(Y_t)) * sqrt(Var(Y_{t-k})))\n</code></pre> <p>For a stationary series, this simplifies to:</p> <pre><code>ACF(k) = Cov(Y_t, Y_{t-k}) / Var(Y_t)\n</code></pre> <p>If the ACF shows a strong positive correlation at lag \\(k\\), this means that values in the time series at time \\(t\\) and time \\(t-k\\) are strongly related. This can be useful in forecasting, as it suggests that past values can be used to predict future values. If the ACF shows a strong negative correlation at lag \\(k\\), this means that values at time \\(t\\) and time \\(t-k\\) are strongly inversely related, which can also be useful in forecasting.</p> <p>The ACF can be used to identify the order of an autoregressive (AR) model, which is a type of model used in time series forecasting. The order of an AR model is the number of lags that are used to predict future values. The ACF can also be used to diagnose the presence of seasonality in a time series.</p> <p>Overall, the autocorrelation function is a valuable tool in time series forecasting, as it helps to identify patterns and relationships between values in a time series that can be used to make predictions about future values.</p> <p>The ACF can be calculated using the <code>acf()</code> function in the <code>statsmodels</code> package in Python. The function takes a time series array as input and returns an array of autocorrelation coefficients at different lags. The significance of the autocorrelation coefficients can be tested using the Ljung-Box test, which tests the null hypothesis that the autocorrelation coefficients are zero up to a certain lag. The Ljung-Box test can be performed using the <code>acorr_ljungbox()</code> function in the <code>statsmodels</code> package. If the p-value of the test is less than a certain significance level (e.g. \\(0.05\\)), then there is evidence of significant autocorrelation in the time series up to the specified lag.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The time series data.</p> required <code>adjusted</code> <code>bool</code> <p>If <code>True</code>, then denominators for auto-covariance are \\(n-k\\), otherwise \\(n\\). Defaults to <code>False</code>.</p> <code>False</code> <code>nlags</code> <code>Optional[int]</code> <p>Number of lags to return autocorrelation for. If not provided, uses \\(\\min(10 \\times \\text{log10}(nobs),nobs-1)\\) (calculated with: <code>min(int(10 * np.log10(nobs)), nobs - 1)</code>). The returned value includes \\(lag 0\\) (ie., \\(1\\)) so size of the acf vector is \\((nlags + 1,)\\). Defaults to <code>None</code>.</p> <code>None</code> <code>qstat</code> <code>bool</code> <p>If <code>True</code>, also returns the Ljung-Box \\(q\\) statistic and corresponding p-values for each autocorrelation coefficient; see the Returns section for details. Defaults to <code>False</code>.</p> <code>False</code> <code>fft</code> <code>bool</code> <p>If <code>True</code>, computes the ACF via FFT. Defaults to <code>True</code>.</p> <code>True</code> <code>alpha</code> <code>Optional[float]</code> <p>If a number is given, the confidence intervals for the given level are returned. For instance if <code>alpha=0.05</code>, a \\(95\\%\\) confidence intervals are returned where the standard deviation is computed according to Bartlett\"s formula. Defaults to <code>None</code>.</p> <code>None</code> <code>bartlett_confint</code> <code>bool</code> <p>Confidence intervals for ACF values are generally placed at 2 standard errors around \\(r_k\\). The formula used for standard error depends upon the situation. If the autocorrelations are being used to test for randomness of residuals as part of the ARIMA routine, the standard errors are determined assuming the residuals are white noise. The approximate formula for any lag is that standard error of each \\(r_k = \\frac{1}{\\sqrt{N}}\\). See section 9.4 of [2] for more details on the \\(\\frac{1}{\\sqrt{N}}\\) result. For more elementary discussion, see section 5.3.2 in [3]. For the ACF of raw data, the standard error at a lag \\(k\\) is found as if the right model was an \\(\\text{MA}(k-1)\\). This allows the possible interpretation that if all autocorrelations past a certain lag are within the limits, the model might be an \\(\\text{MA}\\) of order defined by the last significant autocorrelation. In this case, a moving average model is assumed for the data and the standard errors for the confidence intervals should be generated using Bartlett's formula. For more details on Bartlett formula result, see section 7.2 in [2]. Defaults to <code>True</code>.</p> <code>True</code> <code>missing</code> <code>VALID_ACF_MISSING_OPTIONS</code> <p>A string in <code>[\"none\", \"raise\", \"conservative\", \"drop\"]</code> specifying how the <code>NaN</code>'s are to be treated.</p> <ul> <li><code>\"none\"</code> performs no checks.</li> <li><code>\"raise\"</code> raises an exception if NaN values are found.</li> <li><code>\"drop\"</code> removes the missing observations and then estimates the autocovariances treating the non-missing as contiguous.</li> <li><code>\"conservative\"</code> computes the autocovariance using nan-ops so that nans are removed when computing the mean and cross-products that are used to estimate the autocovariance.</li> </ul> <p>When using <code>\"conservative\"</code>, \\(n\\) is set to the number of non-missing observations. Defaults to <code>\"none\"</code>.</p> <code>'none'</code> <p>Returns:</p> Name Type Description <code>acf</code> <code>ndarray</code> <p>The autocorrelation function for lags <code>0, 1, ..., nlags</code>. Shape <code>(nlags+1,)</code>.</p> <code>confint</code> <code>Optional[ndarray]</code> <p>Confidence intervals for the ACF at lags <code>0, 1, ..., nlags</code>. Shape <code>(nlags + 1, 2)</code>. Returned if <code>alpha</code> is not <code>None</code>.</p> <code>qstat</code> <code>Optional[ndarray]</code> <p>The Ljung-Box Q-Statistic for lags <code>1, 2, ..., nlags</code> (excludes lag zero). Returned if <code>qstat</code> is <code>True</code>.</p> <code>pvalues</code> <code>Optional[ndarray]</code> <p>The p-values associated with the Q-statistics for lags <code>1, 2, ..., nlags</code> (excludes lag zero). Returned if <code>qstat</code> is <code>True</code>.</p> <p>Credit</p> <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> <p>Examples</p> Test ACF without FFT<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from statsmodels.datasets import macrodata\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import acf\n&gt;&gt;&gt; data = macrodata.load_pandas()\n&gt;&gt;&gt; x = data.data[\"realgdp\"]\n&gt;&gt;&gt; res_acf, res_confint, res_qstat, res_pvalues = acf(\n...     x, nlags=40, qstat=True, alpha=0.05, fft=False\n... )\n&gt;&gt;&gt; pprint(res_acf[1:11])\narray([0.94804734, 0.87557484, 0.80668116, 0.75262542, 0.71376997,\n       0.6817336 , 0.66290439, 0.65561048, 0.67094833, 0.70271992])\n&gt;&gt;&gt; pprint(res_confint[1:11])\narray([[0.78471701, 1.11137767],\n       [0.60238868, 1.14876099],\n       [0.46677939, 1.14658292],\n       [0.36500159, 1.14024925],\n       [0.28894752, 1.13859242],\n       [0.22604068, 1.13742653],\n       [0.18077091, 1.14503787],\n       [0.14974636, 1.16147461],\n       [0.1429036 , 1.19899305],\n       [0.15240228, 1.25303756]])\n&gt;&gt;&gt; pprint(res_qstat[:10])\narray([132.14153858, 245.64616028, 342.67482586, 427.73868355,\n       504.79657041, 575.6018536 , 643.03859337, 709.48449817,\n       779.59123116, 857.06863862])\n&gt;&gt;&gt; pprint(res_pvalues[:10])\narray([1.39323140e-030, 4.55631819e-054, 5.75108846e-074, 2.81773062e-091,\n       7.36019524e-107, 4.26400770e-121, 1.30546283e-134, 6.49627091e-148,\n       5.24937010e-162, 1.10078935e-177])\n</code></pre> Test ACF with FFT<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import acf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_acf, res_confint, res_qstat, res_pvalues = acf(\n...     data, nlags=40, qstat=True, alpha=0.05, fft=True\n... )\n&gt;&gt;&gt; pprint(res_acf[1:11])\narray([0.94804734, 0.87557484, 0.80668116, 0.75262542, 0.71376997,\n       0.6817336 , 0.66290439, 0.65561048, 0.67094833, 0.70271992])\n&gt;&gt;&gt; pprint(res_confint[1:11])\narray([[0.78471701, 1.11137767],\n       [0.60238868, 1.14876099],\n       [0.46677939, 1.14658292],\n       [0.36500159, 1.14024925],\n       [0.28894752, 1.13859242],\n       [0.22604068, 1.13742653],\n       [0.18077091, 1.14503787],\n       [0.14974636, 1.16147461],\n       [0.1429036 , 1.19899305],\n       [0.15240228, 1.25303756]])\n&gt;&gt;&gt; pprint(res_qstat[:10])\narray([132.14153858, 245.64616028, 342.67482586, 427.73868355,\n       504.79657041, 575.6018536 , 643.03859337, 709.48449817,\n       779.59123116, 857.06863862])\n&gt;&gt;&gt; pprint(res_pvalues[:10])\narray([1.39323140e-030, 4.55631819e-054, 5.75108846e-074, 2.81773062e-091,\n       7.36019524e-107, 4.26400770e-121, 1.30546283e-134, 6.49627091e-148,\n       5.24937010e-162, 1.10078935e-177])\n</code></pre> References <ol> <li>Parzen, E., 1963. On spectral analysis with missing observations and amplitude modulation. Sankhya: The Indian Journal of Statistics, Series A, pp.383-392.</li> <li>Brockwell and Davis, 1987. Time Series Theory and Methods.</li> <li>Brockwell and Davis, 2010. Introduction to Time Series and Forecasting, 2nd edition.</li> </ol> See Also <ul> <li><code>statsmodels.tsa.stattools.acf</code>: Estimate the autocorrelation function.</li> <li><code>statsmodels.tsa.stattools.pacf</code>: Partial autocorrelation estimation.</li> <li><code>statsmodels.tsa.stattools.ccf</code>: The cross-correlation function.</li> <li><code>ts_stat_tests.algorithms.correlation.acf</code>: Estimate the autocorrelation function</li> <li><code>ts_stat_tests.algorithms.correlation.pacf</code>: Partial autocorrelation estimate.</li> <li><code>ts_stat_tests.algorithms.correlation.ccf</code>: The cross-correlation function.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/correlation.py</code> <pre><code>@typechecked\ndef acf(\n    x: ArrayLike,\n    adjusted: bool = False,\n    nlags: Optional[int] = None,\n    fft: bool = True,\n    bartlett_confint: bool = True,\n    missing: VALID_ACF_MISSING_OPTIONS = \"none\",\n    *,\n    qstat: bool = False,\n    alpha: Optional[float] = None,\n) -&gt; Union[\n    np.ndarray,\n    tuple[np.ndarray, np.ndarray],\n    tuple[np.ndarray, np.ndarray, np.ndarray],\n    tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n]:\n    r\"\"\"\n    !!! note \"Summary\"\n\n        The autocorrelation function (ACF) is a statistical tool used to study the correlation between a time series and its lagged values. In time series forecasting, the ACF is used to identify patterns and relationships between values in a time series at different lags, which can then be used to make predictions about future values.\n\n        This function will implement the [`acf()`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.acf.html) function from the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ???+ abstract \"Details\"\n\n        The acf at lag `0` (ie., `1`) is returned.\n\n        For very long time series it is recommended to use `fft` convolution instead. When `fft` is `False` uses a simple, direct estimator of the autocovariances that only computes the first $nlag + 1$ values. This can be much faster when the time series is long and only a small number of autocovariances are needed.\n\n        If `adjusted` is `True`, the denominator for the autocovariance is adjusted for the loss of data.\n\n        The ACF measures the correlation between a time series and its lagged values at different lags. The correlation is calculated as the ratio of the covariance between the series and its lagged values to the product of their standard deviations. The ACF is typically plotted as a graph, with the lag on the `x`-axis and the correlation coefficient on the `y`-axis.\n\n        The ACF at lag $k$ is defined as:\n\n        $$\n        ACF(k) = \\frac{ Cov(Y_t, Y_{t-k}) } { Var(Y_t) \\times Var(Y_{t-k}) }\n        $$\n\n        where:\n\n        - $Y_t$ and $Y_{t-k}$ are the values of the time series at time $t$ and time $t-k$, respectively,\n        - $Cov(Y_t, Y_{t-k})$ is the covariance between the two values, and\n        - $Var(Y_t)$ and $Var(Y_{t-k})$ are the variances of the two values.\n\n        ```\n        ACF(k) = Cov(Y_t, Y_{t-k}) / (sqrt(Var(Y_t)) * sqrt(Var(Y_{t-k})))\n        ```\n\n        For a stationary series, this simplifies to:\n\n        ```\n        ACF(k) = Cov(Y_t, Y_{t-k}) / Var(Y_t)\n        ```\n\n        If the ACF shows a strong positive correlation at lag $k$, this means that values in the time series at time $t$ and time $t-k$ are strongly related. This can be useful in forecasting, as it suggests that past values can be used to predict future values. If the ACF shows a strong negative correlation at lag $k$, this means that values at time $t$ and time $t-k$ are strongly inversely related, which can also be useful in forecasting.\n\n        The ACF can be used to identify the order of an autoregressive (AR) model, which is a type of model used in time series forecasting. The order of an AR model is the number of lags that are used to predict future values. The ACF can also be used to diagnose the presence of seasonality in a time series.\n\n        Overall, the autocorrelation function is a valuable tool in time series forecasting, as it helps to identify patterns and relationships between values in a time series that can be used to make predictions about future values.\n\n        The ACF can be calculated using the `acf()` function in the `statsmodels` package in Python. The function takes a time series array as input and returns an array of autocorrelation coefficients at different lags. The significance of the autocorrelation coefficients can be tested using the Ljung-Box test, which tests the null hypothesis that the autocorrelation coefficients are zero up to a certain lag. The Ljung-Box test can be performed using the `acorr_ljungbox()` function in the `statsmodels` package. If the p-value of the test is less than a certain significance level (e.g. $0.05$), then there is evidence of significant autocorrelation in the time series up to the specified lag.\n\n    Params:\n        x (ArrayLike):\n            The time series data.\n        adjusted (bool, optional):\n            If `True`, then denominators for auto-covariance are $n-k$, otherwise $n$.&lt;br&gt;\n            Defaults to `False`.\n        nlags (Optional[int], optional):\n            Number of lags to return autocorrelation for. If not provided, uses $\\min(10 \\times \\text{log10}(nobs),nobs-1)$ (calculated with: `min(int(10 * np.log10(nobs)), nobs - 1)`). The returned value includes $lag 0$ (ie., $1$) so size of the acf vector is $(nlags + 1,)$.&lt;br&gt;\n            Defaults to `None`.\n        qstat (bool, optional):\n            If `True`, also returns the Ljung-Box $q$ statistic and corresponding p-values for each autocorrelation coefficient; see the *Returns* section for details.&lt;br&gt;\n            Defaults to `False`.\n        fft (bool, optional):\n            If `True`, computes the ACF via FFT.&lt;br&gt;\n            Defaults to `True`.\n        alpha (Optional[float], optional):\n            If a number is given, the confidence intervals for the given level are returned. For instance if `alpha=0.05`, a $95\\%$ confidence intervals are returned where the standard deviation is computed according to Bartlett\"s formula.&lt;br&gt;\n            Defaults to `None`.\n        bartlett_confint (bool, optional):\n            Confidence intervals for ACF values are generally placed at 2 standard errors around $r_k$. The formula used for standard error depends upon the situation. If the autocorrelations are being used to test for randomness of residuals as part of the ARIMA routine, the standard errors are determined assuming the residuals are white noise. The approximate formula for any lag is that standard error of each $r_k = \\frac{1}{\\sqrt{N}}$. See section 9.4 of [2] for more details on the $\\frac{1}{\\sqrt{N}}$ result. For more elementary discussion, see section 5.3.2 in [3]. For the ACF of raw data, the standard error at a lag $k$ is found as if the right model was an $\\text{MA}(k-1)$. This allows the possible interpretation that if all autocorrelations past a certain lag are within the limits, the model might be an $\\text{MA}$ of order defined by the last significant autocorrelation. In this case, a moving average model is assumed for the data and the standard errors for the confidence intervals should be generated using Bartlett's formula. For more details on Bartlett formula result, see section 7.2 in [2].&lt;br&gt;\n            Defaults to `True`.\n        missing (VALID_ACF_MISSING_OPTIONS, optional):\n            A string in `[\"none\", \"raise\", \"conservative\", \"drop\"]` specifying how the `NaN`'s are to be treated.\n\n            - `\"none\"` performs no checks.\n            - `\"raise\"` raises an exception if NaN values are found.\n            - `\"drop\"` removes the missing observations and then estimates the autocovariances treating the non-missing as contiguous.\n            - `\"conservative\"` computes the autocovariance using nan-ops so that nans are removed when computing the mean and cross-products that are used to estimate the autocovariance.\n\n            When using `\"conservative\"`, $n$ is set to the number of non-missing observations.&lt;br&gt;\n            Defaults to `\"none\"`.\n\n    Returns:\n        acf (np.ndarray):\n            The autocorrelation function for lags `0, 1, ..., nlags`.&lt;br&gt;\n            Shape `(nlags+1,)`.\n        confint (Optional[np.ndarray]):\n            Confidence intervals for the ACF at lags `0, 1, ..., nlags`.&lt;br&gt;\n            Shape `(nlags + 1, 2)`.&lt;br&gt;\n            Returned if `alpha` is not `None`.\n        qstat (Optional[np.ndarray]):\n            The Ljung-Box Q-Statistic for lags `1, 2, ..., nlags` (excludes lag zero).&lt;br&gt;\n            Returned if `qstat` is `True`.\n        pvalues (Optional[np.ndarray]):\n            The p-values associated with the Q-statistics for lags `1, 2, ..., nlags` (excludes lag zero).&lt;br&gt;\n            Returned if `qstat` is `True`.\n\n    !!! success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/) library.\n\n    !!! example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test ACF without FFT\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from statsmodels.datasets import macrodata\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import acf\n        &gt;&gt;&gt; data = macrodata.load_pandas()\n        &gt;&gt;&gt; x = data.data[\"realgdp\"]\n        &gt;&gt;&gt; res_acf, res_confint, res_qstat, res_pvalues = acf(\n        ...     x, nlags=40, qstat=True, alpha=0.05, fft=False\n        ... )\n        &gt;&gt;&gt; pprint(res_acf[1:11])\n        array([0.94804734, 0.87557484, 0.80668116, 0.75262542, 0.71376997,\n               0.6817336 , 0.66290439, 0.65561048, 0.67094833, 0.70271992])\n        &gt;&gt;&gt; pprint(res_confint[1:11])\n        array([[0.78471701, 1.11137767],\n               [0.60238868, 1.14876099],\n               [0.46677939, 1.14658292],\n               [0.36500159, 1.14024925],\n               [0.28894752, 1.13859242],\n               [0.22604068, 1.13742653],\n               [0.18077091, 1.14503787],\n               [0.14974636, 1.16147461],\n               [0.1429036 , 1.19899305],\n               [0.15240228, 1.25303756]])\n        &gt;&gt;&gt; pprint(res_qstat[:10])\n        array([132.14153858, 245.64616028, 342.67482586, 427.73868355,\n               504.79657041, 575.6018536 , 643.03859337, 709.48449817,\n               779.59123116, 857.06863862])\n        &gt;&gt;&gt; pprint(res_pvalues[:10])\n        array([1.39323140e-030, 4.55631819e-054, 5.75108846e-074, 2.81773062e-091,\n               7.36019524e-107, 4.26400770e-121, 1.30546283e-134, 6.49627091e-148,\n               5.24937010e-162, 1.10078935e-177])\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test ACF with FFT\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import acf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_acf, res_confint, res_qstat, res_pvalues = acf(\n        ...     data, nlags=40, qstat=True, alpha=0.05, fft=True\n        ... )\n        &gt;&gt;&gt; pprint(res_acf[1:11])\n        array([0.94804734, 0.87557484, 0.80668116, 0.75262542, 0.71376997,\n               0.6817336 , 0.66290439, 0.65561048, 0.67094833, 0.70271992])\n        &gt;&gt;&gt; pprint(res_confint[1:11])\n        array([[0.78471701, 1.11137767],\n               [0.60238868, 1.14876099],\n               [0.46677939, 1.14658292],\n               [0.36500159, 1.14024925],\n               [0.28894752, 1.13859242],\n               [0.22604068, 1.13742653],\n               [0.18077091, 1.14503787],\n               [0.14974636, 1.16147461],\n               [0.1429036 , 1.19899305],\n               [0.15240228, 1.25303756]])\n        &gt;&gt;&gt; pprint(res_qstat[:10])\n        array([132.14153858, 245.64616028, 342.67482586, 427.73868355,\n               504.79657041, 575.6018536 , 643.03859337, 709.48449817,\n               779.59123116, 857.06863862])\n        &gt;&gt;&gt; pprint(res_pvalues[:10])\n        array([1.39323140e-030, 4.55631819e-054, 5.75108846e-074, 2.81773062e-091,\n               7.36019524e-107, 4.26400770e-121, 1.30546283e-134, 6.49627091e-148,\n               5.24937010e-162, 1.10078935e-177])\n        ```\n\n    ??? question \"References\"\n        1. Parzen, E., 1963. On spectral analysis with missing observations and amplitude modulation. Sankhya: The Indian Journal of Statistics, Series A, pp.383-392.\n        1. Brockwell and Davis, 1987. Time Series Theory and Methods.\n        1. Brockwell and Davis, 2010. Introduction to Time Series and Forecasting, 2nd edition.\n\n    ??? tip \"See Also\"\n        - [`statsmodels.tsa.stattools.acf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.acf.html): Estimate the autocorrelation function.\n        - [`statsmodels.tsa.stattools.pacf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf.html): Partial autocorrelation estimation.\n        - [`statsmodels.tsa.stattools.ccf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.ccf.html): The cross-correlation function.\n        - [`ts_stat_tests.algorithms.correlation.acf`][ts_stat_tests.algorithms.correlation.acf]: Estimate the autocorrelation function\n        - [`ts_stat_tests.algorithms.correlation.pacf`][ts_stat_tests.algorithms.correlation.pacf]: Partial autocorrelation estimate.\n        - [`ts_stat_tests.algorithms.correlation.ccf`][ts_stat_tests.algorithms.correlation.ccf]: The cross-correlation function.\n    \"\"\"\n    return st_acf(\n        x=x,\n        adjusted=adjusted,\n        nlags=nlags,\n        qstat=qstat,\n        fft=fft,\n        alpha=alpha,\n        bartlett_confint=bartlett_confint,\n        missing=missing,\n    )\n</code></pre>"},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation.pacf","title":"pacf","text":"<pre><code>pacf(\n    x: ArrayLike1D,\n    nlags: Optional[int] = None,\n    method: VALID_PACF_METHOD_OPTIONS = \"ywadjusted\",\n    *,\n    alpha: None = None\n) -&gt; np.ndarray\n</code></pre><pre><code>pacf(\n    x: ArrayLike1D,\n    nlags: Optional[int] = None,\n    method: VALID_PACF_METHOD_OPTIONS = \"ywadjusted\",\n    *,\n    alpha: float\n) -&gt; tuple[np.ndarray, np.ndarray]\n</code></pre> <pre><code>pacf(\n    x: ArrayLike1D,\n    nlags: Optional[int] = None,\n    method: VALID_PACF_METHOD_OPTIONS = \"ywadjusted\",\n    *,\n    alpha: Optional[float] = None\n) -&gt; Union[np.ndarray, tuple[np.ndarray, np.ndarray]]\n</code></pre> <p>Summary</p> <p>The partial autocorrelation function (PACF) is a statistical tool used in time series forecasting to identify the direct relationship between two variables, controlling for the effect of the other variables in the time series. In other words, the PACF measures the correlation between a time series and its lagged values, while controlling for the effects of other intermediate lags.</p> <p>This function will implement the <code>pacf()</code> function from the <code>statsmodels</code> library.</p> Details <p>Based on simulation evidence across a range of low-order ARMA models, the best methods based on root MSE are Yule-Walker (MLW), Levinson-Durbin (MLE) and Burg, respectively. The estimators with the lowest bias included these three in addition to OLS and OLS-adjusted. Yule-Walker (adjusted) and Levinson-Durbin (adjusted) performed consistently worse than the other options.</p> <p>The PACF is a plot of the correlation between a time series and its lagged values, controlling for the effect of other lags. The PACF is useful for identifying the order of an autoregressive (AR) model, which is a type of model used in time series forecasting. The order of an AR model is the number of lags that are used to predict future values.</p> <p>The PACF at lag \\(k\\) is defined as:</p> \\[ PACF(k) = Corr \\left( Y_t, Y_{t-k} \\mid Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1} \\right) \\] <p>where:</p> <ul> <li>\\(Y_t\\) and \\(Y_{t-k}\\) are the values of the time series at time \\(t\\) and time \\(t-k\\), respectively, and</li> <li>\\(Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1}\\) are the values of the time series at intervening lags.</li> <li>\\(Corr()\\) denotes the correlation coefficient between two variables.</li> </ul> <pre><code>PACF(k) = Corr(Y_t, Y_{t-k} | Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1})\n</code></pre> <p>The PACF is calculated using the Yule-Walker equations, which are a set of linear equations that describe the relationship between a time series and its lagged values. The PACF is calculated as the difference between the correlation coefficient at lag \\(k\\) and the correlation coefficient at lag \\(k-1\\), controlling for the effects of intermediate lags.</p> <p>The PACF is typically plotted as a graph, with the lag on the <code>x</code>-axis and the correlation coefficient on the <code>y</code>-axis. If the PACF shows a strong positive correlation at lag \\(k\\), this means that values in the time series at time \\(t\\) and time \\(t-k\\) are strongly related, after controlling for the effects of intermediate lags. This suggests that past values can be used to predict future values using an AR model with an order of \\(k\\).</p> <p>Overall, the partial autocorrelation function is a valuable tool in time series forecasting, as it helps to identify the order of an autoregressive model and to control for the effects of intermediate lags. By identifying the direct relationship between two variables, the PACF can help to improve the accuracy of time series forecasting models.</p> <p>The PACF can be calculated using the pacf() function in the statsmodels package in Python. The function takes a time series array as input and returns an array of partial autocorrelation coefficients at different lags. The significance of the partial autocorrelation coefficients can be tested using the same Ljung-Box test as for the ACF. If the p-value of the test is less than a certain significance level (e.g. \\(0.05\\)), then there is evidence of significant partial autocorrelation in the time series up to the specified lag.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike1D</code> <p>Observations of time series for which pacf is calculated.</p> required <code>nlags</code> <code>Optional[int]</code> <p>Number of lags to return autocorrelation for. If not provided, uses \\(min(10 \\times log10(nobs) , (\\frac{nobs}{2}-1))\\) (calculated with: <code>min(int(10*np.log10(nobs)), nobs // 2 - 1)</code>). The returned value includes lag <code>0</code> (ie., <code>1</code>) so size of the pacf vector is \\((nlags + 1,)\\). Defaults to <code>None</code>.</p> <code>None</code> <code>method</code> <code>VALID_PACF_METHOD_OPTIONS</code> <p>Specifies which method for the calculations to use.</p> <ul> <li><code>\"yw\"</code> or <code>\"ywadjusted\"</code>: Yule-Walker with sample-size adjustment in denominator for acovf. Default.</li> <li><code>\"ywm\"</code> or <code>\"ywmle\"</code>: Yule-Walker without adjustment.</li> <li><code>\"ols\"</code>: regression of time series on lags of it and on constant.</li> <li><code>\"ols-inefficient\"</code>: regression of time series on lags using a single common sample to estimate all pacf coefficients.</li> <li><code>\"ols-adjusted\"</code>: regression of time series on lags with a bias adjustment.</li> <li><code>\"ld\"</code> or <code>\"ldadjusted\"</code>: Levinson-Durbin recursion with bias correction.</li> <li><code>\"ldb\"</code> or <code>\"ldbiased\"</code>: Levinson-Durbin recursion without bias correction.</li> </ul> <p>Defaults to <code>\"ywadjusted\"</code>.</p> <code>'ywadjusted'</code> <code>alpha</code> <code>Optional[float]</code> <p>If a number is given, the confidence intervals for the given level are returned. For instance if <code>alpha=.05</code>, \\(95\\%\\) confidence intervals are returned where the standard deviation is computed according to \\(\\frac{1}{\\sqrt{len(x)}}\\). Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>pacf</code> <code>ndarray</code> <p>The partial autocorrelations for lags <code>0, 1, ..., nlags</code>. Shape <code>(nlags+1,)</code>.</p> <code>confint</code> <code>Optional[ndarray]</code> <p>Confidence intervals for the PACF at lags <code>0, 1, ..., nlags</code>. Shape <code>(nlags + 1, 2)</code>. Returned if <code>alpha</code> is not <code>None</code>.</p> <p>Credit</p> <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> <p>Examples</p> Test PACF using Yule-Walker method with sample-size adjustment<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, alpha=0.05)\n&gt;&gt;&gt; pprint(res_acf[1:11])\narray([ 0.95467704, -0.26527732,  0.05546955,  0.10885622,  0.08112579,\n        0.00412541,  0.15616955,  0.10370833,  0.28878144,  0.20691805])\n&gt;&gt;&gt; pprint(res_confint[1:11])\narray([[ 0.79134671,  1.11800737],\n       [-0.42860765, -0.10194698],\n       [-0.10786078,  0.21879988],\n       [-0.05447412,  0.27218655],\n       [-0.08220455,  0.24445612],\n       [-0.15920493,  0.16745574],\n       [-0.00716078,  0.31949988],\n       [-0.059622  ,  0.26703866],\n       [ 0.12545111,  0.45211177],\n       [ 0.04358772,  0.37024838]])\n</code></pre> Test PACF using Yule-Walker method without adjustment<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ywm\", alpha=0.05)\n&gt;&gt;&gt; pprint(res_acf[1:11])\narray([ 0.94804734, -0.22942187,  0.03814778,  0.09378544,  0.0736067 ,\n        0.0077276 ,  0.12559713,  0.08995134,  0.23248854,  0.16605126])\n&gt;&gt;&gt; pprint(res_confint[1:11])\narray([[ 0.78471701,  1.11137767],\n       [-0.39275221, -0.06609154],\n       [-0.12518255,  0.20147811],\n       [-0.06954489,  0.25711577],\n       [-0.08972363,  0.23693703],\n       [-0.15560273,  0.17105793],\n       [-0.0377332 ,  0.28892746],\n       [-0.07337899,  0.25328168],\n       [ 0.06915821,  0.39581887],\n       [ 0.00272093,  0.32938159]])\n</code></pre> Test PACF using regression of time series<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ols\", alpha=0.05)\n&gt;&gt;&gt; pprint(res_acf[1:11])\narray([ 0.95893198, -0.32983096,  0.2018249 ,  0.14500798,  0.25848232,\n       -0.02690283,  0.20433019,  0.15607896,  0.56860841,  0.29256358])\n&gt;&gt;&gt; pprint(res_confint[1:11])\narray([[ 0.79560165,  1.12226231],\n       [-0.49316129, -0.16650062],\n       [ 0.03849457,  0.36515523],\n       [-0.01832235,  0.30833831],\n       [ 0.09515198,  0.42181265],\n       [-0.19023316,  0.1364275 ],\n       [ 0.04099986,  0.36766053],\n       [-0.00725137,  0.31940929],\n       [ 0.40527808,  0.73193874],\n       [ 0.12923325,  0.45589391]])\n</code></pre> Test PACF using regression of time series on lags with inefficient optimisation<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ols-inefficient\", alpha=0.05)\n&gt;&gt;&gt; pprint(res_acf[1:11])\narray([ 0.94692978, -0.3540491 ,  0.18292698,  0.12813227,  0.23647898,\n       -0.04596983,  0.19748537,  0.12877966,  0.55357665,  0.22081591])\n&gt;&gt;&gt; pprint(res_confint[1:11])\narray([[ 0.78359944,  1.11026011],\n       [-0.51737943, -0.19071876],\n       [ 0.01959665,  0.34625731],\n       [-0.03519806,  0.2914626 ],\n       [ 0.07314865,  0.39980932],\n       [-0.20930016,  0.1173605 ],\n       [ 0.03415504,  0.3608157 ],\n       [-0.03455067,  0.29211   ],\n       [ 0.39024632,  0.71690698],\n       [ 0.05748558,  0.38414625]])\n</code></pre> Test PACF using regression of time series on lags with a bias adjustment<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ols-adjusted\", alpha=0.05)\n&gt;&gt;&gt; pprint(res_acf[1:11])\narray([ 0.9656378 , -0.33447646,  0.20611905,  0.14915107,  0.26778024,\n       -0.02807252,  0.21477042,  0.16526008,  0.60651564,  0.31439668])\n&gt;&gt;&gt; pprint(res_confint[1:11])\narray([[ 0.80230746,  1.12896813],\n       [-0.49780679, -0.17114613],\n       [ 0.04278872,  0.36944938],\n       [-0.01417926,  0.3124814 ],\n       [ 0.10444991,  0.43111057],\n       [-0.19140285,  0.13525782],\n       [ 0.05144009,  0.37810076],\n       [ 0.00192974,  0.32859041],\n       [ 0.4431853 ,  0.76984597],\n       [ 0.15106635,  0.47772701]])\n</code></pre> Test PACF using Levinson-Durbin recursion with bias correction<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ldadjusted\", alpha=0.05)\n&gt;&gt;&gt; pprint(res_acf[1:11])\narray([ 0.95467704, -0.26527732,  0.05546955,  0.10885622,  0.08112579,\n        0.00412541,  0.15616955,  0.10370833,  0.28878144,  0.20691805])\n&gt;&gt;&gt; pprint(res_confint[1:11])\narray([[ 0.79134671,  1.11800737],\n       [-0.42860765, -0.10194698],\n       [-0.10786078,  0.21879988],\n       [-0.05447412,  0.27218655],\n       [-0.08220455,  0.24445612],\n       [-0.15920493,  0.16745574],\n       [-0.00716078,  0.31949988],\n       [-0.059622  ,  0.26703866],\n       [ 0.12545111,  0.45211177],\n       [ 0.04358772,  0.37024838]])\n</code></pre> Test PACF using Levinson-Durbin recursion without bias correction<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ldbiased\", alpha=0.05)\n&gt;&gt;&gt; pprint(res_acf[1:11])\narray([ 0.94804734, -0.22942187,  0.03814778,  0.09378544,  0.0736067 ,\n        0.0077276 ,  0.12559713,  0.08995134,  0.23248854,  0.16605126])\n&gt;&gt;&gt; pprint(res_confint[1:11])\narray([[ 0.78471701,  1.11137767],\n       [-0.39275221, -0.06609154],\n       [-0.12518255,  0.20147811],\n       [-0.06954489,  0.25711577],\n       [-0.08972363,  0.23693703],\n       [-0.15560273,  0.17105793],\n       [-0.0377332 ,  0.28892746],\n       [-0.07337899,  0.25328168],\n       [ 0.06915821,  0.39581887],\n       [ 0.00272093,  0.32938159]])\n</code></pre> References <ol> <li>Box, G. E., Jenkins, G. M., Reinsel, G. C., &amp; Ljung, G. M. (2015). Time series analysis: forecasting and control. John Wiley &amp; Sons, p. 66.</li> <li>Brockwell, P.J. and Davis, R.A., 2016. Introduction to time series and forecasting. Springer.</li> </ol> See Also <ul> <li><code>statsmodels.tsa.stattools.acf</code>: Estimate the autocorrelation function.</li> <li><code>statsmodels.tsa.stattools.pacf</code>: Partial autocorrelation estimation.</li> <li><code>statsmodels.tsa.stattools.ccf</code>: The cross-correlation function.</li> <li><code>statsmodels.tsa.stattools.pacf_yw</code>: Partial autocorrelation estimation using Yule-Walker.</li> <li><code>statsmodels.tsa.stattools.pacf_ols</code>: Partial autocorrelation estimation using OLS.</li> <li><code>statsmodels.tsa.stattools.pacf_burg</code>: Partial autocorrelation estimation using Burg's method.</li> <li><code>ts_stat_tests.algorithms.correlation.acf</code>: Estimate the autocorrelation function</li> <li><code>ts_stat_tests.algorithms.correlation.pacf</code>: Partial autocorrelation estimate.</li> <li><code>ts_stat_tests.algorithms.correlation.ccf</code>: The cross-correlation function.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/correlation.py</code> <pre><code>@typechecked\ndef pacf(\n    x: ArrayLike1D,\n    nlags: Optional[int] = None,\n    method: VALID_PACF_METHOD_OPTIONS = \"ywadjusted\",\n    *,\n    alpha: Optional[float] = None,\n) -&gt; Union[np.ndarray, tuple[np.ndarray, np.ndarray]]:\n    r\"\"\"\n    !!! note \"Summary\"\n\n        The partial autocorrelation function (PACF) is a statistical tool used in time series forecasting to identify the direct relationship between two variables, controlling for the effect of the other variables in the time series. In other words, the PACF measures the correlation between a time series and its lagged values, while controlling for the effects of other intermediate lags.\n\n        This function will implement the [`pacf()`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf.html) function from the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ???+ abstract \"Details\"\n\n        Based on simulation evidence across a range of low-order ARMA models, the best methods based on root MSE are Yule-Walker (MLW), Levinson-Durbin (MLE) and Burg, respectively. The estimators with the lowest bias included these three in addition to OLS and OLS-adjusted. Yule-Walker (adjusted) and Levinson-Durbin (adjusted) performed consistently worse than the other options.\n\n        The PACF is a plot of the correlation between a time series and its lagged values, controlling for the effect of other lags. The PACF is useful for identifying the order of an autoregressive (AR) model, which is a type of model used in time series forecasting. The order of an AR model is the number of lags that are used to predict future values.\n\n        The PACF at lag $k$ is defined as:\n\n        $$\n        PACF(k) = Corr \\left( Y_t, Y_{t-k} \\mid Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1} \\right)\n        $$\n\n        where:\n\n        - $Y_t$ and $Y_{t-k}$ are the values of the time series at time $t$ and time $t-k$, respectively, and\n        - $Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1}$ are the values of the time series at intervening lags.\n        - $Corr()$ denotes the correlation coefficient between two variables.\n\n        ```\n        PACF(k) = Corr(Y_t, Y_{t-k} | Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1})\n        ```\n\n        The PACF is calculated using the Yule-Walker equations, which are a set of linear equations that describe the relationship between a time series and its lagged values. The PACF is calculated as the difference between the correlation coefficient at lag $k$ and the correlation coefficient at lag $k-1$, controlling for the effects of intermediate lags.\n\n        The PACF is typically plotted as a graph, with the lag on the `x`-axis and the correlation coefficient on the `y`-axis. If the PACF shows a strong positive correlation at lag $k$, this means that values in the time series at time $t$ and time $t-k$ are strongly related, after controlling for the effects of intermediate lags. This suggests that past values can be used to predict future values using an AR model with an order of $k$.\n\n        Overall, the partial autocorrelation function is a valuable tool in time series forecasting, as it helps to identify the order of an autoregressive model and to control for the effects of intermediate lags. By identifying the direct relationship between two variables, the PACF can help to improve the accuracy of time series forecasting models.\n\n        The PACF can be calculated using the pacf() function in the statsmodels package in Python. The function takes a time series array as input and returns an array of partial autocorrelation coefficients at different lags. The significance of the partial autocorrelation coefficients can be tested using the same Ljung-Box test as for the ACF. If the p-value of the test is less than a certain significance level (e.g. $0.05$), then there is evidence of significant partial autocorrelation in the time series up to the specified lag.\n\n    Params:\n        x (ArrayLike1D):\n            Observations of time series for which pacf is calculated.\n        nlags (Optional[int], optional):\n            Number of lags to return autocorrelation for. If not provided, uses $min(10 \\times log10(nobs) , (\\frac{nobs}{2}-1))$ (calculated with: `min(int(10*np.log10(nobs)), nobs // 2 - 1)`). The returned value includes lag `0` (ie., `1`) so size of the pacf vector is $(nlags + 1,)$.&lt;br&gt;\n            Defaults to `None`.\n        method (VALID_PACF_METHOD_OPTIONS, optional):\n            Specifies which method for the calculations to use.\n\n            - `\"yw\"` or `\"ywadjusted\"`: Yule-Walker with sample-size adjustment in denominator for acovf. Default.\n            - `\"ywm\"` or `\"ywmle\"`: Yule-Walker without adjustment.\n            - `\"ols\"`: regression of time series on lags of it and on constant.\n            - `\"ols-inefficient\"`: regression of time series on lags using a single common sample to estimate all pacf coefficients.\n            - `\"ols-adjusted\"`: regression of time series on lags with a bias adjustment.\n            - `\"ld\"` or `\"ldadjusted\"`: Levinson-Durbin recursion with bias correction.\n            - `\"ldb\"` or `\"ldbiased\"`: Levinson-Durbin recursion without bias correction.&lt;br&gt;\n\n            Defaults to `\"ywadjusted\"`.\n        alpha (Optional[float], optional):\n            If a number is given, the confidence intervals for the given level are returned. For instance if `alpha=.05`, $95\\%$ confidence intervals are returned where the standard deviation is computed according to $\\frac{1}{\\sqrt{len(x)}}$.&lt;br&gt;\n            Defaults to `None`.\n\n    Returns:\n        pacf (np.ndarray):\n            The partial autocorrelations for lags `0, 1, ..., nlags`.&lt;br&gt;\n            Shape `(nlags+1,)`.\n        confint (Optional[np.ndarray]):\n            Confidence intervals for the PACF at lags `0, 1, ..., nlags`.&lt;br&gt;\n            Shape `(nlags + 1, 2)`.&lt;br&gt;\n            Returned if `alpha` is not `None`.\n\n    !!! success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/) library.\n\n    !!! example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test PACF using Yule-Walker method with sample-size adjustment\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, alpha=0.05)\n        &gt;&gt;&gt; pprint(res_acf[1:11])\n        array([ 0.95467704, -0.26527732,  0.05546955,  0.10885622,  0.08112579,\n                0.00412541,  0.15616955,  0.10370833,  0.28878144,  0.20691805])\n        &gt;&gt;&gt; pprint(res_confint[1:11])\n        array([[ 0.79134671,  1.11800737],\n               [-0.42860765, -0.10194698],\n               [-0.10786078,  0.21879988],\n               [-0.05447412,  0.27218655],\n               [-0.08220455,  0.24445612],\n               [-0.15920493,  0.16745574],\n               [-0.00716078,  0.31949988],\n               [-0.059622  ,  0.26703866],\n               [ 0.12545111,  0.45211177],\n               [ 0.04358772,  0.37024838]])\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test PACF using Yule-Walker method without adjustment\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ywm\", alpha=0.05)\n        &gt;&gt;&gt; pprint(res_acf[1:11])\n        array([ 0.94804734, -0.22942187,  0.03814778,  0.09378544,  0.0736067 ,\n                0.0077276 ,  0.12559713,  0.08995134,  0.23248854,  0.16605126])\n        &gt;&gt;&gt; pprint(res_confint[1:11])\n        array([[ 0.78471701,  1.11137767],\n               [-0.39275221, -0.06609154],\n               [-0.12518255,  0.20147811],\n               [-0.06954489,  0.25711577],\n               [-0.08972363,  0.23693703],\n               [-0.15560273,  0.17105793],\n               [-0.0377332 ,  0.28892746],\n               [-0.07337899,  0.25328168],\n               [ 0.06915821,  0.39581887],\n               [ 0.00272093,  0.32938159]])\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test PACF using regression of time series\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ols\", alpha=0.05)\n        &gt;&gt;&gt; pprint(res_acf[1:11])\n        array([ 0.95893198, -0.32983096,  0.2018249 ,  0.14500798,  0.25848232,\n               -0.02690283,  0.20433019,  0.15607896,  0.56860841,  0.29256358])\n        &gt;&gt;&gt; pprint(res_confint[1:11])\n        array([[ 0.79560165,  1.12226231],\n               [-0.49316129, -0.16650062],\n               [ 0.03849457,  0.36515523],\n               [-0.01832235,  0.30833831],\n               [ 0.09515198,  0.42181265],\n               [-0.19023316,  0.1364275 ],\n               [ 0.04099986,  0.36766053],\n               [-0.00725137,  0.31940929],\n               [ 0.40527808,  0.73193874],\n               [ 0.12923325,  0.45589391]])\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test PACF using regression of time series on lags with inefficient optimisation\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ols-inefficient\", alpha=0.05)\n        &gt;&gt;&gt; pprint(res_acf[1:11])\n        array([ 0.94692978, -0.3540491 ,  0.18292698,  0.12813227,  0.23647898,\n               -0.04596983,  0.19748537,  0.12877966,  0.55357665,  0.22081591])\n        &gt;&gt;&gt; pprint(res_confint[1:11])\n        array([[ 0.78359944,  1.11026011],\n               [-0.51737943, -0.19071876],\n               [ 0.01959665,  0.34625731],\n               [-0.03519806,  0.2914626 ],\n               [ 0.07314865,  0.39980932],\n               [-0.20930016,  0.1173605 ],\n               [ 0.03415504,  0.3608157 ],\n               [-0.03455067,  0.29211   ],\n               [ 0.39024632,  0.71690698],\n               [ 0.05748558,  0.38414625]])\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test PACF using regression of time series on lags with a bias adjustment\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ols-adjusted\", alpha=0.05)\n        &gt;&gt;&gt; pprint(res_acf[1:11])\n        array([ 0.9656378 , -0.33447646,  0.20611905,  0.14915107,  0.26778024,\n               -0.02807252,  0.21477042,  0.16526008,  0.60651564,  0.31439668])\n        &gt;&gt;&gt; pprint(res_confint[1:11])\n        array([[ 0.80230746,  1.12896813],\n               [-0.49780679, -0.17114613],\n               [ 0.04278872,  0.36944938],\n               [-0.01417926,  0.3124814 ],\n               [ 0.10444991,  0.43111057],\n               [-0.19140285,  0.13525782],\n               [ 0.05144009,  0.37810076],\n               [ 0.00192974,  0.32859041],\n               [ 0.4431853 ,  0.76984597],\n               [ 0.15106635,  0.47772701]])\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test PACF using Levinson-Durbin recursion with bias correction\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ldadjusted\", alpha=0.05)\n        &gt;&gt;&gt; pprint(res_acf[1:11])\n        array([ 0.95467704, -0.26527732,  0.05546955,  0.10885622,  0.08112579,\n                0.00412541,  0.15616955,  0.10370833,  0.28878144,  0.20691805])\n        &gt;&gt;&gt; pprint(res_confint[1:11])\n        array([[ 0.79134671,  1.11800737],\n               [-0.42860765, -0.10194698],\n               [-0.10786078,  0.21879988],\n               [-0.05447412,  0.27218655],\n               [-0.08220455,  0.24445612],\n               [-0.15920493,  0.16745574],\n               [-0.00716078,  0.31949988],\n               [-0.059622  ,  0.26703866],\n               [ 0.12545111,  0.45211177],\n               [ 0.04358772,  0.37024838]])\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test PACF using Levinson-Durbin recursion without bias correction\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ldbiased\", alpha=0.05)\n        &gt;&gt;&gt; pprint(res_acf[1:11])\n        array([ 0.94804734, -0.22942187,  0.03814778,  0.09378544,  0.0736067 ,\n                0.0077276 ,  0.12559713,  0.08995134,  0.23248854,  0.16605126])\n        &gt;&gt;&gt; pprint(res_confint[1:11])\n        array([[ 0.78471701,  1.11137767],\n               [-0.39275221, -0.06609154],\n               [-0.12518255,  0.20147811],\n               [-0.06954489,  0.25711577],\n               [-0.08972363,  0.23693703],\n               [-0.15560273,  0.17105793],\n               [-0.0377332 ,  0.28892746],\n               [-0.07337899,  0.25328168],\n               [ 0.06915821,  0.39581887],\n               [ 0.00272093,  0.32938159]])\n        ```\n\n    ??? question \"References\"\n        1. Box, G. E., Jenkins, G. M., Reinsel, G. C., &amp; Ljung, G. M. (2015). Time series analysis: forecasting and control. John Wiley &amp; Sons, p. 66.\n        1. Brockwell, P.J. and Davis, R.A., 2016. Introduction to time series and forecasting. Springer.\n\n    ??? tip \"See Also\"\n        - [`statsmodels.tsa.stattools.acf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.acf.html): Estimate the autocorrelation function.\n        - [`statsmodels.tsa.stattools.pacf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf.html): Partial autocorrelation estimation.\n        - [`statsmodels.tsa.stattools.ccf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.ccf.html): The cross-correlation function.\n        - [`statsmodels.tsa.stattools.pacf_yw`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf_yw.html): Partial autocorrelation estimation using Yule-Walker.\n        - [`statsmodels.tsa.stattools.pacf_ols`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf_ols.html): Partial autocorrelation estimation using OLS.\n        - [`statsmodels.tsa.stattools.pacf_burg`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf_burg.html): Partial autocorrelation estimation using Burg's method.\n        - [`ts_stat_tests.algorithms.correlation.acf`][ts_stat_tests.algorithms.correlation.acf]: Estimate the autocorrelation function\n        - [`ts_stat_tests.algorithms.correlation.pacf`][ts_stat_tests.algorithms.correlation.pacf]: Partial autocorrelation estimate.\n        - [`ts_stat_tests.algorithms.correlation.ccf`][ts_stat_tests.algorithms.correlation.ccf]: The cross-correlation function.\n    \"\"\"\n    return st_pacf(\n        x=x,\n        nlags=nlags,\n        method=method,\n        alpha=alpha,\n    )\n</code></pre>"},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation.ccf","title":"ccf","text":"<pre><code>ccf(\n    x: ArrayLike,\n    y: ArrayLike,\n    adjusted: bool = True,\n    fft: bool = True,\n    *,\n    nlags: Optional[int] = None,\n    alpha: None = None\n) -&gt; np.ndarray\n</code></pre><pre><code>ccf(\n    x: ArrayLike,\n    y: ArrayLike,\n    adjusted: bool = True,\n    fft: bool = True,\n    *,\n    nlags: Optional[int] = None,\n    alpha: float\n) -&gt; tuple[np.ndarray, np.ndarray]\n</code></pre> <pre><code>ccf(\n    x: ArrayLike,\n    y: ArrayLike,\n    adjusted: bool = True,\n    fft: bool = True,\n    *,\n    nlags: Optional[int] = None,\n    alpha: Optional[float] = None\n) -&gt; Union[np.ndarray, tuple[np.ndarray, np.ndarray]]\n</code></pre> <p>Summary</p> <p>The cross-correlation function (CCF) is a statistical tool used in time series forecasting to measure the correlation between two time series at different lags. It is used to study the relationship between two time series, and can help to identify lead-lag relationships and causal effects.</p> <p>This function will implement the <code>ccf()</code> function from the <code>statsmodels</code> library.</p> Details <p>If <code>adjusted</code> is <code>True</code>, the denominator for the autocovariance is adjusted.</p> <p>The CCF measures the correlation between two time series at different lags. It is calculated as the ratio of the covariance between the two series at lag k to the product of their standard deviations. The CCF is typically plotted as a graph, with the lag on the <code>x</code>-axis and the correlation coefficient on the <code>y</code>-axis.</p> <p>The CCF at lag k is defined as:</p> \\[ CCF(k) = Corr(X_t, Y_{t-k}) \\] <p>where:</p> <ul> <li>\\(X_t\\) and \\(Y_{t-k}\\) are the values of the two time series at time \\(t\\) and time \\(t-k\\), respectively.</li> <li>\\(Corr()\\) denotes the correlation coefficient between two variables.</li> </ul> <pre><code>CCF(k) = Corr(X_t, Y_{t-k})\n</code></pre> <p>If the CCF shows a strong positive correlation at lag \\(k\\), this means that changes in one time series at time \\(t\\) are strongly related to changes in the other time series at time \\(t-k\\). This suggests a lead-lag relationship between the two time series, where changes in one series lead changes in the other series by a certain number of periods. The CCF can be used to estimate the time lag between the two time series.</p> <p>The CCF can also help to identify causal relationships between two time series. If the CCF shows a strong positive correlation at lag \\(k\\), and the lag is consistent with a causal relationship between the two time series, this suggests that changes in one time series are causing changes in the other time series.</p> <p>Overall, the cross-correlation function is a valuable tool in time series forecasting, as it helps to study the relationship between two time series and to identify lead-lag relationships and causal effects. By identifying the relationship between two time series, the CCF can help to improve the accuracy of time series forecasting models.</p> <p>The CCF can be calculated using the <code>ccf()</code> function in the <code>statsmodels</code> package in Python. The function takes two time series arrays as input and returns an array of cross-correlation coefficients at different lags. The significance of the cross-correlation coefficients can be tested using a similar test to the Ljung-Box test, such as the Box-Pierce test or the Breusch-Godfrey test. These tests can be performed using the <code>boxpierce()</code> and <code>lm()</code> functions in the <code>statsmodels</code> package, respectively. If the p-value of the test is less than a certain significance level (e.g. \\(0.05\\)), then there is evidence of significant cross-correlation between the two time series at the specified lag.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The time series data to use in the calculation.</p> required <code>y</code> <code>ArrayLike</code> <p>The time series data to use in the calculation.</p> required <code>adjusted</code> <code>bool</code> <p>If <code>True</code>, then denominators for cross-correlation is \\(n-k\\), otherwise \\(n\\). Defaults to <code>True</code>.</p> <code>True</code> <code>fft</code> <code>bool</code> <p>If <code>True</code>, use FFT convolution. This method should be preferred for long time series. Defaults to <code>True</code>.</p> <code>True</code> <code>nlags</code> <code>Optional[int]</code> <p>Number of lags to return cross-correlations for. If not provided, the number of lags equals len(x). Defaults to <code>None</code>.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>If a number is given, the confidence intervals for the given level are returned. For instance if <code>alpha=.05</code>, 95% confidence intervals are returned where the standard deviation is computed according to <code>1/sqrt(len(x))</code>. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ccf</code> <code>ndarray</code> <p>The cross-correlation function of <code>x</code> and <code>y</code>.</p> <p>Credit</p> <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> <p>Examples</p> Test CCF without FFT<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import ccf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_ccf = ccf(data, data + 1, adjusted=True, fft=False)\n&gt;&gt;&gt; pprint(res_ccf[1:11])\narray([0.95467704, 0.88790688, 0.82384458, 0.774129  , 0.73944515,\n       0.71137419, 0.69677541, 0.69417581, 0.71567822, 0.75516171])\n</code></pre> Test CCF with FFT<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import ccf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_ccf = ccf(data, data + 1, adjusted=True, fft=True)\n&gt;&gt;&gt; pprint(res_ccf[1:11])\narray([0.95467704, 0.88790688, 0.82384458, 0.774129  , 0.73944515,\n       0.71137419, 0.69677541, 0.69417581, 0.71567822, 0.75516171])\n</code></pre> See Also <ul> <li><code>statsmodels.tsa.stattools.acf</code>: Estimate the autocorrelation function.</li> <li><code>statsmodels.tsa.stattools.pacf</code>: Partial autocorrelation estimation.</li> <li><code>statsmodels.tsa.stattools.ccf</code>: The cross-correlation function.</li> <li><code>ts_stat_tests.algorithms.correlation.acf</code>: Estimate the autocorrelation function</li> <li><code>ts_stat_tests.algorithms.correlation.pacf</code>: Partial autocorrelation estimate.</li> <li><code>ts_stat_tests.algorithms.correlation.ccf</code>: The cross-correlation function.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/correlation.py</code> <pre><code>@typechecked\ndef ccf(\n    x: ArrayLike,\n    y: ArrayLike,\n    adjusted: bool = True,\n    fft: bool = True,\n    *,\n    nlags: Optional[int] = None,\n    alpha: Optional[float] = None,\n) -&gt; Union[np.ndarray, tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    !!! note \"Summary\"\n\n        The cross-correlation function (CCF) is a statistical tool used in time series forecasting to measure the correlation between two time series at different lags. It is used to study the relationship between two time series, and can help to identify lead-lag relationships and causal effects.\n\n        This function will implement the [`ccf()`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.ccf.html) function from the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ???+ abstract \"Details\"\n\n        If `adjusted` is `True`, the denominator for the autocovariance is adjusted.\n\n        The CCF measures the correlation between two time series at different lags. It is calculated as the ratio of the covariance between the two series at lag k to the product of their standard deviations. The CCF is typically plotted as a graph, with the lag on the `x`-axis and the correlation coefficient on the `y`-axis.\n\n        The CCF at lag k is defined as:\n\n        $$\n        CCF(k) = Corr(X_t, Y_{t-k})\n        $$\n\n        where:\n\n        - $X_t$ and $Y_{t-k}$ are the values of the two time series at time $t$ and time $t-k$, respectively.\n        - $Corr()$ denotes the correlation coefficient between two variables.\n\n        ```\n        CCF(k) = Corr(X_t, Y_{t-k})\n        ```\n\n        If the CCF shows a strong positive correlation at lag $k$, this means that changes in one time series at time $t$ are strongly related to changes in the other time series at time $t-k$. This suggests a lead-lag relationship between the two time series, where changes in one series lead changes in the other series by a certain number of periods. The CCF can be used to estimate the time lag between the two time series.\n\n        The CCF can also help to identify causal relationships between two time series. If the CCF shows a strong positive correlation at lag $k$, and the lag is consistent with a causal relationship between the two time series, this suggests that changes in one time series are causing changes in the other time series.\n\n        Overall, the cross-correlation function is a valuable tool in time series forecasting, as it helps to study the relationship between two time series and to identify lead-lag relationships and causal effects. By identifying the relationship between two time series, the CCF can help to improve the accuracy of time series forecasting models.\n\n        The CCF can be calculated using the `ccf()` function in the `statsmodels` package in Python. The function takes two time series arrays as input and returns an array of cross-correlation coefficients at different lags. The significance of the cross-correlation coefficients can be tested using a similar test to the Ljung-Box test, such as the Box-Pierce test or the Breusch-Godfrey test. These tests can be performed using the `boxpierce()` and `lm()` functions in the `statsmodels` package, respectively. If the p-value of the test is less than a certain significance level (e.g. $0.05$), then there is evidence of significant cross-correlation between the two time series at the specified lag.\n\n    Params:\n        x (ArrayLike):\n            The time series data to use in the calculation.\n        y (ArrayLike):\n            The time series data to use in the calculation.\n        adjusted (bool, optional):\n            If `True`, then denominators for cross-correlation is $n-k$, otherwise $n$.&lt;br&gt;\n            Defaults to `True`.\n        fft (bool, optional):\n            If `True`, use FFT convolution. This method should be preferred for long time series.&lt;br&gt;\n            Defaults to `True`.\n        nlags (Optional[int], optional):\n            Number of lags to return cross-correlations for. If not provided, the number of lags equals len(x).\n            Defaults to `None`.\n        alpha (Optional[float], optional):\n            If a number is given, the confidence intervals for the given level are returned. For instance if `alpha=.05`, 95% confidence intervals are returned where the standard deviation is computed according to `1/sqrt(len(x))`.\n            Defaults to `None`.\n\n    Returns:\n        ccf (np.ndarray):\n            The cross-correlation function of `x` and `y`.\n\n    !!! success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/) library.\n\n    !!! example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test CCF without FFT\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import ccf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_ccf = ccf(data, data + 1, adjusted=True, fft=False)\n        &gt;&gt;&gt; pprint(res_ccf[1:11])\n        array([0.95467704, 0.88790688, 0.82384458, 0.774129  , 0.73944515,\n               0.71137419, 0.69677541, 0.69417581, 0.71567822, 0.75516171])\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test CCF with FFT\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import ccf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_ccf = ccf(data, data + 1, adjusted=True, fft=True)\n        &gt;&gt;&gt; pprint(res_ccf[1:11])\n        array([0.95467704, 0.88790688, 0.82384458, 0.774129  , 0.73944515,\n               0.71137419, 0.69677541, 0.69417581, 0.71567822, 0.75516171])\n        ```\n\n    ??? tip \"See Also\"\n        - [`statsmodels.tsa.stattools.acf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.acf.html): Estimate the autocorrelation function.\n        - [`statsmodels.tsa.stattools.pacf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf.html): Partial autocorrelation estimation.\n        - [`statsmodels.tsa.stattools.ccf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.ccf.html): The cross-correlation function.\n        - [`ts_stat_tests.algorithms.correlation.acf`][ts_stat_tests.algorithms.correlation.acf]: Estimate the autocorrelation function\n        - [`ts_stat_tests.algorithms.correlation.pacf`][ts_stat_tests.algorithms.correlation.pacf]: Partial autocorrelation estimate.\n        - [`ts_stat_tests.algorithms.correlation.ccf`][ts_stat_tests.algorithms.correlation.ccf]: The cross-correlation function.\n    \"\"\"\n    return st_ccf(\n        x=x,\n        y=y,\n        adjusted=adjusted,\n        fft=fft,\n        nlags=nlags,\n        alpha=alpha,\n    )\n</code></pre>"},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation.lb","title":"lb","text":"<pre><code>lb(\n    x: ArrayLike,\n    lags: Optional[Union[int, ArrayLike]] = None,\n    boxpierce: bool = False,\n    model_df: int = 0,\n    period: Optional[int] = None,\n    return_df: bool = True,\n    auto_lag: bool = False,\n) -&gt; pd.DataFrame\n</code></pre> <p>Summary</p> <p>The Ljung-Box test is a statistical test used in time series forecasting to test for the presence of autocorrelation in the residuals of a model. The test is based on the autocorrelation function (ACF) of the residuals, and can be used to assess the adequacy of a time series model and to identify areas for improvement.</p> <p>This function will implement the <code>acorr_ljungbox()</code> function from the <code>statsmodels</code> library.</p> Details <p>The Ljung-Box and Box-Pierce statistics differ in how they scale the autocorrelation function; the Ljung-Box test has better finite-sample properties.</p> <p>The test statistic is calculated as:</p> \\[ Q(m) = n(n+2) \\times \\sum_{k=1}^m \\left( \\frac{ r_k^2 }{ n-k } \\right) \\] <p>where:</p> <ul> <li>\\(n\\) is the sample size,</li> <li>\\(m\\) is the maximum lag being tested,</li> <li>\\(r_k\\) is the sample autocorrelation at lag \\(k\\), and</li> <li>\\(\\sum\\) (\\(sum\\)) denotes the sum over \\(k\\) from \\(1\\) to \\(m\\).</li> </ul> <pre><code>Q(m) = n(n+2) * Sum(r_k^2 / (n-k))\n</code></pre> <p>Under the null hypothesis, the test statistic follows a chi-squared distribution with degrees of freedom equal to \\(m-p\\), where \\(p\\) is the number of parameters estimated in fitting the time series model.</p> <p>The Ljung-Box test is performed by calculating the autocorrelation function (ACF) of the residuals from a time series model, and then comparing the ACF values to the expected values under the null hypothesis of no autocorrelation. The test statistic is calculated as the sum of the squared autocorrelations up to a given lag, and is compared to a chi-squared distribution with degrees of freedom equal to the number of lags tested.</p> <p>If the test statistic is greater than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is rejected, indicating that there is evidence of autocorrelation in the residuals. This suggests that the time series model is inadequate, and that additional terms may need to be added to the model to account for the remaining autocorrelation.</p> <p>If the test statistic is less than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is not rejected, indicating that there is no evidence of autocorrelation in the residuals. This suggests that the time series model is adequate, and that no further improvements are needed.</p> <p>Overall, the Ljung-Box test is a valuable tool in time series forecasting, as it helps to assess the adequacy of a time series model and to identify areas for improvement. By testing for autocorrelation in the residuals, the test helps to ensure that the model is accurately capturing the underlying patterns in the time series data.</p> <p>The Ljung-Box test can be calculated using the <code>acorr_ljungbox()</code> function in the <code>statsmodels</code> package in Python. The function takes a time series array and the maximum lag \\(m\\) as input, and returns an array of Q-statistics and associated p-values for each lag up to \\(m\\). If the p-value of the test is less than a certain significance level (e.g. \\(0.05\\)), then there is evidence of significant autocorrelation in the time series up to the specified lag.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data series. The data is demeaned before the test statistic is computed.</p> required <code>lags</code> <code>Optional[Union[int, ArrayLike]]</code> <p>If lags is an integer (<code>int</code>) then this is taken to be the largest lag that is included, the test result is reported for all smaller lag length. If lags is a list or array, then all lags are included up to the largest lag in the list, however only the tests for the lags in the list are reported. If lags is <code>None</code>, then the default maxlag is currently \\(\\min(\\frac{nobs}{2}-2,40)\\) (calculated with: <code>min(nobs // 2 - 2, 40)</code>). The default number of <code>lags</code> changes if <code>period</code> is set.</p> <p>Deprecation</p> <p>After <code>statsmodels</code> version <code>0.12</code>, this will calculation change from</p> \\[ \\min(\\frac{nobs}{2}-2,40) \\] <p>to</p> \\[ \\min(10,\\frac{nobs}{5}) \\] <p>Defaults to <code>None</code>.</p> <code>None</code> <code>boxpierce</code> <code>bool</code> <p>If <code>True</code>, then additional to the results of the Ljung-Box test also the Box-Pierce test results are returned. Defaults to <code>False</code>.</p> <code>False</code> <code>model_df</code> <code>int</code> <p>Number of degrees of freedom consumed by the model. In an ARMA model, this value is usually \\(p+q\\) where \\(p\\) is the AR order and \\(q\\) is the MA order. This value is subtracted from the degrees-of-freedom used in the test so that the adjusted dof for the statistics are \\(lags - model_df\\). If \\(lags - model_df &lt;= 0\\), then <code>NaN</code> is returned. Defaults to <code>0</code>.</p> <code>0</code> <code>period</code> <code>Optional[int]</code> <p>The period of a Seasonal time series. Used to compute the max lag for seasonal data which uses \\(\\min(2 \\times period, \\frac{nobs}{5})\\) (calculated with: <code>min(2*period,nobs//5)</code>) if set. If <code>None</code>, then the default rule is used to set the number of lags. When set, must be \\(&gt;= 2\\). Defaults to <code>None</code>.</p> <code>None</code> <code>return_df</code> <code>bool</code> <p>Flag indicating whether to return the result as a single DataFrame with columns <code>lb_stat</code>, <code>lb_pvalue</code>, and optionally <code>bp_stat</code> and <code>bp_pvalue</code>. Set to <code>True</code> to return the DataFrame or <code>False</code> to continue returning the \\(2-4\\) output. If <code>None</code> (the default), a warning is raised.</p> <p>Deprecation</p> <p>After <code>statsmodels</code> version <code>0.12</code>, this will become the only return method.</p> <p>Defaults to <code>True</code>.</p> <code>True</code> <code>auto_lag</code> <code>bool</code> <p>Flag indicating whether to automatically determine the optimal lag length based on threshold of maximum correlation value. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>lbvalue</code> <code>Union[float, ndarray]</code> <p>The Ljung-Box test statistic.</p> <code>pvalue</code> <code>Union[float, ndarray]</code> <p>The p-value based on chi-square distribution. The p-value is computed as \\(1-\\text{cdf}(lbvalue,dof)\\) where \\(dof\\) is \\(lag - model\\_df\\) (calculated with: <code>1.0 - chi2.cdf(lbvalue, dof)</code>). If \\(lag - model\\_df &lt;= 0\\), then <code>NaN</code> is returned for the <code>pvalue</code>.</p> <code>bpvalue</code> <code>Optional[Union[float, ndarray]]</code> <p>The test statistic for Box-Pierce test.</p> <code>bppvalue</code> <code>Optional[Union[float, ndarray]]</code> <p>The p-value based for Box-Pierce test on chi-square distribution. The p-value is computed as \\(1-\\text{cdf}(bpvalue,dof)\\) where \\(dof\\) is \\(lag - model_df\\) (calculated with: <code>1.0 - chi2.cdf(bpvalue, dof)</code>). If \\(lag - model_df &lt;= 0\\), then <code>NaN</code> is returned for the <code>pvalue</code>.</p> <p>Credit</p> <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> <p>Examples</p> Python<pre><code>&gt;&gt;&gt; import statsmodels.api as sm\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import lb\n&gt;&gt;&gt; data = sm.datasets.sunspots.load_pandas().data\n&gt;&gt;&gt; res = sm.tsa.ARIMA(data[\"SUNACTIVITY\"], order=(1, 0, 1)).fit()\n&gt;&gt;&gt; lb(res.resid, lags=[10], return_df=True)\n    lb_stat     lb_pvalue\n10  214.106992  1.827374e-40\n</code></pre> References <ul> <li>Green, W. \"Econometric Analysis,\" 5th ed., Pearson, 2003.</li> <li>J. Carlos Escanciano, Ignacio N. Lobato \"An automatic Portmanteau test for serial correlation\"., Volume 151, 2009.</li> </ul> See Also <ul> <li><code>statsmodels.regression.linear_model.OLS.fit</code>:</li> <li><code>statsmodels.regression.linear_model.RegressionResults</code>: The output results of a linear regression model.</li> <li><code>statsmodels.stats.diagnostic.acorr_ljungbox</code>: Ljung-Box test for serial correlation.</li> <li><code>statsmodels.stats.diagnostic.acorr_lm</code>: Lagrange Multiplier tests for autocorrelation.</li> <li><code>statsmodels.stats.diagnostic.acorr_breusch_godfrey</code>: Breusch-Godfrey test for serial correlation.</li> <li><code>ts_stat_tests.algorithms.correlation.lb</code>: Ljung-Box test of autocorrelation in residuals.</li> <li><code>ts_stat_tests.algorithms.correlation.lm</code>: Lagrange Multiplier tests for autocorrelation.</li> <li><code>ts_stat_tests.algorithms.correlation.bglm</code>: Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/correlation.py</code> <pre><code>@typechecked\ndef lb(\n    x: ArrayLike,\n    lags: Optional[Union[int, ArrayLike]] = None,\n    boxpierce: bool = False,\n    model_df: int = 0,\n    period: Optional[int] = None,\n    return_df: bool = True,\n    auto_lag: bool = False,\n) -&gt; pd.DataFrame:\n    r\"\"\"\n    !!! note \"Summary\"\n\n        The Ljung-Box test is a statistical test used in time series forecasting to test for the presence of autocorrelation in the residuals of a model. The test is based on the autocorrelation function (ACF) of the residuals, and can be used to assess the adequacy of a time series model and to identify areas for improvement.\n\n        This function will implement the [`acorr_ljungbox()`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_ljungbox.html) function from the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ???+ abstract \"Details\"\n\n        The Ljung-Box and Box-Pierce statistics differ in how they scale the autocorrelation function; the Ljung-Box test has better finite-sample properties.\n\n        The test statistic is calculated as:\n\n        $$\n        Q(m) = n(n+2) \\times \\sum_{k=1}^m \\left( \\frac{ r_k^2 }{ n-k } \\right)\n        $$\n\n        where:\n\n        - $n$ is the sample size,\n        - $m$ is the maximum lag being tested,\n        - $r_k$ is the sample autocorrelation at lag $k$, and\n        - $\\sum$ ($sum$) denotes the sum over $k$ from $1$ to $m$.\n\n        ```\n        Q(m) = n(n+2) * Sum(r_k^2 / (n-k))\n        ```\n\n        Under the null hypothesis, the test statistic follows a chi-squared distribution with degrees of freedom equal to $m-p$, where $p$ is the number of parameters estimated in fitting the time series model.\n\n        The Ljung-Box test is performed by calculating the autocorrelation function (ACF) of the residuals from a time series model, and then comparing the ACF values to the expected values under the null hypothesis of no autocorrelation. The test statistic is calculated as the sum of the squared autocorrelations up to a given lag, and is compared to a chi-squared distribution with degrees of freedom equal to the number of lags tested.\n\n        If the test statistic is greater than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is rejected, indicating that there is evidence of autocorrelation in the residuals. This suggests that the time series model is inadequate, and that additional terms may need to be added to the model to account for the remaining autocorrelation.\n\n        If the test statistic is less than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is not rejected, indicating that there is no evidence of autocorrelation in the residuals. This suggests that the time series model is adequate, and that no further improvements are needed.\n\n        Overall, the Ljung-Box test is a valuable tool in time series forecasting, as it helps to assess the adequacy of a time series model and to identify areas for improvement. By testing for autocorrelation in the residuals, the test helps to ensure that the model is accurately capturing the underlying patterns in the time series data.\n\n        The Ljung-Box test can be calculated using the `acorr_ljungbox()` function in the `statsmodels` package in Python. The function takes a time series array and the maximum lag $m$ as input, and returns an array of Q-statistics and associated p-values for each lag up to $m$. If the p-value of the test is less than a certain significance level (e.g. $0.05$), then there is evidence of significant autocorrelation in the time series up to the specified lag.\n\n    Params:\n        x (ArrayLike):\n            The data series. The data is demeaned before the test statistic is computed.\n        lags (Optional[Union[int, ArrayLike]], optional):\n            If lags is an integer (`int`) then this is taken to be the largest lag that is included, the test result is reported for all smaller lag length. If lags is a list or array, then all lags are included up to the largest lag in the list, however only the tests for the lags in the list are reported. If lags is `None`, then the default maxlag is currently $\\min(\\frac{nobs}{2}-2,40)$ (calculated with: `min(nobs // 2 - 2, 40)`). The default number of `lags` changes if `period` is set.&lt;br&gt;\n            !!! deprecation \"Deprecation\"\n                After `statsmodels` version `0.12`, this will calculation change from\n\n                $$\n                \\min(\\frac{nobs}{2}-2,40)\n                $$\n\n                to\n\n                $$\n                \\min(10,\\frac{nobs}{5})\n                $$\n            Defaults to `None`.\n        boxpierce (bool, optional):\n            If `True`, then additional to the results of the Ljung-Box test also the Box-Pierce test results are returned.&lt;br&gt;\n            Defaults to `False`.\n        model_df (int, optional):\n            Number of degrees of freedom consumed by the model. In an ARMA model, this value is usually $p+q$ where $p$ is the AR order and $q$ is the MA order. This value is subtracted from the degrees-of-freedom used in the test so that the adjusted dof for the statistics are $lags - model_df$. If $lags - model_df &lt;= 0$, then `NaN` is returned.&lt;br&gt;\n            Defaults to `0`.\n        period (Optional[int], optional):\n            The period of a Seasonal time series. Used to compute the max lag for seasonal data which uses $\\min(2 \\times period, \\frac{nobs}{5})$ (calculated with: `min(2*period,nobs//5)`) if set. If `None`, then the default rule is used to set the number of lags. When set, must be $&gt;= 2$.&lt;br&gt;\n            Defaults to `None`.\n        return_df (bool, optional):\n            Flag indicating whether to return the result as a single DataFrame with columns `lb_stat`, `lb_pvalue`, and optionally `bp_stat` and `bp_pvalue`. Set to `True` to return the DataFrame or `False` to continue returning the $2-4$ output. If `None` (the default), a warning is raised.\n            !!! deprecation \"Deprecation\"\n                After `statsmodels` version `0.12`, this will become the only return method.\n            Defaults to `True`.\n        auto_lag (bool, optional):\n            Flag indicating whether to automatically determine the optimal lag length based on threshold of maximum correlation value.&lt;br&gt;\n            Defaults to `False`.\n\n    Returns:\n        lbvalue (Union[float, np.ndarray]):\n            The Ljung-Box test statistic.\n        pvalue (Union[float, np.ndarray]):\n            The p-value based on chi-square distribution. The p-value is computed as $1-\\text{cdf}(lbvalue,dof)$ where $dof$ is $lag - model\\_df$ (calculated with: `1.0 - chi2.cdf(lbvalue, dof)`). If $lag - model\\_df &lt;= 0$, then `NaN` is returned for the `pvalue`.\n        bpvalue (Optional[Union[float, np.ndarray]]):\n            The test statistic for Box-Pierce test.\n        bppvalue (Optional[Union[float, np.ndarray]]):\n            The p-value based for Box-Pierce test on chi-square distribution. The p-value is computed as $1-\\text{cdf}(bpvalue,dof)$ where $dof$ is $lag - model_df$ (calculated with: `1.0 - chi2.cdf(bpvalue, dof)`). If $lag - model_df &lt;= 0$, then `NaN` is returned for the `pvalue`.\n\n    !!! success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/) library.\n\n    !!! example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Python\"}\n        &gt;&gt;&gt; import statsmodels.api as sm\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import lb\n        &gt;&gt;&gt; data = sm.datasets.sunspots.load_pandas().data\n        &gt;&gt;&gt; res = sm.tsa.ARIMA(data[\"SUNACTIVITY\"], order=(1, 0, 1)).fit()\n        &gt;&gt;&gt; lb(res.resid, lags=[10], return_df=True)\n            lb_stat     lb_pvalue\n        10  214.106992  1.827374e-40\n        ```\n\n    ??? question \"References\"\n        - Green, W. \"Econometric Analysis,\" 5th ed., Pearson, 2003.\n        - J. Carlos Escanciano, Ignacio N. Lobato \"An automatic Portmanteau test for serial correlation\"., Volume 151, 2009.\n\n    ??? tip \"See Also\"\n        - [`statsmodels.regression.linear_model.OLS.fit`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.fit.html):\n        - [`statsmodels.regression.linear_model.RegressionResults`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.RegressionResults.html): The output results of a linear regression model.\n        - [`statsmodels.stats.diagnostic.acorr_ljungbox`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_ljungbox.html): Ljung-Box test for serial correlation.\n        - [`statsmodels.stats.diagnostic.acorr_lm`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_lm.html): Lagrange Multiplier tests for autocorrelation.\n        - [`statsmodels.stats.diagnostic.acorr_breusch_godfrey`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_breusch_godfrey.html): Breusch-Godfrey test for serial correlation.\n        - [`ts_stat_tests.algorithms.correlation.lb`][ts_stat_tests.algorithms.correlation.lb]: Ljung-Box test of autocorrelation in residuals.\n        - [`ts_stat_tests.algorithms.correlation.lm`][ts_stat_tests.algorithms.correlation.lm]: Lagrange Multiplier tests for autocorrelation.\n        - [`ts_stat_tests.algorithms.correlation.bglm`][ts_stat_tests.algorithms.correlation.bglm]: Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation.\n    \"\"\"\n    return acorr_ljungbox(\n        x=x,\n        lags=lags,\n        boxpierce=boxpierce,\n        model_df=model_df,\n        period=period,\n        return_df=return_df,\n        auto_lag=auto_lag,\n    )\n</code></pre>"},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation.lm","title":"lm","text":"<pre><code>lm(\n    resid: ArrayLike,\n    nlags: Optional[int] = None,\n    *,\n    store: Literal[False] = False,\n    period: Optional[int] = None,\n    ddof: int = 0,\n    cov_type: Literal[\"nonrobust\"] = \"nonrobust\",\n    cov_kwargs: Optional[dict] = None\n) -&gt; tuple[float, float, float, float]\n</code></pre><pre><code>lm(\n    resid: ArrayLike,\n    nlags: Optional[int] = None,\n    *,\n    store: Literal[True],\n    period: Optional[int] = None,\n    ddof: int = 0,\n    cov_type: Literal[\"nonrobust\"] = \"nonrobust\",\n    cov_kwargs: Optional[dict] = None\n) -&gt; tuple[\n    np.ndarray, np.ndarray, float, float, ResultsStore\n]\n</code></pre><pre><code>lm(\n    resid: ArrayLike,\n    nlags: Optional[int] = None,\n    *,\n    store: Literal[False] = False,\n    period: Optional[int] = None,\n    ddof: int = 0,\n    cov_type: VALID_LM_COV_TYPE_OPTIONS,\n    cov_kwargs: Optional[dict] = None\n) -&gt; tuple[np.ndarray, np.ndarray, float, float]\n</code></pre><pre><code>lm(\n    resid: ArrayLike,\n    nlags: Optional[int] = None,\n    *,\n    store: Literal[True],\n    period: Optional[int] = None,\n    ddof: int = 0,\n    cov_type: VALID_LM_COV_TYPE_OPTIONS,\n    cov_kwargs: Optional[dict] = None\n) -&gt; tuple[float, float, float, float, ResultsStore]\n</code></pre> <pre><code>lm(\n    resid: ArrayLike,\n    nlags: Optional[int] = None,\n    *,\n    store: bool = False,\n    period: Optional[int] = None,\n    ddof: int = 0,\n    cov_type: Union[\n        Literal[\"nonrobust\"], VALID_LM_COV_TYPE_OPTIONS\n    ] = \"nonrobust\",\n    cov_kwargs: Optional[dict] = None\n) -&gt; Union[\n    tuple[np.ndarray, np.ndarray, float, float],\n    tuple[\n        np.ndarray, np.ndarray, float, float, ResultsStore\n    ],\n    tuple[float, float, float, float],\n    tuple[float, float, float, float, ResultsStore],\n]\n</code></pre> <p>Summary</p> <p>The Lagrange Multiplier (LM) test is a statistical test used in time series forecasting to test for the presence of autocorrelation in a model. The test is based on the residual sum of squares (RSS) of a time series model, and can be used to assess the adequacy of the model and to identify areas for improvement.</p> <p>This function will implement the <code>acorr_lm()</code> function from the <code>statsmodels</code> library.</p> Details <p>This is a generic Lagrange Multiplier (LM) test for autocorrelation. It returns Engle's ARCH test if <code>resid</code> is the squared residual array. The Breusch-Godfrey test is a variation on this LM test with additional exogenous variables in the auxiliary regression.</p> <p>The LM test statistic is computed as</p> \\[ LM = (n_{obs} - ddof) \\times R^2, \\] <pre><code>LM = (n_obs - ddof) * R^2\n</code></pre> <p>where \\(R^2\\) is the coefficient of determination from the auxiliary regression of the residuals on their own <code>nlags</code> lags (and any additional regressors included in the model), \\(n_{obs}\\) is the number of observations, and \\(ddof\\) is the model degrees of freedom lost due to parameter estimation.</p> <p>&lt;!-- Previous algorithm included below</p> \\[ LM = n \\times (n+2) \\times \\sum_{k=1}^m \\left( \\frac { r_k^2 }{ n-k } \\right) - 2 \\times (n-1) \\times (n-2) \\times \\sum_{k=1}^m \\left( r_k \\times \\frac { r_{k+1} }{ n-k } \\right) \\] <p>where:</p> <ul> <li>\\(n\\) is the sample size,</li> <li>\\(m\\) is the maximum lag being tested,</li> <li>\\(r_k\\) is the sample autocorrelation at lag \\(k\\), and</li> <li>\\(\\sum\\) (\\(sum\\)) denotes the sum over \\(k\\) from \\(1\\) to \\(m\\).</li> </ul> <pre><code>LM = n * (n+2) * Sum(r_k^2 / (n-k)) - 2 * (n-1) * (n-2) * Sum(r_k * r_(k+1) / (n-k))\n</code></pre> <p>--&gt;</p> <p>In practice, the LM test proceeds by:</p> <ul> <li>Fitting a time series model to the data and obtaining the residuals.</li> <li>Running an auxiliary regression of these residuals on their past <code>nlags</code> values (and any relevant exogenous variables).</li> <li>Computing the LM statistic as \\((n_{obs} - ddof) \\times R^2\\) from this auxiliary regression.</li> </ul> <p>Under the null hypothesis that the autocorrelations up to the specified lag are zero (no serial correlation in the residuals), the LM statistic is asymptotically distributed as a chi-squared random variable with degrees of freedom equal to the number of lagged residual terms included in the auxiliary regression (i.e. the number of lags being tested, adjusted for any restrictions implied by the model).</p> <p>If the test statistic is greater than the critical value from the chi-squared distribution (or equivalently, if the p-value is less than a chosen significance level such as \\(0.05\\)), then the null hypothesis of no autocorrelation is rejected, indicating that there is evidence of autocorrelation in the residuals. This suggests that the time series model may be inadequate and that additional terms may need to be added to the model to account for the remaining autocorrelation.</p> <p>If the test statistic is less than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is not rejected, indicating that there is no evidence of autocorrelation in the residuals. This suggests that the time series model is adequate and that no further improvements are needed with respect to serial correlation.</p> <p>The LM test is a generalization of the Durbin-Watson test, which is a simpler test that only tests for first-order autocorrelation. The LM test can be used to test for higher-order autocorrelation and is more powerful than the Durbin-Watson test.</p> <p>Overall, the Lagrange Multiplier test is a valuable tool in time series forecasting, as it helps to assess the adequacy of a time series model and to identify areas for improvement. By testing for autocorrelation in the residuals, the test helps to ensure that the model is accurately capturing the underlying patterns in the time series data.</p> <p>The LM test can be calculated using the <code>acorr_lm()</code> function in the <code>statsmodels</code> package in Python. The function takes a time series array and the maximum lag <code>m</code> as input, and returns the LM test statistic and associated p-value. If the p-value of the test is less than a certain significance level (e.g. \\(0.05\\)), then there is evidence of significant autocorrelation in the time series up to the specified lag.</p> <p>Parameters:</p> Name Type Description Default <code>resid</code> <code>ArrayLike</code> <p>Time series to test.</p> required <code>nlags</code> <code>Optional[int]</code> <p>Highest lag to use. Defaults to <code>None</code>.</p> <p>Deprecation</p> <p>The behavior of this parameter will change after <code>statsmodels</code> version <code>0.12</code>.</p> <code>None</code> <code>store</code> <code>bool</code> <p>If <code>True</code> then the intermediate results are also returned. Defaults to <code>False</code>.</p> <code>False</code> <code>period</code> <code>Optional[int]</code> <p>The period of a Seasonal time series. Used to compute the max lag for seasonal data which uses \\(\\min(2 \\times period, \\frac{nobs}{5})\\) (calculated with: <code>min(2*period,nobs//5)</code>) if set. If <code>None</code>, then the default rule is used to set the number of lags. When set, must be \\(&gt;=\\) <code>2</code>. Defaults to <code>None</code>.</p> <code>None</code> <code>ddof</code> <code>int</code> <p>The number of degrees of freedom consumed by the model used to produce resid Defaults to <code>0</code>.</p> <code>0</code> <code>cov_type</code> <code>Union[Literal['nonrobust'], VALID_LM_COV_TYPE_OPTIONS]</code> <p>Covariance type. The default is <code>\"nonrobust\"</code> which uses the classic OLS covariance estimator. Specify one of <code>\"HC0\"</code>, <code>\"HC1\"</code>, <code>\"HC2\"</code>, <code>\"HC3\"</code> to use White's covariance estimator. All covariance types supported by <code>OLS.fit</code> are accepted. Defaults to <code>\"nonrobust\"</code>.</p> <code>'nonrobust'</code> <code>cov_kwargs</code> <code>Optional[dict]</code> <p>Dictionary of covariance options passed to <code>OLS.fit</code>. See <code>OLS.fit</code> for more details. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>lm</code> <code>float</code> <p>Lagrange multiplier test statistic.</p> <code>lmpval</code> <code>float</code> <p>The <code>p-value</code> for Lagrange multiplier test.</p> <code>fval</code> <code>float</code> <p>The <code>f-statistic</code> of the F test, alternative version of the same test based on F test for the parameter restriction.</p> <code>fpval</code> <code>float</code> <p>The <code>p-value</code> of the F test.</p> <code>res_store</code> <code>Optional[ResultsStore]</code> <p>Intermediate results. Only returned if <code>store=True</code>.</p> <p>Credit</p> <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> <p>Examples</p> Test Lagrange Multiplier for autocorrelation<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import lm\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_lm, res_lmpval, res_fval, res_fpval = lm(data)\n&gt;&gt;&gt; pprint(res_lm)\n128.09655717844828\n&gt;&gt;&gt; pprint(res_lmpval)\n1.1416848684314836e-22\n&gt;&gt;&gt; pprint(res_fval)\n266.89301496118736\n&gt;&gt;&gt; pprint(res_fpval)\n2.36205831339912e-78\n</code></pre> See Also <ul> <li><code>statsmodels.regression.linear_model.OLS.fit</code>: Fit a linear model.</li> <li><code>statsmodels.regression.linear_model.RegressionResults</code>: The output results of a linear regression model.</li> <li><code>statsmodels.stats.diagnostic.het_arch</code>: Conditional heteroskedasticity testing.</li> <li><code>statsmodels.stats.diagnostic.acorr_ljungbox</code>: Ljung-Box test for serial correlation.</li> <li><code>statsmodels.stats.diagnostic.acorr_lm</code>: Lagrange Multiplier tests for autocorrelation.</li> <li><code>statsmodels.stats.diagnostic.acorr_breusch_godfrey</code>: Breusch-Godfrey test for serial correlation.</li> <li><code>ts_stat_tests.algorithms.correlation.lb</code>: Ljung-Box test of autocorrelation in residuals.</li> <li><code>ts_stat_tests.algorithms.correlation.lm</code>: Lagrange Multiplier tests for autocorrelation.</li> <li><code>ts_stat_tests.algorithms.correlation.bglm</code>: Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/correlation.py</code> <pre><code>@typechecked\ndef lm(\n    resid: ArrayLike,\n    nlags: Optional[int] = None,\n    *,\n    store: bool = False,\n    period: Optional[int] = None,\n    ddof: int = 0,\n    cov_type: Union[Literal[\"nonrobust\"], VALID_LM_COV_TYPE_OPTIONS] = \"nonrobust\",\n    cov_kwargs: Optional[dict] = None,\n) -&gt; Union[\n    tuple[np.ndarray, np.ndarray, float, float],\n    tuple[np.ndarray, np.ndarray, float, float, ResultsStore],\n    tuple[float, float, float, float],\n    tuple[float, float, float, float, ResultsStore],\n]:\n    \"\"\"\n    !!! note \"Summary\"\n\n        The Lagrange Multiplier (LM) test is a statistical test used in time series forecasting to test for the presence of autocorrelation in a model. The test is based on the residual sum of squares (RSS) of a time series model, and can be used to assess the adequacy of the model and to identify areas for improvement.\n\n        This function will implement the [`acorr_lm()`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_lm.html) function from the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ???+ abstract \"Details\"\n\n        This is a generic Lagrange Multiplier (LM) test for autocorrelation. It returns Engle's ARCH test if `resid` is the squared residual array. The Breusch-Godfrey test is a variation on this LM test with additional exogenous variables in the auxiliary regression.\n\n        The LM test statistic is computed as\n\n        $$\n        LM = (n_{obs} - ddof) \\\\times R^2,\n        $$\n\n        ```\n        LM = (n_obs - ddof) * R^2\n        ```\n\n        where $R^2$ is the coefficient of determination from the **auxiliary regression** of the residuals on their own `nlags` lags (and any additional regressors included in the model), $n_{obs}$ is the number of observations, and $ddof$ is the model degrees of freedom lost due to parameter estimation.\n\n        &lt;!-- Previous algorithm included below\n\n        $$\n        LM = n \\\\times (n+2) \\\\times \\\\sum_{k=1}^m \\\\left( \\\\frac { r_k^2 }{ n-k } \\\\right) - 2 \\\\times (n-1) \\\\times (n-2) \\\\times \\\\sum_{k=1}^m \\\\left( r_k \\\\times \\\\frac { r_{k+1} }{ n-k } \\\\right)\n        $$\n\n        where:\n\n        - $n$ is the sample size,\n        - $m$ is the maximum lag being tested,\n        - $r_k$ is the sample autocorrelation at lag $k$, and\n        - $\\\\sum$ ($sum$) denotes the sum over $k$ from $1$ to $m$.\n\n        ```\n        LM = n * (n+2) * Sum(r_k^2 / (n-k)) - 2 * (n-1) * (n-2) * Sum(r_k * r_(k+1) / (n-k))\n        ```\n\n        --&gt;\n\n        In practice, the LM test proceeds by:\n\n        - Fitting a time series model to the data and obtaining the residuals.\n        - Running an auxiliary regression of these residuals on their past `nlags` values (and any relevant exogenous variables).\n        - Computing the LM statistic as $(n_{obs} - ddof) \\\\times R^2$ from this auxiliary regression.\n\n        Under the null hypothesis that the autocorrelations up to the specified lag are zero (no serial correlation in the residuals), the LM statistic is asymptotically distributed as a chi-squared random variable with degrees of freedom equal to the number of lagged residual terms included in the auxiliary regression (i.e. the number of lags being tested, adjusted for any restrictions implied by the model).\n\n        If the test statistic is greater than the critical value from the chi-squared distribution (or equivalently, if the p-value is less than a chosen significance level such as $0.05$), then the null hypothesis of no autocorrelation is rejected, indicating that there is evidence of autocorrelation in the residuals. This suggests that the time series model may be inadequate and that additional terms may need to be added to the model to account for the remaining autocorrelation.\n\n        If the test statistic is less than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is not rejected, indicating that there is no evidence of autocorrelation in the residuals. This suggests that the time series model is adequate and that no further improvements are needed with respect to serial correlation.\n\n        The LM test is a generalization of the Durbin-Watson test, which is a simpler test that only tests for first-order autocorrelation. The LM test can be used to test for higher-order autocorrelation and is more powerful than the Durbin-Watson test.\n\n        Overall, the Lagrange Multiplier test is a valuable tool in time series forecasting, as it helps to assess the adequacy of a time series model and to identify areas for improvement. By testing for autocorrelation in the residuals, the test helps to ensure that the model is accurately capturing the underlying patterns in the time series data.\n\n        The LM test can be calculated using the `acorr_lm()` function in the `statsmodels` package in Python. The function takes a time series array and the maximum lag `m` as input, and returns the LM test statistic and associated p-value. If the p-value of the test is less than a certain significance level (e.g. $0.05$), then there is evidence of significant autocorrelation in the time series up to the specified lag.\n\n    Params:\n        resid (ArrayLike):\n            Time series to test.\n        nlags (Optional[int], optional):\n            Highest lag to use.&lt;br&gt;\n            Defaults to `None`.\n            !!! deprecation \"Deprecation\"\n                The behavior of this parameter will change after `statsmodels` version `0.12`.\n        store (bool, optional):\n            If `True` then the intermediate results are also returned.&lt;br&gt;\n            Defaults to `False`.\n        period (Optional[int], optional):\n            The period of a Seasonal time series. Used to compute the max lag for seasonal data which uses $\\\\min(2 \\\\times period, \\\\frac{nobs}{5})$ (calculated with: `min(2*period,nobs//5)`) if set. If `None`, then the default rule is used to set the number of lags. When set, must be $&gt;=$ `2`.&lt;br&gt;\n            Defaults to `None`.\n        ddof (int, optional):\n            The number of degrees of freedom consumed by the model used to produce resid&lt;br&gt;\n            Defaults to `0`.\n        cov_type (Union[Literal[\"nonrobust\"], VALID_LM_COV_TYPE_OPTIONS], optional):\n            Covariance type. The default is `\"nonrobust\"` which uses the classic OLS covariance estimator. Specify one of `\"HC0\"`, `\"HC1\"`, `\"HC2\"`, `\"HC3\"` to use White's covariance estimator. All covariance types supported by `OLS.fit` are accepted.&lt;br&gt;\n            Defaults to `\"nonrobust\"`.\n        cov_kwargs (Optional[dict], optional):\n            Dictionary of covariance options passed to `OLS.fit`. See [`OLS.fit`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.fit.html) for more details.&lt;br&gt;\n            Defaults to `None`.\n\n    Returns:\n        lm (float):\n            Lagrange multiplier test statistic.\n        lmpval (float):\n            The `p-value` for Lagrange multiplier test.\n        fval (float):\n            The `f-statistic` of the F test, alternative version of the same test based on F test for the parameter restriction.\n        fpval (float):\n            The `p-value` of the F test.\n        res_store (Optional[ResultsStore]):\n            Intermediate results. Only returned if `store=True`.\n\n    !!! success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/) library.\n\n    !!! example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test Lagrange Multiplier for autocorrelation\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import lm\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_lm, res_lmpval, res_fval, res_fpval = lm(data)\n        &gt;&gt;&gt; pprint(res_lm)\n        128.09655717844828\n        &gt;&gt;&gt; pprint(res_lmpval)\n        1.1416848684314836e-22\n        &gt;&gt;&gt; pprint(res_fval)\n        266.89301496118736\n        &gt;&gt;&gt; pprint(res_fpval)\n        2.36205831339912e-78\n        ```\n\n    ??? tip \"See Also\"\n        - [`statsmodels.regression.linear_model.OLS.fit`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.fit.html): Fit a linear model.\n        - [`statsmodels.regression.linear_model.RegressionResults`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.RegressionResults.html): The output results of a linear regression model.\n        - [`statsmodels.stats.diagnostic.het_arch`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.het_arch.html#statsmodels.stats.diagnostic.het_arch): Conditional heteroskedasticity testing.\n        - [`statsmodels.stats.diagnostic.acorr_ljungbox`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_ljungbox.html): Ljung-Box test for serial correlation.\n        - [`statsmodels.stats.diagnostic.acorr_lm`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_lm.html): Lagrange Multiplier tests for autocorrelation.\n        - [`statsmodels.stats.diagnostic.acorr_breusch_godfrey`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_breusch_godfrey.html): Breusch-Godfrey test for serial correlation.\n        - [`ts_stat_tests.algorithms.correlation.lb`][ts_stat_tests.algorithms.correlation.lb]: Ljung-Box test of autocorrelation in residuals.\n        - [`ts_stat_tests.algorithms.correlation.lm`][ts_stat_tests.algorithms.correlation.lm]: Lagrange Multiplier tests for autocorrelation.\n        - [`ts_stat_tests.algorithms.correlation.bglm`][ts_stat_tests.algorithms.correlation.bglm]: Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation.\n    \"\"\"\n    return acorr_lm(  # type: ignore  # statsmodels' acorr_lm has incomplete type hints for these arguments\n        resid=resid,\n        nlags=nlags,\n        store=store,\n        period=period,\n        ddof=ddof,\n        cov_type=cov_type,\n        cov_kwargs=cov_kwargs,\n    )\n</code></pre>"},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation.bglm","title":"bglm","text":"<pre><code>bglm(\n    res: Union[RegressionResults, RegressionResultsWrapper],\n    nlags: Optional[int] = None,\n    *,\n    store: Literal[False] = False\n) -&gt; tuple[float, float, float, float]\n</code></pre><pre><code>bglm(\n    res: Union[RegressionResults, RegressionResultsWrapper],\n    nlags: Optional[int] = None,\n    *,\n    store: Literal[True]\n) -&gt; tuple[float, float, float, float, ResultsStore]\n</code></pre> <pre><code>bglm(\n    res: Union[RegressionResults, RegressionResultsWrapper],\n    nlags: Optional[int] = None,\n    *,\n    store: bool = False\n) -&gt; Union[\n    tuple[float, float, float, float],\n    tuple[float, float, float, float, ResultsStore],\n]\n</code></pre> <p>Summary</p> <p>The Breusch-Godfrey Lagrange Multiplier (BGLM) test is a statistical test used in time series forecasting to test for the presence of autocorrelation in the residuals of a model. The test is a generalization of the LM test and can be used to test for autocorrelation up to a specified order.</p> <p>This function will implement the <code>acorr_breusch_godfrey()</code> function from the <code>statsmodels</code> library.</p> Details <p>BG adds lags of residual to exog in the design matrix for the auxiliary regression with residuals as endog. See Greene (2002), section 12.7.1.</p> <p>The BGLM test is performed by first fitting a time series model to the data and then obtaining the residuals from the model. The residuals are then used to estimate the autocorrelation function (ACF) up to a specified order, typically using the Box-Pierce or Ljung-Box tests. The estimated ACF values are then used to construct the BGLM test statistic, which is compared to a chi-squared distribution with degrees of freedom equal to the number of lags tested.</p> <p>The BGLM test statistic is calculated as:</p> \\[ BGLM = n \\times R^2 \\] <p>where:</p> <ul> <li>\\(n\\) is the sample size and</li> <li>\\(R^2\\) is the coefficient of determination from a regression of the residuals on the lagged values of the residuals and the lagged values of the predictor variable.</li> </ul> <pre><code>BGLM = n * R^2\n</code></pre> <p>Under the null hypothesis that there is no autocorrelation in the residuals of the regression model, the BGLM test statistic follows a chi-squared distribution with degrees of freedom equal to the number of lags included in the model.</p> <p>If the test statistic is greater than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is rejected, indicating that there is evidence of autocorrelation in the residuals. This suggests that the time series model is inadequate, and that additional terms may need to be added to the model to account for the remaining autocorrelation.</p> <p>If the test statistic is less than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is not rejected, indicating that there is no evidence of autocorrelation in the residuals. This suggests that the time series model is adequate, and that no further improvements are needed.</p> <p>Overall, the Breusch-Godfrey Lagrange Multiplier test is a valuable tool in time series forecasting, as it helps to assess the adequacy of a time series model and to identify areas for improvement. By testing for autocorrelation in the residuals, the test helps to ensure that the model is accurately capturing the underlying patterns in the time series data. The test is also useful for determining the appropriate order of an autoregressive integrated moving average (ARIMA) model.</p> <p>The BGLM test can be calculated using the <code>acorr_breusch_godfrey()</code> function in the <code>statsmodels</code> package in Python. The function takes a fitted regression model and the maximum number of lags to include in the test as input, and returns the BGLM test statistic and associated p-value. If the p-value of the test is less than a certain significance level (e.g. \\(0.05\\)), then there is evidence of significant autocorrelation in the residuals of the regression model up to the specified lag.</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>Union[RegressionResults, RegressionResultsWrapper]</code> <p>Estimation results for which the residuals are tested for serial correlation.</p> required <code>nlags</code> <code>Optional[int]</code> <p>Number of lags to include in the auxiliary regression. (<code>nlags</code> is highest lag). Defaults to <code>None</code>.</p> <code>None</code> <code>store</code> <code>bool</code> <p>If <code>store</code> is <code>True</code>, then an additional class instance that contains intermediate results is returned. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>lm</code> <code>float</code> <p>Lagrange multiplier test statistic.</p> <code>lmpval</code> <code>float</code> <p>The <code>p-value</code> for Lagrange multiplier test.</p> <code>fval</code> <code>float</code> <p>The value of the <code>f-statistic</code> for F test, alternative version of the same test based on F test for the parameter restriction.</p> <code>fpval</code> <code>float</code> <p>The <code>p-value</code> of the F test.</p> <code>res_store</code> <code>Optional[ResultsStore]</code> <p>A class instance that holds intermediate results. Only returned if <code>store=True</code>.</p> <p>Credit</p> <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> <p>Examples</p> Test for Breusch-Godfrey Lagrange Multiplier in residual autocorrelation<pre><code>&gt;&gt;&gt; from statsmodels import api as sm\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import bglm\n&gt;&gt;&gt; y = sm.datasets.longley.load_pandas().endog\n&gt;&gt;&gt; X = sm.datasets.longley.load_pandas().exog\n&gt;&gt;&gt; X = sm.add_constant(X)\n&gt;&gt;&gt; res_lm, res_lmpval, res_fval, res_fpval = bglm(sm.OLS(y, X).fit())\n&gt;&gt;&gt; print(res_lm)\n5.1409448555268185\n&gt;&gt;&gt; print(res_lmpval)\n0.16176265367835008\n&gt;&gt;&gt; print(res_fval)\n0.9468493873718188\n&gt;&gt;&gt; print(res_fpval)\n0.4751521243357578\n</code></pre> References <ol> <li>Greene, W. H. Econometric Analysis. New Jersey. Prentice Hall; 5th edition. (2002).</li> </ol> See Also <ul> <li><code>statsmodels.regression.linear_model.OLS.fit</code>: Fit a linear model.</li> <li><code>statsmodels.regression.linear_model.RegressionResults</code>: The output results of a linear regression model.</li> <li><code>statsmodels.stats.diagnostic.het_arch</code>: Conditional heteroskedasticity testing.</li> <li><code>statsmodels.stats.diagnostic.acorr_ljungbox</code>: Ljung-Box test for serial correlation.</li> <li><code>statsmodels.stats.diagnostic.acorr_lm</code>: Lagrange Multiplier tests for autocorrelation.</li> <li><code>statsmodels.stats.diagnostic.acorr_breusch_godfrey</code>: Breusch-Godfrey test for serial correlation.</li> <li><code>ts_stat_tests.algorithms.correlation.lb</code>: Ljung-Box test of autocorrelation in residuals.</li> <li><code>ts_stat_tests.algorithms.correlation.lm</code>: Lagrange Multiplier tests for autocorrelation.</li> <li><code>ts_stat_tests.algorithms.correlation.bglm</code>: Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/correlation.py</code> <pre><code>@typechecked\ndef bglm(\n    res: Union[RegressionResults, RegressionResultsWrapper],\n    nlags: Optional[int] = None,\n    *,\n    store: bool = False,\n) -&gt; Union[\n    tuple[float, float, float, float],\n    tuple[float, float, float, float, ResultsStore],\n]:\n    \"\"\"\n    !!! note \"Summary\"\n\n        The Breusch-Godfrey Lagrange Multiplier (BGLM) test is a statistical test used in time series forecasting to test for the presence of autocorrelation in the residuals of a model. The test is a generalization of the LM test and can be used to test for autocorrelation up to a specified order.\n\n        This function will implement the [`acorr_breusch_godfrey()`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_breusch_godfrey.html) function from the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ???+ abstract \"Details\"\n\n        BG adds lags of residual to exog in the design matrix for the auxiliary regression with residuals as endog. See Greene (2002), section 12.7.1.\n\n        The BGLM test is performed by first fitting a time series model to the data and then obtaining the residuals from the model. The residuals are then used to estimate the autocorrelation function (ACF) up to a specified order, typically using the Box-Pierce or Ljung-Box tests. The estimated ACF values are then used to construct the BGLM test statistic, which is compared to a chi-squared distribution with degrees of freedom equal to the number of lags tested.\n\n        The BGLM test statistic is calculated as:\n\n        $$\n        BGLM = n \\\\times R^2\n        $$\n\n        where:\n\n        - $n$ is the sample size and\n        - $R^2$ is the coefficient of determination from a regression of the residuals on the lagged values of the residuals and the lagged values of the predictor variable.\n\n        ```\n        BGLM = n * R^2\n        ```\n\n        Under the null hypothesis that there is no autocorrelation in the residuals of the regression model, the BGLM test statistic follows a chi-squared distribution with degrees of freedom equal to the number of lags included in the model.\n\n        If the test statistic is greater than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is rejected, indicating that there is evidence of autocorrelation in the residuals. This suggests that the time series model is inadequate, and that additional terms may need to be added to the model to account for the remaining autocorrelation.\n\n        If the test statistic is less than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is not rejected, indicating that there is no evidence of autocorrelation in the residuals. This suggests that the time series model is adequate, and that no further improvements are needed.\n\n        Overall, the Breusch-Godfrey Lagrange Multiplier test is a valuable tool in time series forecasting, as it helps to assess the adequacy of a time series model and to identify areas for improvement. By testing for autocorrelation in the residuals, the test helps to ensure that the model is accurately capturing the underlying patterns in the time series data. The test is also useful for determining the appropriate order of an autoregressive integrated moving average (ARIMA) model.\n\n        The BGLM test can be calculated using the `acorr_breusch_godfrey()` function in the `statsmodels` package in Python. The function takes a fitted regression model and the maximum number of lags to include in the test as input, and returns the BGLM test statistic and associated p-value. If the p-value of the test is less than a certain significance level (e.g. $0.05$), then there is evidence of significant autocorrelation in the residuals of the regression model up to the specified lag.\n\n    Params:\n        res (Union[RegressionResults, RegressionResultsWrapper]):\n            Estimation results for which the residuals are tested for serial correlation.\n        nlags (Optional[int], optional):\n            Number of lags to include in the auxiliary regression. (`nlags` is highest lag).&lt;br&gt;\n            Defaults to `None`.\n        store (bool, optional):\n            If `store` is `True`, then an additional class instance that contains intermediate results is returned.&lt;br&gt;\n            Defaults to `False`.\n\n    Returns:\n        lm (float):\n            Lagrange multiplier test statistic.\n        lmpval (float):\n            The `p-value` for Lagrange multiplier test.\n        fval (float):\n            The value of the `f-statistic` for F test, alternative version of the same test based on F test for the parameter restriction.\n        fpval (float):\n            The `p-value` of the F test.\n        res_store (Optional[ResultsStore]):\n            A class instance that holds intermediate results. Only returned if `store=True`.\n\n    !!! success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/) library.\n\n    !!! example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test for Breusch-Godfrey Lagrange Multiplier in residual autocorrelation\"}\n        &gt;&gt;&gt; from statsmodels import api as sm\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import bglm\n        &gt;&gt;&gt; y = sm.datasets.longley.load_pandas().endog\n        &gt;&gt;&gt; X = sm.datasets.longley.load_pandas().exog\n        &gt;&gt;&gt; X = sm.add_constant(X)\n        &gt;&gt;&gt; res_lm, res_lmpval, res_fval, res_fpval = bglm(sm.OLS(y, X).fit())\n        &gt;&gt;&gt; print(res_lm)\n        5.1409448555268185\n        &gt;&gt;&gt; print(res_lmpval)\n        0.16176265367835008\n        &gt;&gt;&gt; print(res_fval)\n        0.9468493873718188\n        &gt;&gt;&gt; print(res_fpval)\n        0.4751521243357578\n        ```\n\n    ??? question \"References\"\n        1. Greene, W. H. Econometric Analysis. New Jersey. Prentice Hall; 5th edition. (2002).\n\n    ??? tip \"See Also\"\n        - [`statsmodels.regression.linear_model.OLS.fit`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.fit.html): Fit a linear model.\n        - [`statsmodels.regression.linear_model.RegressionResults`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.RegressionResults.html): The output results of a linear regression model.\n        - [`statsmodels.stats.diagnostic.het_arch`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.het_arch.html#statsmodels.stats.diagnostic.het_arch): Conditional heteroskedasticity testing.\n        - [`statsmodels.stats.diagnostic.acorr_ljungbox`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_ljungbox.html): Ljung-Box test for serial correlation.\n        - [`statsmodels.stats.diagnostic.acorr_lm`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_lm.html): Lagrange Multiplier tests for autocorrelation.\n        - [`statsmodels.stats.diagnostic.acorr_breusch_godfrey`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_breusch_godfrey.html): Breusch-Godfrey test for serial correlation.\n        - [`ts_stat_tests.algorithms.correlation.lb`][ts_stat_tests.algorithms.correlation.lb]: Ljung-Box test of autocorrelation in residuals.\n        - [`ts_stat_tests.algorithms.correlation.lm`][ts_stat_tests.algorithms.correlation.lm]: Lagrange Multiplier tests for autocorrelation.\n        - [`ts_stat_tests.algorithms.correlation.bglm`][ts_stat_tests.algorithms.correlation.bglm]: Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation.\n    \"\"\"\n    return acorr_breusch_godfrey(  # type: ignore  # statsmodels typing for acorr_breusch_godfrey is incomplete/incompatible with our RegressionResults types\n        res=res,\n        nlags=nlags,\n        store=store,\n    )\n</code></pre>"},{"location":"code/normality/","title":"Test the <code>normality</code> of a given Time-Series Dataset","text":""},{"location":"code/normality/#introduction","title":"Introduction","text":"<p>Summary</p> <p>As stated by the NIST/SEMATECH e-Handbook of Statistical Methods:</p> <p>Normal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known. Their importance is partly due to the central limit theorem.</p> <p> For more info, see: Engineering Statistics Handbook: Measures of Skewness and Kurtosis.</p> <p>Info</p> <p>The normality test is used to determine whether a data set is well-modeled by a normal distribution. In time series forecasting, we primarily test the residuals (errors) of a model for normality. If the residuals follow a normal distribution, it suggests that the model has successfully captured the systematic patterns in the data, and the remaining errors are random white noise.</p> <p>If the residuals are not normally distributed, it may indicate that the model is missing important features, such as seasonal patterns or long-term trends, or that a transformation of the data (e.g., Log or Box-Cox) is required before modeling.</p> <p> For more info, see: Hyndman &amp; Athanasopoulos: Forecasting: Principles and Practice.</p> <p>Source Library</p> <p>The <code>scipy</code> and <code>statsmodels</code> packages were chosen because they provide standard, reliable implementations of classical statistical tests. <code>scipy.stats</code> provides implementations for Shapiro-Wilk, D'Agostino-Pearson, and Anderson-Darling tests, while <code>statsmodels</code> provides the Jarque-Bera and Omnibus tests.</p> <p>Source Module</p> <p>All of the source code can be found within these modules:</p> <ul> <li><code>ts_stat_tests.algorithms.normality</code>.</li> <li><code>ts_stat_tests.tests.normality</code>.</li> </ul>"},{"location":"code/normality/#normality-tests","title":"Normality Tests","text":""},{"location":"code/normality/#ts_stat_tests.tests.normality","title":"ts_stat_tests.tests.normality","text":"<p>Summary</p> <p>This module contains convenience functions and tests for normality measures, allowing for easy access to different normality algorithms.</p>"},{"location":"code/normality/#ts_stat_tests.tests.normality.normality","title":"normality","text":"<pre><code>normality(\n    x: ArrayLike,\n    algorithm: str = \"dp\",\n    axis: int = 0,\n    nan_policy: VALID_DP_NAN_POLICY_OPTIONS = \"propagate\",\n    dist: VALID_AD_DIST_OPTIONS = \"norm\",\n) -&gt; Any\n</code></pre> <p>Summary</p> <p>Perform a normality test on the given data.</p> Details <p>This function is a convenience wrapper around the five underlying algorithms: - <code>jb()</code> - <code>ob()</code> - <code>sw()</code> - <code>dp()</code> - <code>ad()</code></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked. Should be a <code>1-D</code> or <code>N-D</code> data array.</p> required <code>algorithm</code> <code>str</code> <p>Which normality algorithm to use. - <code>jb()</code>: <code>[\"jb\", \"jarque\", \"jarque-bera\"]</code> - <code>ob()</code>: <code>[\"ob\", \"omni\", \"omnibus\"]</code> - <code>sw()</code>: <code>[\"sw\", \"shapiro\", \"shapiro-wilk\"]</code> - <code>dp()</code>: <code>[\"dp\", \"dagostino\", \"dagostino-pearson\"]</code> - <code>ad()</code>: <code>[\"ad\", \"anderson\", \"anderson-darling\"]</code> Defaults to <code>\"dp\"</code>.</p> <code>'dp'</code> <code>axis</code> <code>int</code> <p>Axis along which to compute the test. Default is <code>0</code>.</p> <code>0</code> <code>nan_policy</code> <code>VALID_DP_NAN_POLICY_OPTIONS</code> <p>Defines how to handle when input contains <code>NaN</code>. - <code>propagate</code>: returns <code>NaN</code> - <code>raise</code>: throws an error - <code>omit</code>: performs the calculations ignoring <code>NaN</code> values Defaults to <code>\"propagate\"</code>.</p> <code>'propagate'</code> <code>dist</code> <code>VALID_AD_DIST_OPTIONS</code> <p>The type of distribution to test against. Only relevant when <code>algorithm=anderson</code>. Defaults to <code>\"norm\"</code>.</p> <code>'norm'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the given value for <code>algorithm</code> is not valid.</p> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the normality test.</p> <p>Credit</p> <p>Calculations are performed by <code>scipy.stats</code> and <code>statsmodels.stats</code>.</p> Examples <p><code>normality</code> with <code>dagostino-pearson</code> algorithm: Basic usage<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from ts_stat_tests.tests.normality import normality\n&gt;&gt;&gt; data = np.random.normal(0, 1, 100)\n&gt;&gt;&gt; result = normality(data, algorithm=\"dp\")\n</code></pre></p> Source code in <code>src/ts_stat_tests/tests/normality.py</code> <pre><code>@typechecked\ndef normality(\n    x: ArrayLike,\n    algorithm: str = \"dp\",\n    axis: int = 0,\n    nan_policy: VALID_DP_NAN_POLICY_OPTIONS = \"propagate\",\n    dist: VALID_AD_DIST_OPTIONS = \"norm\",\n) -&gt; Any:\n    \"\"\"\n    !!! note \"Summary\"\n        Perform a normality test on the given data.\n\n    ???+ abstract \"Details\"\n        This function is a convenience wrapper around the five underlying algorithms:&lt;br&gt;\n        - [`jb()`][ts_stat_tests.algorithms.normality.jb]&lt;br&gt;\n        - [`ob()`][ts_stat_tests.algorithms.normality.ob]&lt;br&gt;\n        - [`sw()`][ts_stat_tests.algorithms.normality.sw]&lt;br&gt;\n        - [`dp()`][ts_stat_tests.algorithms.normality.dp]&lt;br&gt;\n        - [`ad()`][ts_stat_tests.algorithms.normality.ad]\n\n    Params:\n        x (ArrayLike):\n            The data to be checked. Should be a `1-D` or `N-D` data array.\n        algorithm (str):\n            Which normality algorithm to use.&lt;br&gt;\n            - `jb()`: `[\"jb\", \"jarque\", \"jarque-bera\"]`&lt;br&gt;\n            - `ob()`: `[\"ob\", \"omni\", \"omnibus\"]`&lt;br&gt;\n            - `sw()`: `[\"sw\", \"shapiro\", \"shapiro-wilk\"]`&lt;br&gt;\n            - `dp()`: `[\"dp\", \"dagostino\", \"dagostino-pearson\"]`&lt;br&gt;\n            - `ad()`: `[\"ad\", \"anderson\", \"anderson-darling\"]`&lt;br&gt;\n            Defaults to `\"dp\"`.\n        axis (int):\n            Axis along which to compute the test. Default is `0`.\n        nan_policy (VALID_DP_NAN_POLICY_OPTIONS):\n            Defines how to handle when input contains `NaN`.&lt;br&gt;\n            - `propagate`: returns `NaN`&lt;br&gt;\n            - `raise`: throws an error&lt;br&gt;\n            - `omit`: performs the calculations ignoring `NaN` values&lt;br&gt;\n            Defaults to `\"propagate\"`.\n        dist (VALID_AD_DIST_OPTIONS):\n            The type of distribution to test against.&lt;br&gt;\n            Only relevant when `algorithm=anderson`.&lt;br&gt;\n            Defaults to `\"norm\"`.\n\n    Raises:\n        ValueError: When the given value for `algorithm` is not valid.\n\n    Returns:\n        (Any):\n            The result of the normality test.\n\n    !!! Success \"Credit\"\n        Calculations are performed by `scipy.stats` and `statsmodels.stats`.\n\n    ???+ example \"Examples\"\n\n        `normality` with `dagostino-pearson` algorithm:\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from ts_stat_tests.tests.normality import normality\n        &gt;&gt;&gt; data = np.random.normal(0, 1, 100)\n        &gt;&gt;&gt; result = normality(data, algorithm=\"dp\")\n        ```\n    \"\"\"\n    options: dict[str, tuple[str, ...]] = {\n        \"jb\": (\"jb\", \"jarque\", \"jarque-bera\"),\n        \"ob\": (\"ob\", \"omni\", \"omnibus\"),\n        \"sw\": (\"sw\", \"shapiro\", \"shapiro-wilk\"),\n        \"dp\": (\"dp\", \"dagostino\", \"dagostino-pearson\"),\n        \"ad\": (\"ad\", \"anderson\", \"anderson-darling\"),\n    }\n    if algorithm in options[\"jb\"]:\n        return _jb(x=x, axis=axis)\n    if algorithm in options[\"ob\"]:\n        return _ob(x=x, axis=axis)\n    if algorithm in options[\"sw\"]:\n        return _sw(x=x)\n    if algorithm in options[\"dp\"]:\n        return _dp(x=x, axis=axis, nan_policy=nan_policy)\n    if algorithm in options[\"ad\"]:\n        return _ad(x=x, dist=dist)\n\n    raise ValueError(\n        generate_error_message(\n            parameter_name=\"algorithm\",\n            value_parsed=algorithm,\n            options=options,\n        )\n    )\n</code></pre>"},{"location":"code/normality/#ts_stat_tests.tests.normality.is_normal","title":"is_normal","text":"<pre><code>is_normal(\n    x: ArrayLike,\n    algorithm: str = \"dp\",\n    alpha: float = 0.05,\n    axis: int = 0,\n    nan_policy: VALID_DP_NAN_POLICY_OPTIONS = \"propagate\",\n    dist: VALID_AD_DIST_OPTIONS = \"norm\",\n) -&gt; dict[str, Union[str, float, bool, Any]]\n</code></pre> <p>Summary</p> <p>Test whether a given data set is <code>normal</code> or not.</p> Details <p>This function implements the given algorithm (defined in the parameter <code>algorithm</code>), and returns a dictionary containing the relevant data: <pre><code>{\n    \"result\": ...,  # The result of the test. Will be `True` if `p-value &gt;= alpha`, and `False` otherwise\n    \"statistic\": ...,  # The test statistic\n    \"p_value\": ...,  # The p-value of the test (if applicable)\n    \"alpha\": ...,  # The significance level used\n}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked. Should be a <code>1-D</code> or <code>N-D</code> data array.</p> required <code>algorithm</code> <code>str</code> <p>Which normality algorithm to use. - <code>jb()</code>: <code>[\"jb\", \"jarque\", \"jarque-bera\"]</code> - <code>ob()</code>: <code>[\"ob\", \"omni\", \"omnibus\"]</code> - <code>sw()</code>: <code>[\"sw\", \"shapiro\", \"shapiro-wilk\"]</code> - <code>dp()</code>: <code>[\"dp\", \"dagostino\", \"dagostino-pearson\"]</code> - <code>ad()</code>: <code>[\"ad\", \"anderson\", \"anderson-darling\"]</code> Defaults to <code>\"dp\"</code>.</p> <code>'dp'</code> <code>alpha</code> <code>float</code> <p>Significance level. Default is <code>0.05</code>.</p> <code>0.05</code> <code>axis</code> <code>int</code> <p>Axis along which to compute the test. Default is <code>0</code>.</p> <code>0</code> <code>nan_policy</code> <code>VALID_DP_NAN_POLICY_OPTIONS</code> <p>Defines how to handle when input contains <code>NaN</code>. - <code>propagate</code>: returns <code>NaN</code> - <code>raise</code>: throws an error - <code>omit</code>: performs the calculations ignoring <code>NaN</code> values Defaults to <code>\"propagate\"</code>.</p> <code>'propagate'</code> <code>dist</code> <code>VALID_AD_DIST_OPTIONS</code> <p>The type of distribution to test against. Only relevant when <code>algorithm=anderson</code>. Defaults to <code>\"norm\"</code>.</p> <code>'norm'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the results of the test.</p> <p>Credit</p> <p>Calculations are performed by <code>scipy.stats</code> and <code>statsmodels.stats</code>.</p> Examples <p><code>is_normal</code> with <code>dagostino-pearson</code> algorithm: Basic usage<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from ts_stat_tests.tests.normality import is_normal\n&gt;&gt;&gt; data = np.random.normal(0, 1, 100)\n&gt;&gt;&gt; result = is_normal(data, algorithm=\"dp\")\n&gt;&gt;&gt; result[\"result\"]\nTrue\n</code></pre></p> Source code in <code>src/ts_stat_tests/tests/normality.py</code> <pre><code>@typechecked\ndef is_normal(\n    x: ArrayLike,\n    algorithm: str = \"dp\",\n    alpha: float = 0.05,\n    axis: int = 0,\n    nan_policy: VALID_DP_NAN_POLICY_OPTIONS = \"propagate\",\n    dist: VALID_AD_DIST_OPTIONS = \"norm\",\n) -&gt; dict[str, Union[str, float, bool, Any]]:\n    \"\"\"\n    !!! note \"Summary\"\n        Test whether a given data set is `normal` or not.\n\n    ???+ abstract \"Details\"\n        This function implements the given algorithm (defined in the parameter `algorithm`), and returns a dictionary containing the relevant data:\n        ```python\n        {\n            \"result\": ...,  # The result of the test. Will be `True` if `p-value &gt;= alpha`, and `False` otherwise\n            \"statistic\": ...,  # The test statistic\n            \"p_value\": ...,  # The p-value of the test (if applicable)\n            \"alpha\": ...,  # The significance level used\n        }\n        ```\n\n    Params:\n        x (ArrayLike):\n            The data to be checked. Should be a `1-D` or `N-D` data array.\n        algorithm (str):\n            Which normality algorithm to use.&lt;br&gt;\n            - `jb()`: `[\"jb\", \"jarque\", \"jarque-bera\"]`&lt;br&gt;\n            - `ob()`: `[\"ob\", \"omni\", \"omnibus\"]`&lt;br&gt;\n            - `sw()`: `[\"sw\", \"shapiro\", \"shapiro-wilk\"]`&lt;br&gt;\n            - `dp()`: `[\"dp\", \"dagostino\", \"dagostino-pearson\"]`&lt;br&gt;\n            - `ad()`: `[\"ad\", \"anderson\", \"anderson-darling\"]`&lt;br&gt;\n            Defaults to `\"dp\"`.\n        alpha (float):\n            Significance level. Default is `0.05`.\n        axis (int):\n            Axis along which to compute the test. Default is `0`.\n        nan_policy (VALID_DP_NAN_POLICY_OPTIONS):\n            Defines how to handle when input contains `NaN`.&lt;br&gt;\n            - `propagate`: returns `NaN`&lt;br&gt;\n            - `raise`: throws an error&lt;br&gt;\n            - `omit`: performs the calculations ignoring `NaN` values&lt;br&gt;\n            Defaults to `\"propagate\"`.\n        dist (VALID_AD_DIST_OPTIONS):\n            The type of distribution to test against.&lt;br&gt;\n            Only relevant when `algorithm=anderson`.&lt;br&gt;\n            Defaults to `\"norm\"`.\n\n    Returns:\n        (dict):\n            A dictionary containing the results of the test.\n\n    !!! Success \"Credit\"\n        Calculations are performed by `scipy.stats` and `statsmodels.stats`.\n\n    ???+ example \"Examples\"\n\n        `is_normal` with `dagostino-pearson` algorithm:\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from ts_stat_tests.tests.normality import is_normal\n        &gt;&gt;&gt; data = np.random.normal(0, 1, 100)\n        &gt;&gt;&gt; result = is_normal(data, algorithm=\"dp\")\n        &gt;&gt;&gt; result[\"result\"]\n        True\n        ```\n    \"\"\"\n    res = normality(x=x, algorithm=algorithm, axis=axis, nan_policy=nan_policy, dist=dist)\n\n    # Anderson-Darling is a bit different\n    options: dict[str, tuple[str, ...]] = {\n        \"ad\": (\"ad\", \"anderson\", \"anderson-darling\"),\n    }\n\n    if algorithm in options[\"ad\"]:\n        # res is AndersonResult(statistic, critical_values, significance_level, fit_result)\n        # indexing only gives the first 3 elements\n        stat, crit, sig = res[0], res[1], res[2]\n        # sig is something like [15. , 10. ,  5. ,  2.5,  1. ]\n        # alpha is something like 0.05 (which is 5%)\n        idx = np.argmin(np.abs(sig - (alpha * 100)))\n        critical_value = crit[idx]\n        is_norm = stat &lt; critical_value\n        return {\n            \"result\": bool(is_norm),\n            \"statistic\": float(stat),\n            \"critical_value\": float(critical_value),\n            \"significance_level\": float(sig[idx]),\n            \"alpha\": float(alpha),\n        }\n\n    # For others, they return (statistic, pvalue) or similar\n    if hasattr(res, \"pvalue\"):\n        p_val = res.pvalue\n        stat = res.statistic\n    elif isinstance(res, (tuple, list)):\n        stat, p_val = res[0], res[1]\n    else:\n        # Fallback\n        stat = res\n        p_val = None\n\n    is_norm = p_val &gt;= alpha if p_val is not None else False\n\n    return {\n        \"result\": bool(is_norm),\n        \"statistic\": float(stat) if isinstance(stat, (float, int)) else stat,\n        \"p_value\": float(p_val) if p_val is not None else None,\n        \"alpha\": float(alpha),\n    }\n</code></pre>"},{"location":"code/normality/#normality-algorithms","title":"Normality Algorithms","text":""},{"location":"code/normality/#ts_stat_tests.tests.normality","title":"ts_stat_tests.tests.normality","text":"<p>Summary</p> <p>This module contains convenience functions and tests for normality measures, allowing for easy access to different normality algorithms.</p>"},{"location":"code/normality/#ts_stat_tests.tests.normality.normality","title":"normality","text":"<pre><code>normality(\n    x: ArrayLike,\n    algorithm: str = \"dp\",\n    axis: int = 0,\n    nan_policy: VALID_DP_NAN_POLICY_OPTIONS = \"propagate\",\n    dist: VALID_AD_DIST_OPTIONS = \"norm\",\n) -&gt; Any\n</code></pre> <p>Summary</p> <p>Perform a normality test on the given data.</p> Details <p>This function is a convenience wrapper around the five underlying algorithms: - <code>jb()</code> - <code>ob()</code> - <code>sw()</code> - <code>dp()</code> - <code>ad()</code></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked. Should be a <code>1-D</code> or <code>N-D</code> data array.</p> required <code>algorithm</code> <code>str</code> <p>Which normality algorithm to use. - <code>jb()</code>: <code>[\"jb\", \"jarque\", \"jarque-bera\"]</code> - <code>ob()</code>: <code>[\"ob\", \"omni\", \"omnibus\"]</code> - <code>sw()</code>: <code>[\"sw\", \"shapiro\", \"shapiro-wilk\"]</code> - <code>dp()</code>: <code>[\"dp\", \"dagostino\", \"dagostino-pearson\"]</code> - <code>ad()</code>: <code>[\"ad\", \"anderson\", \"anderson-darling\"]</code> Defaults to <code>\"dp\"</code>.</p> <code>'dp'</code> <code>axis</code> <code>int</code> <p>Axis along which to compute the test. Default is <code>0</code>.</p> <code>0</code> <code>nan_policy</code> <code>VALID_DP_NAN_POLICY_OPTIONS</code> <p>Defines how to handle when input contains <code>NaN</code>. - <code>propagate</code>: returns <code>NaN</code> - <code>raise</code>: throws an error - <code>omit</code>: performs the calculations ignoring <code>NaN</code> values Defaults to <code>\"propagate\"</code>.</p> <code>'propagate'</code> <code>dist</code> <code>VALID_AD_DIST_OPTIONS</code> <p>The type of distribution to test against. Only relevant when <code>algorithm=anderson</code>. Defaults to <code>\"norm\"</code>.</p> <code>'norm'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the given value for <code>algorithm</code> is not valid.</p> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the normality test.</p> <p>Credit</p> <p>Calculations are performed by <code>scipy.stats</code> and <code>statsmodels.stats</code>.</p> Examples <p><code>normality</code> with <code>dagostino-pearson</code> algorithm: Basic usage<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from ts_stat_tests.tests.normality import normality\n&gt;&gt;&gt; data = np.random.normal(0, 1, 100)\n&gt;&gt;&gt; result = normality(data, algorithm=\"dp\")\n</code></pre></p> Source code in <code>src/ts_stat_tests/tests/normality.py</code> <pre><code>@typechecked\ndef normality(\n    x: ArrayLike,\n    algorithm: str = \"dp\",\n    axis: int = 0,\n    nan_policy: VALID_DP_NAN_POLICY_OPTIONS = \"propagate\",\n    dist: VALID_AD_DIST_OPTIONS = \"norm\",\n) -&gt; Any:\n    \"\"\"\n    !!! note \"Summary\"\n        Perform a normality test on the given data.\n\n    ???+ abstract \"Details\"\n        This function is a convenience wrapper around the five underlying algorithms:&lt;br&gt;\n        - [`jb()`][ts_stat_tests.algorithms.normality.jb]&lt;br&gt;\n        - [`ob()`][ts_stat_tests.algorithms.normality.ob]&lt;br&gt;\n        - [`sw()`][ts_stat_tests.algorithms.normality.sw]&lt;br&gt;\n        - [`dp()`][ts_stat_tests.algorithms.normality.dp]&lt;br&gt;\n        - [`ad()`][ts_stat_tests.algorithms.normality.ad]\n\n    Params:\n        x (ArrayLike):\n            The data to be checked. Should be a `1-D` or `N-D` data array.\n        algorithm (str):\n            Which normality algorithm to use.&lt;br&gt;\n            - `jb()`: `[\"jb\", \"jarque\", \"jarque-bera\"]`&lt;br&gt;\n            - `ob()`: `[\"ob\", \"omni\", \"omnibus\"]`&lt;br&gt;\n            - `sw()`: `[\"sw\", \"shapiro\", \"shapiro-wilk\"]`&lt;br&gt;\n            - `dp()`: `[\"dp\", \"dagostino\", \"dagostino-pearson\"]`&lt;br&gt;\n            - `ad()`: `[\"ad\", \"anderson\", \"anderson-darling\"]`&lt;br&gt;\n            Defaults to `\"dp\"`.\n        axis (int):\n            Axis along which to compute the test. Default is `0`.\n        nan_policy (VALID_DP_NAN_POLICY_OPTIONS):\n            Defines how to handle when input contains `NaN`.&lt;br&gt;\n            - `propagate`: returns `NaN`&lt;br&gt;\n            - `raise`: throws an error&lt;br&gt;\n            - `omit`: performs the calculations ignoring `NaN` values&lt;br&gt;\n            Defaults to `\"propagate\"`.\n        dist (VALID_AD_DIST_OPTIONS):\n            The type of distribution to test against.&lt;br&gt;\n            Only relevant when `algorithm=anderson`.&lt;br&gt;\n            Defaults to `\"norm\"`.\n\n    Raises:\n        ValueError: When the given value for `algorithm` is not valid.\n\n    Returns:\n        (Any):\n            The result of the normality test.\n\n    !!! Success \"Credit\"\n        Calculations are performed by `scipy.stats` and `statsmodels.stats`.\n\n    ???+ example \"Examples\"\n\n        `normality` with `dagostino-pearson` algorithm:\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from ts_stat_tests.tests.normality import normality\n        &gt;&gt;&gt; data = np.random.normal(0, 1, 100)\n        &gt;&gt;&gt; result = normality(data, algorithm=\"dp\")\n        ```\n    \"\"\"\n    options: dict[str, tuple[str, ...]] = {\n        \"jb\": (\"jb\", \"jarque\", \"jarque-bera\"),\n        \"ob\": (\"ob\", \"omni\", \"omnibus\"),\n        \"sw\": (\"sw\", \"shapiro\", \"shapiro-wilk\"),\n        \"dp\": (\"dp\", \"dagostino\", \"dagostino-pearson\"),\n        \"ad\": (\"ad\", \"anderson\", \"anderson-darling\"),\n    }\n    if algorithm in options[\"jb\"]:\n        return _jb(x=x, axis=axis)\n    if algorithm in options[\"ob\"]:\n        return _ob(x=x, axis=axis)\n    if algorithm in options[\"sw\"]:\n        return _sw(x=x)\n    if algorithm in options[\"dp\"]:\n        return _dp(x=x, axis=axis, nan_policy=nan_policy)\n    if algorithm in options[\"ad\"]:\n        return _ad(x=x, dist=dist)\n\n    raise ValueError(\n        generate_error_message(\n            parameter_name=\"algorithm\",\n            value_parsed=algorithm,\n            options=options,\n        )\n    )\n</code></pre>"},{"location":"code/normality/#ts_stat_tests.tests.normality.is_normal","title":"is_normal","text":"<pre><code>is_normal(\n    x: ArrayLike,\n    algorithm: str = \"dp\",\n    alpha: float = 0.05,\n    axis: int = 0,\n    nan_policy: VALID_DP_NAN_POLICY_OPTIONS = \"propagate\",\n    dist: VALID_AD_DIST_OPTIONS = \"norm\",\n) -&gt; dict[str, Union[str, float, bool, Any]]\n</code></pre> <p>Summary</p> <p>Test whether a given data set is <code>normal</code> or not.</p> Details <p>This function implements the given algorithm (defined in the parameter <code>algorithm</code>), and returns a dictionary containing the relevant data: <pre><code>{\n    \"result\": ...,  # The result of the test. Will be `True` if `p-value &gt;= alpha`, and `False` otherwise\n    \"statistic\": ...,  # The test statistic\n    \"p_value\": ...,  # The p-value of the test (if applicable)\n    \"alpha\": ...,  # The significance level used\n}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked. Should be a <code>1-D</code> or <code>N-D</code> data array.</p> required <code>algorithm</code> <code>str</code> <p>Which normality algorithm to use. - <code>jb()</code>: <code>[\"jb\", \"jarque\", \"jarque-bera\"]</code> - <code>ob()</code>: <code>[\"ob\", \"omni\", \"omnibus\"]</code> - <code>sw()</code>: <code>[\"sw\", \"shapiro\", \"shapiro-wilk\"]</code> - <code>dp()</code>: <code>[\"dp\", \"dagostino\", \"dagostino-pearson\"]</code> - <code>ad()</code>: <code>[\"ad\", \"anderson\", \"anderson-darling\"]</code> Defaults to <code>\"dp\"</code>.</p> <code>'dp'</code> <code>alpha</code> <code>float</code> <p>Significance level. Default is <code>0.05</code>.</p> <code>0.05</code> <code>axis</code> <code>int</code> <p>Axis along which to compute the test. Default is <code>0</code>.</p> <code>0</code> <code>nan_policy</code> <code>VALID_DP_NAN_POLICY_OPTIONS</code> <p>Defines how to handle when input contains <code>NaN</code>. - <code>propagate</code>: returns <code>NaN</code> - <code>raise</code>: throws an error - <code>omit</code>: performs the calculations ignoring <code>NaN</code> values Defaults to <code>\"propagate\"</code>.</p> <code>'propagate'</code> <code>dist</code> <code>VALID_AD_DIST_OPTIONS</code> <p>The type of distribution to test against. Only relevant when <code>algorithm=anderson</code>. Defaults to <code>\"norm\"</code>.</p> <code>'norm'</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing the results of the test.</p> <p>Credit</p> <p>Calculations are performed by <code>scipy.stats</code> and <code>statsmodels.stats</code>.</p> Examples <p><code>is_normal</code> with <code>dagostino-pearson</code> algorithm: Basic usage<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from ts_stat_tests.tests.normality import is_normal\n&gt;&gt;&gt; data = np.random.normal(0, 1, 100)\n&gt;&gt;&gt; result = is_normal(data, algorithm=\"dp\")\n&gt;&gt;&gt; result[\"result\"]\nTrue\n</code></pre></p> Source code in <code>src/ts_stat_tests/tests/normality.py</code> <pre><code>@typechecked\ndef is_normal(\n    x: ArrayLike,\n    algorithm: str = \"dp\",\n    alpha: float = 0.05,\n    axis: int = 0,\n    nan_policy: VALID_DP_NAN_POLICY_OPTIONS = \"propagate\",\n    dist: VALID_AD_DIST_OPTIONS = \"norm\",\n) -&gt; dict[str, Union[str, float, bool, Any]]:\n    \"\"\"\n    !!! note \"Summary\"\n        Test whether a given data set is `normal` or not.\n\n    ???+ abstract \"Details\"\n        This function implements the given algorithm (defined in the parameter `algorithm`), and returns a dictionary containing the relevant data:\n        ```python\n        {\n            \"result\": ...,  # The result of the test. Will be `True` if `p-value &gt;= alpha`, and `False` otherwise\n            \"statistic\": ...,  # The test statistic\n            \"p_value\": ...,  # The p-value of the test (if applicable)\n            \"alpha\": ...,  # The significance level used\n        }\n        ```\n\n    Params:\n        x (ArrayLike):\n            The data to be checked. Should be a `1-D` or `N-D` data array.\n        algorithm (str):\n            Which normality algorithm to use.&lt;br&gt;\n            - `jb()`: `[\"jb\", \"jarque\", \"jarque-bera\"]`&lt;br&gt;\n            - `ob()`: `[\"ob\", \"omni\", \"omnibus\"]`&lt;br&gt;\n            - `sw()`: `[\"sw\", \"shapiro\", \"shapiro-wilk\"]`&lt;br&gt;\n            - `dp()`: `[\"dp\", \"dagostino\", \"dagostino-pearson\"]`&lt;br&gt;\n            - `ad()`: `[\"ad\", \"anderson\", \"anderson-darling\"]`&lt;br&gt;\n            Defaults to `\"dp\"`.\n        alpha (float):\n            Significance level. Default is `0.05`.\n        axis (int):\n            Axis along which to compute the test. Default is `0`.\n        nan_policy (VALID_DP_NAN_POLICY_OPTIONS):\n            Defines how to handle when input contains `NaN`.&lt;br&gt;\n            - `propagate`: returns `NaN`&lt;br&gt;\n            - `raise`: throws an error&lt;br&gt;\n            - `omit`: performs the calculations ignoring `NaN` values&lt;br&gt;\n            Defaults to `\"propagate\"`.\n        dist (VALID_AD_DIST_OPTIONS):\n            The type of distribution to test against.&lt;br&gt;\n            Only relevant when `algorithm=anderson`.&lt;br&gt;\n            Defaults to `\"norm\"`.\n\n    Returns:\n        (dict):\n            A dictionary containing the results of the test.\n\n    !!! Success \"Credit\"\n        Calculations are performed by `scipy.stats` and `statsmodels.stats`.\n\n    ???+ example \"Examples\"\n\n        `is_normal` with `dagostino-pearson` algorithm:\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from ts_stat_tests.tests.normality import is_normal\n        &gt;&gt;&gt; data = np.random.normal(0, 1, 100)\n        &gt;&gt;&gt; result = is_normal(data, algorithm=\"dp\")\n        &gt;&gt;&gt; result[\"result\"]\n        True\n        ```\n    \"\"\"\n    res = normality(x=x, algorithm=algorithm, axis=axis, nan_policy=nan_policy, dist=dist)\n\n    # Anderson-Darling is a bit different\n    options: dict[str, tuple[str, ...]] = {\n        \"ad\": (\"ad\", \"anderson\", \"anderson-darling\"),\n    }\n\n    if algorithm in options[\"ad\"]:\n        # res is AndersonResult(statistic, critical_values, significance_level, fit_result)\n        # indexing only gives the first 3 elements\n        stat, crit, sig = res[0], res[1], res[2]\n        # sig is something like [15. , 10. ,  5. ,  2.5,  1. ]\n        # alpha is something like 0.05 (which is 5%)\n        idx = np.argmin(np.abs(sig - (alpha * 100)))\n        critical_value = crit[idx]\n        is_norm = stat &lt; critical_value\n        return {\n            \"result\": bool(is_norm),\n            \"statistic\": float(stat),\n            \"critical_value\": float(critical_value),\n            \"significance_level\": float(sig[idx]),\n            \"alpha\": float(alpha),\n        }\n\n    # For others, they return (statistic, pvalue) or similar\n    if hasattr(res, \"pvalue\"):\n        p_val = res.pvalue\n        stat = res.statistic\n    elif isinstance(res, (tuple, list)):\n        stat, p_val = res[0], res[1]\n    else:\n        # Fallback\n        stat = res\n        p_val = None\n\n    is_norm = p_val &gt;= alpha if p_val is not None else False\n\n    return {\n        \"result\": bool(is_norm),\n        \"statistic\": float(stat) if isinstance(stat, (float, int)) else stat,\n        \"p_value\": float(p_val) if p_val is not None else None,\n        \"alpha\": float(alpha),\n    }\n</code></pre>"},{"location":"code/normality/#ts_stat_tests.algorithms.normality","title":"ts_stat_tests.algorithms.normality","text":"<p>Summary</p> <p>This module provides implementations of various statistical tests to assess the normality of data distributions. These tests are essential in statistical analysis and time series forecasting, as many models assume that the underlying data follows a normal distribution.</p>"},{"location":"code/normality/#ts_stat_tests.algorithms.normality.jb","title":"jb","text":"<pre><code>jb(x: ArrayLike, axis: int = 0) -&gt; tuple[\n    Union[float, ArrayLike],\n    Union[float, ArrayLike],\n    Union[float, ArrayLike],\n    Union[float, ArrayLike],\n]\n</code></pre> <p>Summary</p> <p>The Jarque-Bera test is a statistical test used to determine whether a dataset follows a normal distribution. In time series forecasting, the test can be used to evaluate whether the residuals of a model follow a normal distribution, which is an assumption of many time series forecasting models.</p> Details <p>To apply the Jarque-Bera test to time series data, we first need to estimate the residuals of the forecasting model. The residuals represent the difference between the actual values of the time series and the values predicted by the model. We can then use the Jarque-Bera test to evaluate whether the residuals follow a normal distribution.</p> <p>The Jarque-Bera test is based on two statistics, skewness and kurtosis, which measure the degree of asymmetry and peakedness in the distribution of the residuals. The test compares the observed skewness and kurtosis of the residuals to the expected values for a normal distribution. If the observed values are significantly different from the expected values, the test rejects the null hypothesis that the residuals follow a normal distribution.</p> <p>In practice, we can use statistical software to perform the Jarque-Bera test on the residuals of a time series forecasting model. If the test indicates that the residuals do not follow a normal distribution, we may need to consider modifying the forecasting model or using a different modeling approach.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>Data to test for normality. Usually regression model residuals that are mean 0.</p> required <code>axis</code> <code>int</code> <p>Axis to use if data has more than 1 dimension. Defaults to <code>0</code>.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>JB</code> <code>Union[float, ArrayLike]</code> <p>The Jarque-Bera test statistic.</p> <code>JBpv</code> <code>Union[float, ArrayLike]</code> <p>The pvalue of the test statistic.</p> <code>skew</code> <code>Union[float, ArrayLike]</code> <p>Estimated skewness of the data.</p> <code>kurtosis</code> <code>Union[float, ArrayLike]</code> <p>Estimated kurtosis of the data.</p> Examples <p>Example one, using the <code>airline</code> data from the <code>sktime</code> package.</p> Python<pre><code>&gt;&gt;&gt; # Import packages\n&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; from statsmodels.stats.stattools import jarque_bera\n\n&gt;&gt;&gt; # Load the airline dataset\n&gt;&gt;&gt; y = load_airline()\n\n&gt;&gt;&gt; # Apply Jarque-Bera test\n&gt;&gt;&gt; jb_value, p_value, skewness, kurtosis = jarque_bera(y)\n\n&gt;&gt;&gt; # Print the results\n&gt;&gt;&gt; print(\"Jarque-Bera test statistic:\", jb_value)\nJarque-Bera test statistic: 4.588031669436549\n&gt;&gt;&gt; print(\"p-value:\", p_value)\np-value: 0.10134805179561781\n\n&gt;&gt;&gt; # Check the test\n&gt;&gt;&gt; if p_value &lt; 0.05:\n...     print(\"Reject the null hypothesis that the data is normally distributed\")\n... else:\n...     print(\"Cannot reject the null hypothesis that the data is normally distributed\")\n...\nCannot reject the null hypothesis that the data is normally distributed\n</code></pre> <p>In this example, the p-value is greater than <code>0.05</code>, indicating that we cannot reject the null hypothesis that the data is normally distributed. Therefore, we can assume that the airline data does follow a normal distribution.</p> <p>Example two, using the <code>sine</code> wave data generated from the <code>numpy</code> package.</p> Python<pre><code>&gt;&gt;&gt; # Import packages\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statsmodels.stats.stattools import jarque_bera\n\n&gt;&gt;&gt; # Generate sine wave data\n&gt;&gt;&gt; t = np.linspace(0, 10, 100)\n&gt;&gt;&gt; y = np.sin(t)\n\n&gt;&gt;&gt; # Apply Jarque-Bera test\n&gt;&gt;&gt; jb_value, p_value, skewness, kurtosis = jarque_bera(y)\n\n&gt;&gt;&gt; # Print the results\n&gt;&gt;&gt; print(\"Jarque-Bera test statistic:\", jb_value)\nJarque-Bera test statistic: 15.830310292715973\n&gt;&gt;&gt; print(\"p-value:\", p_value)\np-value: 0.00036833142556487206\n\n&gt;&gt;&gt; if p_value &lt; 0.05:\n...     print(\"Reject the null hypothesis that the data is normally distributed\")\n... else:\n...     print(\"Cannot reject the null hypothesis that the data is normally distributed\")\n...\nReject the null hypothesis that the data is normally distributed\n</code></pre> <p>In this example, the p-value is less than <code>0.05</code>, indicating that we can reject the null hypothesis that the data is normally distributed. Therefore, we can assume that the sine wave data does not follow a normal distribution.</p> <p>Example three, using the <code>FractionalGaussianNoise</code> random data generated from the <code>stochastic</code> package.</p> Python<pre><code>&gt;&gt;&gt; # Import packages\n&gt;&gt;&gt; from stochastic.noise import FractionalGaussianNoise\n&gt;&gt;&gt; from statsmodels.stats.stattools import jarque_bera\n\n&gt;&gt;&gt; # Generate Fractional Gaussian Noise\n&gt;&gt;&gt; fgn = FractionalGaussianNoise(t=1, hurst=0.5, length=100, method=\"daviesharte\")\n&gt;&gt;&gt; noise = fgn.sample()\n\n&gt;&gt;&gt; # Apply Jarque-Bera test\n&gt;&gt;&gt; jb_value, p_value, skewness, kurtosis = jarque_bera(y)\n\n&gt;&gt;&gt; # Print the results\n&gt;&gt;&gt; print(\"Jarque-Bera test statistic:\", jb_value)\nJarque-Bera test statistic: 8.94626982252318\n&gt;&gt;&gt; print(\"p-value:\", p_value)\np-value: 0.011411891515478784\n\n&gt;&gt;&gt; if p_value &lt; 0.05:\n...     print(\"Reject the null hypothesis that the data is normally distributed\")\n... else:\n...     print(\"Cannot reject the null hypothesis that the data is normally distributed\")\n...\nReject the null hypothesis that the data is normally distributed\n</code></pre> <p>In this example, the p-value is less than <code>0.05</code>, indicating that we can reject the null hypothesis that the data is normally distributed. Therefore, we can assume that the random noise generated by <code>FractionalGaussianNoise</code> does not follow a normal distribution.</p> Notes <p>Each output returned has 1 dimension fewer than data.</p> <p>The Jarque-Bera test statistic tests the null that the data is normally distributed against an alternative that the data follow some other distribution. The test statistic is based on two moments of the data, the skewness, and the kurtosis, and has an asymptotic \\(x_2^2\\) distribution.</p> <p>The test statistic is defined as:</p> \\[ JB = n \\left( \\frac{S^2}{6} + \\frac{(K-3)^2}{24} \\right) \\] <p>where:</p> <ul> <li>\\(n\\) is the number of data points,</li> <li>\\(S\\) is the sample skewness, and</li> <li>\\(K\\) is the sample kurtosis of the data.</li> </ul> Credit <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> References <ul> <li>Jarque, C. and Bera, A. (1980) \"Efficient tests for normality, homoscedasticity and serial independence of regression residuals\", 6 Econometric Letters 255-259.</li> </ul> See Also <ul> <li><code>ob()</code></li> <li><code>sw()</code></li> <li><code>dp()</code></li> <li><code>ad()</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/normality.py</code> <pre><code>@typechecked\ndef jb(\n    x: ArrayLike,\n    axis: int = 0,\n) -&gt; tuple[\n    Union[float, ArrayLike],\n    Union[float, ArrayLike],\n    Union[float, ArrayLike],\n    Union[float, ArrayLike],\n]:\n    \"\"\"\n    !!! note \"Summary\"\n        The Jarque-Bera test is a statistical test used to determine whether a dataset follows a normal distribution. In time series forecasting, the test can be used to evaluate whether the residuals of a model follow a normal distribution, which is an assumption of many time series forecasting models.\n\n    ???+ abstract \"Details\"\n        To apply the Jarque-Bera test to time series data, we first need to estimate the residuals of the forecasting model. The residuals represent the difference between the actual values of the time series and the values predicted by the model. We can then use the Jarque-Bera test to evaluate whether the residuals follow a normal distribution.\n\n        The Jarque-Bera test is based on two statistics, skewness and kurtosis, which measure the degree of asymmetry and peakedness in the distribution of the residuals. The test compares the observed skewness and kurtosis of the residuals to the expected values for a normal distribution. If the observed values are significantly different from the expected values, the test rejects the null hypothesis that the residuals follow a normal distribution.\n\n        In practice, we can use statistical software to perform the Jarque-Bera test on the residuals of a time series forecasting model. If the test indicates that the residuals do not follow a normal distribution, we may need to consider modifying the forecasting model or using a different modeling approach.\n\n    Params:\n        x (ArrayLike):\n            Data to test for normality. Usually regression model residuals that are mean 0.\n        axis (int):\n            Axis to use if data has more than 1 dimension.&lt;br&gt;\n            Defaults to `0`.\n\n    Returns:\n        JB (Union[float, ArrayLike]):\n            The Jarque-Bera test statistic.\n        JBpv (Union[float, ArrayLike]):\n            The pvalue of the test statistic.\n        skew (Union[float, ArrayLike]):\n            Estimated skewness of the data.\n        kurtosis (Union[float, ArrayLike]):\n            Estimated kurtosis of the data.\n\n    ???+ example \"Examples\"\n\n        Example one, using the `airline` data from the `sktime` package.\n\n        ```pycon {.py .python linenums=\"1\" title=\"Python\"}\n        &gt;&gt;&gt; # Import packages\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; from statsmodels.stats.stattools import jarque_bera\n\n        &gt;&gt;&gt; # Load the airline dataset\n        &gt;&gt;&gt; y = load_airline()\n\n        &gt;&gt;&gt; # Apply Jarque-Bera test\n        &gt;&gt;&gt; jb_value, p_value, skewness, kurtosis = jarque_bera(y)\n\n        &gt;&gt;&gt; # Print the results\n        &gt;&gt;&gt; print(\"Jarque-Bera test statistic:\", jb_value)\n        Jarque-Bera test statistic: 4.588031669436549\n        &gt;&gt;&gt; print(\"p-value:\", p_value)\n        p-value: 0.10134805179561781\n\n        &gt;&gt;&gt; # Check the test\n        &gt;&gt;&gt; if p_value &lt; 0.05:\n        ...     print(\"Reject the null hypothesis that the data is normally distributed\")\n        ... else:\n        ...     print(\"Cannot reject the null hypothesis that the data is normally distributed\")\n        ...\n        Cannot reject the null hypothesis that the data is normally distributed\n        ```\n\n        In this example, the p-value is **greater** than `0.05`, indicating that we _cannot_ reject the null hypothesis that the data _is_ normally distributed. Therefore, we can assume that the airline data **does** follow a normal distribution.\n\n        ---\n\n        Example two, using the `sine` wave data generated from the `numpy` package.\n\n        ```pycon {.py .python linenums=\"1\" title=\"Python\"}\n        &gt;&gt;&gt; # Import packages\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from statsmodels.stats.stattools import jarque_bera\n\n        &gt;&gt;&gt; # Generate sine wave data\n        &gt;&gt;&gt; t = np.linspace(0, 10, 100)\n        &gt;&gt;&gt; y = np.sin(t)\n\n        &gt;&gt;&gt; # Apply Jarque-Bera test\n        &gt;&gt;&gt; jb_value, p_value, skewness, kurtosis = jarque_bera(y)\n\n        &gt;&gt;&gt; # Print the results\n        &gt;&gt;&gt; print(\"Jarque-Bera test statistic:\", jb_value)\n        Jarque-Bera test statistic: 15.830310292715973\n        &gt;&gt;&gt; print(\"p-value:\", p_value)\n        p-value: 0.00036833142556487206\n\n        &gt;&gt;&gt; if p_value &lt; 0.05:\n        ...     print(\"Reject the null hypothesis that the data is normally distributed\")\n        ... else:\n        ...     print(\"Cannot reject the null hypothesis that the data is normally distributed\")\n        ...\n        Reject the null hypothesis that the data is normally distributed\n        ```\n\n        In this example, the p-value is **less** than `0.05`, indicating that we _can_ reject the null hypothesis that the data _is_ normally distributed. Therefore, we can assume that the sine wave data does **not** follow a normal distribution.\n\n        ---\n\n        Example three, using the `FractionalGaussianNoise` random data generated from the `stochastic` package.\n\n        ```pycon {.py .python linenums=\"1\" title=\"Python\"}\n        &gt;&gt;&gt; # Import packages\n        &gt;&gt;&gt; from stochastic.noise import FractionalGaussianNoise\n        &gt;&gt;&gt; from statsmodels.stats.stattools import jarque_bera\n\n        &gt;&gt;&gt; # Generate Fractional Gaussian Noise\n        &gt;&gt;&gt; fgn = FractionalGaussianNoise(t=1, hurst=0.5, length=100, method=\"daviesharte\")\n        &gt;&gt;&gt; noise = fgn.sample()\n\n        &gt;&gt;&gt; # Apply Jarque-Bera test\n        &gt;&gt;&gt; jb_value, p_value, skewness, kurtosis = jarque_bera(y)\n\n        &gt;&gt;&gt; # Print the results\n        &gt;&gt;&gt; print(\"Jarque-Bera test statistic:\", jb_value)\n        Jarque-Bera test statistic: 8.94626982252318\n        &gt;&gt;&gt; print(\"p-value:\", p_value)\n        p-value: 0.011411891515478784\n\n        &gt;&gt;&gt; if p_value &lt; 0.05:\n        ...     print(\"Reject the null hypothesis that the data is normally distributed\")\n        ... else:\n        ...     print(\"Cannot reject the null hypothesis that the data is normally distributed\")\n        ...\n        Reject the null hypothesis that the data is normally distributed\n        ```\n\n        In this example, the p-value is **less** than `0.05`, indicating that we _can_ reject the null hypothesis that the data _is_ normally distributed. Therefore, we can assume that the random noise generated by `FractionalGaussianNoise` does **not** follow a normal distribution.\n\n    ??? note \"Notes\"\n        Each output returned has 1 dimension fewer than data.\n\n        The Jarque-Bera test statistic tests the null that the data is normally distributed against an alternative that the data follow some other distribution. The test statistic is based on two moments of the data, the skewness, and the kurtosis, and has an asymptotic $x_2^2$ distribution.\n\n        The test statistic is defined as:\n\n        $$\n        JB = n \\\\left( \\\\frac{S^2}{6} + \\\\frac{(K-3)^2}{24} \\\\right)\n        $$\n\n        where:\n\n        - $n$ is the number of data points,\n        - $S$ is the sample skewness, and\n        - $K$ is the sample kurtosis of the data.\n\n    ??? success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ??? question \"References\"\n        - Jarque, C. and Bera, A. (1980) \"Efficient tests for normality, homoscedasticity and serial independence of regression residuals\", 6 Econometric Letters 255-259.\n\n    ??? tip \"See Also\"\n        - [`ob()`][ts_stat_tests.algorithms.normality.ob]\n        - [`sw()`][ts_stat_tests.algorithms.normality.sw]\n        - [`dp()`][ts_stat_tests.algorithms.normality.dp]\n        - [`ad()`][ts_stat_tests.algorithms.normality.ad]\n    \"\"\"\n    return _jb(resids=x, axis=axis)\n</code></pre>"},{"location":"code/normality/#ts_stat_tests.algorithms.normality.ob","title":"ob","text":"<pre><code>ob(\n    x: ArrayLike, axis: int = 0\n) -&gt; Union[tuple[float, float], NormaltestResult]\n</code></pre> <p>Summary</p> <p>The Omnibus test is a statistical test used to evaluate the normality of a dataset, including time series data. In time series forecasting, the Omnibus test can be used to assess whether the residuals of a model follow a normal distribution, which is an important assumption for many statistical models.</p> Details <p>The Omnibus test uses a combination of skewness and kurtosis measures to assess whether the residuals follow a normal distribution. Skewness measures the degree of asymmetry in the distribution of the residuals, while kurtosis measures the degree of peakedness or flatness. If the residuals follow a normal distribution, their skewness and kurtosis should be close to zero.</p> <p>To apply the Omnibus test to time series data, we first need to estimate the residuals of the forecasting model. We can then use a statistical software package to perform the Omnibus test on the residuals. The test produces a single p-value, which indicates the probability of observing the observed skewness and kurtosis values if the residuals follow a normal distribution. If the p-value is greater than the significance level (usually 0.05), we can conclude that the residuals follow a normal distribution.</p> <p>If the Omnibus test indicates that the residuals do not follow a normal distribution, we may need to consider using a different modeling approach or modifying the forecasting model. It is important to ensure that the residuals of a time series forecasting model follow a normal distribution to ensure that the model is valid and reliable for making predictions.</p> <p>The Omnibus test for normality is a statistical test used to evaluate whether a dataset, including time series data, follows a normal distribution. The mathematical equation for the Omnibus test is:</p> \\[ O = N \\times (b_1^2 + b_2^2) \\] <p>where:</p> <ul> <li>\\(O\\) is the Omnibus test statistic</li> <li>\\(N\\) is the sample size</li> <li>\\(b1\\) and \\(b2\\) are the coefficients of the first two terms of a third-order polynomial fit to the data</li> </ul> <p>To calculate the Omnibus test statistic for time series data, we need to perform the following steps:</p> <ol> <li> <p>Estimate the residuals of the forecasting model: The residuals are the difference between the actual values and the predicted values of the time series model.</p> </li> <li> <p>Calculate the sample mean and standard deviation of the residuals: These are the mean and standard deviation of the residuals, respectively.</p> </li> <li> <p>Calculate the skewness and kurtosis of the residuals: These are measures of the asymmetry and peakedness of the distribution of the residuals, respectively.</p> </li> <li> <p>Fit a third-order polynomial to the standardized residuals: The standardized residuals are the residuals divided by their sample standard deviation. The third-order polynomial has the form:</p> \\[ z = b_0 + (b_1 \\times x) + (b_2 \\times x^2) + (b_3 \\times x^3) \\] <p>where:</p> <ul> <li>\\(z\\) is the standardized residual,</li> <li>\\(x\\) is the normal deviate (i.e., the value that would be expected if the data followed a normal distribution), and</li> <li>\\(b_0\\), \\(b_1\\), \\(b_2\\), and \\(b_3\\) are the coefficients of the polynomial fit.</li> </ul> </li> <li> <p>Calculate the values of \\(b_1\\) and \\(b_2\\): These are the coefficients of the first two terms of the polynomial fit.</p> </li> <li> <p>Substitute the values for sample size, \\(b_1\\), and \\(b_2\\) into the Omnibus formula: The formula calculates a single test statistic, which is the Omnibus value.</p> </li> <li> <p>Compare the Omnibus value to a critical value from a chi-squared distribution with 2 degrees of freedom: If the Omnibus value is greater than the critical value, we can reject the null hypothesis of normality and conclude that the residuals do not follow a normal distribution. If the Omnibus value is less than the critical value, we cannot reject the null hypothesis of normality and can conclude that the residuals follow a normal distribution.</p> </li> </ol> <p>In summary, the Omnibus test is a statistical test that evaluates normality of time series residuals using a third-order polynomial fit to the standardized residuals. It calculates a single test statistic using the coefficients of the polynomial fit and compares it to a critical value to determine whether the residuals follow a normal distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>Data to test for normality. Usually regression model residuals that are mean 0.</p> required <code>axis</code> <code>int</code> <p>Axis to use if data has more than 1 dimension. Defaults to <code>0</code>.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>statistic</code> <code>Union[float, ndarray]</code> <p>The Omnibus test statistic.</p> <code>pvalue</code> <code>Union[float, ndarray]</code> <p>The p-value for the hypothesis test.</p> Examples <p>Example one, using the <code>airline</code> data from the <code>sktime</code> package.</p> Python<pre><code>&gt;&gt;&gt; # Import packages\n&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; from statsmodels.stats.stattools import omni_normtest\n\n&gt;&gt;&gt; # load the airline dataset\n&gt;&gt;&gt; airline = load_airline()\n\n&gt;&gt;&gt; # run the Omnibus test on the dataset\n&gt;&gt;&gt; statistic, p_value = omni_normtest(airline)\n\n&gt;&gt;&gt; # print the results\n&gt;&gt;&gt; print(f\"Omnibus test statistic: {statistic:.3f}\")\nOmnibus test statistic: 1.753\n&gt;&gt;&gt; print(f\"Omnibus test p-value: {p_value:.3f}\")\nOmnibus test p-value: 0.416\n\n&gt;&gt;&gt; # check if null hypothesis is rejected\n&gt;&gt;&gt; alpha = 0.05\n&gt;&gt;&gt; if p_value &lt; alpha:\n...     print(\"Reject null hypothesis that data is normally distributed\")\n... else:\n...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n...\nFail to reject null hypothesis that data is normally distributed\n</code></pre> <p>The null hypothesis of the Omnibus test is that the data is normally distributed. In this case, the p-value is <code>0.416</code>, which is greater than the significance level of <code>0.05</code>, indicating that we fail to reject the null hypothesis. Therefore, we can conclude that the Airline dataset is likely normally distributed.</p> <p>Example two, using the <code>sine</code> wave data generated from the <code>numpy</code> package.</p> Python<pre><code>&gt;&gt;&gt; # Import packages\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statsmodels.stats.stattools import omni_normtest\n\n&gt;&gt;&gt; # generate sine wave data\n&gt;&gt;&gt; data = np.sin(np.linspace(0, 2 * np.pi, num=100))\n\n&gt;&gt;&gt; # run the Omnibus test on the data\n&gt;&gt;&gt; statistic, p_value = omni_normtest(data)\n\n&gt;&gt;&gt; # print the results\n&gt;&gt;&gt; print(f\"Omnibus test statistic: {statistic:.3f}\")\nOmnibus test statistic: 24.750\n&gt;&gt;&gt; print(f\"Omnibus test p-value: {p_value:.3f}\")\nOmnibus test p-value: 4.326e-06\n\n&gt;&gt;&gt; # check if null hypothesis is rejected\n&gt;&gt;&gt; alpha = 0.05\n&gt;&gt;&gt; if p_value &lt; alpha:\n...     print(\"Reject null hypothesis that data is normally distributed\")\n... else:\n...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n...\nReject null hypothesis that data is normally distributed\n</code></pre> <p>The null hypothesis of the Omnibus test is that the data is normally distributed. In this case, the p-value is <code>4.326e-06</code>, which is much smaller than the significance level of <code>0.05</code>, indicating strong evidence to reject the null hypothesis. Therefore, we can conclude that the sine wave data is not normally distributed.</p> <p>Example three, using the <code>FractionalGaussianNoise</code> random data generated from the <code>stochastic</code> package.</p> Python<pre><code>&gt;&gt;&gt; # Import packages\n&gt;&gt;&gt; from stochastic.noise import FractionalGaussianNoise\n&gt;&gt;&gt; from statsmodels.stats.stattools import omni_normtest\n\n&gt;&gt;&gt; # Generate Fractional Gaussian Noise\n&gt;&gt;&gt; fgn = FractionalGaussianNoise(t=1, hurst=0.5, length=1000, method=\"daviesharte\")\n&gt;&gt;&gt; noise = fgn.sample()\n\n&gt;&gt;&gt; # run the Omnibus test on the data\n&gt;&gt;&gt; statistic, p_value = omni_normtest(noise)\n\n&gt;&gt;&gt; # print the results\n&gt;&gt;&gt; print(f\"Omnibus test statistic: {statistic:.3f}\")\nOmnibus test statistic: 4.717\n&gt;&gt;&gt; print(f\"Omnibus test p-value: {p_value:.3f}\")\nOmnibus test p-value: 0.094\n\n&gt;&gt;&gt; # check if null hypothesis is rejected\n&gt;&gt;&gt; alpha = 0.05\n&gt;&gt;&gt; if p_value &lt; alpha:\n...     print(\"Reject null hypothesis that data is normally distributed\")\n... else:\n...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n...\nFail to reject null hypothesis that data is normally distributed\n</code></pre> <p>The null hypothesis of the Omnibus test is that the data is normally distributed. In this case, the p-value is <code>0.094</code>, which is greater than the significance level of <code>0.05</code>, indicating that we fail to reject the null hypothesis. Therefore, we can conclude that the random noise generated by the <code>FractionalGaussianNoise</code> class is likely normally distributed.</p> Notes <p>The Omnibus test statistic tests the null that the data is normally distributed against an alternative that the data follow some other distribution. It is based on D'Agostino's \\(K^2\\) test statistic.</p> Credit <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> References <ul> <li>D'Agostino, R. B. and Pearson, E. S. (1973), \"Tests for departure from normality,\" Biometrika, 60, 613-622.</li> <li>D'Agostino, R. B. and Stephens, M. A. (1986), \"Goodness-of-fit techniques,\" New York: Marcel Dekker.</li> </ul> See Also <ul> <li><code>jb()</code></li> <li><code>sw()</code></li> <li><code>dp()</code></li> <li><code>ad()</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/normality.py</code> <pre><code>@typechecked\ndef ob(\n    x: ArrayLike,\n    axis: int = 0,\n) -&gt; Union[tuple[float, float], NormaltestResult]:\n    \"\"\"\n    !!! note \"Summary\"\n        The Omnibus test is a statistical test used to evaluate the normality of a dataset, including time series data. In time series forecasting, the Omnibus test can be used to assess whether the residuals of a model follow a normal distribution, which is an important assumption for many statistical models.\n\n    ???+ abstract \"Details\"\n        The Omnibus test uses a combination of skewness and kurtosis measures to assess whether the residuals follow a normal distribution. Skewness measures the degree of asymmetry in the distribution of the residuals, while kurtosis measures the degree of peakedness or flatness. If the residuals follow a normal distribution, their skewness and kurtosis should be close to zero.\n\n        To apply the Omnibus test to time series data, we first need to estimate the residuals of the forecasting model. We can then use a statistical software package to perform the Omnibus test on the residuals. The test produces a single p-value, which indicates the probability of observing the observed skewness and kurtosis values if the residuals follow a normal distribution. If the p-value is greater than the significance level (usually 0.05), we can conclude that the residuals follow a normal distribution.\n\n        If the Omnibus test indicates that the residuals do not follow a normal distribution, we may need to consider using a different modeling approach or modifying the forecasting model. It is important to ensure that the residuals of a time series forecasting model follow a normal distribution to ensure that the model is valid and reliable for making predictions.\n\n        The Omnibus test for normality is a statistical test used to evaluate whether a dataset, including time series data, follows a normal distribution. The mathematical equation for the Omnibus test is:\n\n        $$\n        O = N \\\\times (b_1^2 + b_2^2)\n        $$\n\n        where:\n\n        - $O$ is the Omnibus test statistic\n        - $N$ is the sample size\n        - $b1$ and $b2$ are the coefficients of the first two terms of a third-order polynomial fit to the data\n\n        To calculate the Omnibus test statistic for time series data, we need to perform the following steps:\n\n        1. Estimate the residuals of the forecasting model: The residuals are the difference between the actual values and the predicted values of the time series model.\n\n        1. Calculate the sample mean and standard deviation of the residuals: These are the mean and standard deviation of the residuals, respectively.\n\n        1. Calculate the skewness and kurtosis of the residuals: These are measures of the asymmetry and peakedness of the distribution of the residuals, respectively.\n\n        1. Fit a third-order polynomial to the standardized residuals: The standardized residuals are the residuals divided by their sample standard deviation. The third-order polynomial has the form:\n\n            $$\n            z = b_0 + (b_1 \\\\times x) + (b_2 \\\\times x^2) + (b_3 \\\\times x^3)\n            $$\n\n            where:\n\n            - $z$ is the standardized residual,\n            - $x$ is the normal deviate (i.e., the value that would be expected if the data followed a normal distribution), and\n            - $b_0$, $b_1$, $b_2$, and $b_3$ are the coefficients of the polynomial fit.\n\n        1. Calculate the values of $b_1$ and $b_2$: These are the coefficients of the first two terms of the polynomial fit.\n\n        1. Substitute the values for sample size, $b_1$, and $b_2$ into the Omnibus formula: The formula calculates a single test statistic, which is the Omnibus value.\n\n        1. Compare the Omnibus value to a critical value from a chi-squared distribution with 2 degrees of freedom: If the Omnibus value is greater than the critical value, we can reject the null hypothesis of normality and conclude that the residuals do not follow a normal distribution. If the Omnibus value is less than the critical value, we cannot reject the null hypothesis of normality and can conclude that the residuals follow a normal distribution.\n\n        In summary, the Omnibus test is a statistical test that evaluates normality of time series residuals using a third-order polynomial fit to the standardized residuals. It calculates a single test statistic using the coefficients of the polynomial fit and compares it to a critical value to determine whether the residuals follow a normal distribution.\n\n    Params:\n        x (ArrayLike):\n            Data to test for normality. Usually regression model residuals that are mean 0.\n        axis (int):\n            Axis to use if data has more than 1 dimension.&lt;br&gt;\n            Defaults to `0`.\n\n    Returns:\n        statistic (Union[float, np.ndarray]):\n            The Omnibus test statistic.\n        pvalue (Union[float, np.ndarray]):\n            The p-value for the hypothesis test.\n\n    ???+ example \"Examples\"\n        Example one, using the `airline` data from the `sktime` package.\n\n        ```pycon {.py .python linenums=\"1\" title=\"Python\"}\n        &gt;&gt;&gt; # Import packages\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; from statsmodels.stats.stattools import omni_normtest\n\n        &gt;&gt;&gt; # load the airline dataset\n        &gt;&gt;&gt; airline = load_airline()\n\n        &gt;&gt;&gt; # run the Omnibus test on the dataset\n        &gt;&gt;&gt; statistic, p_value = omni_normtest(airline)\n\n        &gt;&gt;&gt; # print the results\n        &gt;&gt;&gt; print(f\"Omnibus test statistic: {statistic:.3f}\")\n        Omnibus test statistic: 1.753\n        &gt;&gt;&gt; print(f\"Omnibus test p-value: {p_value:.3f}\")\n        Omnibus test p-value: 0.416\n\n        &gt;&gt;&gt; # check if null hypothesis is rejected\n        &gt;&gt;&gt; alpha = 0.05\n        &gt;&gt;&gt; if p_value &lt; alpha:\n        ...     print(\"Reject null hypothesis that data is normally distributed\")\n        ... else:\n        ...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n        ...\n        Fail to reject null hypothesis that data is normally distributed\n        ```\n\n        The null hypothesis of the Omnibus test is that the data _is_ normally distributed. In this case, the p-value is `0.416`, which is **greater** than the significance level of `0.05`, indicating that we _fail_ to reject the null hypothesis. Therefore, we can conclude that the Airline dataset **is** likely normally distributed.\n\n        ---\n\n        Example two, using the `sine` wave data generated from the `numpy` package.\n\n        ```pycon {.py .python linenums=\"1\" title=\"Python\"}\n        &gt;&gt;&gt; # Import packages\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from statsmodels.stats.stattools import omni_normtest\n\n        &gt;&gt;&gt; # generate sine wave data\n        &gt;&gt;&gt; data = np.sin(np.linspace(0, 2 * np.pi, num=100))\n\n        &gt;&gt;&gt; # run the Omnibus test on the data\n        &gt;&gt;&gt; statistic, p_value = omni_normtest(data)\n\n        &gt;&gt;&gt; # print the results\n        &gt;&gt;&gt; print(f\"Omnibus test statistic: {statistic:.3f}\")\n        Omnibus test statistic: 24.750\n        &gt;&gt;&gt; print(f\"Omnibus test p-value: {p_value:.3f}\")\n        Omnibus test p-value: 4.326e-06\n\n        &gt;&gt;&gt; # check if null hypothesis is rejected\n        &gt;&gt;&gt; alpha = 0.05\n        &gt;&gt;&gt; if p_value &lt; alpha:\n        ...     print(\"Reject null hypothesis that data is normally distributed\")\n        ... else:\n        ...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n        ...\n        Reject null hypothesis that data is normally distributed\n        ```\n\n        The null hypothesis of the Omnibus test _is_ that the data is normally distributed. In this case, the p-value is `4.326e-06`, which is much **smaller** than the significance level of `0.05`, indicating strong evidence to _reject_ the null hypothesis. Therefore, we can conclude that the sine wave data is **not** normally distributed.\n\n        ---\n\n        Example three, using the `FractionalGaussianNoise` random data generated from the `stochastic` package.\n\n        ```pycon {.py .python linenums=\"1\" title=\"Python\"}\n        &gt;&gt;&gt; # Import packages\n        &gt;&gt;&gt; from stochastic.noise import FractionalGaussianNoise\n        &gt;&gt;&gt; from statsmodels.stats.stattools import omni_normtest\n\n        &gt;&gt;&gt; # Generate Fractional Gaussian Noise\n        &gt;&gt;&gt; fgn = FractionalGaussianNoise(t=1, hurst=0.5, length=1000, method=\"daviesharte\")\n        &gt;&gt;&gt; noise = fgn.sample()\n\n        &gt;&gt;&gt; # run the Omnibus test on the data\n        &gt;&gt;&gt; statistic, p_value = omni_normtest(noise)\n\n        &gt;&gt;&gt; # print the results\n        &gt;&gt;&gt; print(f\"Omnibus test statistic: {statistic:.3f}\")\n        Omnibus test statistic: 4.717\n        &gt;&gt;&gt; print(f\"Omnibus test p-value: {p_value:.3f}\")\n        Omnibus test p-value: 0.094\n\n        &gt;&gt;&gt; # check if null hypothesis is rejected\n        &gt;&gt;&gt; alpha = 0.05\n        &gt;&gt;&gt; if p_value &lt; alpha:\n        ...     print(\"Reject null hypothesis that data is normally distributed\")\n        ... else:\n        ...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n        ...\n        Fail to reject null hypothesis that data is normally distributed\n        ```\n\n        The null hypothesis of the Omnibus test _is_ that the data is normally distributed. In this case, the p-value is `0.094`, which is **greater** than the significance level of `0.05`, indicating that we _fail_ to reject the null hypothesis. Therefore, we can conclude that the random noise generated by the `FractionalGaussianNoise` class **is** likely normally distributed.\n\n    ??? note \"Notes\"\n        The Omnibus test statistic tests the null that the data is normally distributed against an alternative that the data follow some other distribution. It is based on D'Agostino's $K^2$ test statistic.\n\n    ??? success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ??? question \"References\"\n        - D'Agostino, R. B. and Pearson, E. S. (1973), \"Tests for departure from normality,\" Biometrika, 60, 613-622.\n        - D'Agostino, R. B. and Stephens, M. A. (1986), \"Goodness-of-fit techniques,\" New York: Marcel Dekker.\n\n    ??? tip \"See Also\"\n        - [`jb()`][ts_stat_tests.algorithms.normality.jb]\n        - [`sw()`][ts_stat_tests.algorithms.normality.sw]\n        - [`dp()`][ts_stat_tests.algorithms.normality.dp]\n        - [`ad()`][ts_stat_tests.algorithms.normality.ad]\n    \"\"\"\n    return _ob(resids=x, axis=axis)\n</code></pre>"},{"location":"code/normality/#ts_stat_tests.algorithms.normality.sw","title":"sw","text":"<pre><code>sw(\n    x: ArrayLike,\n) -&gt; Union[tuple[float, float], ShapiroResult]\n</code></pre> <p>Summary</p> <p>The Shapiro-Wilk test is a statistical test used to determine whether a dataset, including time series data, follows a normal distribution. In time series forecasting, the Shapiro-Wilk test can be used to evaluate whether the residuals of a model follow a normal distribution, which is an assumption of many statistical models.</p> Details <p>The Shapiro-Wilk test is based on the null hypothesis that the residuals of the forecasting model are normally distributed. The test calculates a test statistic that compares the observed distribution of the residuals to the expected distribution under the null hypothesis of normality. If the observed distribution of the residuals deviates significantly from the expected distribution under normality, the test rejects the null hypothesis and concludes that the residuals do not follow a normal distribution.</p> <p>To apply the Shapiro-Wilk test to time series data, we first need to estimate the residuals of the forecasting model. We can then use a statistical software package to perform the Shapiro-Wilk test on the residuals. The test produces a p-value, which indicates the probability of observing the observed distribution of the residuals if the null hypothesis of normality is true. If the p-value is less than the significance level (usually 0.05), we can conclude that the residuals do not follow a normal distribution.</p> <p>If the Shapiro-Wilk test indicates that the residuals do not follow a normal distribution, we may need to consider using a different modeling approach or modifying the forecasting model. It is important to ensure that the residuals of a time series forecasting model follow a normal distribution to ensure that the model is valid and reliable for making predictions.</p> <p>The Shapiro-Wilk test is a statistical test used to evaluate whether a dataset, including time series data, follows a normal distribution. The mathematical equation for the Shapiro-Wilk test is:</p> \\[ W = \\frac { \\left( \\sum_{i=1}^n (a_i \\times z_i) \\right)^2 } { \\sum_{i=1}^n (x_i - \\bar{x})^2 } \\] <p>where:</p> <ul> <li>\\(W\\) is the test statistic</li> <li>\\(a_i\\) are the coefficients calculated from the ordered sample values</li> <li>\\(z_i\\) are the corresponding normal deviates for the ai coefficients</li> <li>\\(x_i\\) are the ordered sample values</li> <li>\\(\\bar{x}\\) is the sample mean</li> </ul> <p>To calculate the Shapiro-Wilk test statistic for time series data, we need to perform the following steps:</p> <ol> <li> <p>Estimate the residuals of the forecasting model: The residuals are the difference between the actual values and the predicted values of the time series model.</p> </li> <li> <p>Calculate the sample mean and standard deviation of the residuals: These are the mean and standard deviation of the residuals, respectively.</p> </li> <li> <p>Standardize the residuals: The standardized residuals are the residuals divided by their sample standard deviation.</p> </li> <li> <p>Order the standardized residuals from smallest to largest: This step ensures that the Shapiro-Wilk test is performed on a sample that is in ascending order.</p> </li> <li> <p>Calculate the \\(a_i\\) coefficients: The \\(a_i\\) coefficients are calculated from the ordered sample values using the formula:</p> \\[ a_i = \\frac { \\sum_{j=1}^{n} (a_{i_j} \\times x_j) }{ s^2 } \\] <p>where:</p> <ul> <li>\\(s^2\\) is the sample variance and</li> <li>\\(a_{i_j}\\) are constants that depend on the sample size and the order of the sample values. These constants are pre-calculated and available in statistical software packages.</li> </ul> </li> <li> <p>Calculate the \\(z_i\\) normal deviates: The \\(z_i\\) normal deviates are the corresponding values of the standard normal distribution for the ai coefficients. These values are pre-calculated and available in statistical software packages.</p> </li> <li> <p>Calculate the numerator of the test statistic: This is the sum of the product of the ai coefficients and the corresponding \\(z_i\\) normal deviates:</p> \\[ \\sum (a_i \\times z_i) \\] </li> <li> <p>Calculate the denominator of the test statistic: This is the sum of the squared differences between the ordered sample values and the sample mean:</p> \\[ \\sum (x_i - \\bar{x})^2 \\] </li> <li> <p>Calculate the test statistic: This is the ratio of the squared numerator to the denominator:</p> \\[ W = \\frac { (\\sum(a_i \\times z_i))^2 } { \\sum(x_i - \\bar{x})^2 } \\] </li> <li> <p>Compare the test statistic to a critical value: If the test statistic is less than the critical value, we cannot reject the null hypothesis of normality and can conclude that the residuals follow a normal distribution. If the test statistic is greater than the critical value, we reject the null hypothesis of normality and conclude that the residuals do not follow a normal distribution.</p> </li> </ol> <p>In summary, the Shapiro-Wilk test is a statistical test that evaluates normality of time series residuals by standardizing and ordering the residuals, calculating ai coefficients and corresponding \\(z_i\\) normal deviates, and computing the test statistic using a ratio of the squared sum of \\(ai \\times zi\\) to the sum of squared differences between the sample values and sample mean. Finally, we compare the test statistic to a critical value to determine whether the residuals follow a normal distribution or not.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>Array of sample data</p> required <p>Returns:</p> Name Type Description <code>statistic</code> <code>float</code> <p>The test statistic.</p> <code>pvalue</code> <code>float</code> <p>The p-value for the hypothesis test.</p> Examples <p>Test the null hypothesis that a random sample was drawn from a normal distribution.</p> From the `scipy` docs<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy import stats\n&gt;&gt;&gt; rng = np.random.default_rng()\n&gt;&gt;&gt; x = stats.norm.rvs(loc=5, scale=3, size=100, random_state=rng)\n&gt;&gt;&gt; shapiro_test = stats.shapiro(x)\n&gt;&gt;&gt; shapiro_test\nShapiroResult(statistic=0.9813305735588074, pvalue=0.16855233907699585)\n&gt;&gt;&gt; shapiro_test.statistic\n0.9813305735588074\n&gt;&gt;&gt; shapiro_test.pvalue\n0.16855233907699585\n</code></pre> <p>Example one, using the <code>airline</code> data from the <code>sktime</code> package.</p> Python<pre><code>&gt;&gt;&gt; # Import packages\n&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; from scipy.stats import shapiro\n\n&gt;&gt;&gt; # load the airline data\n&gt;&gt;&gt; data = load_airline()\n\n&gt;&gt;&gt; # run the Shapiro-Wilk test on the data\n&gt;&gt;&gt; statistic, p_value = shapiro(data)\n\n&gt;&gt;&gt; # print the results\n&gt;&gt;&gt; print(f\"Shapiro-Wilk test statistic: {statistic:.3f}\")\nShapiro-Wilk test statistic: 0.910\n&gt;&gt;&gt; print(f\"Shapiro-Wilk test p-value: {p_value:.3f}\")\nShapiro-Wilk test p-value: 0.054\n\n&gt;&gt;&gt; # check if null hypothesis is rejected\n&gt;&gt;&gt; alpha = 0.05\n&gt;&gt;&gt; if p_value &lt; alpha:\n...     print(\"Reject null hypothesis that data is normally distributed\")\n... else:\n...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n...\nFail to reject null hypothesis that data is normally distributed\n</code></pre> <p>The null hypothesis of the Shapiro-Wilk test is that the data is normally distributed. In this case, the p-value is <code>0.054</code>, which is greater than the significance level of <code>0.05</code>, indicating that we fail to reject the null hypothesis. Therefore, we can conclude that the airline data is likely normally distributed.</p> <p>Example two, using the <code>sine</code> wave data generated from the <code>numpy</code> package.</p> Python<pre><code>&gt;&gt;&gt; # Import packages\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.stats import shapiro\n\n&gt;&gt;&gt; # generate sine wave data\n&gt;&gt;&gt; x = np.linspace(0, 2 * np.pi, 100)\n&gt;&gt;&gt; data = np.sin(x)\n\n&gt;&gt;&gt; # run the Shapiro-Wilk test on the data\n&gt;&gt;&gt; statistic, p_value = shapiro(data)\n\n&gt;&gt;&gt; # print the results\n&gt;&gt;&gt; print(f\"Shapiro-Wilk test statistic: {statistic:.3f}\")\n&gt;&gt;&gt; print(f\"Shapiro-Wilk test p-value: {p_value:.3f}\")\n\n&gt;&gt;&gt; # check if null hypothesis is rejected\n&gt;&gt;&gt; alpha = 0.05\n&gt;&gt;&gt; if p_value &lt; alpha:\n...     print(\"Reject null hypothesis that data is normally distributed\")\n... else:\n...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n...\n</code></pre> <p>The null hypothesis of the Shapiro-Wilk test is that the data is normally distributed. In this case, the p-value is <code>0.002</code>, which is less than the significance level of <code>0.05</code>, indicating that we can reject the null hypothesis. Therefore, we can conclude that the sine wave data is not normally distributed.</p> <p>Example three, using the <code>FractionalGaussianNoise</code> random data generated from the <code>stochastic</code> package.</p> Python<pre><code>&gt;&gt;&gt; # Import packages\n&gt;&gt;&gt; from stochastic.noise import FractionalGaussianNoise\n&gt;&gt;&gt; from scipy.stats import shapiro\n\n&gt;&gt;&gt; # Generate Fractional Gaussian Noise\n&gt;&gt;&gt; fgn = FractionalGaussianNoise(t=1, hurst=0.5, length=100, method=\"daviesharte\")\n&gt;&gt;&gt; data = fgn.sample()\n\n&gt;&gt;&gt; # run the Shapiro-Wilk test on the data\n&gt;&gt;&gt; statistic, p_value = shapiro(data)\n\n&gt;&gt;&gt; # print the results\n&gt;&gt;&gt; print(f\"Shapiro-Wilk test statistic: {statistic:.3f}\")\nShapiro-Wilk test statistic: 0.979\n&gt;&gt;&gt; print(f\"Shapiro-Wilk test p-value: {p_value:.3f}\")\nShapiro-Wilk test p-value: 0.417\n\n&gt;&gt;&gt; # check if null hypothesis is rejected\n&gt;&gt;&gt; alpha = 0.05\n&gt;&gt;&gt; if p_value &lt; alpha:\n...     print(\"Reject null hypothesis that data is normally distributed\")\n... else:\n...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n...\nFail to reject null hypothesis that data is normally distributed\n</code></pre> <p>The null hypothesis of the Shapiro-Wilk test is that the data is normally distributed. In this case, the p-value is <code>0.417</code>, which is greater than the significance level of <code>0.05</code>, indicating that we fail to reject the null hypothesis. Therefore, we can conclude that the random noise generated by the <code>FractionalGaussianNoise</code> class is likely normally distributed.</p> Notes <p>The algorithm used is described in (Algorithm as R94 Appl. Statist. (1995)) but censoring parameters as described are not implemented. For \\(N &gt; 5000\\) the \\(W\\) test statistic is accurate but the \\(p-value\\) may not be.</p> <p>The chance of rejecting the null hypothesis when it is true is close to \\(5%\\) regardless of sample size.</p> Credit <ul> <li>All credit goes to the <code>scipy</code> library.</li> </ul> References <ul> <li>https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm</li> <li>Shapiro, S. S. &amp; Wilk, M.B (1965). An analysis of variance test for normality (complete samples), Biometrika, Vol. 52, pp. 591-611.</li> <li>Razali, N. M. &amp; Wah, Y. B. (2011) Power comparisons of Shapiro-Wilk, Kolmogorov-Smirnov, Lilliefors and Anderson-Darling tests, Journal of Statistical Modeling and Analytics, Vol. 2, pp. 21-33.</li> <li>Algorithm as R94 Appl. Statist. (1995) VOL. 44, NO. 4.</li> </ul> See Also <ul> <li><code>jb()</code></li> <li><code>ob()</code></li> <li><code>dp()</code></li> <li><code>ad()</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/normality.py</code> <pre><code>@typechecked\ndef sw(\n    x: ArrayLike,\n) -&gt; Union[tuple[float, float], ShapiroResult]:\n    \"\"\"\n    !!! note \"Summary\"\n        The Shapiro-Wilk test is a statistical test used to determine whether a dataset, including time series data, follows a normal distribution. In time series forecasting, the Shapiro-Wilk test can be used to evaluate whether the residuals of a model follow a normal distribution, which is an assumption of many statistical models.\n\n    ???+ abstract \"Details\"\n        The Shapiro-Wilk test is based on the null hypothesis that the residuals of the forecasting model are normally distributed. The test calculates a test statistic that compares the observed distribution of the residuals to the expected distribution under the null hypothesis of normality. If the observed distribution of the residuals deviates significantly from the expected distribution under normality, the test rejects the null hypothesis and concludes that the residuals do not follow a normal distribution.\n\n        To apply the Shapiro-Wilk test to time series data, we first need to estimate the residuals of the forecasting model. We can then use a statistical software package to perform the Shapiro-Wilk test on the residuals. The test produces a p-value, which indicates the probability of observing the observed distribution of the residuals if the null hypothesis of normality is true. If the p-value is less than the significance level (usually 0.05), we can conclude that the residuals do not follow a normal distribution.\n\n        If the Shapiro-Wilk test indicates that the residuals do not follow a normal distribution, we may need to consider using a different modeling approach or modifying the forecasting model. It is important to ensure that the residuals of a time series forecasting model follow a normal distribution to ensure that the model is valid and reliable for making predictions.\n\n        The Shapiro-Wilk test is a statistical test used to evaluate whether a dataset, including time series data, follows a normal distribution. The mathematical equation for the Shapiro-Wilk test is:\n\n        $$\n        W = \\\\frac { \\\\left( \\\\sum_{i=1}^n (a_i \\\\times z_i) \\\\right)^2 } { \\\\sum_{i=1}^n (x_i - \\\\bar{x})^2 }\n        $$\n\n        where:\n\n        - $W$ is the test statistic\n        - $a_i$ are the coefficients calculated from the ordered sample values\n        - $z_i$ are the corresponding normal deviates for the ai coefficients\n        - $x_i$ are the ordered sample values\n        - $\\\\bar{x}$ is the sample mean\n\n        To calculate the Shapiro-Wilk test statistic for time series data, we need to perform the following steps:\n\n        1. Estimate the residuals of the forecasting model: The residuals are the difference between the actual values and the predicted values of the time series model.\n\n        1. Calculate the sample mean and standard deviation of the residuals: These are the mean and standard deviation of the residuals, respectively.\n\n        1. Standardize the residuals: The standardized residuals are the residuals divided by their sample standard deviation.\n\n        1. Order the standardized residuals from smallest to largest: This step ensures that the Shapiro-Wilk test is performed on a sample that is in ascending order.\n\n        1. Calculate the $a_i$ coefficients: The $a_i$ coefficients are calculated from the ordered sample values using the formula:\n\n            $$\n            a_i = \\\\frac { \\\\sum_{j=1}^{n} (a_{i_j} \\\\times x_j) }{ s^2 }\n            $$\n\n            where:\n\n            - $s^2$ is the sample variance and\n            - $a_{i_j}$ are constants that depend on the sample size and the order of the sample values. These constants are pre-calculated and available in statistical software packages.\n\n        1. Calculate the $z_i$ normal deviates: The $z_i$ normal deviates are the corresponding values of the standard normal distribution for the ai coefficients. These values are pre-calculated and available in statistical software packages.\n\n        1. Calculate the numerator of the test statistic: This is the sum of the product of the ai coefficients and the corresponding $z_i$ normal deviates:\n\n            $$\n            \\\\sum (a_i \\\\times z_i)\n            $$\n\n        1. Calculate the denominator of the test statistic: This is the sum of the squared differences between the ordered sample values and the sample mean:\n\n            $$\n            \\\\sum (x_i - \\\\bar{x})^2\n            $$\n\n        1. Calculate the test statistic: This is the ratio of the squared numerator to the denominator:\n\n            $$\n            W = \\\\frac { (\\\\sum(a_i \\\\times z_i))^2 } { \\\\sum(x_i - \\\\bar{x})^2 }\n            $$\n\n        1. Compare the test statistic to a critical value: If the test statistic is less than the critical value, we cannot reject the null hypothesis of normality and can conclude that the residuals follow a normal distribution. If the test statistic is greater than the critical value, we reject the null hypothesis of normality and conclude that the residuals do not follow a normal distribution.\n\n        In summary, the Shapiro-Wilk test is a statistical test that evaluates normality of time series residuals by standardizing and ordering the residuals, calculating ai coefficients and corresponding $z_i$ normal deviates, and computing the test statistic using a ratio of the squared sum of $ai \\\\times zi$ to the sum of squared differences between the sample values and sample mean. Finally, we compare the test statistic to a critical value to determine whether the residuals follow a normal distribution or not.\n\n    Params:\n        x (ArrayLike):\n            Array of sample data\n\n    Returns:\n        statistic (float):\n            The test statistic.\n        pvalue (float):\n            The p-value for the hypothesis test.\n\n    ???+ example \"Examples\"\n        Test the null hypothesis that a random sample was drawn from a normal distribution.\n\n        ```pycon {.py .python linenums=\"1\" title=\"From the `scipy` docs\"}\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from scipy import stats\n        &gt;&gt;&gt; rng = np.random.default_rng()\n        &gt;&gt;&gt; x = stats.norm.rvs(loc=5, scale=3, size=100, random_state=rng)\n        &gt;&gt;&gt; shapiro_test = stats.shapiro(x)\n        &gt;&gt;&gt; shapiro_test\n        ShapiroResult(statistic=0.9813305735588074, pvalue=0.16855233907699585)\n        &gt;&gt;&gt; shapiro_test.statistic\n        0.9813305735588074\n        &gt;&gt;&gt; shapiro_test.pvalue\n        0.16855233907699585\n        ```\n\n        ---\n\n        Example one, using the `airline` data from the `sktime` package.\n\n        ```pycon {.py .python linenums=\"1\" title=\"Python\"}\n        &gt;&gt;&gt; # Import packages\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; from scipy.stats import shapiro\n\n        &gt;&gt;&gt; # load the airline data\n        &gt;&gt;&gt; data = load_airline()\n\n        &gt;&gt;&gt; # run the Shapiro-Wilk test on the data\n        &gt;&gt;&gt; statistic, p_value = shapiro(data)\n\n        &gt;&gt;&gt; # print the results\n        &gt;&gt;&gt; print(f\"Shapiro-Wilk test statistic: {statistic:.3f}\")\n        Shapiro-Wilk test statistic: 0.910\n        &gt;&gt;&gt; print(f\"Shapiro-Wilk test p-value: {p_value:.3f}\")\n        Shapiro-Wilk test p-value: 0.054\n\n        &gt;&gt;&gt; # check if null hypothesis is rejected\n        &gt;&gt;&gt; alpha = 0.05\n        &gt;&gt;&gt; if p_value &lt; alpha:\n        ...     print(\"Reject null hypothesis that data is normally distributed\")\n        ... else:\n        ...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n        ...\n        Fail to reject null hypothesis that data is normally distributed\n        ```\n\n        The null hypothesis of the Shapiro-Wilk test is that the data _is_ normally distributed. In this case, the p-value is `0.054`, which is **greater** than the significance level of `0.05`, indicating that we _fail_ to reject the null hypothesis. Therefore, we can conclude that the airline data **is** likely normally distributed.\n\n        ---\n\n        Example two, using the `sine` wave data generated from the `numpy` package.\n\n        ```pycon {.py .python linenums=\"1\" title=\"Python\"}\n        &gt;&gt;&gt; # Import packages\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from scipy.stats import shapiro\n\n        &gt;&gt;&gt; # generate sine wave data\n        &gt;&gt;&gt; x = np.linspace(0, 2 * np.pi, 100)\n        &gt;&gt;&gt; data = np.sin(x)\n\n        &gt;&gt;&gt; # run the Shapiro-Wilk test on the data\n        &gt;&gt;&gt; statistic, p_value = shapiro(data)\n\n        &gt;&gt;&gt; # print the results\n        &gt;&gt;&gt; print(f\"Shapiro-Wilk test statistic: {statistic:.3f}\")\n        &gt;&gt;&gt; print(f\"Shapiro-Wilk test p-value: {p_value:.3f}\")\n\n        &gt;&gt;&gt; # check if null hypothesis is rejected\n        &gt;&gt;&gt; alpha = 0.05\n        &gt;&gt;&gt; if p_value &lt; alpha:\n        ...     print(\"Reject null hypothesis that data is normally distributed\")\n        ... else:\n        ...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n        ...\n        ```\n\n        The null hypothesis of the Shapiro-Wilk test _is_ that the data is normally distributed. In this case, the p-value is `0.002`, which is **less** than the significance level of `0.05`, indicating that we _can_ reject the null hypothesis. Therefore, we can conclude that the sine wave data is **not** normally distributed.\n\n        ---\n\n        Example three, using the `FractionalGaussianNoise` random data generated from the `stochastic` package.\n\n        ```pycon {.py .python linenums=\"1\" title=\"Python\"}\n        &gt;&gt;&gt; # Import packages\n        &gt;&gt;&gt; from stochastic.noise import FractionalGaussianNoise\n        &gt;&gt;&gt; from scipy.stats import shapiro\n\n        &gt;&gt;&gt; # Generate Fractional Gaussian Noise\n        &gt;&gt;&gt; fgn = FractionalGaussianNoise(t=1, hurst=0.5, length=100, method=\"daviesharte\")\n        &gt;&gt;&gt; data = fgn.sample()\n\n        &gt;&gt;&gt; # run the Shapiro-Wilk test on the data\n        &gt;&gt;&gt; statistic, p_value = shapiro(data)\n\n        &gt;&gt;&gt; # print the results\n        &gt;&gt;&gt; print(f\"Shapiro-Wilk test statistic: {statistic:.3f}\")\n        Shapiro-Wilk test statistic: 0.979\n        &gt;&gt;&gt; print(f\"Shapiro-Wilk test p-value: {p_value:.3f}\")\n        Shapiro-Wilk test p-value: 0.417\n\n        &gt;&gt;&gt; # check if null hypothesis is rejected\n        &gt;&gt;&gt; alpha = 0.05\n        &gt;&gt;&gt; if p_value &lt; alpha:\n        ...     print(\"Reject null hypothesis that data is normally distributed\")\n        ... else:\n        ...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n        ...\n        Fail to reject null hypothesis that data is normally distributed\n        ```\n\n        The null hypothesis of the Shapiro-Wilk test _is_ that the data is normally distributed. In this case, the p-value is `0.417`, which is **greater** than the significance level of `0.05`, indicating that we _fail_ to reject the null hypothesis. Therefore, we can conclude that the random noise generated by the `FractionalGaussianNoise` class **is** likely normally distributed.\n\n    ??? note \"Notes\"\n        The algorithm used is described in (Algorithm as R94 Appl. Statist. (1995)) but censoring parameters as described are not implemented. For $N &gt; 5000$ the $W$ test statistic is accurate but the $p-value$ may not be.\n\n        The chance of rejecting the null hypothesis when it is true is close to $5%$ regardless of sample size.\n\n    ??? success \"Credit\"\n        - All credit goes to the [`scipy`](https://docs.scipy.org/) library.\n\n    ??? question \"References\"\n        - https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm\n        - Shapiro, S. S. &amp; Wilk, M.B (1965). An analysis of variance test for normality (complete samples), Biometrika, Vol. 52, pp. 591-611.\n        - Razali, N. M. &amp; Wah, Y. B. (2011) Power comparisons of Shapiro-Wilk, Kolmogorov-Smirnov, Lilliefors and Anderson-Darling tests, Journal of Statistical Modeling and Analytics, Vol. 2, pp. 21-33.\n        - Algorithm as R94 Appl. Statist. (1995) VOL. 44, NO. 4.\n\n    ??? tip \"See Also\"\n        - [`jb()`][ts_stat_tests.algorithms.normality.jb]\n        - [`ob()`][ts_stat_tests.algorithms.normality.ob]\n        - [`dp()`][ts_stat_tests.algorithms.normality.dp]\n        - [`ad()`][ts_stat_tests.algorithms.normality.ad]\n    \"\"\"\n    return _sw(x=x)\n</code></pre>"},{"location":"code/normality/#ts_stat_tests.algorithms.normality.dp","title":"dp","text":"<pre><code>dp(\n    x: ArrayLike,\n    axis: int = 0,\n    nan_policy: VALID_DP_NAN_POLICY_OPTIONS = \"propagate\",\n) -&gt; Union[\n    tuple[Union[float, ArrayLike], Union[float, ArrayLike]],\n    NormaltestResult,\n]\n</code></pre> <p>Summary</p> <p>The D'Agostino and Pearson's test is a statistical test used to evaluate whether a dataset, including time series data, follows a normal distribution. In time series forecasting, the D'Agostino and Pearson's test can be used to assess whether the residuals of a model follow a normal distribution, which is an assumption of many statistical models.</p> Details <p>The D'Agostino and Pearson's test uses a combination of skewness and kurtosis measures to assess whether the residuals follow a normal distribution. Skewness measures the degree of asymmetry in the distribution of the residuals, while kurtosis measures the degree of peakedness or flatness. If the residuals follow a normal distribution, their skewness and kurtosis should be close to zero.</p> <p>To apply the D'Agostino and Pearson's test to time series data, we first need to estimate the residuals of the forecasting model. We can then use a statistical software package to perform the test on the residuals. The test produces a test statistic that compares the observed skewness and kurtosis values to the expected values under the null hypothesis of normality. If the observed values deviate significantly from the expected values under normality, the test rejects the null hypothesis and concludes that the residuals do not follow a normal distribution.</p> <p>The D'Agostino and Pearson's test produces a p-value, which indicates the probability of observing the observed test statistic if the null hypothesis of normality is true. If the p-value is less than the significance level (usually 0.05), we can conclude that the residuals do not follow a normal distribution.</p> <p>If the D'Agostino and Pearson's test indicates that the residuals do not follow a normal distribution, we may need to consider using a different modeling approach or modifying the forecasting model. It is important to ensure that the residuals of a time series forecasting model follow a normal distribution to ensure that the model is valid and reliable for making predictions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The array containing the sample to be tested.</p> required <code>axis</code> <code>int</code> <p>Axis along which to compute test. If <code>None</code>, compute over the whole array <code>a</code>. Defaults to <code>0</code>.</p> <code>0</code> <code>nan_policy</code> <code>VALID_DP_NAN_POLICY_OPTIONS</code> <p>Defines how to handle when input contains nan. The following options are available (default is 'propagate'):</p> <ul> <li>'propagate': returns nan</li> <li>'raise': throws an error</li> <li>'omit': performs the calculations ignoring nan values.</li> </ul> <p>Defaults to <code>\"propagate\"</code>.</p> <code>'propagate'</code> <p>Returns:</p> Name Type Description <code>statistic</code> <code>Union[float, ndarray]</code> <p>Value \\(s^2 + k^2\\), where \\(s\\) is the z-score returned by <code>skewtest</code> and \\(k\\) is the z-score returned by <code>kurtosistest</code>.</p> <code>pvalue</code> <code>Union[float, ndarray]</code> <p>A 2-sided chi-squared probability for the hypothesis test.</p> Examples <p>Test the null hypothesis that a random sample was drawn from a normal distribution.</p> From the `scipy` docs<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy import stats\n&gt;&gt;&gt; rng = np.random.default_rng()\n&gt;&gt;&gt; pts = 1000\n&gt;&gt;&gt; a = rng.normal(0, 1, size=pts)\n&gt;&gt;&gt; b = rng.normal(2, 1, size=pts)\n&gt;&gt;&gt; x = np.concatenate((a, b))\n&gt;&gt;&gt; k2, p = stats.normaltest(x)\n&gt;&gt;&gt; alpha = 1e-3\n&gt;&gt;&gt; print(\"p = {:g}\".format(p))\np = 8.4713e-19\n&gt;&gt;&gt; if p &lt; alpha:  # null hypothesis: x comes from a normal distribution\n...     print(\"The null hypothesis can be rejected\")\n... else:\n...     print(\"The null hypothesis cannot be rejected\")\n...\n\"The null hypothesis can be rejected\"\n</code></pre> <p>Example one, using the <code>airline</code> data from the <code>sktime</code> package.</p> Python<pre><code>&gt;&gt;&gt; # Import packages\n&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; from scipy.stats import normaltest\n\n&gt;&gt;&gt; # load the airline data\n&gt;&gt;&gt; data = load_airline()\n\n&gt;&gt;&gt; # run D'Agostino and Pearson's test on the data\n&gt;&gt;&gt; statistic, p_value = normaltest(data)\n\n&gt;&gt;&gt; # print the results\n&gt;&gt;&gt; print(f\"D'Agostino and Pearson's test statistic: {statistic:.3f}\")\nD'Agostino and Pearson's test statistic: 7.764\n&gt;&gt;&gt; print(f\"D'Agostino and Pearson's test p-value: {p_value:.3f}\")\nD'Agostino and Pearson's test p-value: 0.021\n\n&gt;&gt;&gt; # check if null hypothesis is rejected\n&gt;&gt;&gt; alpha = 0.05\n&gt;&gt;&gt; if p_value &lt; alpha:\n...     print(\"Reject null hypothesis that data is normally distributed\")\n... else:\n...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n...\nReject null hypothesis that data is normally distributed\n</code></pre> <p>The null hypothesis of D'Agostino and Pearson's test is that the data is normally distributed. In this case, the p-value is <code>0.021</code>, which is less than the significance level of <code>0.05</code>, indicating that we can reject the null hypothesis. Therefore, we can conclude that the airline data from the sktime library is not normally distributed.</p> <p>Example two, using the <code>sine</code> wave data generated from the <code>numpy</code> package.</p> Python<pre><code>&gt;&gt;&gt; # Import packages\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.stats import normaltest\n\n&gt;&gt;&gt; # generate sine wave data\n&gt;&gt;&gt; data = np.sin(np.linspace(0, 2 * np.pi, num=100))\n\n&gt;&gt;&gt; # run D'Agostino and Pearson's test on the data\n&gt;&gt;&gt; statistic, p_value = normaltest(data)\n\n&gt;&gt;&gt; # print the results\n&gt;&gt;&gt; print(f\"D'Agostino and Pearson's test statistic: {statistic:.3f}\")\nD'Agostino and Pearson's test statistic: 50.583\n&gt;&gt;&gt; print(f\"D'Agostino and Pearson's test p-value: {p_value:.3f}\")\nD'Agostino and Pearson's test p-value: 0.000\n\n&gt;&gt;&gt; # check if null hypothesis is rejected\n&gt;&gt;&gt; alpha = 0.05\n&gt;&gt;&gt; if p_value &lt; alpha:\n...     print(\"Reject null hypothesis that data is normally distributed\")\n... else:\n...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n...\nReject null hypothesis that data is normally distributed\n</code></pre> <p>The null hypothesis of D'Agostino and Pearson's test is that the data is normally distributed. In this case, the p-value is <code>0.000</code>, which is less than the significance level of <code>0.05</code>, indicating that we can reject the null hypothesis. Therefore, we can conclude that the sine wave data generated from the numpy library is not normally distributed.</p> <p>Example three, using the <code>FractionalGaussianNoise</code> random data generated from the <code>stochastic</code> package.</p> Python<pre><code>&gt;&gt;&gt; # Import packages\n&gt;&gt;&gt; from stochastic import FractionalGaussianNoise\n&gt;&gt;&gt; from scipy.stats import normaltest\n\n&gt;&gt;&gt; # generate random noise using FractionalGaussianNoise\n&gt;&gt;&gt; fgn = FractionalGaussianNoise(H=0.7, length=100)\n&gt;&gt;&gt; data = fgn.generate()\n\n&gt;&gt;&gt; # run D'Agostino and Pearson's test on the data\n&gt;&gt;&gt; statistic, p_value = normaltest(data)\n\n&gt;&gt;&gt; # print the results\n&gt;&gt;&gt; print(f\"D'Agostino and Pearson's test statistic: {statistic:.3f}\")\nD'Agostino and Pearson's test statistic: 0.388\n&gt;&gt;&gt; print(f\"D'Agostino and Pearson's test p-value: {p_value:.3f}\")\nD'Agostino and Pearson's test p-value: 0.823\n\n&gt;&gt;&gt; # check if null hypothesis is rejected\n&gt;&gt;&gt; alpha = 0.05\n&gt;&gt;&gt; if p_value &lt; alpha:\n...     print(\"Reject null hypothesis that data is normally distributed\")\n... else:\n...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n...\nFail to reject null hypothesis that data is normally distributed\n</code></pre> <p>The null hypothesis of D'Agostino and Pearson's test is that the data is normally distributed. In this case, the p-value is <code>0.823</code>, which is greater than the significance level of <code>0.05</code>, indicating that we fail to reject the null hypothesis. Therefore, we can conclude that the random noise generated by the FractionalGaussianNoise class from the stochastic library is likely normally distributed.</p> Notes <p>This function is a wrapper for the <code>scipy.stats.normaltest</code> function.</p> <p>The D'Agostino and Pearson's test is a statistical test used to determine if a dataset, including time series data, is normally distributed. The test is based on the sample skewness and sample kurtosis of the dataset. The mathematical equation for the D'Agostino and Pearson's test is:</p> \\[ D^2 = \\left( \\frac{n+1}{6} \\right) \\times \\left( S^2 + K^2 \\right) \\] <p>where:</p> <ul> <li>\\(D^2\\) is the test statistic</li> <li>\\(n\\) is the sample size</li> <li>\\(S\\) is the sample skewness</li> <li>\\(K\\) is the sample kurtosis</li> </ul> <p>To calculate the D'Agostino and Pearson's test statistic for time series data, we need to perform the following steps:</p> <ol> <li> <p>Estimate the residuals of the forecasting model: The residuals are the difference between the actual values and the predicted values of the time series model.</p> </li> <li> <p>Calculate the sample mean and standard deviation of the residuals: These are the mean and standard deviation of the residuals, respectively.</p> </li> <li> <p>Standardize the residuals: The standardized residuals are the residuals divided by their sample standard deviation.</p> </li> <li> <p>Calculate the sample skewness: The sample skewness is a measure of the asymmetry of the distribution of the residuals. It is calculated as:</p> \\[ S = \\left( \\frac {n} {(n-1) \\times (n-2)} \\right) \\times \\left( \\frac {\\sum_{i=1}^{n}(x_i - \\bar{x})^3 } { s^3 } \\right) \\] <p>where:</p> <ul> <li>\\(x_i\\) are the standardized residuals,</li> <li>\\(\\bar{x}\\) is their mean,</li> <li>\\(s\\) is their standard deviation, and</li> <li>\\(n\\) is the sample size.</li> </ul> </li> <li> <p>Calculate the sample kurtosis: The sample kurtosis is a measure of the \"peakedness\" of the distribution of the residuals. It is calculated as:</p> \\[ K = \\left( \\frac { n \\times (n+1) } { (n-1) \\times (n-2) \\times (n-3) } \\right) \\times \\left( \\frac { \\sum_{i=1}^{n} (x_i - \\bar{x})^4 } { s^4 } \\right) - \\left( \\frac { 3 \\times (n-1)^2 } { (n-2) \\times (n-3) } \\right) \\] <p>where:</p> <ul> <li>\\(x_i\\) are the standardized residuals,</li> <li>\\(\\bar{x}\\) is their mean,</li> <li>\\(s\\) is their standard deviation, and</li> <li>\\(n\\) is the sample size.</li> </ul> </li> <li> <p>Calculate the test statistic: The test statistic is calculated using the formula:</p> \\[ D^2 = \\left( \\frac { n+1 } {6} \\right) \\times \\left( S^2 + K^2 \\right) \\] </li> <li> <p>Compare the test statistic to a critical value: If the test statistic is less than the critical value, we cannot reject the null hypothesis of normality and can conclude that the residuals follow a normal distribution. If the test statistic is greater than the critical value, we reject the null hypothesis of normality and conclude that the residuals do not follow a normal distribution.</p> </li> </ol> <p>In summary, the D'Agostino and Pearson's test is a statistical test that evaluates normality of time series residuals by standardizing the residuals, calculating their sample skewness and sample kurtosis, and computing the test statistic using a formula that takes into account both skewness and kurtosis. Finally, we compare the test statistic to a critical value to determine whether the residuals follow a normal distribution or not.</p> Credit <ul> <li>All credit goes to the <code>scipy</code> library.</li> </ul> References <ul> <li>D'Agostino, R. B. (1971), \"An omnibus test of normality for moderate and large sample size\", Biometrika, 58, 341-348</li> <li>D'Agostino, R. and Pearson, E. S. (1973), \"Tests for departure from normality\", Biometrika, 60, 613-622</li> </ul> See Also <ul> <li><code>jb()</code></li> <li><code>ob()</code></li> <li><code>sw()</code></li> <li><code>ad()</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/normality.py</code> <pre><code>@typechecked\ndef dp(\n    x: ArrayLike,\n    axis: int = 0,\n    nan_policy: VALID_DP_NAN_POLICY_OPTIONS = \"propagate\",\n) -&gt; Union[\n    tuple[Union[float, ArrayLike], Union[float, ArrayLike]],\n    NormaltestResult,\n]:\n    \"\"\"\n    !!! note \"Summary\"\n        The D'Agostino and Pearson's test is a statistical test used to evaluate whether a dataset, including time series data, follows a normal distribution. In time series forecasting, the D'Agostino and Pearson's test can be used to assess whether the residuals of a model follow a normal distribution, which is an assumption of many statistical models.\n\n    ???+ abstract \"Details\"\n        The D'Agostino and Pearson's test uses a combination of skewness and kurtosis measures to assess whether the residuals follow a normal distribution. Skewness measures the degree of asymmetry in the distribution of the residuals, while kurtosis measures the degree of peakedness or flatness. If the residuals follow a normal distribution, their skewness and kurtosis should be close to zero.\n\n        To apply the D'Agostino and Pearson's test to time series data, we first need to estimate the residuals of the forecasting model. We can then use a statistical software package to perform the test on the residuals. The test produces a test statistic that compares the observed skewness and kurtosis values to the expected values under the null hypothesis of normality. If the observed values deviate significantly from the expected values under normality, the test rejects the null hypothesis and concludes that the residuals do not follow a normal distribution.\n\n        The D'Agostino and Pearson's test produces a p-value, which indicates the probability of observing the observed test statistic if the null hypothesis of normality is true. If the p-value is less than the significance level (usually 0.05), we can conclude that the residuals do not follow a normal distribution.\n\n        If the D'Agostino and Pearson's test indicates that the residuals do not follow a normal distribution, we may need to consider using a different modeling approach or modifying the forecasting model. It is important to ensure that the residuals of a time series forecasting model follow a normal distribution to ensure that the model is valid and reliable for making predictions.\n\n    Params:\n        x (ArrayLike):\n            The array containing the sample to be tested.\n        axis (int):\n            Axis along which to compute test. If `None`, compute over the whole array `a`.&lt;br&gt;\n            Defaults to `0`.\n        nan_policy (VALID_DP_NAN_POLICY_OPTIONS):\n            Defines how to handle when input contains nan. The following options are available (default is 'propagate'):\n\n            - 'propagate': returns nan\n            - 'raise': throws an error\n            - 'omit': performs the calculations ignoring nan values.\n\n            Defaults to `\"propagate\"`.\n\n    Returns:\n        statistic (Union[float, np.ndarray]):\n            Value $s^2 + k^2$, where $s$ is the z-score returned by [`skewtest`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.skewtest.html) and $k$ is the z-score returned by [`kurtosistest`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kurtosistest.html).\n        pvalue (Union[float, np.ndarray]):\n            A 2-sided chi-squared probability for the hypothesis test.\n\n    ???+ example \"Examples\"\n\n        Test the null hypothesis that a random sample was drawn from a normal distribution.\n\n        ```pycon {.py .python linenums=\"1\" title=\"From the `scipy` docs\"}\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from scipy import stats\n        &gt;&gt;&gt; rng = np.random.default_rng()\n        &gt;&gt;&gt; pts = 1000\n        &gt;&gt;&gt; a = rng.normal(0, 1, size=pts)\n        &gt;&gt;&gt; b = rng.normal(2, 1, size=pts)\n        &gt;&gt;&gt; x = np.concatenate((a, b))\n        &gt;&gt;&gt; k2, p = stats.normaltest(x)\n        &gt;&gt;&gt; alpha = 1e-3\n        &gt;&gt;&gt; print(\"p = {:g}\".format(p))\n        p = 8.4713e-19\n        &gt;&gt;&gt; if p &lt; alpha:  # null hypothesis: x comes from a normal distribution\n        ...     print(\"The null hypothesis can be rejected\")\n        ... else:\n        ...     print(\"The null hypothesis cannot be rejected\")\n        ...\n        \"The null hypothesis can be rejected\"\n        ```\n\n        ---\n\n        Example one, using the `airline` data from the `sktime` package.\n\n        ```pycon {.py .python linenums=\"1\" title=\"Python\"}\n        &gt;&gt;&gt; # Import packages\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; from scipy.stats import normaltest\n\n        &gt;&gt;&gt; # load the airline data\n        &gt;&gt;&gt; data = load_airline()\n\n        &gt;&gt;&gt; # run D'Agostino and Pearson's test on the data\n        &gt;&gt;&gt; statistic, p_value = normaltest(data)\n\n        &gt;&gt;&gt; # print the results\n        &gt;&gt;&gt; print(f\"D'Agostino and Pearson's test statistic: {statistic:.3f}\")\n        D'Agostino and Pearson's test statistic: 7.764\n        &gt;&gt;&gt; print(f\"D'Agostino and Pearson's test p-value: {p_value:.3f}\")\n        D'Agostino and Pearson's test p-value: 0.021\n\n        &gt;&gt;&gt; # check if null hypothesis is rejected\n        &gt;&gt;&gt; alpha = 0.05\n        &gt;&gt;&gt; if p_value &lt; alpha:\n        ...     print(\"Reject null hypothesis that data is normally distributed\")\n        ... else:\n        ...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n        ...\n        Reject null hypothesis that data is normally distributed\n        ```\n\n        The null hypothesis of D'Agostino and Pearson's test is that the data _is_ normally distributed. In this case, the p-value is `0.021`, which is **less** than the significance level of `0.05`, indicating that we _can_ reject the null hypothesis. Therefore, we can conclude that the airline data from the sktime library is **not** normally distributed.\n\n        ---\n\n        Example two, using the `sine` wave data generated from the `numpy` package.\n\n        ```pycon {.py .python linenums=\"1\" title=\"Python\"}\n        &gt;&gt;&gt; # Import packages\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from scipy.stats import normaltest\n\n        &gt;&gt;&gt; # generate sine wave data\n        &gt;&gt;&gt; data = np.sin(np.linspace(0, 2 * np.pi, num=100))\n\n        &gt;&gt;&gt; # run D'Agostino and Pearson's test on the data\n        &gt;&gt;&gt; statistic, p_value = normaltest(data)\n\n        &gt;&gt;&gt; # print the results\n        &gt;&gt;&gt; print(f\"D'Agostino and Pearson's test statistic: {statistic:.3f}\")\n        D'Agostino and Pearson's test statistic: 50.583\n        &gt;&gt;&gt; print(f\"D'Agostino and Pearson's test p-value: {p_value:.3f}\")\n        D'Agostino and Pearson's test p-value: 0.000\n\n        &gt;&gt;&gt; # check if null hypothesis is rejected\n        &gt;&gt;&gt; alpha = 0.05\n        &gt;&gt;&gt; if p_value &lt; alpha:\n        ...     print(\"Reject null hypothesis that data is normally distributed\")\n        ... else:\n        ...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n        ...\n        Reject null hypothesis that data is normally distributed\n        ```\n\n        The null hypothesis of D'Agostino and Pearson's test is that the data _is_ normally distributed. In this case, the p-value is `0.000`, which is **less** than the significance level of `0.05`, indicating that we _can_ reject the null hypothesis. Therefore, we can conclude that the sine wave data generated from the numpy library is **not** normally distributed.\n\n        ---\n\n        Example three, using the `FractionalGaussianNoise` random data generated from the `stochastic` package.\n\n        ```pycon {.py .python linenums=\"1\" title=\"Python\"}\n        &gt;&gt;&gt; # Import packages\n        &gt;&gt;&gt; from stochastic import FractionalGaussianNoise\n        &gt;&gt;&gt; from scipy.stats import normaltest\n\n        &gt;&gt;&gt; # generate random noise using FractionalGaussianNoise\n        &gt;&gt;&gt; fgn = FractionalGaussianNoise(H=0.7, length=100)\n        &gt;&gt;&gt; data = fgn.generate()\n\n        &gt;&gt;&gt; # run D'Agostino and Pearson's test on the data\n        &gt;&gt;&gt; statistic, p_value = normaltest(data)\n\n        &gt;&gt;&gt; # print the results\n        &gt;&gt;&gt; print(f\"D'Agostino and Pearson's test statistic: {statistic:.3f}\")\n        D'Agostino and Pearson's test statistic: 0.388\n        &gt;&gt;&gt; print(f\"D'Agostino and Pearson's test p-value: {p_value:.3f}\")\n        D'Agostino and Pearson's test p-value: 0.823\n\n        &gt;&gt;&gt; # check if null hypothesis is rejected\n        &gt;&gt;&gt; alpha = 0.05\n        &gt;&gt;&gt; if p_value &lt; alpha:\n        ...     print(\"Reject null hypothesis that data is normally distributed\")\n        ... else:\n        ...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n        ...\n        Fail to reject null hypothesis that data is normally distributed\n        ```\n\n        The null hypothesis of D'Agostino and Pearson's test is that the data _is_ normally distributed. In this case, the p-value is `0.823`, which is *greater* than the significance level of `0.05`, indicating that we _fail_ to reject the null hypothesis. Therefore, we can conclude that the random noise generated by the FractionalGaussianNoise class from the stochastic library *is* likely normally distributed.\n\n    ??? note \"Notes\"\n        This function is a wrapper for the `scipy.stats.normaltest` function.\n\n        The D'Agostino and Pearson's test is a statistical test used to determine if a dataset, including time series data, is normally distributed. The test is based on the sample skewness and sample kurtosis of the dataset. The mathematical equation for the D'Agostino and Pearson's test is:\n\n        $$\n        D^2 = \\\\left( \\\\frac{n+1}{6} \\\\right) \\\\times \\\\left( S^2 + K^2 \\\\right)\n        $$\n\n        where:\n\n        - $D^2$ is the test statistic\n        - $n$ is the sample size\n        - $S$ is the sample skewness\n        - $K$ is the sample kurtosis\n\n        To calculate the D'Agostino and Pearson's test statistic for time series data, we need to perform the following steps:\n\n        1. Estimate the residuals of the forecasting model: The residuals are the difference between the actual values and the predicted values of the time series model.\n\n        1. Calculate the sample mean and standard deviation of the residuals: These are the mean and standard deviation of the residuals, respectively.\n\n        1. Standardize the residuals: The standardized residuals are the residuals divided by their sample standard deviation.\n\n        1. Calculate the sample skewness: The sample skewness is a measure of the asymmetry of the distribution of the residuals. It is calculated as:\n\n            $$\n            S = \\\\left( \\\\frac {n} {(n-1) \\\\times (n-2)} \\\\right) \\\\times \\\\left( \\\\frac {\\\\sum_{i=1}^{n}(x_i - \\\\bar{x})^3 } { s^3 } \\\\right)\n            $$\n\n            where:\n\n            - $x_i$ are the standardized residuals,\n            - $\\\\bar{x}$ is their mean,\n            - $s$ is their standard deviation, and\n            - $n$ is the sample size.\n\n        1. Calculate the sample kurtosis: The sample kurtosis is a measure of the \"peakedness\" of the distribution of the residuals. It is calculated as:\n\n            $$\n            K = \\\\left( \\\\frac { n \\\\times (n+1) } { (n-1) \\\\times (n-2) \\\\times (n-3) } \\\\right) \\\\times \\\\left( \\\\frac { \\\\sum_{i=1}^{n} (x_i - \\\\bar{x})^4 } { s^4 } \\\\right) - \\\\left( \\\\frac { 3 \\\\times (n-1)^2 } { (n-2) \\\\times (n-3) } \\\\right)\n            $$\n\n            where:\n\n            - $x_i$ are the standardized residuals,\n            - $\\\\bar{x}$ is their mean,\n            - $s$ is their standard deviation, and\n            - $n$ is the sample size.\n\n        1. Calculate the test statistic: The test statistic is calculated using the formula:\n\n            $$\n            D^2 = \\\\left( \\\\frac { n+1 } {6} \\\\right) \\\\times \\\\left( S^2 + K^2 \\\\right)\n            $$\n\n        1. Compare the test statistic to a critical value: If the test statistic is less than the critical value, we cannot reject the null hypothesis of normality and can conclude that the residuals follow a normal distribution. If the test statistic is greater than the critical value, we reject the null hypothesis of normality and conclude that the residuals do not follow a normal distribution.\n\n        In summary, the D'Agostino and Pearson's test is a statistical test that evaluates normality of time series residuals by standardizing the residuals, calculating their sample skewness and sample kurtosis, and computing the test statistic using a formula that takes into account both skewness and kurtosis. Finally, we compare the test statistic to a critical value to determine whether the residuals follow a normal distribution or not.\n\n    ??? success \"Credit\"\n        - All credit goes to the [`scipy`](https://docs.scipy.org/) library.\n\n    ??? question \"References\"\n        - D'Agostino, R. B. (1971), \"An omnibus test of normality for moderate and large sample size\", Biometrika, 58, 341-348\n        - D'Agostino, R. and Pearson, E. S. (1973), \"Tests for departure from normality\", Biometrika, 60, 613-622\n\n    ??? tip \"See Also\"\n        - [`jb()`][ts_stat_tests.algorithms.normality.jb]\n        - [`ob()`][ts_stat_tests.algorithms.normality.ob]\n        - [`sw()`][ts_stat_tests.algorithms.normality.sw]\n        - [`ad()`][ts_stat_tests.algorithms.normality.ad]\n    \"\"\"\n    return _dp(a=x, axis=axis, nan_policy=nan_policy)\n</code></pre>"},{"location":"code/normality/#ts_stat_tests.algorithms.normality.ad","title":"ad","text":"<pre><code>ad(\n    x: ArrayLike, dist: VALID_AD_DIST_OPTIONS = \"norm\"\n) -&gt; AndersonResult\n</code></pre> <p>Summary</p> <p>The Anderson-Darling test is a statistical test used to evaluate whether a dataset, including time series data, follows a normal distribution. In time series forecasting, the Anderson-Darling test can be used to assess whether the residuals of a model follow a normal distribution, which is an assumption of many statistical models.</p> <p>The Anderson-Darling test tests the null hypothesis that a sample is drawn from a population that follows a particular distribution. For the Anderson-Darling test, the critical values depend on which distribution is being tested against. This function works for normal, exponential, logistic, or Gumbel (Extreme Value Type I) distributions.</p> Details <p>The Anderson-Darling test is based on the null hypothesis that the residuals of the forecasting model are normally distributed. The test calculates a test statistic that measures the distance between the observed distribution of the residuals and the expected distribution under the null hypothesis of normality. If the observed distribution of the residuals deviates significantly from the expected distribution under normality, the test rejects the null hypothesis and concludes that the residuals do not follow a normal distribution.</p> <p>To apply the Anderson-Darling test to time series data, we first need to estimate the residuals of the forecasting model. We can then use a statistical software package to perform the test on the residuals. The test produces a p-value, which indicates the probability of observing the observed distribution of the residuals if the null hypothesis of normality is true. If the p-value is less than the significance level (usually \\(0.05\\)), we can conclude that the residuals do not follow a normal distribution.</p> <p>The Anderson-Darling test is more sensitive to deviations from normality in the tails of the distribution than other tests, such as the Shapiro-Wilk test. This makes it a useful test when assessing whether the residuals of a time series forecasting model exhibit heavy tails or other non-normal features.</p> <p>If the Anderson-Darling test indicates that the residuals do not follow a normal distribution, we may need to consider using a different modeling approach or modifying the forecasting model. It is important to ensure that the residuals of a time series forecasting model follow a normal distribution to ensure that the model is valid and reliable for making predictions.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>Array of sample data.</p> required <code>dist</code> <code>VALID_AD_DIST_OPTIONS</code> <p>The type of distribution to test against. The default is <code>'norm'</code>. The names <code>'extreme1'</code>, <code>'gumbel_l'</code> and <code>'gumbel'</code> are synonyms for the same distribution. Defaults to <code>\"norm\"</code>.</p> <code>'norm'</code> <p>Returns:</p> Name Type Description <code>statistic</code> <code>float</code> <p>The Anderson-Darling test statistic.</p> <code>critical_values</code> <code>list</code> <p>The critical values for this distribution.</p> <code>significance_level</code> <code>list</code> <p>The significance levels for the corresponding critical values in percents. The function returns critical values for a differing set of significance levels depending on the distribution that is being tested against.</p> <code>fit_result</code> <code>Any</code> <p>An object containing the results of fitting the distribution to the data. Note that the <code>FitResult</code> class was added to SciPy in version <code>1.10.0</code>. In the same release, this <code>anderson</code> function from SciPy had the outputs extended to include the <code>fit_result</code> object. The SciPy version <code>1.10.0</code> requires Python version <code>&gt;=3.8</code>.Therefore, when this function is executed on Python <code>3.7</code>, it will default to the highest compatible SciPy version, which is <code>1.7.0</code>. Hence, to ensure that this algorithm can still be used in Python <code>3.7</code>, the type of this output object is changed to <code>Any</code>, and is only returned when the Python version is <code>&gt;=3.8</code>.</p> Examples <p>Test the null hypothesis that a random sample was drawn from a normal distribution (with unspecified mean and standard deviation).</p> From the `scipy` docs<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.stats import anderson\n&gt;&gt;&gt; rng = np.random.default_rng()\n&gt;&gt;&gt; data = rng.random(size=35)\n&gt;&gt;&gt; res = anderson(data)\n&gt;&gt;&gt; res.statistic\n0.8398018749744764\n&gt;&gt;&gt; res.critical_values\narray([0.527, 0.6  , 0.719, 0.839, 0.998])\n&gt;&gt;&gt; res.significance_level\narray([15. , 10. ,  5. ,  2.5,  1. ])\n</code></pre> <p>The value of the statistic (barely) exceeds the critical value associated with a significance level of \\(2.5%\\), so the null hypothesis may be rejected at a significance level of \\(2.5%\\), but not at a significance level of \\(1%\\).</p> <p>Example one, using the <code>airline</code> data from the <code>sktime</code> package.</p> Python<pre><code>&gt;&gt;&gt; # Import packages\n&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; from scipy.stats import anderson\n\n&gt;&gt;&gt; # load the airline data\n&gt;&gt;&gt; airline_data = load_airline()\n\n&gt;&gt;&gt; # run Anderson-Darling test on the data\n&gt;&gt;&gt; result = anderson(airline_data)\n\n&gt;&gt;&gt; # print the results\n&gt;&gt;&gt; print(f\"Anderson-Darling test statistic: {result.statistic:.3f}\")\nAnderson-Darling test statistic: 3.089\n&gt;&gt;&gt; print(f\"Anderson-Darling test critical values: {result.critical_values}\")\nAnderson-Darling test critical values: [0.565 0.644 0.772 0.901 1.072]\n&gt;&gt;&gt; print(f\"Anderson-Darling test significance levels: {result.significance_level}\")\nAnderson-Darling test significance levels: [15.  10.   5.   2.5  1. ]\n\n&gt;&gt;&gt; # check if null hypothesis is rejected\n&gt;&gt;&gt; alpha = 0.05\n&gt;&gt;&gt; if result.statistic &gt; result.critical_values[2]:\n...     print(\"Reject null hypothesis that data is normally distributed\")\n... else:\n...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n...\nReject null hypothesis that data is normally distributed\n</code></pre> <p>The null hypothesis of Anderson-Darling test is that the data is normally distributed. In this case, the test statistic is 3.089, which is greater than the critical value at 5% significance level of 0.772, indicating that we reject the null hypothesis. Therefore, we can conclude that the airline data from the sktime library is not normally distributed.</p> <p>Example two, using the <code>sine</code> wave data generated from the <code>numpy</code> package.</p> Python<pre><code>&gt;&gt;&gt; # Import packages\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.stats import anderson\n\n&gt;&gt;&gt; # Generate sine wave data\n&gt;&gt;&gt; x = np.sin(np.linspace(0, 2 * np.pi, 100))\n\n&gt;&gt;&gt; # Perform Anderson-Darling test\n&gt;&gt;&gt; result = anderson(x)\n\n&gt;&gt;&gt; print(\"Statistic: %.3f\" % result.statistic)\nStatistic: 0.161\n&gt;&gt;&gt; for i in range(len(result.critical_values)):\n...     sl, cv = result.significance_level[i], result.critical_values[i]\n...     if result.statistic &lt; cv:\n...         print(\"%.3f: %.3f, data looks normal (fail to reject H0)\" % (sl, cv))\n...     else:\n...         print(\"%.3f: %.3f, data does not look normal (reject H0)\" % (sl, cv))\n...\n15.000: 0.561, data looks normal (fail to reject H0)\n10.000: 0.638, data looks normal (fail to reject H0)\n5.000: 0.765, data looks normal (fail to reject H0)\n2.500: 0.892, data looks normal (fail to reject H0)\n1.000: 1.061, data looks normal (fail to reject H0)\n</code></pre> <p>In this case, the Anderson-Darling test statistic is 0.161. The critical values and significance levels are also printed. The null hypothesis is that the data is drawn from a normal distribution. Based on the output, we can see that the statistic value is less than all of the critical values for the chosen significance levels, meaning we fail to reject the null hypothesis. Therefore, we can conclude that the sine wave data generated from NumPy is normally distributed.</p> <p>Example three, using the <code>FractionalGaussianNoise</code> random data generated from the <code>stochastic</code> package.</p> Python<pre><code>&gt;&gt;&gt; from stochastic.noise import FractionalGaussianNoise\n&gt;&gt;&gt; from scipy.stats import anderson\n\n&gt;&gt;&gt; # generate fractional Gaussian noise\n&gt;&gt;&gt; fgn = FractionalGaussianNoise(t=1000, hurst=0.5)\n\n&gt;&gt;&gt; # calculate the anderson-darling test\n&gt;&gt;&gt; result = anderson(fgn.fgn(), dist=\"norm\")\n\n&gt;&gt;&gt; # print the result\n&gt;&gt;&gt; print(\"Statistic: %.3f\" % result.statistic)\n&gt;&gt;&gt; p = 0\n&gt;&gt;&gt; for i in range(len(result.critical_values)):\n...     sl, cv = result.significance_level[i], result.critical_values[i]\n...     if result.statistic &lt; result.critical_values[i]:\n...         print(\"%.3f: %.3f, data looks normal (fail to reject H0)\" % (sl, cv))\n...     else:\n...         print(\"%.3f: %.3f, data does not look normal (reject H0)\" % (sl, cv))\n...\n</code></pre> <p>In this example, the test statistic is 1.171, and the critical values at different significance levels are 0.570, 0.648, 0.777, 0.906, and 1.076. Since the test statistic is greater than all of the critical values, we can reject the null hypothesis and conclude that the data is not normally distributed.</p> Notes <p>Critical values provided are for the following significance levels:</p> <ul> <li>normal/exponential<ul> <li>15%, 10%, 5%, 2.5%, 1%</li> </ul> </li> <li>logistic<ul> <li>25%, 10%, 5%, 2.5%, 1%, 0.5%</li> </ul> </li> <li>Gumbel<ul> <li>25%, 10%, 5%, 2.5%, 1%</li> </ul> </li> </ul> <p>If the returned statistic is larger than these critical values then for the corresponding significance level, the null hypothesis that the data come from the chosen distribution can be rejected. The returned statistic is referred to as 'A2' in the references.</p> <p>The Anderson-Darling test is a statistical test used to determine whether a dataset, including time series data, is normally distributed. The test is based on the deviations of the sample distribution from the theoretical normal distribution. The mathematical equation for the Anderson-Darling test is:</p> \\[ A^2 = -n - \\sum_{i=1}^{n} \\left( \\left( \\frac{2 \\times i - 1}{n} \\right) \\times (log(F(x_i)) + log(1-F(n-i+1))) \\right) \\] <p>where:</p> <ul> <li>\\(A^2\\) is the test statistic</li> <li>\\(n\\) is the sample size</li> <li>\\(F(x_i)\\) is the empirical distribution function of the sample at \\(x_i\\)</li> <li>\\(log\\) is a natural logarithm</li> </ul> <p>To calculate the Anderson-Darling test statistic for time series data, we need to perform the following steps:</p> <ol> <li> <p>Estimate the residuals of the forecasting model: The residuals are the difference between the actual values and the predicted values of the time series model.</p> </li> <li> <p>Calculate the sample mean and standard deviation of the residuals: These are the mean and standard deviation of the residuals, respectively.</p> </li> <li> <p>Standardize the residuals: The standardized residuals are the residuals divided by their sample standard deviation.</p> </li> <li> <p>Sort the standardized residuals in ascending order.</p> </li> <li> <p>Calculate the empirical distribution function (EDF) of the residuals: The EDF is the proportion of the standardized residuals that are less than or equal to a given value. It is calculated as:</p> \\[ F(x_i) = \\left( \\frac{1}{n} \\right) \\times \\sum_{j=1}^{n} \\left( I(x_i &lt;= x_j) \\right) \\] <p>where:</p> <ul> <li>\\(x_i\\) are the sorted standardized residuals,</li> <li>\\(x_j\\) are the sample values, and</li> <li>\\(I()\\) is the indicator function.</li> </ul> </li> <li> <p>Calculate the test statistic: The test statistic is calculated using the formula:</p> \\[ A^2 = -n - \\sum_{i=1}^{n} \\left( \\left( \\frac{2 \\times i - 1}{n} \\right) \\times (log(F(x_i)) + log(1-F(n-i+1))) \\right) \\] <p>where:</p> <ul> <li>\\(x_i\\) are the sorted standardized residuals.</li> </ul> </li> <li> <p>Compare the test statistic to a critical value: If the test statistic is less than the critical value, we cannot reject the null hypothesis of normality and can conclude that the residuals follow a normal distribution. If the test statistic is greater than the critical value, we reject the null hypothesis of normality and conclude that the residuals do not follow a normal distribution.</p> </li> </ol> <p>In summary, the Anderson-Darling test is a statistical test that evaluates normality of time series residuals by comparing the empirical distribution function of the sample data to the cumulative distribution function of the normal distribution. The test statistic is calculated by summing the product of weights and logarithms of the empirical distribution function and the complement of the normal distribution CDF. Finally, we compare the test statistic to a critical value to determine whether the residuals follow a normal distribution or not.</p> Credit <ul> <li>All credit goes to the <code>scipy</code> library.</li> </ul> References <ul> <li>https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm</li> <li>Stephens, M. A. (1974). EDF Statistics for Goodness of Fit and Some Comparisons, Journal of the American Statistical Association, Vol. 69, pp. 730-737.</li> <li>Stephens, M. A. (1976). Asymptotic Results for Goodness-of-Fit Statistics with Unknown Parameters, Annals of Statistics, Vol. 4, pp. 357-369.</li> <li>Stephens, M. A. (1977). Goodness of Fit for the Extreme Value Distribution, Biometrika, Vol. 64, pp. 583-588.</li> <li>Stephens, M. A. (1977). Goodness of Fit with Special Reference to Tests for Exponentiality , Technical Report No. 262, Department of Statistics, Stanford University, Stanford, CA.</li> <li>Stephens, M. A. (1979). Tests of Fit for the Logistic Distribution Based on the Empirical Distribution Function, Biometrika, Vol. 66, pp. 591-595.</li> </ul> See Also <ul> <li><code>jb()</code></li> <li><code>ob()</code></li> <li><code>sw()</code></li> <li><code>dp()</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/normality.py</code> <pre><code>@typechecked\ndef ad(\n    x: ArrayLike,\n    dist: VALID_AD_DIST_OPTIONS = \"norm\",\n) -&gt; AndersonResult:\n    \"\"\"\n    !!! note \"Summary\"\n        The Anderson-Darling test is a statistical test used to evaluate whether a dataset, including time series data, follows a normal distribution. In time series forecasting, the Anderson-Darling test can be used to assess whether the residuals of a model follow a normal distribution, which is an assumption of many statistical models.\n\n        The Anderson-Darling test tests the null hypothesis that a sample is drawn from a population that follows a particular distribution. For the Anderson-Darling test, the critical values depend on which distribution is being tested against. This function works for normal, exponential, logistic, or Gumbel (Extreme Value Type I) distributions.\n\n    ???+ abstract \"Details\"\n        The Anderson-Darling test is based on the null hypothesis that the residuals of the forecasting model are normally distributed. The test calculates a test statistic that measures the distance between the observed distribution of the residuals and the expected distribution under the null hypothesis of normality. If the observed distribution of the residuals deviates significantly from the expected distribution under normality, the test rejects the null hypothesis and concludes that the residuals do not follow a normal distribution.\n\n        To apply the Anderson-Darling test to time series data, we first need to estimate the residuals of the forecasting model. We can then use a statistical software package to perform the test on the residuals. The test produces a p-value, which indicates the probability of observing the observed distribution of the residuals if the null hypothesis of normality is true. If the p-value is less than the significance level (usually $0.05$), we can conclude that the residuals do not follow a normal distribution.\n\n        The Anderson-Darling test is more sensitive to deviations from normality in the tails of the distribution than other tests, such as the Shapiro-Wilk test. This makes it a useful test when assessing whether the residuals of a time series forecasting model exhibit heavy tails or other non-normal features.\n\n        If the Anderson-Darling test indicates that the residuals do not follow a normal distribution, we may need to consider using a different modeling approach or modifying the forecasting model. It is important to ensure that the residuals of a time series forecasting model follow a normal distribution to ensure that the model is valid and reliable for making predictions.\n\n    Params:\n        x (ArrayLike):\n            Array of sample data.\n        dist (VALID_AD_DIST_OPTIONS):\n            The type of distribution to test against. The default is `'norm'`. The names `'extreme1'`, `'gumbel_l'` and `'gumbel'` are synonyms for the same distribution.&lt;br&gt;\n            Defaults to `\"norm\"`.\n\n    Returns:\n        statistic (float):\n            The Anderson-Darling test statistic.\n        critical_values (list):\n            The critical values for this distribution.\n        significance_level (list):\n            The significance levels for the corresponding critical values in percents. The function returns critical values for a differing set of significance levels depending on the distribution that is being tested against.\n        fit_result (Any):\n            An object containing the results of fitting the distribution to the data.\n            Note that the [`FitResult`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats._result_classes.FitResult.html) class was added to SciPy in version `1.10.0`. In the same release, this `anderson` function from SciPy had the outputs extended to include the `fit_result` object. The SciPy version `1.10.0` requires Python version `&gt;=3.8`.&lt;br&gt;&lt;br&gt;Therefore, when this function is executed on Python `3.7`, it will default to the highest compatible SciPy version, which is `1.7.0`. Hence, to ensure that this algorithm can still be used in Python `3.7`, the type of this output object is changed to `Any`, and is only returned when the Python version is `&gt;=3.8`.\n\n    ???+ example \"Examples\"\n\n        Test the null hypothesis that a random sample was drawn from a normal distribution (with unspecified mean and standard deviation).\n\n        ```pycon {.py .python linenums=\"1\" title=\"From the `scipy` docs\"}\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from scipy.stats import anderson\n        &gt;&gt;&gt; rng = np.random.default_rng()\n        &gt;&gt;&gt; data = rng.random(size=35)\n        &gt;&gt;&gt; res = anderson(data)\n        &gt;&gt;&gt; res.statistic\n        0.8398018749744764\n        &gt;&gt;&gt; res.critical_values\n        array([0.527, 0.6  , 0.719, 0.839, 0.998])\n        &gt;&gt;&gt; res.significance_level\n        array([15. , 10. ,  5. ,  2.5,  1. ])\n        ```\n\n        The value of the statistic (barely) exceeds the critical value associated with a significance level of $2.5%$, so the null hypothesis may be rejected at a significance level of $2.5%$, but not at a significance level of $1%$.\n\n        ---\n\n        Example one, using the `airline` data from the `sktime` package.\n\n        ```pycon {.py .python linenums=\"1\" title=\"Python\"}\n        &gt;&gt;&gt; # Import packages\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; from scipy.stats import anderson\n\n        &gt;&gt;&gt; # load the airline data\n        &gt;&gt;&gt; airline_data = load_airline()\n\n        &gt;&gt;&gt; # run Anderson-Darling test on the data\n        &gt;&gt;&gt; result = anderson(airline_data)\n\n        &gt;&gt;&gt; # print the results\n        &gt;&gt;&gt; print(f\"Anderson-Darling test statistic: {result.statistic:.3f}\")\n        Anderson-Darling test statistic: 3.089\n        &gt;&gt;&gt; print(f\"Anderson-Darling test critical values: {result.critical_values}\")\n        Anderson-Darling test critical values: [0.565 0.644 0.772 0.901 1.072]\n        &gt;&gt;&gt; print(f\"Anderson-Darling test significance levels: {result.significance_level}\")\n        Anderson-Darling test significance levels: [15.  10.   5.   2.5  1. ]\n\n        &gt;&gt;&gt; # check if null hypothesis is rejected\n        &gt;&gt;&gt; alpha = 0.05\n        &gt;&gt;&gt; if result.statistic &gt; result.critical_values[2]:\n        ...     print(\"Reject null hypothesis that data is normally distributed\")\n        ... else:\n        ...     print(\"Fail to reject null hypothesis that data is normally distributed\")\n        ...\n        Reject null hypothesis that data is normally distributed\n        ```\n\n        The null hypothesis of Anderson-Darling test is that the data is normally distributed. In this case, the test statistic is 3.089, which is greater than the critical value at 5% significance level of 0.772, indicating that we reject the null hypothesis. Therefore, we can conclude that the airline data from the sktime library is not normally distributed.\n\n        ---\n\n        Example two, using the `sine` wave data generated from the `numpy` package.\n\n        ```pycon {.py .python linenums=\"1\" title=\"Python\"}\n        &gt;&gt;&gt; # Import packages\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from scipy.stats import anderson\n\n        &gt;&gt;&gt; # Generate sine wave data\n        &gt;&gt;&gt; x = np.sin(np.linspace(0, 2 * np.pi, 100))\n\n        &gt;&gt;&gt; # Perform Anderson-Darling test\n        &gt;&gt;&gt; result = anderson(x)\n\n        &gt;&gt;&gt; print(\"Statistic: %.3f\" % result.statistic)\n        Statistic: 0.161\n        &gt;&gt;&gt; for i in range(len(result.critical_values)):\n        ...     sl, cv = result.significance_level[i], result.critical_values[i]\n        ...     if result.statistic &lt; cv:\n        ...         print(\"%.3f: %.3f, data looks normal (fail to reject H0)\" % (sl, cv))\n        ...     else:\n        ...         print(\"%.3f: %.3f, data does not look normal (reject H0)\" % (sl, cv))\n        ...\n        15.000: 0.561, data looks normal (fail to reject H0)\n        10.000: 0.638, data looks normal (fail to reject H0)\n        5.000: 0.765, data looks normal (fail to reject H0)\n        2.500: 0.892, data looks normal (fail to reject H0)\n        1.000: 1.061, data looks normal (fail to reject H0)\n        ```\n\n        In this case, the Anderson-Darling test statistic is 0.161. The critical values and significance levels are also printed. The null hypothesis is that the data is drawn from a normal distribution. Based on the output, we can see that the statistic value is less than all of the critical values for the chosen significance levels, meaning we fail to reject the null hypothesis. Therefore, we can conclude that the sine wave data generated from NumPy is normally distributed.\n\n        ---\n\n        Example three, using the `FractionalGaussianNoise` random data generated from the `stochastic` package.\n\n        ```pycon {.py .python linenums=\"1\" title=\"Python\"}\n        &gt;&gt;&gt; from stochastic.noise import FractionalGaussianNoise\n        &gt;&gt;&gt; from scipy.stats import anderson\n\n        &gt;&gt;&gt; # generate fractional Gaussian noise\n        &gt;&gt;&gt; fgn = FractionalGaussianNoise(t=1000, hurst=0.5)\n\n        &gt;&gt;&gt; # calculate the anderson-darling test\n        &gt;&gt;&gt; result = anderson(fgn.fgn(), dist=\"norm\")\n\n        &gt;&gt;&gt; # print the result\n        &gt;&gt;&gt; print(\"Statistic: %.3f\" % result.statistic)\n        &gt;&gt;&gt; p = 0\n        &gt;&gt;&gt; for i in range(len(result.critical_values)):\n        ...     sl, cv = result.significance_level[i], result.critical_values[i]\n        ...     if result.statistic &lt; result.critical_values[i]:\n        ...         print(\"%.3f: %.3f, data looks normal (fail to reject H0)\" % (sl, cv))\n        ...     else:\n        ...         print(\"%.3f: %.3f, data does not look normal (reject H0)\" % (sl, cv))\n        ...\n        ```\n\n        In this example, the test statistic is 1.171, and the critical values at different significance levels are 0.570, 0.648, 0.777, 0.906, and 1.076. Since the test statistic is greater than all of the critical values, we can reject the null hypothesis and conclude that the data is not normally distributed.\n\n    ??? note \"Notes\"\n        Critical values provided are for the following significance levels:\n\n        - normal/exponential\n            - 15%, 10%, 5%, 2.5%, 1%\n        - logistic\n            - 25%, 10%, 5%, 2.5%, 1%, 0.5%\n        - Gumbel\n            - 25%, 10%, 5%, 2.5%, 1%\n\n        If the returned statistic is larger than these critical values then for the corresponding significance level, the null hypothesis that the data come from the chosen distribution can be rejected. The returned statistic is referred to as 'A2' in the references.\n\n        The Anderson-Darling test is a statistical test used to determine whether a dataset, including time series data, is normally distributed. The test is based on the deviations of the sample distribution from the theoretical normal distribution. The mathematical equation for the Anderson-Darling test is:\n\n        $$\n        A^2 = -n - \\\\sum_{i=1}^{n} \\\\left( \\\\left( \\\\frac{2 \\\\times i - 1}{n} \\\\right) \\\\times (log(F(x_i)) + log(1-F(n-i+1))) \\\\right)\n        $$\n\n        where:\n\n        - $A^2$ is the test statistic\n        - $n$ is the sample size\n        - $F(x_i)$ is the empirical distribution function of the sample at $x_i$\n        - $log$ is a natural logarithm\n\n        To calculate the Anderson-Darling test statistic for time series data, we need to perform the following steps:\n\n        1. Estimate the residuals of the forecasting model: The residuals are the difference between the actual values and the predicted values of the time series model.\n\n        1. Calculate the sample mean and standard deviation of the residuals: These are the mean and standard deviation of the residuals, respectively.\n\n        1. Standardize the residuals: The standardized residuals are the residuals divided by their sample standard deviation.\n\n        1. Sort the standardized residuals in ascending order.\n\n        1. Calculate the empirical distribution function (EDF) of the residuals: The EDF is the proportion of the standardized residuals that are less than or equal to a given value. It is calculated as:\n\n            $$\n            F(x_i) = \\\\left( \\\\frac{1}{n} \\\\right) \\\\times \\\\sum_{j=1}^{n} \\\\left( I(x_i &lt;= x_j) \\\\right)\n            $$\n\n            where:\n\n            - $x_i$ are the sorted standardized residuals,\n            - $x_j$ are the sample values, and\n            - $I()$ is the indicator function.\n\n        1. Calculate the test statistic: The test statistic is calculated using the formula:\n\n            $$\n            A^2 = -n - \\\\sum_{i=1}^{n} \\\\left( \\\\left( \\\\frac{2 \\\\times i - 1}{n} \\\\right) \\\\times (log(F(x_i)) + log(1-F(n-i+1))) \\\\right)\n            $$\n\n            where:\n\n            - $x_i$ are the sorted standardized residuals.\n\n        1. Compare the test statistic to a critical value: If the test statistic is less than the critical value, we cannot reject the null hypothesis of normality and can conclude that the residuals follow a normal distribution. If the test statistic is greater than the critical value, we reject the null hypothesis of normality and conclude that the residuals do not follow a normal distribution.\n\n        In summary, the Anderson-Darling test is a statistical test that evaluates normality of time series residuals by comparing the empirical distribution function of the sample data to the cumulative distribution function of the normal distribution. The test statistic is calculated by summing the product of weights and logarithms of the empirical distribution function and the complement of the normal distribution CDF. Finally, we compare the test statistic to a critical value to determine whether the residuals follow a normal distribution or not.\n\n    ??? success \"Credit\"\n        - All credit goes to the [`scipy`](https://docs.scipy.org/) library.\n\n    ??? question \"References\"\n        - https://www.itl.nist.gov/div898/handbook/prc/section2/prc213.htm\n        - Stephens, M. A. (1974). EDF Statistics for Goodness of Fit and Some Comparisons, Journal of the American Statistical Association, Vol. 69, pp. 730-737.\n        - Stephens, M. A. (1976). Asymptotic Results for Goodness-of-Fit Statistics with Unknown Parameters, Annals of Statistics, Vol. 4, pp. 357-369.\n        - Stephens, M. A. (1977). Goodness of Fit for the Extreme Value Distribution, Biometrika, Vol. 64, pp. 583-588.\n        - Stephens, M. A. (1977). Goodness of Fit with Special Reference to Tests for Exponentiality , Technical Report No. 262, Department of Statistics, Stanford University, Stanford, CA.\n        - Stephens, M. A. (1979). Tests of Fit for the Logistic Distribution Based on the Empirical Distribution Function, Biometrika, Vol. 66, pp. 591-595.\n\n    ??? tip \"See Also\"\n        - [`jb()`][ts_stat_tests.algorithms.normality.jb]\n        - [`ob()`][ts_stat_tests.algorithms.normality.ob]\n        - [`sw()`][ts_stat_tests.algorithms.normality.sw]\n        - [`dp()`][ts_stat_tests.algorithms.normality.dp]\n    \"\"\"\n    return _ad(x=x, dist=dist)\n</code></pre>"},{"location":"code/regularity/","title":"Test the <code>regularity</code> of a given Time-Series Dataset","text":""},{"location":"code/regularity/#introduction","title":"Introduction","text":"<p>Summary</p> <p>As stated by Selva Prabhakaran:</p> <p>The more regular and repeatable patterns a time series has, the easier it is to forecast.</p> <p>The 'Approximate Entropy' algorithm can be used to quantify the regularity and unpredictability of fluctuations in a time series.</p> <p>The higher the approximate entropy, the more difficult it is to forecast it.</p> <p>Another better alternate is the 'Sample Entropy'.</p> <p>Sample Entropy is similar to approximate entropy but is more consistent in estimating the complexity even for smaller time series.</p> <p>For example, a random time series with fewer data points can have a lower 'approximate entropy' than a more 'regular' time series, whereas, a longer random time series will have a higher 'approximate entropy'.</p> <p> For more info, see: Time Series Analysis in Python: A Comprehensive Guide with Examples.</p> <p>Info</p> <p>To state that the data is 'regular' is to say that the data points are evenly spaced, regularly collected, and not missing data points (ie. do not contain excessive <code>NA</code> values). Logically, it is not always necessary to conduct the Test for Regularity on automatically collected data (like for example with Energy Prices, or Daily Temperature), however if this data was collected manually then it is highly recommended. If the data does not meet the requirements of Regularity, then it is necessary to return to the data collection plan, and revise the methodology used.</p> <p> For more info, see: The Future of Australian Energy Prices: Time-Series Analysis of Historic Prices and Forecast for Future Prices.</p> <p>Source Library</p> <p>The <code>AntroPy</code> package was chosen because it provides well-tested and efficient implementations of approximate entropy, sample entropy, and related complexity measures for time-series data, is built on top of the scientific Python stack (NumPy/SciPy), and is actively maintained and open source, making it a reliable choice for reproducible statistical analysis.</p> <p>Source Module</p> <p>All of the source code can be found within the modules:</p> <ul> <li><code>ts_stat_tests.algorithms.regularity</code>.</li> <li><code>ts_stat_tests.tests.regularity</code>.</li> </ul>"},{"location":"code/regularity/#regularity-tests","title":"Regularity Tests","text":""},{"location":"code/regularity/#ts_stat_tests.tests.regularity","title":"ts_stat_tests.tests.regularity","text":"<p>Summary</p> <p>This module contains convenience functions and tests for regularity measures, allowing for easy access to different entropy algorithms.</p>"},{"location":"code/regularity/#ts_stat_tests.tests.regularity.entropy","title":"entropy","text":"<pre><code>entropy(\n    x: ArrayLike,\n    algorithm: str = \"sample\",\n    order: int = 2,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n    sf: float = 1,\n    normalize: bool = True,\n) -&gt; float\n</code></pre> <p>Summary</p> <p>Test for the entropy of a given data set.</p> Details <p>This function is a convenience wrapper around the five underlying algorithms: - <code>approx_entropy()</code> - <code>sample_entropy()</code> - <code>spectral_entropy()</code> - <code>permutation_entropy()</code> - <code>svd_entropy()</code></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked. Should be a <code>1-D</code> or <code>N-D</code> data array.</p> required <code>algorithm</code> <code>str</code> <p>Which entropy algorithm to use. - <code>sample_entropy()</code>: <code>[\"sample\", \"sampl\", \"samp\"]</code> - <code>approx_entropy()</code>: <code>[\"app\", \"approx\"]</code> - <code>spectral_entropy()</code>: <code>[\"spec\", \"spect\", \"spectral\"]</code> - <code>permutation_entropy()</code>: <code>[\"perm\", \"permutation\"]</code> - <code>svd_entropy()</code>: <code>[\"svd\", \"svd_entropy\"]</code> Defaults to <code>\"sample\"</code>.</p> <code>'sample'</code> <code>order</code> <code>int</code> <p>Embedding dimension. Only relevant when <code>algorithm=sample</code> or <code>algorithm=approx</code>. Defaults to <code>2</code>.</p> <code>2</code> <code>metric</code> <code>VALID_KDTREE_METRIC_OPTIONS</code> <p>Name of the distance metric function used with <code>sklearn.neighbors.KDTree</code>. Default is to use the Chebyshev distance. Only relevant when <code>algorithm=sample</code> or <code>algorithm=approx</code>. Defaults to <code>\"chebyshev\"</code>.</p> <code>'chebyshev'</code> <code>sf</code> <code>float</code> <p>Sampling frequency, in Hz. Only relevant when <code>algorithm=spectral</code>. Defaults to <code>1</code>.</p> <code>1</code> <code>normalize</code> <code>bool</code> <p>If <code>True</code>, divide by \\(log2(psd.size)\\) to normalize the spectral entropy to be between \\(0\\) and \\(1\\). Otherwise, return the spectral entropy in bit. Only relevant when <code>algorithm=spectral</code>. Defaults to <code>True</code>.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the given value for <code>algorithm</code> is not valid.</p> <p>Returns:</p> Type Description <code>float</code> <p>The Entropy value.</p> <p>Credit</p> <p>All credit goes to the <code>AntroPy</code> library.</p> Examples <p><code>approx_entropy</code>: Basic usage<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; entropy(x=data, algorithm=\"approx\")\n0.6451264780416452\n</code></pre></p> <p><code>sample_entropy</code>: Basic usage<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; entropy(x=data, algorithm=\"sample\")\n0.6177074729583698\n</code></pre></p> <p><code>spectral_entropy</code>: Basic usage<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; entropy(x=data, algorithm=\"spectral\", sf=1)\n2.6538040647031726\n</code></pre></p> Advanced usage<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; spectral_entropy(data, 2, \"welch\", normalize=True)\n0.3371369604224553\n</code></pre> References <ul> <li>Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.</li> <li>https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html</li> <li>Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.</li> <li>https://en.wikipedia.org/wiki/Spectral_density</li> <li>https://en.wikipedia.org/wiki/Welch%27s_method</li> </ul> See Also <ul> <li><code>regularity()</code></li> <li><code>approx_entropy()</code></li> <li><code>sample_entropy()</code></li> <li><code>spectral_entropy()</code></li> <li><code>permutation_entropy()</code></li> <li><code>svd_entropy()</code></li> </ul> Source code in <code>src/ts_stat_tests/tests/regularity.py</code> <pre><code>@typechecked\ndef entropy(\n    x: ArrayLike,\n    algorithm: str = \"sample\",\n    order: int = 2,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n    sf: float = 1,\n    normalize: bool = True,\n) -&gt; float:\n    \"\"\"\n    !!! note \"Summary\"\n        Test for the entropy of a given data set.\n\n    ???+ abstract \"Details\"\n        This function is a convenience wrapper around the five underlying algorithms:&lt;br&gt;\n        - [`approx_entropy()`][ts_stat_tests.algorithms.regularity.approx_entropy]&lt;br&gt;\n        - [`sample_entropy()`][ts_stat_tests.algorithms.regularity.sample_entropy]&lt;br&gt;\n        - [`spectral_entropy()`][ts_stat_tests.algorithms.regularity.spectral_entropy]&lt;br&gt;\n        - [`permutation_entropy()`][ts_stat_tests.algorithms.regularity.permutation_entropy]&lt;br&gt;\n        - [`svd_entropy()`][ts_stat_tests.algorithms.regularity.svd_entropy]\n\n    Params:\n        x (ArrayLike):\n            The data to be checked. Should be a `1-D` or `N-D` data array.\n        algorithm (str, optional):\n            Which entropy algorithm to use.&lt;br&gt;\n            - `sample_entropy()`: `[\"sample\", \"sampl\", \"samp\"]`&lt;br&gt;\n            - `approx_entropy()`: `[\"app\", \"approx\"]`&lt;br&gt;\n            - `spectral_entropy()`: `[\"spec\", \"spect\", \"spectral\"]`&lt;br&gt;\n            - `permutation_entropy()`: `[\"perm\", \"permutation\"]`&lt;br&gt;\n            - `svd_entropy()`: `[\"svd\", \"svd_entropy\"]`&lt;br&gt;\n            Defaults to `\"sample\"`.\n        order (int, optional):\n            Embedding dimension.&lt;br&gt;\n            Only relevant when `algorithm=sample` or `algorithm=approx`.&lt;br&gt;\n            Defaults to `2`.\n        metric (VALID_KDTREE_METRIC_OPTIONS):\n            Name of the distance metric function used with [`sklearn.neighbors.KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree). Default is to use the [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance).&lt;br&gt;\n            Only relevant when `algorithm=sample` or `algorithm=approx`.&lt;br&gt;\n            Defaults to `\"chebyshev\"`.\n        sf (float, optional):\n            Sampling frequency, in Hz.&lt;br&gt;\n            Only relevant when `algorithm=spectral`.&lt;br&gt;\n            Defaults to `1`.\n        normalize (bool, optional):\n            If `True`, divide by $log2(psd.size)$ to normalize the spectral entropy to be between $0$ and $1$. Otherwise, return the spectral entropy in bit.&lt;br&gt;\n            Only relevant when `algorithm=spectral`.&lt;br&gt;\n            Defaults to `True`.\n\n    Raises:\n        ValueError: When the given value for `algorithm` is not valid.\n\n    Returns:\n        (float):\n            The Entropy value.\n\n    !!! Success \"Credit\"\n        All credit goes to the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ???+ Example \"Examples\"\n\n        `approx_entropy`:\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; entropy(x=data, algorithm=\"approx\")\n        0.6451264780416452\n        ```\n\n        ---\n\n        `sample_entropy`:\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; entropy(x=data, algorithm=\"sample\")\n        0.6177074729583698\n        ```\n\n        ---\n\n        `spectral_entropy`:\n        ```pycon {.py .python linenums=\"1\"  title=\"Basic usage\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; entropy(x=data, algorithm=\"spectral\", sf=1)\n        2.6538040647031726\n        ```\n\n        ```pycon {.py .python linenums=\"1\"  title=\"Advanced usage\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; spectral_entropy(data, 2, \"welch\", normalize=True)\n        0.3371369604224553\n        ```\n\n    ??? Question \"References\"\n        - Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n        - https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n        - Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.\n        - https://en.wikipedia.org/wiki/Spectral_density\n        - https://en.wikipedia.org/wiki/Welch%27s_method\n\n    ??? Tip \"See Also\"\n        - [`regularity()`][ts_stat_tests.tests.regularity.regularity]\n        - [`approx_entropy()`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`sample_entropy()`][ts_stat_tests.algorithms.regularity.sample_entropy]\n        - [`spectral_entropy()`][ts_stat_tests.algorithms.regularity.spectral_entropy]\n        - [`permutation_entropy()`][ts_stat_tests.algorithms.regularity.permutation_entropy]\n        - [`svd_entropy()`][ts_stat_tests.algorithms.regularity.svd_entropy]\n    \"\"\"\n    options: dict[str, tuple[str, ...]] = {\n        \"sampl\": (\"sample\", \"sampl\", \"samp\"),\n        \"approx\": (\"app\", \"approx\"),\n        \"spect\": (\"spec\", \"spect\", \"spectral\"),\n        \"perm\": (\"perm\", \"permutation\"),\n        \"svd\": (\"svd\", \"svd_entropy\"),\n    }\n    if algorithm in options[\"sampl\"]:\n        return sample_entropy(x=x, order=order, metric=metric)\n    if algorithm in options[\"approx\"]:\n        return approx_entropy(x=x, order=order, metric=metric)\n    if algorithm in options[\"spect\"]:\n        return cast(float, spectral_entropy(x=x, sf=sf, normalize=normalize))\n    if algorithm in options[\"perm\"]:\n        return permutation_entropy(x=x, order=order, normalize=normalize)\n    if algorithm in options[\"svd\"]:\n        return svd_entropy(x=x, order=order, normalize=normalize)\n    raise ValueError(\n        generate_error_message(\n            parameter_name=\"algorithm\",\n            value_parsed=algorithm,\n            options=options,\n        )\n    )\n</code></pre>"},{"location":"code/regularity/#ts_stat_tests.tests.regularity.regularity","title":"regularity","text":"<pre><code>regularity(\n    x: ArrayLike,\n    algorithm: str = \"sample\",\n    order: int = 2,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n    sf: float = 1,\n    normalize: bool = True,\n) -&gt; float\n</code></pre> <p>Summary</p> <p>Test for the regularity of a given data set.</p> Details <p>This is a pass-through, convenience wrapper around the <code>entropy()</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked. Should be a <code>1-D</code> or <code>N-D</code> data array.</p> required <code>algorithm</code> <code>str</code> <p>Which entropy algorithm to use. - <code>sample_entropy()</code>: <code>[\"sample\", \"sampl\", \"samp\"]</code> - <code>approx_entropy()</code>: <code>[\"app\", \"approx\"]</code> - <code>spectral_entropy()</code>: <code>[\"spec\", \"spect\", \"spectral\"]</code> - <code>permutation_entropy()</code>: <code>[\"perm\", \"permutation\"]</code> - <code>svd_entropy()</code>: <code>[\"svd\", \"svd_entropy\"]</code> Defaults to <code>\"sample\"</code>.</p> <code>'sample'</code> <code>order</code> <code>int</code> <p>Embedding dimension. Only relevant when <code>algorithm=sample</code> or <code>algorithm=approx</code>. Defaults to <code>2</code>.</p> <code>2</code> <code>metric</code> <code>VALID_KDTREE_METRIC_OPTIONS</code> <p>Name of the distance metric function used with <code>sklearn.neighbors.KDTree</code>. Default is to use the Chebyshev distance. Only relevant when <code>algorithm=sample</code> or <code>algorithm=approx</code>. Defaults to <code>\"chebyshev\"</code>.</p> <code>'chebyshev'</code> <code>sf</code> <code>float</code> <p>Sampling frequency, in Hz. Only relevant when <code>algorithm=spectral</code>. Defaults to <code>1</code>.</p> <code>1</code> <code>normalize</code> <code>bool</code> <p>If <code>True</code>, divide by \\(log2(psd.size)\\) to normalize the spectral entropy to be between \\(0\\) and \\(1\\). Otherwise, return the spectral entropy in bit. Only relevant when <code>algorithm=spectral</code>. Defaults to <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>float</code> <p>The Regularity value.</p> <p>Credit</p> <p>All credit goes to the <code>AntroPy</code> library.</p> Examples <p><code>regularity</code> with <code>approx_entropy</code> algorithm: Basic usage<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; from ts_stat_tests.tests.regularity import regularity\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; regularity(x=data, algorithm=\"approx_entropy\")\n0.6451264780416452\n</code></pre></p> <p><code>regularity</code> with <code>sample_entropy</code> algorithm: Basic usage<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; from ts_stat_tests.tests.regularity import regularity\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; regularity(x=data, algorithm=\"sample_entropy\")\n0.6177074729583698\n</code></pre></p> <p><code>regularity</code> with <code>spectral_entropy</code> algorithm: Basic usage<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; from ts_stat_tests.tests.regularity import regularity\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; regularity(x=data, algorithm=\"spectral_entropy\", sf=1)\n2.6538040647031726\n</code></pre></p> Advanced usage<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; from ts_stat_tests.tests.regularity import regularity\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; regularity(data, algorithm=\"spectral_entropy\", sf=2, method=\"welch\", normalize=True)\n0.3371369604224553\n</code></pre> References <ul> <li>Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.</li> <li>https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html</li> <li>Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.</li> <li>https://en.wikipedia.org/wiki/Spectral_density</li> <li>https://en.wikipedia.org/wiki/Welch%27s_method</li> </ul> See Also <ul> <li><code>entropy()</code></li> <li><code>approx_entropy()</code></li> <li><code>sample_entropy()</code></li> <li><code>spectral_entropy()</code></li> <li><code>permutation_entropy()</code></li> <li><code>svd_entropy()</code></li> </ul> Source code in <code>src/ts_stat_tests/tests/regularity.py</code> <pre><code>@typechecked\ndef regularity(\n    x: ArrayLike,\n    algorithm: str = \"sample\",\n    order: int = 2,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n    sf: float = 1,\n    normalize: bool = True,\n) -&gt; float:\n    \"\"\"\n    !!! note \"Summary\"\n        Test for the regularity of a given data set.\n\n    ???+ abstract \"Details\"\n        This is a pass-through, convenience wrapper around the [`entropy()`][ts_stat_tests.tests.regularity.entropy] function.\n\n    Params:\n        x (ArrayLike):\n            The data to be checked. Should be a `1-D` or `N-D` data array.\n        algorithm (str, optional):\n            Which entropy algorithm to use.&lt;br&gt;\n            - `sample_entropy()`: `[\"sample\", \"sampl\", \"samp\"]`&lt;br&gt;\n            - `approx_entropy()`: `[\"app\", \"approx\"]`&lt;br&gt;\n            - `spectral_entropy()`: `[\"spec\", \"spect\", \"spectral\"]`&lt;br&gt;\n            - `permutation_entropy()`: `[\"perm\", \"permutation\"]`&lt;br&gt;\n            - `svd_entropy()`: `[\"svd\", \"svd_entropy\"]`&lt;br&gt;\n            Defaults to `\"sample\"`.\n        order (int, optional):\n            Embedding dimension.&lt;br&gt;\n            Only relevant when `algorithm=sample` or `algorithm=approx`.&lt;br&gt;\n            Defaults to `2`.\n        metric (VALID_KDTREE_METRIC_OPTIONS):\n            Name of the distance metric function used with [`sklearn.neighbors.KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree). Default is to use the [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance).&lt;br&gt;\n            Only relevant when `algorithm=sample` or `algorithm=approx`.&lt;br&gt;\n            Defaults to `\"chebyshev\"`.\n        sf (float, optional):\n            Sampling frequency, in Hz.&lt;br&gt;\n            Only relevant when `algorithm=spectral`.&lt;br&gt;\n            Defaults to `1`.\n        normalize (bool, optional):\n            If `True`, divide by $log2(psd.size)$ to normalize the spectral entropy to be between $0$ and $1$. Otherwise, return the spectral entropy in bit.&lt;br&gt;\n            Only relevant when `algorithm=spectral`.&lt;br&gt;\n            Defaults to `True`.\n\n    Returns:\n        (float):\n            The Regularity value.\n\n    !!! Success \"Credit\"\n        All credit goes to the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ???+ Example \"Examples\"\n\n        `regularity` with `approx_entropy` algorithm:\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.tests.regularity import regularity\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; regularity(x=data, algorithm=\"approx_entropy\")\n        0.6451264780416452\n        ```\n\n        ---\n\n        `regularity` with `sample_entropy` algorithm:\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.tests.regularity import regularity\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; regularity(x=data, algorithm=\"sample_entropy\")\n        0.6177074729583698\n        ```\n\n        ---\n\n        `regularity` with `spectral_entropy` algorithm:\n        ```pycon {.py .python linenums=\"1\"  title=\"Basic usage\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.tests.regularity import regularity\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; regularity(x=data, algorithm=\"spectral_entropy\", sf=1)\n        2.6538040647031726\n        ```\n\n        ```pycon {.py .python linenums=\"1\"  title=\"Advanced usage\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.tests.regularity import regularity\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; regularity(data, algorithm=\"spectral_entropy\", sf=2, method=\"welch\", normalize=True)\n        0.3371369604224553\n        ```\n\n    ??? Question \"References\"\n        - Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n        - https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n        - Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.\n        - https://en.wikipedia.org/wiki/Spectral_density\n        - https://en.wikipedia.org/wiki/Welch%27s_method\n\n    ??? Tip \"See Also\"\n        - [`entropy()`][ts_stat_tests.tests.regularity.entropy]\n        - [`approx_entropy()`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`sample_entropy()`][ts_stat_tests.algorithms.regularity.sample_entropy]\n        - [`spectral_entropy()`][ts_stat_tests.algorithms.regularity.spectral_entropy]\n        - [`permutation_entropy()`][ts_stat_tests.algorithms.regularity.permutation_entropy]\n        - [`svd_entropy()`][ts_stat_tests.algorithms.regularity.svd_entropy]\n    \"\"\"\n    return entropy(x=x, algorithm=algorithm, order=order, metric=metric, sf=sf, normalize=normalize)\n</code></pre>"},{"location":"code/regularity/#ts_stat_tests.tests.regularity.is_regular","title":"is_regular","text":"<pre><code>is_regular(\n    x: ArrayLike,\n    algorithm: str = \"sample\",\n    order: int = 2,\n    sf: float = 1,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n    normalize: bool = True,\n    tolerance: Union[str, float, int, None] = \"default\",\n) -&gt; dict[str, Union[str, float, bool]]\n</code></pre> <p>Summary</p> <p>Test whether a given data set is <code>regular</code> or not.</p> Details <p>This function implements the given algorithm (defined in the parameter <code>algorithm</code>), and returns a dictionary containing the relevant data: <pre><code>{\n    \"result\": ...,  # The result of the test. Will be `True` if `entropy&lt;tolerance`, and `False` otherwise\n    \"entropy\": ...,  # A `float` value, the result of the `entropy()` function\n    \"tolerance\": ...,  # A `float` value, which is the tolerance used for determining whether or not the `entropy` is `regular` or not\n}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked. Should be a <code>1-D</code> or <code>N-D</code> data array.</p> required <code>algorithm</code> <code>str</code> <p>Which entropy algorithm to use. - <code>sample_entropy()</code>: <code>[\"sample\", \"sampl\", \"samp\"]</code> - <code>approx_entropy()</code>: <code>[\"app\", \"approx\"]</code> - <code>spectral_entropy()</code>: <code>[\"spec\", \"spect\", \"spectral\"]</code> - <code>permutation_entropy()</code>: <code>[\"perm\", \"permutation\"]</code> - <code>svd_entropy()</code>: <code>[\"svd\", \"svd_entropy\"]</code> Defaults to <code>\"sample\"</code>.</p> <code>'sample'</code> <code>order</code> <code>int</code> <p>Embedding dimension. Only relevant when <code>algorithm=sample</code> or <code>algorithm=approx</code>. Defaults to <code>2</code>.</p> <code>2</code> <code>metric</code> <code>VALID_KDTREE_METRIC_OPTIONS</code> <p>Name of the distance metric function used with <code>sklearn.neighbors.KDTree</code>. Default is to use the Chebyshev distance. Only relevant when <code>algorithm=sample</code> or <code>algorithm=approx</code>. Defaults to <code>\"chebyshev\"</code>.</p> <code>'chebyshev'</code> <code>sf</code> <code>float</code> <p>Sampling frequency, in Hz. Only relevant when <code>algorithm=spectral</code>. Defaults to <code>1</code>.</p> <code>1</code> <code>normalize</code> <code>bool</code> <p>If <code>True</code>, divide by \\(log2(psd.size)\\) to normalize the spectral entropy to be between \\(0\\) and \\(1\\). Otherwise, return the spectral entropy in bit. Only relevant when <code>algorithm=spectral</code>. Defaults to <code>True</code>.</p> <code>True</code> <code>tolerance</code> <code>Union[str, float, int, None]</code> <p>The tolerance value used to determine whether or not the result is <code>regular</code> or not. - If <code>tolerance</code> is either type <code>int</code> or <code>float</code>, then this value will be used. - If <code>tolerance</code> is either <code>\"default\"</code> or <code>None</code>, then <code>tolerance</code> will be derived from <code>x</code> using the calculation:     <pre><code>tolerance = 0.2 * np.std(a=x)\n</code></pre> - If any other value is given, then a <code>ValueError</code> error will be raised. Defaults to <code>\"default\"</code>.</p> <code>'default'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the given <code>tolerance</code> parameter is invalid.</p> <p>Valid options are:</p> <ul> <li>A number with type <code>float</code> or <code>int</code>, or</li> <li>A string with value <code>default</code>, or</li> <li>The value <code>None</code>.</li> </ul> <p>Returns:</p> Type Description <code>Dict[str, Union[str, float, bool]]</code> <p>A dictionary with only 3 keys containing the results of the test: <pre><code>{\n    \"result\": ...,\n    \"entropy\": ...,\n    \"tolerance\": ...,\n}\n</code></pre></p> <p>Credit</p> <p>All credit goes to the <code>AntroPy</code> library.</p> Examples Sample Entropy<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; is_regular(x=data, algorithm=\"sample\")\n{\"entropy\": 0.6177074729583698, \"tolerance\": 23.909808306554297, \"result\": True}\n</code></pre> Approx Entropy<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; is_regular(x=data, algorithm=\"approx\", tolerance=20)\n{\"entropy\": 0.6451264780416452, \"tolerance\": 20, \"result\": True}\n</code></pre> Spectral Entropy<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; is_regular(x=data, algorithm=\"spectral\", sf=1)\n{\"entropy\": 0.4287365561752448, \"tolerance\": 23.909808306554297, \"result\": True}\n</code></pre> References <ul> <li>Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.</li> <li>https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html</li> <li>Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.</li> <li>https://en.wikipedia.org/wiki/Spectral_density</li> <li>https://en.wikipedia.org/wiki/Welch%27s_method</li> </ul> See Also <ul> <li><code>entropy()</code></li> <li><code>regularity()</code></li> <li><code>approx_entropy()</code></li> <li><code>sample_entropy()</code></li> <li><code>spectral_entropy()</code></li> <li><code>permutation_entropy()</code></li> <li><code>svd_entropy()</code></li> </ul> Source code in <code>src/ts_stat_tests/tests/regularity.py</code> <pre><code>@typechecked\ndef is_regular(\n    x: ArrayLike,\n    algorithm: str = \"sample\",\n    order: int = 2,\n    sf: float = 1,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n    normalize: bool = True,\n    tolerance: Union[str, float, int, None] = \"default\",\n) -&gt; dict[str, Union[str, float, bool]]:\n    \"\"\"\n    !!! note \"Summary\"\n        Test whether a given data set is `regular` or not.\n\n    ???+ abstract \"Details\"\n        This function implements the given algorithm (defined in the parameter `algorithm`), and returns a dictionary containing the relevant data:\n        ```python\n        {\n            \"result\": ...,  # The result of the test. Will be `True` if `entropy&lt;tolerance`, and `False` otherwise\n            \"entropy\": ...,  # A `float` value, the result of the `entropy()` function\n            \"tolerance\": ...,  # A `float` value, which is the tolerance used for determining whether or not the `entropy` is `regular` or not\n        }\n        ```\n\n    Params:\n        x (ArrayLike):\n            The data to be checked. Should be a `1-D` or `N-D` data array.\n        algorithm (str, optional):\n            Which entropy algorithm to use.&lt;br&gt;\n            - `sample_entropy()`: `[\"sample\", \"sampl\", \"samp\"]`&lt;br&gt;\n            - `approx_entropy()`: `[\"app\", \"approx\"]`&lt;br&gt;\n            - `spectral_entropy()`: `[\"spec\", \"spect\", \"spectral\"]`&lt;br&gt;\n            - `permutation_entropy()`: `[\"perm\", \"permutation\"]`&lt;br&gt;\n            - `svd_entropy()`: `[\"svd\", \"svd_entropy\"]`&lt;br&gt;\n            Defaults to `\"sample\"`.\n        order (int, optional):\n            Embedding dimension.&lt;br&gt;\n            Only relevant when `algorithm=sample` or `algorithm=approx`.&lt;br&gt;\n            Defaults to `2`.\n        metric (VALID_KDTREE_METRIC_OPTIONS):\n            Name of the distance metric function used with [`sklearn.neighbors.KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree). Default is to use the [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance).&lt;br&gt;\n            Only relevant when `algorithm=sample` or `algorithm=approx`.&lt;br&gt;\n            Defaults to `\"chebyshev\"`.\n        sf (float, optional):\n            Sampling frequency, in Hz.&lt;br&gt;\n            Only relevant when `algorithm=spectral`.&lt;br&gt;\n            Defaults to `1`.\n        normalize (bool, optional):\n            If `True`, divide by $log2(psd.size)$ to normalize the spectral entropy to be between $0$ and $1$. Otherwise, return the spectral entropy in bit.&lt;br&gt;\n            Only relevant when `algorithm=spectral`.&lt;br&gt;\n            Defaults to `True`.\n        tolerance (Union[str, float, int, None], optional):\n            The tolerance value used to determine whether or not the result is `regular` or not.&lt;br&gt;\n            - If `tolerance` is either type `int` or `float`, then this value will be used.&lt;br&gt;\n            - If `tolerance` is either `\"default\"` or `None`, then `tolerance` will be derived from `x` using the calculation:\n                ```python\n                tolerance = 0.2 * np.std(a=x)\n                ```\n            - If any other value is given, then a `ValueError` error will be raised.&lt;br&gt;\n            Defaults to `\"default\"`.\n\n    Raises:\n        (ValueError): If the given `tolerance` parameter is invalid.\n\n            Valid options are:\n\n            - A number with type `float` or `int`, or\n            - A string with value `default`, or\n            - The value `None`.\n\n    Returns:\n        (Dict[str, Union[str, float, bool]]):\n            A dictionary with only 3 keys containing the results of the test:\n            ```python\n            {\n                \"result\": ...,\n                \"entropy\": ...,\n                \"tolerance\": ...,\n            }\n            ```\n\n    !!! Success \"Credit\"\n        All credit goes to the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ???+ Example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Sample Entropy\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; is_regular(x=data, algorithm=\"sample\")\n        {\"entropy\": 0.6177074729583698, \"tolerance\": 23.909808306554297, \"result\": True}\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Approx Entropy\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; is_regular(x=data, algorithm=\"approx\", tolerance=20)\n        {\"entropy\": 0.6451264780416452, \"tolerance\": 20, \"result\": True}\n        ```\n\n        ```pycon {.py .python linenums=\"1\"  title=\"Spectral Entropy\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; is_regular(x=data, algorithm=\"spectral\", sf=1)\n        {\"entropy\": 0.4287365561752448, \"tolerance\": 23.909808306554297, \"result\": True}\n        ```\n\n    ??? Question \"References\"\n        - Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n        - https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n        - Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.\n        - https://en.wikipedia.org/wiki/Spectral_density\n        - https://en.wikipedia.org/wiki/Welch%27s_method\n\n    ??? Tip \"See Also\"\n        - [`entropy()`][ts_stat_tests.tests.regularity.entropy]\n        - [`regularity()`][ts_stat_tests.tests.regularity.regularity]\n        - [`approx_entropy()`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`sample_entropy()`][ts_stat_tests.algorithms.regularity.sample_entropy]\n        - [`spectral_entropy()`][ts_stat_tests.algorithms.regularity.spectral_entropy]\n        - [`permutation_entropy()`][ts_stat_tests.algorithms.regularity.permutation_entropy]\n        - [`svd_entropy()`][ts_stat_tests.algorithms.regularity.svd_entropy]\n    \"\"\"\n    if isinstance(tolerance, (float, int)):\n        tol = tolerance\n    elif tolerance in [\"default\", None]:\n        tol = 0.2 * np.std(a=np.asarray(x))\n    else:\n        raise ValueError(\n            f\"Invalid option for `tolerance` parameter: {tolerance}.\\n\"\n            f\"Valid options are:\\n\"\n            f\"- A number with type `float` or `int`,\\n\"\n            f\"- A string with value `default`,\\n\"\n            f\"- The value `None`.\"\n        )\n    value = regularity(x=x, order=order, sf=sf, metric=metric, algorithm=algorithm, normalize=normalize)\n    result = value &lt; tol\n    return {\n        \"result\": bool(result),\n        \"entropy\": float(value),\n        \"tolerance\": float(tol),\n    }\n</code></pre>"},{"location":"code/regularity/#regularity-algorithms","title":"Regularity Algorithms","text":""},{"location":"code/regularity/#ts_stat_tests.tests.regularity","title":"ts_stat_tests.tests.regularity","text":"<p>Summary</p> <p>This module contains convenience functions and tests for regularity measures, allowing for easy access to different entropy algorithms.</p>"},{"location":"code/regularity/#ts_stat_tests.tests.regularity.entropy","title":"entropy","text":"<pre><code>entropy(\n    x: ArrayLike,\n    algorithm: str = \"sample\",\n    order: int = 2,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n    sf: float = 1,\n    normalize: bool = True,\n) -&gt; float\n</code></pre> <p>Summary</p> <p>Test for the entropy of a given data set.</p> Details <p>This function is a convenience wrapper around the five underlying algorithms: - <code>approx_entropy()</code> - <code>sample_entropy()</code> - <code>spectral_entropy()</code> - <code>permutation_entropy()</code> - <code>svd_entropy()</code></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked. Should be a <code>1-D</code> or <code>N-D</code> data array.</p> required <code>algorithm</code> <code>str</code> <p>Which entropy algorithm to use. - <code>sample_entropy()</code>: <code>[\"sample\", \"sampl\", \"samp\"]</code> - <code>approx_entropy()</code>: <code>[\"app\", \"approx\"]</code> - <code>spectral_entropy()</code>: <code>[\"spec\", \"spect\", \"spectral\"]</code> - <code>permutation_entropy()</code>: <code>[\"perm\", \"permutation\"]</code> - <code>svd_entropy()</code>: <code>[\"svd\", \"svd_entropy\"]</code> Defaults to <code>\"sample\"</code>.</p> <code>'sample'</code> <code>order</code> <code>int</code> <p>Embedding dimension. Only relevant when <code>algorithm=sample</code> or <code>algorithm=approx</code>. Defaults to <code>2</code>.</p> <code>2</code> <code>metric</code> <code>VALID_KDTREE_METRIC_OPTIONS</code> <p>Name of the distance metric function used with <code>sklearn.neighbors.KDTree</code>. Default is to use the Chebyshev distance. Only relevant when <code>algorithm=sample</code> or <code>algorithm=approx</code>. Defaults to <code>\"chebyshev\"</code>.</p> <code>'chebyshev'</code> <code>sf</code> <code>float</code> <p>Sampling frequency, in Hz. Only relevant when <code>algorithm=spectral</code>. Defaults to <code>1</code>.</p> <code>1</code> <code>normalize</code> <code>bool</code> <p>If <code>True</code>, divide by \\(log2(psd.size)\\) to normalize the spectral entropy to be between \\(0\\) and \\(1\\). Otherwise, return the spectral entropy in bit. Only relevant when <code>algorithm=spectral</code>. Defaults to <code>True</code>.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the given value for <code>algorithm</code> is not valid.</p> <p>Returns:</p> Type Description <code>float</code> <p>The Entropy value.</p> <p>Credit</p> <p>All credit goes to the <code>AntroPy</code> library.</p> Examples <p><code>approx_entropy</code>: Basic usage<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; entropy(x=data, algorithm=\"approx\")\n0.6451264780416452\n</code></pre></p> <p><code>sample_entropy</code>: Basic usage<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; entropy(x=data, algorithm=\"sample\")\n0.6177074729583698\n</code></pre></p> <p><code>spectral_entropy</code>: Basic usage<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; entropy(x=data, algorithm=\"spectral\", sf=1)\n2.6538040647031726\n</code></pre></p> Advanced usage<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; spectral_entropy(data, 2, \"welch\", normalize=True)\n0.3371369604224553\n</code></pre> References <ul> <li>Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.</li> <li>https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html</li> <li>Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.</li> <li>https://en.wikipedia.org/wiki/Spectral_density</li> <li>https://en.wikipedia.org/wiki/Welch%27s_method</li> </ul> See Also <ul> <li><code>regularity()</code></li> <li><code>approx_entropy()</code></li> <li><code>sample_entropy()</code></li> <li><code>spectral_entropy()</code></li> <li><code>permutation_entropy()</code></li> <li><code>svd_entropy()</code></li> </ul> Source code in <code>src/ts_stat_tests/tests/regularity.py</code> <pre><code>@typechecked\ndef entropy(\n    x: ArrayLike,\n    algorithm: str = \"sample\",\n    order: int = 2,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n    sf: float = 1,\n    normalize: bool = True,\n) -&gt; float:\n    \"\"\"\n    !!! note \"Summary\"\n        Test for the entropy of a given data set.\n\n    ???+ abstract \"Details\"\n        This function is a convenience wrapper around the five underlying algorithms:&lt;br&gt;\n        - [`approx_entropy()`][ts_stat_tests.algorithms.regularity.approx_entropy]&lt;br&gt;\n        - [`sample_entropy()`][ts_stat_tests.algorithms.regularity.sample_entropy]&lt;br&gt;\n        - [`spectral_entropy()`][ts_stat_tests.algorithms.regularity.spectral_entropy]&lt;br&gt;\n        - [`permutation_entropy()`][ts_stat_tests.algorithms.regularity.permutation_entropy]&lt;br&gt;\n        - [`svd_entropy()`][ts_stat_tests.algorithms.regularity.svd_entropy]\n\n    Params:\n        x (ArrayLike):\n            The data to be checked. Should be a `1-D` or `N-D` data array.\n        algorithm (str, optional):\n            Which entropy algorithm to use.&lt;br&gt;\n            - `sample_entropy()`: `[\"sample\", \"sampl\", \"samp\"]`&lt;br&gt;\n            - `approx_entropy()`: `[\"app\", \"approx\"]`&lt;br&gt;\n            - `spectral_entropy()`: `[\"spec\", \"spect\", \"spectral\"]`&lt;br&gt;\n            - `permutation_entropy()`: `[\"perm\", \"permutation\"]`&lt;br&gt;\n            - `svd_entropy()`: `[\"svd\", \"svd_entropy\"]`&lt;br&gt;\n            Defaults to `\"sample\"`.\n        order (int, optional):\n            Embedding dimension.&lt;br&gt;\n            Only relevant when `algorithm=sample` or `algorithm=approx`.&lt;br&gt;\n            Defaults to `2`.\n        metric (VALID_KDTREE_METRIC_OPTIONS):\n            Name of the distance metric function used with [`sklearn.neighbors.KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree). Default is to use the [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance).&lt;br&gt;\n            Only relevant when `algorithm=sample` or `algorithm=approx`.&lt;br&gt;\n            Defaults to `\"chebyshev\"`.\n        sf (float, optional):\n            Sampling frequency, in Hz.&lt;br&gt;\n            Only relevant when `algorithm=spectral`.&lt;br&gt;\n            Defaults to `1`.\n        normalize (bool, optional):\n            If `True`, divide by $log2(psd.size)$ to normalize the spectral entropy to be between $0$ and $1$. Otherwise, return the spectral entropy in bit.&lt;br&gt;\n            Only relevant when `algorithm=spectral`.&lt;br&gt;\n            Defaults to `True`.\n\n    Raises:\n        ValueError: When the given value for `algorithm` is not valid.\n\n    Returns:\n        (float):\n            The Entropy value.\n\n    !!! Success \"Credit\"\n        All credit goes to the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ???+ Example \"Examples\"\n\n        `approx_entropy`:\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; entropy(x=data, algorithm=\"approx\")\n        0.6451264780416452\n        ```\n\n        ---\n\n        `sample_entropy`:\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; entropy(x=data, algorithm=\"sample\")\n        0.6177074729583698\n        ```\n\n        ---\n\n        `spectral_entropy`:\n        ```pycon {.py .python linenums=\"1\"  title=\"Basic usage\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; entropy(x=data, algorithm=\"spectral\", sf=1)\n        2.6538040647031726\n        ```\n\n        ```pycon {.py .python linenums=\"1\"  title=\"Advanced usage\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; spectral_entropy(data, 2, \"welch\", normalize=True)\n        0.3371369604224553\n        ```\n\n    ??? Question \"References\"\n        - Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n        - https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n        - Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.\n        - https://en.wikipedia.org/wiki/Spectral_density\n        - https://en.wikipedia.org/wiki/Welch%27s_method\n\n    ??? Tip \"See Also\"\n        - [`regularity()`][ts_stat_tests.tests.regularity.regularity]\n        - [`approx_entropy()`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`sample_entropy()`][ts_stat_tests.algorithms.regularity.sample_entropy]\n        - [`spectral_entropy()`][ts_stat_tests.algorithms.regularity.spectral_entropy]\n        - [`permutation_entropy()`][ts_stat_tests.algorithms.regularity.permutation_entropy]\n        - [`svd_entropy()`][ts_stat_tests.algorithms.regularity.svd_entropy]\n    \"\"\"\n    options: dict[str, tuple[str, ...]] = {\n        \"sampl\": (\"sample\", \"sampl\", \"samp\"),\n        \"approx\": (\"app\", \"approx\"),\n        \"spect\": (\"spec\", \"spect\", \"spectral\"),\n        \"perm\": (\"perm\", \"permutation\"),\n        \"svd\": (\"svd\", \"svd_entropy\"),\n    }\n    if algorithm in options[\"sampl\"]:\n        return sample_entropy(x=x, order=order, metric=metric)\n    if algorithm in options[\"approx\"]:\n        return approx_entropy(x=x, order=order, metric=metric)\n    if algorithm in options[\"spect\"]:\n        return cast(float, spectral_entropy(x=x, sf=sf, normalize=normalize))\n    if algorithm in options[\"perm\"]:\n        return permutation_entropy(x=x, order=order, normalize=normalize)\n    if algorithm in options[\"svd\"]:\n        return svd_entropy(x=x, order=order, normalize=normalize)\n    raise ValueError(\n        generate_error_message(\n            parameter_name=\"algorithm\",\n            value_parsed=algorithm,\n            options=options,\n        )\n    )\n</code></pre>"},{"location":"code/regularity/#ts_stat_tests.tests.regularity.regularity","title":"regularity","text":"<pre><code>regularity(\n    x: ArrayLike,\n    algorithm: str = \"sample\",\n    order: int = 2,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n    sf: float = 1,\n    normalize: bool = True,\n) -&gt; float\n</code></pre> <p>Summary</p> <p>Test for the regularity of a given data set.</p> Details <p>This is a pass-through, convenience wrapper around the <code>entropy()</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked. Should be a <code>1-D</code> or <code>N-D</code> data array.</p> required <code>algorithm</code> <code>str</code> <p>Which entropy algorithm to use. - <code>sample_entropy()</code>: <code>[\"sample\", \"sampl\", \"samp\"]</code> - <code>approx_entropy()</code>: <code>[\"app\", \"approx\"]</code> - <code>spectral_entropy()</code>: <code>[\"spec\", \"spect\", \"spectral\"]</code> - <code>permutation_entropy()</code>: <code>[\"perm\", \"permutation\"]</code> - <code>svd_entropy()</code>: <code>[\"svd\", \"svd_entropy\"]</code> Defaults to <code>\"sample\"</code>.</p> <code>'sample'</code> <code>order</code> <code>int</code> <p>Embedding dimension. Only relevant when <code>algorithm=sample</code> or <code>algorithm=approx</code>. Defaults to <code>2</code>.</p> <code>2</code> <code>metric</code> <code>VALID_KDTREE_METRIC_OPTIONS</code> <p>Name of the distance metric function used with <code>sklearn.neighbors.KDTree</code>. Default is to use the Chebyshev distance. Only relevant when <code>algorithm=sample</code> or <code>algorithm=approx</code>. Defaults to <code>\"chebyshev\"</code>.</p> <code>'chebyshev'</code> <code>sf</code> <code>float</code> <p>Sampling frequency, in Hz. Only relevant when <code>algorithm=spectral</code>. Defaults to <code>1</code>.</p> <code>1</code> <code>normalize</code> <code>bool</code> <p>If <code>True</code>, divide by \\(log2(psd.size)\\) to normalize the spectral entropy to be between \\(0\\) and \\(1\\). Otherwise, return the spectral entropy in bit. Only relevant when <code>algorithm=spectral</code>. Defaults to <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>float</code> <p>The Regularity value.</p> <p>Credit</p> <p>All credit goes to the <code>AntroPy</code> library.</p> Examples <p><code>regularity</code> with <code>approx_entropy</code> algorithm: Basic usage<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; from ts_stat_tests.tests.regularity import regularity\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; regularity(x=data, algorithm=\"approx_entropy\")\n0.6451264780416452\n</code></pre></p> <p><code>regularity</code> with <code>sample_entropy</code> algorithm: Basic usage<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; from ts_stat_tests.tests.regularity import regularity\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; regularity(x=data, algorithm=\"sample_entropy\")\n0.6177074729583698\n</code></pre></p> <p><code>regularity</code> with <code>spectral_entropy</code> algorithm: Basic usage<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; from ts_stat_tests.tests.regularity import regularity\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; regularity(x=data, algorithm=\"spectral_entropy\", sf=1)\n2.6538040647031726\n</code></pre></p> Advanced usage<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; from ts_stat_tests.tests.regularity import regularity\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; regularity(data, algorithm=\"spectral_entropy\", sf=2, method=\"welch\", normalize=True)\n0.3371369604224553\n</code></pre> References <ul> <li>Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.</li> <li>https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html</li> <li>Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.</li> <li>https://en.wikipedia.org/wiki/Spectral_density</li> <li>https://en.wikipedia.org/wiki/Welch%27s_method</li> </ul> See Also <ul> <li><code>entropy()</code></li> <li><code>approx_entropy()</code></li> <li><code>sample_entropy()</code></li> <li><code>spectral_entropy()</code></li> <li><code>permutation_entropy()</code></li> <li><code>svd_entropy()</code></li> </ul> Source code in <code>src/ts_stat_tests/tests/regularity.py</code> <pre><code>@typechecked\ndef regularity(\n    x: ArrayLike,\n    algorithm: str = \"sample\",\n    order: int = 2,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n    sf: float = 1,\n    normalize: bool = True,\n) -&gt; float:\n    \"\"\"\n    !!! note \"Summary\"\n        Test for the regularity of a given data set.\n\n    ???+ abstract \"Details\"\n        This is a pass-through, convenience wrapper around the [`entropy()`][ts_stat_tests.tests.regularity.entropy] function.\n\n    Params:\n        x (ArrayLike):\n            The data to be checked. Should be a `1-D` or `N-D` data array.\n        algorithm (str, optional):\n            Which entropy algorithm to use.&lt;br&gt;\n            - `sample_entropy()`: `[\"sample\", \"sampl\", \"samp\"]`&lt;br&gt;\n            - `approx_entropy()`: `[\"app\", \"approx\"]`&lt;br&gt;\n            - `spectral_entropy()`: `[\"spec\", \"spect\", \"spectral\"]`&lt;br&gt;\n            - `permutation_entropy()`: `[\"perm\", \"permutation\"]`&lt;br&gt;\n            - `svd_entropy()`: `[\"svd\", \"svd_entropy\"]`&lt;br&gt;\n            Defaults to `\"sample\"`.\n        order (int, optional):\n            Embedding dimension.&lt;br&gt;\n            Only relevant when `algorithm=sample` or `algorithm=approx`.&lt;br&gt;\n            Defaults to `2`.\n        metric (VALID_KDTREE_METRIC_OPTIONS):\n            Name of the distance metric function used with [`sklearn.neighbors.KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree). Default is to use the [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance).&lt;br&gt;\n            Only relevant when `algorithm=sample` or `algorithm=approx`.&lt;br&gt;\n            Defaults to `\"chebyshev\"`.\n        sf (float, optional):\n            Sampling frequency, in Hz.&lt;br&gt;\n            Only relevant when `algorithm=spectral`.&lt;br&gt;\n            Defaults to `1`.\n        normalize (bool, optional):\n            If `True`, divide by $log2(psd.size)$ to normalize the spectral entropy to be between $0$ and $1$. Otherwise, return the spectral entropy in bit.&lt;br&gt;\n            Only relevant when `algorithm=spectral`.&lt;br&gt;\n            Defaults to `True`.\n\n    Returns:\n        (float):\n            The Regularity value.\n\n    !!! Success \"Credit\"\n        All credit goes to the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ???+ Example \"Examples\"\n\n        `regularity` with `approx_entropy` algorithm:\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.tests.regularity import regularity\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; regularity(x=data, algorithm=\"approx_entropy\")\n        0.6451264780416452\n        ```\n\n        ---\n\n        `regularity` with `sample_entropy` algorithm:\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.tests.regularity import regularity\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; regularity(x=data, algorithm=\"sample_entropy\")\n        0.6177074729583698\n        ```\n\n        ---\n\n        `regularity` with `spectral_entropy` algorithm:\n        ```pycon {.py .python linenums=\"1\"  title=\"Basic usage\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.tests.regularity import regularity\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; regularity(x=data, algorithm=\"spectral_entropy\", sf=1)\n        2.6538040647031726\n        ```\n\n        ```pycon {.py .python linenums=\"1\"  title=\"Advanced usage\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.tests.regularity import regularity\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; regularity(data, algorithm=\"spectral_entropy\", sf=2, method=\"welch\", normalize=True)\n        0.3371369604224553\n        ```\n\n    ??? Question \"References\"\n        - Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n        - https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n        - Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.\n        - https://en.wikipedia.org/wiki/Spectral_density\n        - https://en.wikipedia.org/wiki/Welch%27s_method\n\n    ??? Tip \"See Also\"\n        - [`entropy()`][ts_stat_tests.tests.regularity.entropy]\n        - [`approx_entropy()`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`sample_entropy()`][ts_stat_tests.algorithms.regularity.sample_entropy]\n        - [`spectral_entropy()`][ts_stat_tests.algorithms.regularity.spectral_entropy]\n        - [`permutation_entropy()`][ts_stat_tests.algorithms.regularity.permutation_entropy]\n        - [`svd_entropy()`][ts_stat_tests.algorithms.regularity.svd_entropy]\n    \"\"\"\n    return entropy(x=x, algorithm=algorithm, order=order, metric=metric, sf=sf, normalize=normalize)\n</code></pre>"},{"location":"code/regularity/#ts_stat_tests.tests.regularity.is_regular","title":"is_regular","text":"<pre><code>is_regular(\n    x: ArrayLike,\n    algorithm: str = \"sample\",\n    order: int = 2,\n    sf: float = 1,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n    normalize: bool = True,\n    tolerance: Union[str, float, int, None] = \"default\",\n) -&gt; dict[str, Union[str, float, bool]]\n</code></pre> <p>Summary</p> <p>Test whether a given data set is <code>regular</code> or not.</p> Details <p>This function implements the given algorithm (defined in the parameter <code>algorithm</code>), and returns a dictionary containing the relevant data: <pre><code>{\n    \"result\": ...,  # The result of the test. Will be `True` if `entropy&lt;tolerance`, and `False` otherwise\n    \"entropy\": ...,  # A `float` value, the result of the `entropy()` function\n    \"tolerance\": ...,  # A `float` value, which is the tolerance used for determining whether or not the `entropy` is `regular` or not\n}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked. Should be a <code>1-D</code> or <code>N-D</code> data array.</p> required <code>algorithm</code> <code>str</code> <p>Which entropy algorithm to use. - <code>sample_entropy()</code>: <code>[\"sample\", \"sampl\", \"samp\"]</code> - <code>approx_entropy()</code>: <code>[\"app\", \"approx\"]</code> - <code>spectral_entropy()</code>: <code>[\"spec\", \"spect\", \"spectral\"]</code> - <code>permutation_entropy()</code>: <code>[\"perm\", \"permutation\"]</code> - <code>svd_entropy()</code>: <code>[\"svd\", \"svd_entropy\"]</code> Defaults to <code>\"sample\"</code>.</p> <code>'sample'</code> <code>order</code> <code>int</code> <p>Embedding dimension. Only relevant when <code>algorithm=sample</code> or <code>algorithm=approx</code>. Defaults to <code>2</code>.</p> <code>2</code> <code>metric</code> <code>VALID_KDTREE_METRIC_OPTIONS</code> <p>Name of the distance metric function used with <code>sklearn.neighbors.KDTree</code>. Default is to use the Chebyshev distance. Only relevant when <code>algorithm=sample</code> or <code>algorithm=approx</code>. Defaults to <code>\"chebyshev\"</code>.</p> <code>'chebyshev'</code> <code>sf</code> <code>float</code> <p>Sampling frequency, in Hz. Only relevant when <code>algorithm=spectral</code>. Defaults to <code>1</code>.</p> <code>1</code> <code>normalize</code> <code>bool</code> <p>If <code>True</code>, divide by \\(log2(psd.size)\\) to normalize the spectral entropy to be between \\(0\\) and \\(1\\). Otherwise, return the spectral entropy in bit. Only relevant when <code>algorithm=spectral</code>. Defaults to <code>True</code>.</p> <code>True</code> <code>tolerance</code> <code>Union[str, float, int, None]</code> <p>The tolerance value used to determine whether or not the result is <code>regular</code> or not. - If <code>tolerance</code> is either type <code>int</code> or <code>float</code>, then this value will be used. - If <code>tolerance</code> is either <code>\"default\"</code> or <code>None</code>, then <code>tolerance</code> will be derived from <code>x</code> using the calculation:     <pre><code>tolerance = 0.2 * np.std(a=x)\n</code></pre> - If any other value is given, then a <code>ValueError</code> error will be raised. Defaults to <code>\"default\"</code>.</p> <code>'default'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the given <code>tolerance</code> parameter is invalid.</p> <p>Valid options are:</p> <ul> <li>A number with type <code>float</code> or <code>int</code>, or</li> <li>A string with value <code>default</code>, or</li> <li>The value <code>None</code>.</li> </ul> <p>Returns:</p> Type Description <code>Dict[str, Union[str, float, bool]]</code> <p>A dictionary with only 3 keys containing the results of the test: <pre><code>{\n    \"result\": ...,\n    \"entropy\": ...,\n    \"tolerance\": ...,\n}\n</code></pre></p> <p>Credit</p> <p>All credit goes to the <code>AntroPy</code> library.</p> Examples Sample Entropy<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; is_regular(x=data, algorithm=\"sample\")\n{\"entropy\": 0.6177074729583698, \"tolerance\": 23.909808306554297, \"result\": True}\n</code></pre> Approx Entropy<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; is_regular(x=data, algorithm=\"approx\", tolerance=20)\n{\"entropy\": 0.6451264780416452, \"tolerance\": 20, \"result\": True}\n</code></pre> Spectral Entropy<pre><code>&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; is_regular(x=data, algorithm=\"spectral\", sf=1)\n{\"entropy\": 0.4287365561752448, \"tolerance\": 23.909808306554297, \"result\": True}\n</code></pre> References <ul> <li>Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.</li> <li>https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html</li> <li>Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.</li> <li>https://en.wikipedia.org/wiki/Spectral_density</li> <li>https://en.wikipedia.org/wiki/Welch%27s_method</li> </ul> See Also <ul> <li><code>entropy()</code></li> <li><code>regularity()</code></li> <li><code>approx_entropy()</code></li> <li><code>sample_entropy()</code></li> <li><code>spectral_entropy()</code></li> <li><code>permutation_entropy()</code></li> <li><code>svd_entropy()</code></li> </ul> Source code in <code>src/ts_stat_tests/tests/regularity.py</code> <pre><code>@typechecked\ndef is_regular(\n    x: ArrayLike,\n    algorithm: str = \"sample\",\n    order: int = 2,\n    sf: float = 1,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n    normalize: bool = True,\n    tolerance: Union[str, float, int, None] = \"default\",\n) -&gt; dict[str, Union[str, float, bool]]:\n    \"\"\"\n    !!! note \"Summary\"\n        Test whether a given data set is `regular` or not.\n\n    ???+ abstract \"Details\"\n        This function implements the given algorithm (defined in the parameter `algorithm`), and returns a dictionary containing the relevant data:\n        ```python\n        {\n            \"result\": ...,  # The result of the test. Will be `True` if `entropy&lt;tolerance`, and `False` otherwise\n            \"entropy\": ...,  # A `float` value, the result of the `entropy()` function\n            \"tolerance\": ...,  # A `float` value, which is the tolerance used for determining whether or not the `entropy` is `regular` or not\n        }\n        ```\n\n    Params:\n        x (ArrayLike):\n            The data to be checked. Should be a `1-D` or `N-D` data array.\n        algorithm (str, optional):\n            Which entropy algorithm to use.&lt;br&gt;\n            - `sample_entropy()`: `[\"sample\", \"sampl\", \"samp\"]`&lt;br&gt;\n            - `approx_entropy()`: `[\"app\", \"approx\"]`&lt;br&gt;\n            - `spectral_entropy()`: `[\"spec\", \"spect\", \"spectral\"]`&lt;br&gt;\n            - `permutation_entropy()`: `[\"perm\", \"permutation\"]`&lt;br&gt;\n            - `svd_entropy()`: `[\"svd\", \"svd_entropy\"]`&lt;br&gt;\n            Defaults to `\"sample\"`.\n        order (int, optional):\n            Embedding dimension.&lt;br&gt;\n            Only relevant when `algorithm=sample` or `algorithm=approx`.&lt;br&gt;\n            Defaults to `2`.\n        metric (VALID_KDTREE_METRIC_OPTIONS):\n            Name of the distance metric function used with [`sklearn.neighbors.KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree). Default is to use the [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance).&lt;br&gt;\n            Only relevant when `algorithm=sample` or `algorithm=approx`.&lt;br&gt;\n            Defaults to `\"chebyshev\"`.\n        sf (float, optional):\n            Sampling frequency, in Hz.&lt;br&gt;\n            Only relevant when `algorithm=spectral`.&lt;br&gt;\n            Defaults to `1`.\n        normalize (bool, optional):\n            If `True`, divide by $log2(psd.size)$ to normalize the spectral entropy to be between $0$ and $1$. Otherwise, return the spectral entropy in bit.&lt;br&gt;\n            Only relevant when `algorithm=spectral`.&lt;br&gt;\n            Defaults to `True`.\n        tolerance (Union[str, float, int, None], optional):\n            The tolerance value used to determine whether or not the result is `regular` or not.&lt;br&gt;\n            - If `tolerance` is either type `int` or `float`, then this value will be used.&lt;br&gt;\n            - If `tolerance` is either `\"default\"` or `None`, then `tolerance` will be derived from `x` using the calculation:\n                ```python\n                tolerance = 0.2 * np.std(a=x)\n                ```\n            - If any other value is given, then a `ValueError` error will be raised.&lt;br&gt;\n            Defaults to `\"default\"`.\n\n    Raises:\n        (ValueError): If the given `tolerance` parameter is invalid.\n\n            Valid options are:\n\n            - A number with type `float` or `int`, or\n            - A string with value `default`, or\n            - The value `None`.\n\n    Returns:\n        (Dict[str, Union[str, float, bool]]):\n            A dictionary with only 3 keys containing the results of the test:\n            ```python\n            {\n                \"result\": ...,\n                \"entropy\": ...,\n                \"tolerance\": ...,\n            }\n            ```\n\n    !!! Success \"Credit\"\n        All credit goes to the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ???+ Example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Sample Entropy\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; is_regular(x=data, algorithm=\"sample\")\n        {\"entropy\": 0.6177074729583698, \"tolerance\": 23.909808306554297, \"result\": True}\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Approx Entropy\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; is_regular(x=data, algorithm=\"approx\", tolerance=20)\n        {\"entropy\": 0.6451264780416452, \"tolerance\": 20, \"result\": True}\n        ```\n\n        ```pycon {.py .python linenums=\"1\"  title=\"Spectral Entropy\"}\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; is_regular(x=data, algorithm=\"spectral\", sf=1)\n        {\"entropy\": 0.4287365561752448, \"tolerance\": 23.909808306554297, \"result\": True}\n        ```\n\n    ??? Question \"References\"\n        - Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n        - https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n        - Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.\n        - https://en.wikipedia.org/wiki/Spectral_density\n        - https://en.wikipedia.org/wiki/Welch%27s_method\n\n    ??? Tip \"See Also\"\n        - [`entropy()`][ts_stat_tests.tests.regularity.entropy]\n        - [`regularity()`][ts_stat_tests.tests.regularity.regularity]\n        - [`approx_entropy()`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`sample_entropy()`][ts_stat_tests.algorithms.regularity.sample_entropy]\n        - [`spectral_entropy()`][ts_stat_tests.algorithms.regularity.spectral_entropy]\n        - [`permutation_entropy()`][ts_stat_tests.algorithms.regularity.permutation_entropy]\n        - [`svd_entropy()`][ts_stat_tests.algorithms.regularity.svd_entropy]\n    \"\"\"\n    if isinstance(tolerance, (float, int)):\n        tol = tolerance\n    elif tolerance in [\"default\", None]:\n        tol = 0.2 * np.std(a=np.asarray(x))\n    else:\n        raise ValueError(\n            f\"Invalid option for `tolerance` parameter: {tolerance}.\\n\"\n            f\"Valid options are:\\n\"\n            f\"- A number with type `float` or `int`,\\n\"\n            f\"- A string with value `default`,\\n\"\n            f\"- The value `None`.\"\n        )\n    value = regularity(x=x, order=order, sf=sf, metric=metric, algorithm=algorithm, normalize=normalize)\n    result = value &lt; tol\n    return {\n        \"result\": bool(result),\n        \"entropy\": float(value),\n        \"tolerance\": float(tol),\n    }\n</code></pre>"},{"location":"code/regularity/#ts_stat_tests.algorithms.regularity","title":"ts_stat_tests.algorithms.regularity","text":"<p>Summary</p> <p>This module contains algorithms to compute regularity measures for time series data, including approximate entropy, sample entropy, spectral entropy, and permutation entropy.</p>"},{"location":"code/regularity/#ts_stat_tests.algorithms.regularity.approx_entropy","title":"approx_entropy","text":"<pre><code>approx_entropy(\n    x: ArrayLike,\n    order: int = 2,\n    tolerance: Optional[float] = None,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n) -&gt; float\n</code></pre> <p>Summary</p> <p>Approximate entropy is a measure of the amount of regularity or predictability in a time series. It is used to quantify the degree of self-similarity of a signal over different time scales, and can be useful for detecting underlying patterns or trends in data</p> Details <p>Approximate entropy is a technique used to quantify the amount of regularity and the unpredictability of fluctuations over time-series data. Smaller values indicates that the data is more regular and predictable.</p> <p>The tolerance value (\\(r\\)) is set to \\(0.2 \\times std(x)\\).</p> <p>To calculate approximate entropy, we first need to define a window size or scale factor, which determines the length of the subsequences that are used to compare the similarity of the time series. We then compare all possible pairs of subsequences within the time series and calculate the probability that two subsequences are within a certain tolerance level of each other, where the tolerance level is usually expressed as a percentage of the standard deviation of the time series.</p> <p>The approximate entropy is then defined as the negative natural logarithm of the average probability of similarity across all possible pairs of subsequences, normalized by the length of the time series and the scale factor.</p> <p>The approximate entropy measure is useful in a variety of applications, such as the analysis of physiological signals, financial time series, and climate data. It can be used to detect changes in the regularity or predictability of a time series over time, and can provide insights into the underlying dynamics or mechanisms that generate the signal. For example, a decrease in approximate entropy may indicate the onset of a disease or a shift in the underlying physiological state, while an increase in approximate entropy may suggest the presence of noise or other external influences on the system.</p> <p>The equation for ApEn is:</p> \\[ ApEn(m, r, N) = \u03c6m(r) - \u03c6m+1(r) \\] <p>where:</p> <ul> <li>\\(m\\) is the embedding dimension,</li> <li>\\(r\\) is the tolerance or similarity criterion,</li> <li>\\(N\\) is the length of the time series, and</li> <li>\\(\u03c6m(r)\\) and \\(\u03c6m+1(r)\\) are the logarithms of the probabilities that two sequences of m data points in the time series that are similar to each other within a tolerance \\(r\\) remain similar for the next data point, for \\(m\\) and \\(m+1\\), respectively.</li> </ul> <pre><code>ApEn(m, r, N) = \u03c6m(r) - \u03c6m+1(r)\n</code></pre> <p>The calculation of ApEn involves the following steps:</p> <ol> <li>Create a set of vectors, each containing \\(m\\) data points from the time series, where \\(m\\) is the embedding dimension.</li> <li>Calculate the Euclidean distance between each pair of vectors and count the number of pairs that are within a distance \\(r\\) of each other.</li> <li>Compute the probabilities \\(\u03c6m(r)\\) and \\(\u03c6m+1(r)\\) using the counts from step 2.</li> <li>Compute \\(ApEn(m, r, N)\\) using the equation above.</li> </ol> <p>The value of ApEn ranges from zero (\\(0\\)) to infinity (\\(\\infty\\)), with lower values indicating higher regularity or predictability in the time series. A time series with high ApEn is more unpredictable or irregular, whereas a time series with low ApEn is more regular or predictable.</p> <p>ApEn is often used in time series forecasting to assess the complexity of the data and to determine whether a time series is suitable for modeling with a particular forecasting method, such as ARIMA or neural networks.</p> <p>When calculating the Approximate entropy requires the specification of a set of parameters that determine the characteristics of the time series. One of these parameters is the embedding dimension, which determines the number of values that are used to construct each permutation pattern.</p> <p>The embedding dimension is important in the calculation of permutation entropy because it affects the sensitivity of the measure to different patterns in the data. If the embedding dimension is too small, we may miss important patterns or variations in the time series, and the resulting permutation entropy value may not accurately reflect the underlying complexity of the signal. On the other hand, if the embedding dimension is too large, we may overfit the data and produce a permutation entropy value that is overly sensitive to noise or other random fluctuations.</p> <p>Choosing an appropriate embedding dimension is therefore crucial in ensuring that the permutation entropy calculation is robust and reliable, and captures the essential features of the time series in a meaningful way. This allows us to make more accurate and informative inferences about the behavior of the system that generated the data, and can be useful in a wide range of applications, from signal processing to data analysis and beyond.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>One-dimensional time series of shape (n_times).</p> required <code>order</code> <code>int</code> <p>Embedding dimension. Defaults to <code>2</code>.</p> <code>2</code> <code>tolerance</code> <code>Optional[float]</code> <p>Tolerance level or similarity criterion. If <code>None</code> (default), it is set to \\(0.2     imes std(x)\\). Defaults to <code>None</code>.</p> <code>None</code> <code>metric</code> <code>VALID_KDTREE_METRIC_OPTIONS</code> <p>Name of the distance metric function used with <code>sklearn.neighbors.KDTree</code>. Default is to use the Chebyshev distance. For a full list of all available metrics, see <code>sklearn.metrics.pairwise.distance_metrics</code> and <code>scipy.spatial.distance</code> Defaults to <code>\"chebyshev\"</code>.</p> <code>'chebyshev'</code> <p>Returns:</p> Type Description <code>float</code> <p>Approximate Entropy score.</p> Notes <p>Inputs:</p> <ul> <li><code>x</code> is a 1-dimensional array.     It represents time-series data, ideally with each element in the array being a measurement or value taken at regular time intervuls over the length of the array. The exact number of elements (or length of the array) can realistically be greater than 30 elements long, but ideally should be a few hundred elements long, and is even better when it is a few thousand elements long.</li> </ul> <p>Settings:</p> <ul> <li> <p><code>order</code> is used for determining the number of values that are used to construct each permutation pattern.</p> <ul> <li>If the embedding dimension is too small, we may miss important patterns or variations in the time series, and the resulting approximate entropy value may not accurately reflect the underlying complexity of the signal.</li> <li>If the embedding dimension is too large, we may overfit the data and produce an approximate entropy value that is overly sensitive to noise or other random fluctuations.</li> </ul> </li> <li> <p><code>metric</code> is used for determining which distance metric to use for the underlying distance-space between two time series.</p> <ul> <li>The Chebyshev metric is often used when calculating approximate entropy because it is a robust and computationally efficient way to measure the distance between two time series. The Chebyshev distance between two vectors is defined as the maximum absolute difference between their corresponding components. When comparing two subsequences in the time series, the Chebyshev distance is calculated by taking the maximum absolute difference between their corresponding values at each point in time.</li> <li>The use of the Chebyshev metric in approximate entropy calculation has been found to be effective in detecting the presence of patterns or regularities in a time series. This is because the Chebyshev metric is less sensitive to outliers and noise in the data than other metrics, such as Euclidean distance or Manhattan distance.</li> <li>However, other metrics can also be used to calculate approximate entropy, depending on the specific characteristics of the time series being analyzed and the research question at hand. For example, the Euclidean distance can be used as an alternative to the Chebyshev metric, especially when the time series is relatively smooth and does not contain sharp spikes or discontinuities.</li> <li>In addition, other metrics such as Hamming distance or Cosine distance can be used in cases where the time series represents binary or categorical data, or when the time series has a natural geometric interpretation, respectively.</li> <li>Ultimately, the choice of metric depends on the specific application and the properties of the time series being analyzed. The use of different metrics can lead to different results and insights, and it is important to carefully consider the advantages and limitations of each approach before making a decision.</li> </ul> </li> </ul> <p>Outputs:</p> <ul> <li>A single number is returned, which represents the entropy score. It will be a float value, where numbers close to \\(0\\) indicate less entropy; meaning that it is more stable, regular and predictable.</li> </ul> <p>Expectations:</p> <ul> <li>A returned value close to \\(0\\) means that it is quite stable.</li> <li>'close to zero' is highly interpretable and context specific. For example, if you generate a sequence of random numbers between \\(0\\) and \\(1\\), then 'close to zero' would need to be down to 5 or 6 decimal places. But if you're looking at a sequence of numbers which range between \\(0\\) and \\(1000\\), then 'close to zero' would be anything less than, say, \\(20\\).</li> <li>When using other available metrics (such as <code>'euclidean'</code>, '<code>hamming'</code>, <code>'cosine'</code>, etc), the results may be a little unexpected. So, it's best to do some research around which metric is best to use for the specific purposes at hand.</li> </ul> <p>Credit</p> <p>All credit goes to the <code>AntroPy</code> library.</p> Examples Prepare data<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; from stochastic.processes import noise as sn\n&gt;&gt;&gt; data_airline = load_airline()\n&gt;&gt;&gt; rng = np.random.default_rng(seed=42)\n&gt;&gt;&gt; data_noise = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)\n&gt;&gt;&gt; data_random = rng.random(1000)\n</code></pre> Basic usage<pre><code>&gt;&gt;&gt; print(f\"{approx_entropy(x=data_airline):.4f}\")\n0.6451\n</code></pre> Gaussian noise<pre><code>&gt;&gt;&gt; print(f\"{approx_entropy(x=data_noise, order=2):.4f}\")\n2.1958\n</code></pre> Euclidean metric<pre><code>&gt;&gt;&gt; print(f\"{approx_entropy(x=data_noise, order=3, metric='euclidean'):.4f}\")\n1.5120\n</code></pre> Random data<pre><code>&gt;&gt;&gt; print(f\"{approx_entropy(x=data_random):.4f}\")\n1.8030\n</code></pre> References <ul> <li>Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049</li> <li>SK-Learn: Pairwise metrics, Affinities and Kernels</li> <li>Spatial data structures and algorithms</li> </ul> See Also <ul> <li><code>antropy.app_entropy</code></li> <li><code>antropy.sample_entropy</code></li> <li><code>antropy.perm_entropy</code></li> <li><code>antropy.spectral_entropy</code></li> <li><code>ts_stat_tests.algorithms.app_entropy</code></li> <li><code>ts_stat_tests.algorithms.approx_entropy</code></li> <li><code>ts_stat_tests.algorithms.perm_entropy</code></li> <li><code>ts_stat_tests.algorithms.spectral_entropy</code></li> <li><code>sklearn.neighbors.KDTree</code></li> <li><code>sklearn.metrics.pairwise_distances</code></li> <li><code>scipy.spatial.distance</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/regularity.py</code> <pre><code>@typechecked\ndef approx_entropy(\n    x: ArrayLike,\n    order: int = 2,\n    tolerance: Optional[float] = None,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n) -&gt; float:\n    \"\"\"\n    !!! note \"Summary\"\n        Approximate entropy is a measure of the amount of regularity or predictability in a time series. It is used to quantify the degree of self-similarity of a signal over different time scales, and can be useful for detecting underlying patterns or trends in data\n\n    ???+ abstract \"Details\"\n        Approximate entropy is a technique used to quantify the amount of regularity and the unpredictability of fluctuations over time-series data. Smaller values indicates that the data is more regular and predictable.\n\n        The tolerance value ($r$) is set to $0.2 \\\\times std(x)$.\n\n        To calculate approximate entropy, we first need to define a window size or scale factor, which determines the length of the subsequences that are used to compare the similarity of the time series. We then compare all possible pairs of subsequences within the time series and calculate the probability that two subsequences are within a certain tolerance level of each other, where the tolerance level is usually expressed as a percentage of the standard deviation of the time series.\n\n        The approximate entropy is then defined as the negative natural logarithm of the average probability of similarity across all possible pairs of subsequences, normalized by the length of the time series and the scale factor.\n\n        The approximate entropy measure is useful in a variety of applications, such as the analysis of physiological signals, financial time series, and climate data. It can be used to detect changes in the regularity or predictability of a time series over time, and can provide insights into the underlying dynamics or mechanisms that generate the signal. For example, a decrease in approximate entropy may indicate the onset of a disease or a shift in the underlying physiological state, while an increase in approximate entropy may suggest the presence of noise or other external influences on the system.\n\n        The equation for ApEn is:\n\n        $$\n        ApEn(m, r, N) = \u03c6m(r) - \u03c6m+1(r)\n        $$\n\n        where:\n\n        - $m$ is the embedding dimension,\n        - $r$ is the tolerance or similarity criterion,\n        - $N$ is the length of the time series, and\n        - $\u03c6m(r)$ and $\u03c6m+1(r)$ are the logarithms of the probabilities that two sequences of m data points in the time series that are similar to each other within a tolerance $r$ remain similar for the next data point, for $m$ and $m+1$, respectively.\n\n        ```\n        ApEn(m, r, N) = \u03c6m(r) - \u03c6m+1(r)\n        ```\n\n        The calculation of ApEn involves the following steps:\n\n        1. Create a set of vectors, each containing $m$ data points from the time series, where $m$ is the embedding dimension.\n        1. Calculate the Euclidean distance between each pair of vectors and count the number of pairs that are within a distance $r$ of each other.\n        1. Compute the probabilities $\u03c6m(r)$ and $\u03c6m+1(r)$ using the counts from step 2.\n        1. Compute $ApEn(m, r, N)$ using the equation above.\n\n        The value of ApEn ranges from zero ($0$) to infinity ($\\\\infty$), with lower values indicating higher regularity or predictability in the time series. A time series with high ApEn is more unpredictable or irregular, whereas a time series with low ApEn is more regular or predictable.\n\n        ApEn is often used in time series forecasting to assess the complexity of the data and to determine whether a time series is suitable for modeling with a particular forecasting method, such as ARIMA or neural networks.\n\n        When calculating the Approximate entropy requires the specification of a set of parameters that determine the characteristics of the time series. One of these parameters is the _embedding dimension_, which determines the number of values that are used to construct each permutation pattern.\n\n        The embedding dimension is important in the calculation of permutation entropy because it affects the sensitivity of the measure to different patterns in the data. If the embedding dimension is too small, we may miss important patterns or variations in the time series, and the resulting permutation entropy value may not accurately reflect the underlying complexity of the signal. On the other hand, if the embedding dimension is too large, we may overfit the data and produce a permutation entropy value that is overly sensitive to noise or other random fluctuations.\n\n        Choosing an appropriate embedding dimension is therefore crucial in ensuring that the permutation entropy calculation is robust and reliable, and captures the essential features of the time series in a meaningful way. This allows us to make more accurate and informative inferences about the behavior of the system that generated the data, and can be useful in a wide range of applications, from signal processing to data analysis and beyond.\n\n    Params:\n        x (ArrayLike):\n            One-dimensional time series of shape (n_times).\n        order (int, optional):\n            Embedding dimension.&lt;br&gt;\n            Defaults to `2`.\n        tolerance (Optional[float]):\n            Tolerance level or similarity criterion. If `None` (default), it is set to $0.2 \\times std(x)$.&lt;br&gt;\n            Defaults to `None`.\n        metric (VALID_KDTREE_METRIC_OPTIONS):\n            Name of the distance metric function used with [`sklearn.neighbors.KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree). Default is to use the [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance). For a full list of all available metrics, see [`sklearn.metrics.pairwise.distance_metrics`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html) and [`scipy.spatial.distance`](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)&lt;br&gt;\n            Defaults to `\"chebyshev\"`.\n\n    Returns:\n        (float):\n            Approximate Entropy score.\n\n    ???+ note \"Notes\"\n\n        **Inputs**:\n\n        - `x` is a 1-dimensional array.&lt;br&gt;\n            It represents time-series data, ideally with each element in the array being a measurement or value taken at regular time intervuls over the length of the array. The exact number of elements (or length of the array) can realistically be greater than 30 elements long, but ideally should be a few hundred elements long, and is even better when it is a few thousand elements long.\n\n        **Settings**:\n\n        - `order` is used for determining the number of values that are used to construct each permutation pattern.\n            - If the embedding dimension is too small, we may miss important patterns or variations in the time series, and the resulting approximate entropy value may not accurately reflect the underlying complexity of the signal.\n            - If the embedding dimension is too large, we may overfit the data and produce an approximate entropy value that is overly sensitive to noise or other random fluctuations.\n\n        - `metric` is used for determining which distance metric to use for the underlying distance-space between two time series.\n            - The Chebyshev metric is often used when calculating approximate entropy because it is a robust and computationally efficient way to measure the distance between two time series. The Chebyshev distance between two vectors is defined as the maximum absolute difference between their corresponding components. When comparing two subsequences in the time series, the Chebyshev distance is calculated by taking the maximum absolute difference between their corresponding values at each point in time.\n            - The use of the Chebyshev metric in approximate entropy calculation has been found to be effective in detecting the presence of patterns or regularities in a time series. This is because the Chebyshev metric is less sensitive to outliers and noise in the data than other metrics, such as Euclidean distance or Manhattan distance.\n            - However, other metrics can also be used to calculate approximate entropy, depending on the specific characteristics of the time series being analyzed and the research question at hand. For example, the Euclidean distance can be used as an alternative to the Chebyshev metric, especially when the time series is relatively smooth and does not contain sharp spikes or discontinuities.\n            - In addition, other metrics such as Hamming distance or Cosine distance can be used in cases where the time series represents binary or categorical data, or when the time series has a natural geometric interpretation, respectively.\n            - Ultimately, the choice of metric depends on the specific application and the properties of the time series being analyzed. The use of different metrics can lead to different results and insights, and it is important to carefully consider the advantages and limitations of each approach before making a decision.\n\n        **Outputs**:\n\n        - A single number is returned, which represents the entropy score. It will be a float value, where numbers close to $0$ indicate _less_ entropy; meaning that it is _more_ stable, regular and predictable.\n\n        **Expectations**:\n\n        - A returned value close to $0$ means that it is quite stable.\n        - 'close to zero' is highly interpretable and context specific. For example, if you generate a sequence of random numbers between $0$ and $1$, then 'close to zero' would need to be down to 5 or 6 decimal places. But if you're looking at a sequence of numbers which range between $0$ and $1000$, then 'close to zero' would be anything less than, say, $20$.\n        - When using other available metrics (such as `'euclidean'`, '`hamming'`, `'cosine'`, etc), the results may be a little unexpected. So, it's best to do some research around which metric is best to use for the specific purposes at hand.\n\n    !!! Success \"Credit\"\n        All credit goes to the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ???+ Example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Prepare data\"}\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; from stochastic.processes import noise as sn\n        &gt;&gt;&gt; data_airline = load_airline()\n        &gt;&gt;&gt; rng = np.random.default_rng(seed=42)\n        &gt;&gt;&gt; data_noise = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)\n        &gt;&gt;&gt; data_random = rng.random(1000)\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; print(f\"{approx_entropy(x=data_airline):.4f}\")\n        0.6451\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Gaussian noise\"}\n        &gt;&gt;&gt; print(f\"{approx_entropy(x=data_noise, order=2):.4f}\")\n        2.1958\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Euclidean metric\"}\n        &gt;&gt;&gt; print(f\"{approx_entropy(x=data_noise, order=3, metric='euclidean'):.4f}\")\n        1.5120\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Random data\"}\n        &gt;&gt;&gt; print(f\"{approx_entropy(x=data_random):.4f}\")\n        1.8030\n        ```\n\n    ??? Question \"References\"\n        - [Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049](https://journals.physiology.org/doi/epdf/10.1152/ajpheart.2000.278.6.H2039)\n        - [SK-Learn: Pairwise metrics, Affinities and Kernels](https://scikit-learn.org/stable/modules/metrics.html#metrics)\n        - [Spatial data structures and algorithms](https://docs.scipy.org/doc/scipy/tutorial/spatial.html)\n\n    ??? Tip \"See Also\"\n        - [`antropy.app_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.app_entropy.html)\n        - [`antropy.sample_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.sample_entropy.html)\n        - [`antropy.perm_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.perm_entropy.html)\n        - [`antropy.spectral_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.spectral_entropy.html)\n        - [`ts_stat_tests.algorithms.app_entropy`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`ts_stat_tests.algorithms.approx_entropy`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`ts_stat_tests.algorithms.perm_entropy`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`ts_stat_tests.algorithms.spectral_entropy`][ts_stat_tests.algorithms.regularity.spectral_entropy]\n        - [`sklearn.neighbors.KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html)\n        - [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html)\n        - [`scipy.spatial.distance`](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)\n    \"\"\"\n    return a_app_entropy(\n        x=x,\n        order=order,\n        tolerance=tolerance,\n        metric=metric,\n    )\n</code></pre>"},{"location":"code/regularity/#ts_stat_tests.algorithms.regularity.sample_entropy","title":"sample_entropy","text":"<pre><code>sample_entropy(\n    x: ArrayLike,\n    order: int = 2,\n    tolerance: Optional[float] = None,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n) -&gt; float\n</code></pre> <p>Summary</p> <p>Sample entropy is a measure of the amount of regularity or predictability in a time series. It is used to quantify the degree of self-similarity of a signal over different time scales, and can be useful for detecting underlying patterns or trends in data.</p> Details <p>Sample entropy is a modification of approximate entropy, used for assessing the complexity of physiological time-series signals. It has two advantages over approximate entropy: data length independence and a relatively trouble-free implementation. Large values indicate high complexity whereas smaller values characterize more self-similar and regular signals.</p> <p>The equation for sample entropy (SampEn) is as follows:</p> \\[ SampEn(m, r, N) = - \\log \\left( \\frac {Cm(r)} {Cm+1(r)} \\right) \\] <p>where:</p> <ul> <li>\\(m\\) is the embedding dimension,</li> <li>\\(r\\) is the tolerance or similarity criterion,</li> <li>\\(N\\) is the length of the time series, and</li> <li>\\(Cm(r)\\) and \\(Cm+1(r)\\) are the number of \\(m\\)-tuples (vectors of \\(m\\) consecutive data points) that have a distance less than or equal to \\(r\\), and \\((m+1)\\)-tuples with the same property, respectively. The log function is the natural logarithm.</li> </ul> <pre><code>SampEn(m, r, N) = -log( Cm(r) / Cm+1(r) )\n</code></pre> <p>The calculation of sample entropy involves the following steps:</p> <ol> <li>Choose the values of \\(m\\) and \\(r\\).</li> <li>Construct \\(m\\)-tuples from the time series data.</li> <li>Compute the number of \\(m\\)-tuples that are within a distance r of each other (\\(Cm(r)\\)).</li> <li>Compute the number of \\((m+1)\\)-tuples that are within a distance r of each other (\\(Cm+1(r)\\)).</li> <li>Compute the value of \\(SampEn\\) using the formula above.</li> </ol> <p>The value of SampEn ranges from zero (\\(0\\)) to infinity (\\(\\infty\\)), with lower values indicating higher regularity or predictability in the time series. A time series with high \\(SampEn\\) is more unpredictable or irregular, whereas a time series with low \\(SampEn\\) is more regular or predictable.</p> <p>Sample entropy is often used in time series forecasting to assess the complexity of the data and to determine whether a time series is suitable for modeling with a particular forecasting method, such as ARIMA or neural networks.</p> <p>Note that if <code>metric == 'chebyshev'</code> and <code>len(x) &lt; 5000</code> points, then the sample entropy is computed using a fast custom Numba script. For other distance metric or longer time-series, the sample entropy is computed using a code from the <code>mne-features</code> package by Jean-Baptiste Schiratti and Alexandre Gramfort (requires sklearn).</p> <p>To calculate the Sample entropy requires the specification of a set of parameters that determine the characteristics of the time series. One of these parameters is the embedding dimension, which determines the number of values that are used to construct each permutation pattern.</p> <p>The embedding dimension is important in the calculation of permutation entropy because it affects the sensitivity of the measure to different patterns in the data. If the embedding dimension is too small, we may miss important patterns or variations in the time series, and the resulting permutation entropy value may not accurately reflect the underlying complexity of the signal. On the other hand, if the embedding dimension is too large, we may overfit the data and produce a permutation entropy value that is overly sensitive to noise or other random fluctuations.</p> <p>Choosing an appropriate embedding dimension is therefore crucial in ensuring that the permutation entropy calculation is robust and reliable, and captures the essential features of the time series in a meaningful way. This allows us to make more accurate and informative inferences about the behavior of the system that generated the data, and can be useful in a wide range of applications, from signal processing to data analysis and beyond.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>One-dimensional time series of shape (n_times).</p> required <code>order</code> <code>int</code> <p>Embedding dimension. Defaults to <code>2</code>.</p> <code>2</code> <code>tolerance</code> <code>Optional[float]</code> <p>Tolerance level or similarity criterion. If <code>None</code> (default), it is set to \\(0.2     imes std(x)\\). Defaults to <code>None</code>.</p> <code>None</code> <code>metric</code> <code>VALID_KDTREE_METRIC_OPTIONS</code> <p>Name of the distance metric function used with <code>sklearn.neighbors.KDTree</code>. Default is to use the Chebyshev distance. For a full list of all available metrics, see <code>sklearn.metrics.pairwise.distance_metrics</code> and <code>scipy.spatial.distance</code> Defaults to <code>\"chebyshev\"</code>.</p> <code>'chebyshev'</code> <p>Returns:</p> Type Description <code>float</code> <p>Sample Entropy score.</p> <p>Credit</p> <p>All credit goes to the <code>AntroPy</code> library.</p> Examples Prepare data<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; from stochastic.processes import noise as sn\n&gt;&gt;&gt; data_airline = load_airline()\n&gt;&gt;&gt; rng = np.random.default_rng(seed=42)\n&gt;&gt;&gt; data_noise = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)\n&gt;&gt;&gt; data_random = rng.random(1000)\n&gt;&gt;&gt; data_sine = np.sin(2 * np.pi * 1 * np.arange(3000) / 100)\n&gt;&gt;&gt; data_line = np.arange(1000)\n</code></pre> Basic usage<pre><code>&gt;&gt;&gt; print(f\"{sample_entropy(x=data_airline):.4f}\")\n0.6177\n</code></pre> Gaussian noise<pre><code>&gt;&gt;&gt; print(f\"{sample_entropy(x=data_noise, order=2):.4f}\")\n2.1819\n</code></pre> Euclidean metric<pre><code>&gt;&gt;&gt; print(f\"{sample_entropy(x=data_noise, order=3, metric='euclidean'):.4f}\")\n2.6806\n</code></pre> Random data<pre><code>&gt;&gt;&gt; print(f\"{sample_entropy(x=data_random):.4f}\")\n2.1595\n</code></pre> Sine wave<pre><code>&gt;&gt;&gt; print(f\"{sample_entropy(x=data_sine):.4f}\")\n0.1633\n</code></pre> Straight line<pre><code>&gt;&gt;&gt; print(f\"{sample_entropy(x=data_line):.4f}\")\n0.0000\n</code></pre> References <ul> <li>Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049</li> <li>SK-Learn: Pairwise metrics, Affinities and Kernels</li> <li>Spatial data structures and algorithms</li> </ul> See Also <ul> <li><code>antropy.app_entropy</code></li> <li><code>antropy.sample_entropy</code></li> <li><code>antropy.perm_entropy</code></li> <li><code>antropy.spectral_entropy</code></li> <li><code>ts_stat_tests.algorithms.app_entropy</code></li> <li><code>ts_stat_tests.algorithms.approx_entropy</code></li> <li><code>ts_stat_tests.algorithms.perm_entropy</code></li> <li><code>ts_stat_tests.algorithms.spectral_entropy</code></li> <li><code>sklearn.neighbors.KDTree</code></li> <li><code>sklearn.metrics.pairwise_distances</code></li> <li><code>scipy.spatial.distance</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/regularity.py</code> <pre><code>@typechecked\ndef sample_entropy(\n    x: ArrayLike,\n    order: int = 2,\n    tolerance: Optional[float] = None,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n) -&gt; float:\n    \"\"\"\n    !!! note \"Summary\"\n        Sample entropy is a measure of the amount of regularity or predictability in a time series. It is used to quantify the degree of self-similarity of a signal over different time scales, and can be useful for detecting underlying patterns or trends in data.\n\n    ???+ abstract \"Details\"\n        Sample entropy is a modification of approximate entropy, used for assessing the complexity of physiological time-series signals. It has two advantages over approximate entropy: data length independence and a relatively trouble-free implementation. Large values indicate high complexity whereas smaller values characterize more self-similar and regular signals.\n\n        The equation for sample entropy (SampEn) is as follows:\n\n        $$\n        SampEn(m, r, N) = - \\\\log \\\\left( \\\\frac {Cm(r)} {Cm+1(r)} \\\\right)\n        $$\n\n        where:\n\n        - $m$ is the embedding dimension,\n        - $r$ is the tolerance or similarity criterion,\n        - $N$ is the length of the time series, and\n        - $Cm(r)$ and $Cm+1(r)$ are the number of $m$-tuples (vectors of $m$ consecutive data points) that have a distance less than or equal to $r$, and $(m+1)$-tuples with the same property, respectively. The log function is the natural logarithm.\n\n        ```\n        SampEn(m, r, N) = -log( Cm(r) / Cm+1(r) )\n        ```\n\n        The calculation of sample entropy involves the following steps:\n\n        1. Choose the values of $m$ and $r$.\n        1. Construct $m$-tuples from the time series data.\n        1. Compute the number of $m$-tuples that are within a distance r of each other ($Cm(r)$).\n        1. Compute the number of $(m+1)$-tuples that are within a distance r of each other ($Cm+1(r)$).\n        1. Compute the value of $SampEn$ using the formula above.\n\n        The value of SampEn ranges from zero ($0$) to infinity ($\\\\infty$), with lower values indicating higher regularity or predictability in the time series. A time series with high $SampEn$ is more unpredictable or irregular, whereas a time series with low $SampEn$ is more regular or predictable.\n\n        Sample entropy is often used in time series forecasting to assess the complexity of the data and to determine whether a time series is suitable for modeling with a particular forecasting method, such as ARIMA or neural networks.\n\n        Note that if `metric == 'chebyshev'` and `len(x) &lt; 5000` points, then the sample entropy is computed using a fast custom Numba script. For other distance metric or longer time-series, the sample entropy is computed using a code from the [`mne-features`](https://mne.tools/mne-features/) package by Jean-Baptiste Schiratti and Alexandre Gramfort (requires sklearn).\n\n        To calculate the Sample entropy requires the specification of a set of parameters that determine the characteristics of the time series. One of these parameters is the _embedding dimension_, which determines the number of values that are used to construct each permutation pattern.\n\n        The embedding dimension is important in the calculation of permutation entropy because it affects the sensitivity of the measure to different patterns in the data. If the embedding dimension is too small, we may miss important patterns or variations in the time series, and the resulting permutation entropy value may not accurately reflect the underlying complexity of the signal. On the other hand, if the embedding dimension is too large, we may overfit the data and produce a permutation entropy value that is overly sensitive to noise or other random fluctuations.\n\n        Choosing an appropriate embedding dimension is therefore crucial in ensuring that the permutation entropy calculation is robust and reliable, and captures the essential features of the time series in a meaningful way. This allows us to make more accurate and informative inferences about the behavior of the system that generated the data, and can be useful in a wide range of applications, from signal processing to data analysis and beyond.\n\n    Params:\n        x (ArrayLike):\n            One-dimensional time series of shape (n_times).\n        order (int, optional):\n            Embedding dimension.&lt;br&gt;\n            Defaults to `2`.\n        tolerance (Optional[float]):\n            Tolerance level or similarity criterion. If `None` (default), it is set to $0.2 \\times std(x)$.&lt;br&gt;\n            Defaults to `None`.\n        metric (VALID_KDTREE_METRIC_OPTIONS):\n            Name of the distance metric function used with [`sklearn.neighbors.KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree). Default is to use the [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance). For a full list of all available metrics, see [`sklearn.metrics.pairwise.distance_metrics`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html) and [`scipy.spatial.distance`](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)&lt;br&gt;\n            Defaults to `\"chebyshev\"`.\n\n    Returns:\n        (float):\n            Sample Entropy score.\n\n    !!! Success \"Credit\"\n        All credit goes to the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ???+ Example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Prepare data\"}\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; from stochastic.processes import noise as sn\n        &gt;&gt;&gt; data_airline = load_airline()\n        &gt;&gt;&gt; rng = np.random.default_rng(seed=42)\n        &gt;&gt;&gt; data_noise = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)\n        &gt;&gt;&gt; data_random = rng.random(1000)\n        &gt;&gt;&gt; data_sine = np.sin(2 * np.pi * 1 * np.arange(3000) / 100)\n        &gt;&gt;&gt; data_line = np.arange(1000)\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; print(f\"{sample_entropy(x=data_airline):.4f}\")\n        0.6177\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Gaussian noise\"}\n        &gt;&gt;&gt; print(f\"{sample_entropy(x=data_noise, order=2):.4f}\")\n        2.1819\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Euclidean metric\"}\n        &gt;&gt;&gt; print(f\"{sample_entropy(x=data_noise, order=3, metric='euclidean'):.4f}\")\n        2.6806\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Random data\"}\n        &gt;&gt;&gt; print(f\"{sample_entropy(x=data_random):.4f}\")\n        2.1595\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Sine wave\"}\n        &gt;&gt;&gt; print(f\"{sample_entropy(x=data_sine):.4f}\")\n        0.1633\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Straight line\"}\n        &gt;&gt;&gt; print(f\"{sample_entropy(x=data_line):.4f}\")\n        0.0000\n        ```\n\n    ??? Question \"References\"\n        - [Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049](https://journals.physiology.org/doi/epdf/10.1152/ajpheart.2000.278.6.H2039)\n        - [SK-Learn: Pairwise metrics, Affinities and Kernels](https://scikit-learn.org/stable/modules/metrics.html#metrics)\n        - [Spatial data structures and algorithms](https://docs.scipy.org/doc/scipy/tutorial/spatial.html)\n\n    ??? Tip \"See Also\"\n        - [`antropy.app_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.app_entropy.html)\n        - [`antropy.sample_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.sample_entropy.html)\n        - [`antropy.perm_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.perm_entropy.html)\n        - [`antropy.spectral_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.spectral_entropy.html)\n        - [`ts_stat_tests.algorithms.app_entropy`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`ts_stat_tests.algorithms.approx_entropy`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`ts_stat_tests.algorithms.perm_entropy`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`ts_stat_tests.algorithms.spectral_entropy`][ts_stat_tests.algorithms.regularity.spectral_entropy]\n        - [`sklearn.neighbors.KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html)\n        - [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html)\n        - [`scipy.spatial.distance`](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)\n    \"\"\"\n    return a_sample_entropy(\n        x=x,\n        order=order,\n        tolerance=tolerance,\n        metric=metric,\n    )\n</code></pre>"},{"location":"code/regularity/#ts_stat_tests.algorithms.regularity.permutation_entropy","title":"permutation_entropy","text":"<pre><code>permutation_entropy(\n    x: ArrayLike,\n    order: int = 3,\n    delay: Union[int, list, ndarray] = 1,\n    normalize: bool = False,\n) -&gt; float\n</code></pre> <p>Summary</p> <p>Permutation entropy is a measure of the complexity or randomness of a time series. It is based on the idea of permuting the order of the values in the time series and calculating the entropy of the resulting permutation patterns.</p> Details <p>The permutation entropy is a complexity measure for time-series first introduced by Bandt and Pompe in 2002.</p> <p>The formula for permutation entropy is as follows:</p> \\[ PE(n) = - \\sum_{n=0}^{n!} \\times p(i) \\times \\text{log2}(p(i)) \\] <p>where:</p> <ul> <li>\\(n\\) is the length of the sliding window,</li> <li>\\(p(i)\\) is the probability of the i-th ordinal pattern, and</li> <li>the sum is taken over all possible ordinal patterns.</li> <li>The logarithm function is base 2.</li> </ul> <pre><code>PE(n) = - \u2211 p(i) * log2(p(i))\n</code></pre> <p>The calculation of permutation entropy involves the following steps:</p> <ol> <li>Choose the length of the sliding window (\\(n\\)).</li> <li>Construct the set of all possible ordinal patterns of length \\(n\\).</li> <li>Compute the frequency of occurrence of each ordinal pattern in the time series.</li> <li>Compute the probability of occurrence of each ordinal pattern by dividing its frequency by the total number of ordinal patterns.</li> <li>Compute the value of permutation entropy using the formula above.</li> </ol> <p>The value of permutation entropy ranges from \\(0\\) to \\(log2(n!)\\), with lower values indicating higher regularity or predictability in the time series. A time series with high permutation entropy is more unpredictable or irregular, whereas a time series with low permutation entropy is more regular or predictable.</p> <p>Permutation entropy is often used in time series forecasting to assess the complexity of the data and to determine whether a time series is suitable for modeling with a particular forecasting method, such as ARIMA or neural networks. It is particularly useful for detecting nonlinear dynamics and nonstationarity in the data.</p> <p>This is the information contained in comparing \\(n\\) consecutive values of the time series. It is clear that \\(0 \u2264 H (n) \u2264 \\text{log2}(n!)\\) where the lower bound is attained for an increasing or decreasing sequence of values, and the upper bound for a completely random system where all \\(n!\\) possible permutations appear with the same probability.</p> <p>To calculate the Permutation entropy requires the specification of a set of parameters that determine the characteristics of the time series. One of these parameters is the embedding dimension, which determines the number of values that are used to construct each permutation pattern.</p> <p>The embedding dimension is important in the calculation of permutation entropy because it affects the sensitivity of the measure to different patterns in the data. If the embedding dimension is too small, we may miss important patterns or variations in the time series, and the resulting permutation entropy value may not accurately reflect the underlying complexity of the signal. On the other hand, if the embedding dimension is too large, we may overfit the data and produce a permutation entropy value that is overly sensitive to noise or other random fluctuations.</p> <p>Choosing an appropriate embedding dimension is therefore crucial in ensuring that the permutation entropy calculation is robust and reliable, and captures the essential features of the time series in a meaningful way. This allows us to make more accurate and informative inferences about the behavior of the system that generated the data, and can be useful in a wide range of applications, from signal processing to data analysis and beyond.</p> <p>The embedded matrix \\(Y\\) is created by:</p> \\[ \\begin{align}     y(i) &amp;= [x_i,x_{i+\\text{delay}}, ...,x_{i+(\\text{order}-1) * \\text{delay}}] \\\\     Y &amp;= [y(1),y(2),...,y(N-(\\text{order}-1))*\\text{delay})]^T \\end{align} \\] <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>One-dimensional time series of shape (n_times).</p> required <code>order</code> <code>int</code> <p>Order of permutation entropy. Defaults to <code>3</code>.</p> <code>3</code> <code>delay</code> <code>Union[int, list, ndarray]</code> <p>Time delay (lag). If multiple values are passed (e.g. [1, 2, 3]), AntroPy will calculate the average permutation entropy across all these delays. Defaults to <code>1</code>.</p> <code>1</code> <code>normalize</code> <code>bool</code> <p>If True, divide by \\(log2(order!)\\) to normalize the entropy between \\(0\\) and \\(1\\). Otherwise, return the permutation entropy in bit. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>The entropy of the data set.</p> <p>Credit</p> <ul> <li>All credit goes to the <code>entropy.perm_entropy</code> library.</li> </ul> <p>Examples</p> Prepare data<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; from stochastic.processes import noise as sn\n&gt;&gt;&gt; x = [4, 7, 9, 10, 6, 11, 3]\n&gt;&gt;&gt; data_airline = load_airline()\n&gt;&gt;&gt; rng = np.random.default_rng(seed=42)\n&gt;&gt;&gt; data_noise = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)\n&gt;&gt;&gt; data_random = rng.random(1000)\n&gt;&gt;&gt; data_sine = np.sin(2 * np.pi * 1 * np.arange(3000) / 100)\n&gt;&gt;&gt; data_line = np.arange(1000)\n</code></pre> Basic usage<pre><code>&gt;&gt;&gt; print(f\"{permutation_entropy(x=data_airline):.4f}\")\n2.3601\n</code></pre> Simple series<pre><code>&gt;&gt;&gt; print(f\"{permutation_entropy(x=x, order=2):.4f}\")\n0.9183\n</code></pre> Normalised series<pre><code>&gt;&gt;&gt; print(f\"{permutation_entropy(x=x, order=2, normalize=True):.4f}\")\n0.9183\n</code></pre> Gaussian noise<pre><code>&gt;&gt;&gt; print(f\"{permutation_entropy(x=data_noise, order=2):.4f}\")\n0.9999\n</code></pre> Normalized noise<pre><code>&gt;&gt;&gt; print(f\"{permutation_entropy(x=data_noise, order=2, normalize=True):.4f}\")\n0.9999\n</code></pre> Multiple delays<pre><code>&gt;&gt;&gt; print(f\"{permutation_entropy(x=data_noise, delay=[1, 2, 3], normalize=True):.4f}\")\n0.9999\n</code></pre> <p>Random data<pre><code>&gt;&gt;&gt; print(f\"{permutation_entropy(x=data_random, normalize=True):.4f}\")\n0.9991\n</code></pre> Sine wave<pre><code>&gt;&gt;&gt; print(f\"{permutation_entropy(x=data_sine, normalize=True):.4f}\")\n0.4463\n</code></pre></p> Straight line<pre><code>&gt;&gt;&gt; print(f\"{permutation_entropy(x=data_line, normalize=True):.4f}\")\n0.0000\n</code></pre> References <ul> <li>Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for time series.\" Physical review letters 88.17 (2002): 174102</li> </ul> See Also <ul> <li><code>antropy.app_entropy</code></li> <li><code>antropy.sample_entropy</code></li> <li><code>antropy.perm_entropy</code></li> <li><code>antropy.spectral_entropy</code></li> <li><code>ts_stat_tests.algorithms.app_entropy</code></li> <li><code>ts_stat_tests.algorithms.approx_entropy</code></li> <li><code>ts_stat_tests.algorithms.perm_entropy</code></li> <li><code>ts_stat_tests.algorithms.spectral_entropy</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/regularity.py</code> <pre><code>@typechecked\ndef permutation_entropy(\n    x: ArrayLike,\n    order: int = 3,\n    delay: Union[int, list, np.ndarray] = 1,\n    normalize: bool = False,\n) -&gt; float:\n    \"\"\"\n    !!! note \"Summary\"\n        Permutation entropy is a measure of the complexity or randomness of a time series. It is based on the idea of permuting the order of the values in the time series and calculating the entropy of the resulting permutation patterns.\n\n    ???+ abstract \"Details\"\n        The permutation entropy is a complexity measure for time-series first introduced by Bandt and Pompe in 2002.\n\n        The formula for permutation entropy is as follows:\n\n        $$\n        PE(n) = - \\\\sum_{n=0}^{n!} \\\\times p(i) \\\\times \\\\text{log2}(p(i))\n        $$\n\n        where:\n\n        - $n$ is the length of the sliding window,\n        - $p(i)$ is the probability of the i-th ordinal pattern, and\n        - the sum is taken over all possible ordinal patterns.\n        - The logarithm function is base 2.\n\n        ```\n        PE(n) = - \u2211 p(i) * log2(p(i))\n        ```\n\n        The calculation of permutation entropy involves the following steps:\n\n        1. Choose the length of the sliding window ($n$).\n        1. Construct the set of all possible ordinal patterns of length $n$.\n        1. Compute the frequency of occurrence of each ordinal pattern in the time series.\n        1. Compute the probability of occurrence of each ordinal pattern by dividing its frequency by the total number of ordinal patterns.\n        1. Compute the value of permutation entropy using the formula above.\n\n        The value of permutation entropy ranges from $0$ to $log2(n!)$, with lower values indicating higher regularity or predictability in the time series. A time series with high permutation entropy is more unpredictable or irregular, whereas a time series with low permutation entropy is more regular or predictable.\n\n        Permutation entropy is often used in time series forecasting to assess the complexity of the data and to determine whether a time series is suitable for modeling with a particular forecasting method, such as ARIMA or neural networks. It is particularly useful for detecting nonlinear dynamics and nonstationarity in the data.\n\n        This is the information contained in comparing $n$ consecutive values of the time series. It is clear that $0 \u2264 H (n) \u2264 \\\\text{log2}(n!)$ where the lower bound is attained for an increasing or decreasing sequence of values, and the upper bound for a completely random system where all $n!$ possible permutations appear with the same probability.\n\n        To calculate the Permutation entropy requires the specification of a set of parameters that determine the characteristics of the time series. One of these parameters is the _embedding dimension_, which determines the number of values that are used to construct each permutation pattern.\n\n        The embedding dimension is important in the calculation of permutation entropy because it affects the sensitivity of the measure to different patterns in the data. If the embedding dimension is too small, we may miss important patterns or variations in the time series, and the resulting permutation entropy value may not accurately reflect the underlying complexity of the signal. On the other hand, if the embedding dimension is too large, we may overfit the data and produce a permutation entropy value that is overly sensitive to noise or other random fluctuations.\n\n        Choosing an appropriate embedding dimension is therefore crucial in ensuring that the permutation entropy calculation is robust and reliable, and captures the essential features of the time series in a meaningful way. This allows us to make more accurate and informative inferences about the behavior of the system that generated the data, and can be useful in a wide range of applications, from signal processing to data analysis and beyond.\n\n        The embedded matrix $Y$ is created by:\n\n        $$\n        \\\\begin{align}\n            y(i) &amp;= [x_i,x_{i+\\\\text{delay}}, ...,x_{i+(\\\\text{order}-1) * \\\\text{delay}}] \\\\\\\\\n            Y &amp;= [y(1),y(2),...,y(N-(\\\\text{order}-1))*\\\\text{delay})]^T\n        \\\\end{align}\n        $$\n\n    Params:\n        x (ArrayLike):\n            One-dimensional time series of shape (n_times).\n        order (int, optional):\n            Order of permutation entropy.&lt;br&gt;\n            Defaults to `3`.\n        delay (Union[int, list, np.ndarray], optional):\n            Time delay (lag). If multiple values are passed (e.g. [1, 2, 3]), AntroPy will calculate the average permutation entropy across all these delays.&lt;br&gt;\n            Defaults to `1`.\n        normalize (bool, optional):\n            If True, divide by $log2(order!)$ to normalize the entropy between $0$ and $1$. Otherwise, return the permutation entropy in bit.&lt;br&gt;\n            Defaults to `False`.\n\n    Returns:\n        (float):\n            The entropy of the data set.\n\n    !!! success \"Credit\"\n        - All credit goes to the [`entropy.perm_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.perm_entropy.html) library.\n\n    !!! example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Prepare data\"}\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; from stochastic.processes import noise as sn\n        &gt;&gt;&gt; x = [4, 7, 9, 10, 6, 11, 3]\n        &gt;&gt;&gt; data_airline = load_airline()\n        &gt;&gt;&gt; rng = np.random.default_rng(seed=42)\n        &gt;&gt;&gt; data_noise = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)\n        &gt;&gt;&gt; data_random = rng.random(1000)\n        &gt;&gt;&gt; data_sine = np.sin(2 * np.pi * 1 * np.arange(3000) / 100)\n        &gt;&gt;&gt; data_line = np.arange(1000)\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; print(f\"{permutation_entropy(x=data_airline):.4f}\")\n        2.3601\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Simple series\"}\n        &gt;&gt;&gt; print(f\"{permutation_entropy(x=x, order=2):.4f}\")\n        0.9183\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Normalised series\"}\n        &gt;&gt;&gt; print(f\"{permutation_entropy(x=x, order=2, normalize=True):.4f}\")\n        0.9183\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Gaussian noise\"}\n        &gt;&gt;&gt; print(f\"{permutation_entropy(x=data_noise, order=2):.4f}\")\n        0.9999\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Normalized noise\"}\n        &gt;&gt;&gt; print(f\"{permutation_entropy(x=data_noise, order=2, normalize=True):.4f}\")\n        0.9999\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Multiple delays\"}\n        &gt;&gt;&gt; print(f\"{permutation_entropy(x=data_noise, delay=[1, 2, 3], normalize=True):.4f}\")\n        0.9999\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Random data\"}\n        &gt;&gt;&gt; print(f\"{permutation_entropy(x=data_random, normalize=True):.4f}\")\n        0.9991\n        ```\n        ```pycon {.py .python linenums=\"1\" title=\"Sine wave\"}\n        &gt;&gt;&gt; print(f\"{permutation_entropy(x=data_sine, normalize=True):.4f}\")\n        0.4463\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Straight line\"}\n        &gt;&gt;&gt; print(f\"{permutation_entropy(x=data_line, normalize=True):.4f}\")\n        0.0000\n        ```\n\n    ??? question \"References\"\n        - [Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for time series.\" Physical review letters 88.17 (2002): 174102](http://materias.df.uba.ar/dnla2019c1/files/2019/03/permutation_entropy.pdf)\n\n    ??? tip \"See Also\"\n        - [`antropy.app_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.app_entropy.html)\n        - [`antropy.sample_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.sample_entropy.html)\n        - [`antropy.perm_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.perm_entropy.html)\n        - [`antropy.spectral_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.spectral_entropy.html)\n        - [`ts_stat_tests.algorithms.app_entropy`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`ts_stat_tests.algorithms.approx_entropy`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`ts_stat_tests.algorithms.perm_entropy`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`ts_stat_tests.algorithms.spectral_entropy`][ts_stat_tests.algorithms.regularity.spectral_entropy]\n    \"\"\"\n    return cast(\n        float,\n        a_perm_entropy(\n            x=x,\n            order=order,\n            delay=cast(Any, delay),\n            normalize=normalize,\n        ),\n    )\n</code></pre>"},{"location":"code/regularity/#ts_stat_tests.algorithms.regularity.spectral_entropy","title":"spectral_entropy","text":"<pre><code>spectral_entropy(\n    x: ArrayLike,\n    sf: float = 1,\n    method: VALID_SPECTRAL_ENTROPY_METHOD_OPTIONS = \"fft\",\n    nperseg: Optional[int] = None,\n    normalize: bool = False,\n    axis: int = -1,\n) -&gt; Union[float, np.ndarray]\n</code></pre> <p>Summary</p> <p>Spectral entropy is a measure of the amount of complexity or unpredictability in a signal's frequency domain representation. It is used to quantify the degree of randomness or regularity in the power spectrum of a signal, which is a graphical representation of the distribution of power across different frequencies.</p> Details <p>Spectral Entropy is also a measure of the distribution of power or energy in the frequency domain of a time series. It is based on the Shannon entropy, which is a measure of the uncertainty or information content of a probability distribution</p> <p>Spectral Entropy is defined to be the Shannon entropy of the power spectral density (\\(PSD\\)) of the data:</p> \\[ H(x,f_s) =  -\\sum_{i=0}^{f_s/2} \\times P(i) \\times \\text{log2}(P(i)) \\] <p>where:</p> <ul> <li>\\(P\\) is the normalised \\(PSD\\), which is the proportion of power or energy at the \\(i\\)-th frequency band, and</li> <li>\\(f_s\\) is the sampling frequency.</li> <li>The logarithm function is base 2.</li> </ul> <pre><code>SE = - \u2211 p(i) * log2(p(i))\n</code></pre> <p>The calculation of spectral entropy involves the following steps:</p> <ol> <li>Compute the power or energy spectral density of the time series using a spectral analysis technique, such as the fast Fourier transform (FFT).</li> <li>Divide the frequency range of interest into non-overlapping frequency bands.</li> <li>Compute the proportion of power or energy in each frequency band by integrating the spectral density over the band.</li> <li>Compute the value of spectral entropy using the formula above.</li> </ol> <p>The value of spectral entropy ranges from \\(0\\) to \\(\\text{log2}(N)\\), where \\(N\\) is the number of frequency bands. Lower values indicate a more concentrated or regular distribution of power or energy in the frequency domain, while higher values indicate a more spread-out or irregular distribution.</p> <p>Spectral entropy is often used in time series forecasting to assess the complexity of the data and to determine whether a time series is suitable for modeling with a particular forecasting method, such as spectral analysis or machine learning algorithms. It is particularly useful for detecting periodicity and cyclical patterns in the data, as well as changes in the frequency distribution over time.</p> <p>To calculate spectral entropy, we first need to compute the power spectrum of the signal using a Fourier transform or other spectral analysis method. The power spectrum represents the energy of the signal at different frequencies, and can be visualized as a graph of power versus frequency.</p> <p>Once we have the power spectrum, we can calculate the spectral entropy by applying Shannon entropy to the distribution of power across different frequencies. Shannon entropy is a measure of the amount of information or uncertainty in a probability distribution, and is given by the negative sum of the product of the probability of each frequency bin and the logarithm of that probability.</p> <p>Spectral entropy is useful in a variety of applications, such as signal processing, acoustics, and neuroscience. It can be used to characterize the complexity or regularity of a signal's frequency content, and can provide insights into the underlying processes or mechanisms that generated the signal. For example, high spectral entropy may indicate the presence of multiple sources or processes with different frequencies, while low spectral entropy may suggest the presence of a single dominant frequency or periodicity.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p><code>1-D</code> or <code>N-D</code> data array.</p> required <code>sf</code> <code>float</code> <p>Sampling frequency, in Hz. Defaults to <code>1</code>.</p> <code>1</code> <code>method</code> <code>VALID_SPECTRAL_ENTROPY_METHOD_OPTIONS</code> <p>Spectral estimation method: - <code>'fft'</code>: Fourier Transformation (<code>scipy.signal.periodogram()</code>) - <code>'welch'</code>: Welch periodogram (<code>scipy.signal.welch()</code>) Defaults to <code>\"fft\"</code>.</p> <code>'fft'</code> <code>nperseg</code> <code>Optional[int]</code> <p>Length of each FFT segment for Welch method. If <code>None</code>, uses <code>scipy</code>'s default of 256 samples. Defaults to <code>None</code>.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>If <code>True</code>, divide by \\(log2(psd.size)\\) to normalize the spectral entropy to be between \\(0\\) and \\(1\\). Otherwise, return the spectral entropy in bit. Defaults to <code>False</code>.</p> <code>False</code> <code>axis</code> <code>int</code> <p>The axis along which the entropy is calculated. Default is the last axis. Defaults to <code>-1</code>.</p> <code>-1</code> <p>Returns:</p> Type Description <code>float</code> <p>Spectral Entropy score.</p> <p>Credit</p> <p>All credit goes to the <code>AntroPy</code> library.</p> Examples Prepare data<pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from sktime.datasets import load_airline\n&gt;&gt;&gt; from stochastic.processes import noise as sn\n&gt;&gt;&gt; sf, dur = 100, 4\n&gt;&gt;&gt; N = sf * dur\n&gt;&gt;&gt; data_time = np.arange(N)\n&gt;&gt;&gt; data_sine = np.sin(2 * np.pi * 1 * data_time)\n&gt;&gt;&gt; rng = np.random.default_rng(seed=42)\n&gt;&gt;&gt; data_noise = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)\n&gt;&gt;&gt; data_2d = rng.normal(size=(4, 3000))\n&gt;&gt;&gt; data_airline = load_airline()\n</code></pre> Basic usage<pre><code>&gt;&gt;&gt; print(f\"{spectral_entropy(x=data_airline, sf=12):.4f}\")\n2.6538\n</code></pre> Sine wave<pre><code>&gt;&gt;&gt; print(f\"{spectral_entropy(x=data_sine, sf=100, method='fft'):.4f}\")\n6.2329\n</code></pre> Welch method<pre><code>&gt;&gt;&gt; print(f\"{spectral_entropy(x=data_sine, sf=100, method='welch'):.4f}\")\n1.2924\n</code></pre> Normalised calculation<pre><code>&gt;&gt;&gt; print(f\"{spectral_entropy(x=data_sine, sf=100, method='welch', normalize=True):.4f}\")\n0.9956\n</code></pre> 2D data<pre><code>&gt;&gt;&gt; print(spectral_entropy(x=data_2d, sf=100, normalize=True).tolist())\n[0.9426, 0.9382, 0.9410, 0.9376]\n</code></pre> Gaussian noise<pre><code>&gt;&gt;&gt; print(f\"{spectral_entropy(x=data_noise, sf=100, normalize=True):.4f}\")\n0.9505\n</code></pre> References <ul> <li>Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.</li> <li>https://en.wikipedia.org/wiki/Spectral_density</li> <li>https://en.wikipedia.org/wiki/Welch%27s_method</li> </ul> See Also <ul> <li><code>antropy.app_entropy</code></li> <li><code>antropy.sample_entropy</code></li> <li><code>antropy.perm_entropy</code></li> <li><code>antropy.spectral_entropy</code></li> <li><code>ts_stat_tests.algorithms.app_entropy</code></li> <li><code>ts_stat_tests.algorithms.approx_entropy</code></li> <li><code>ts_stat_tests.algorithms.perm_entropy</code></li> <li><code>ts_stat_tests.algorithms.spectral_entropy</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/regularity.py</code> <pre><code>@typechecked\ndef spectral_entropy(\n    x: ArrayLike,\n    sf: float = 1,\n    method: VALID_SPECTRAL_ENTROPY_METHOD_OPTIONS = \"fft\",\n    nperseg: Optional[int] = None,\n    normalize: bool = False,\n    axis: int = -1,\n) -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    !!! note \"Summary\"\n\n        Spectral entropy is a measure of the amount of complexity or unpredictability in a signal's frequency domain representation. It is used to quantify the degree of randomness or regularity in the power spectrum of a signal, which is a graphical representation of the distribution of power across different frequencies.\n\n\n    ???+ abstract \"Details\"\n\n        Spectral Entropy is also a measure of the distribution of power or energy in the frequency domain of a time series. It is based on the Shannon entropy, which is a measure of the uncertainty or information content of a probability distribution\n\n        Spectral Entropy is defined to be the Shannon entropy of the power spectral density ($PSD$) of the data:\n\n        $$\n        H(x,f_s) =  -\\\\sum_{i=0}^{f_s/2} \\\\times P(i) \\\\times \\\\text{log2}(P(i))\n        $$\n\n        where:\n\n        - $P$ is the normalised $PSD$, which is the proportion of power or energy at the $i$-th frequency band, and\n        - $f_s$ is the sampling frequency.\n        - The logarithm function is base 2.\n\n        ```\n        SE = - \u2211 p(i) * log2(p(i))\n        ```\n\n        The calculation of spectral entropy involves the following steps:\n\n        1. Compute the power or energy spectral density of the time series using a spectral analysis technique, such as the fast Fourier transform (FFT).\n        1. Divide the frequency range of interest into non-overlapping frequency bands.\n        1. Compute the proportion of power or energy in each frequency band by integrating the spectral density over the band.\n        1. Compute the value of spectral entropy using the formula above.\n\n        The value of spectral entropy ranges from $0$ to $\\\\text{log2}(N)$, where $N$ is the number of frequency bands. Lower values indicate a more concentrated or regular distribution of power or energy in the frequency domain, while higher values indicate a more spread-out or irregular distribution.\n\n        Spectral entropy is often used in time series forecasting to assess the complexity of the data and to determine whether a time series is suitable for modeling with a particular forecasting method, such as spectral analysis or machine learning algorithms. It is particularly useful for detecting periodicity and cyclical patterns in the data, as well as changes in the frequency distribution over time.\n\n        To calculate spectral entropy, we first need to compute the power spectrum of the signal using a Fourier transform or other spectral analysis method. The power spectrum represents the energy of the signal at different frequencies, and can be visualized as a graph of power versus frequency.\n\n        Once we have the power spectrum, we can calculate the spectral entropy by applying Shannon entropy to the distribution of power across different frequencies. Shannon entropy is a measure of the amount of information or uncertainty in a probability distribution, and is given by the negative sum of the product of the probability of each frequency bin and the logarithm of that probability.\n\n        Spectral entropy is useful in a variety of applications, such as signal processing, acoustics, and neuroscience. It can be used to characterize the complexity or regularity of a signal's frequency content, and can provide insights into the underlying processes or mechanisms that generated the signal. For example, high spectral entropy may indicate the presence of multiple sources or processes with different frequencies, while low spectral entropy may suggest the presence of a single dominant frequency or periodicity.\n\n    Params:\n        x (ArrayLike):\n            `1-D` or `N-D` data array.\n        sf (float, optional):\n            Sampling frequency, in Hz.&lt;br&gt;\n            Defaults to `1`.\n        method (VALID_SPECTRAL_ENTROPY_METHOD_OPTIONS):\n            Spectral estimation method:&lt;br&gt;\n            - `'fft'`: Fourier Transformation ([`scipy.signal.periodogram()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.periodogram.html#scipy.signal.periodogram))&lt;br&gt;\n            - `'welch'`: Welch periodogram ([`scipy.signal.welch()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.welch.html#scipy.signal.welch))&lt;br&gt;\n            Defaults to `\"fft\"`.\n        nperseg (Optional[int]):\n            Length of each FFT segment for Welch method. If `None`, uses `scipy`'s default of 256 samples.&lt;br&gt;\n            Defaults to `None`.\n        normalize (bool, optional):\n            If `True`, divide by $log2(psd.size)$ to normalize the spectral entropy to be between $0$ and $1$. Otherwise, return the spectral entropy in bit.&lt;br&gt;\n            Defaults to `False`.\n        axis (int, optional):\n            The axis along which the entropy is calculated. Default is the last axis.&lt;br&gt;\n            Defaults to `-1`.\n\n    Returns:\n        (float):\n            Spectral Entropy score.\n\n    !!! Success \"Credit\"\n        All credit goes to the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ???+ Example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Prepare data\"}\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from sktime.datasets import load_airline\n        &gt;&gt;&gt; from stochastic.processes import noise as sn\n        &gt;&gt;&gt; sf, dur = 100, 4\n        &gt;&gt;&gt; N = sf * dur\n        &gt;&gt;&gt; data_time = np.arange(N)\n        &gt;&gt;&gt; data_sine = np.sin(2 * np.pi * 1 * data_time)\n        &gt;&gt;&gt; rng = np.random.default_rng(seed=42)\n        &gt;&gt;&gt; data_noise = sn.FractionalGaussianNoise(hurst=0.5, rng=rng).sample(10000)\n        &gt;&gt;&gt; data_2d = rng.normal(size=(4, 3000))\n        &gt;&gt;&gt; data_airline = load_airline()\n        ```\n\n        ```pycon {.py .python linenums=\"1\"  title=\"Basic usage\"}\n        &gt;&gt;&gt; print(f\"{spectral_entropy(x=data_airline, sf=12):.4f}\")\n        2.6538\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Sine wave\"}\n        &gt;&gt;&gt; print(f\"{spectral_entropy(x=data_sine, sf=100, method='fft'):.4f}\")\n        6.2329\n        ```\n\n        ```pycon {.py .python linenums=\"1\"  title=\"Welch method\"}\n        &gt;&gt;&gt; print(f\"{spectral_entropy(x=data_sine, sf=100, method='welch'):.4f}\")\n        1.2924\n        ```\n\n        ```pycon {.py .python linenums=\"1\"  title=\"Normalised calculation\"}\n        &gt;&gt;&gt; print(f\"{spectral_entropy(x=data_sine, sf=100, method='welch', normalize=True):.4f}\")\n        0.9956\n        ```\n\n        ```pycon {.py .python linenums=\"1\"  title=\"2D data\"}\n        &gt;&gt;&gt; print(spectral_entropy(x=data_2d, sf=100, normalize=True).tolist())\n        [0.9426, 0.9382, 0.9410, 0.9376]\n        ```\n\n        ```pycon {.py .python linenums=\"1\"  title=\"Gaussian noise\"}\n        &gt;&gt;&gt; print(f\"{spectral_entropy(x=data_noise, sf=100, normalize=True):.4f}\")\n        0.9505\n        ```\n\n    ??? Question \"References\"\n        - Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.\n        - https://en.wikipedia.org/wiki/Spectral_density\n        - https://en.wikipedia.org/wiki/Welch%27s_method\n\n    ??? Tip \"See Also\"\n        - [`antropy.app_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.app_entropy.html)\n        - [`antropy.sample_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.sample_entropy.html)\n        - [`antropy.perm_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.perm_entropy.html)\n        - [`antropy.spectral_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.spectral_entropy.html)\n        - [`ts_stat_tests.algorithms.app_entropy`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`ts_stat_tests.algorithms.approx_entropy`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`ts_stat_tests.algorithms.perm_entropy`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`ts_stat_tests.algorithms.spectral_entropy`][ts_stat_tests.algorithms.regularity.spectral_entropy]\n    \"\"\"\n    return a_spectral_entropy(\n        x=x,\n        sf=sf,\n        method=method,\n        nperseg=nperseg,\n        normalize=normalize,\n        axis=axis,\n    )\n</code></pre>"},{"location":"code/regularity/#ts_stat_tests.algorithms.regularity.svd_entropy","title":"svd_entropy","text":"<pre><code>svd_entropy(\n    x: ArrayLike,\n    order: int = 3,\n    delay: int = 1,\n    normalize: bool = False,\n) -&gt; float\n</code></pre> <p>Summary</p> <p>SVD entropy is a measure of the complexity or randomness of a time series based on Singular Value Decomposition (SVD).</p> Details <p>SVD entropy is calculated by first embedding the time series into a matrix, then performing SVD on that matrix to obtain the singular values. The entropy is then calculated from the normalized singular values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>One-dimensional time series of shape (n_times).</p> required <code>order</code> <code>int</code> <p>Order of the SVD entropy (embedding dimension). Defaults to <code>3</code>.</p> <code>3</code> <code>delay</code> <code>int</code> <p>Time delay (lag). Defaults to <code>1</code>.</p> <code>1</code> <code>normalize</code> <code>bool</code> <p>If True, divide by \\(log2(order!)\\) to normalize the entropy between \\(0\\) and \\(1\\). Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>The SVD entropy of the data set.</p> <p>Credit</p> <p>All credit goes to the <code>AntroPy</code> library.</p> See Also <ul> <li><code>antropy.svd_entropy</code></li> <li><code>ts_stat_tests.algorithms.approx_entropy</code></li> <li><code>ts_stat_tests.algorithms.sample_entropy</code></li> <li><code>ts_stat_tests.algorithms.perm_entropy</code></li> <li><code>ts_stat_tests.algorithms.spectral_entropy</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/regularity.py</code> <pre><code>@typechecked\ndef svd_entropy(\n    x: ArrayLike,\n    order: int = 3,\n    delay: int = 1,\n    normalize: bool = False,\n) -&gt; float:\n    \"\"\"\n    !!! note \"Summary\"\n        SVD entropy is a measure of the complexity or randomness of a time series based on Singular Value Decomposition (SVD).\n\n    ???+ abstract \"Details\"\n        SVD entropy is calculated by first embedding the time series into a matrix, then performing SVD on that matrix to obtain the singular values. The entropy is then calculated from the normalized singular values.\n\n    Params:\n        x (ArrayLike):\n            One-dimensional time series of shape (n_times).\n        order (int, optional):\n            Order of the SVD entropy (embedding dimension).&lt;br&gt;\n            Defaults to `3`.\n        delay (int, optional):\n            Time delay (lag).&lt;br&gt;\n            Defaults to `1`.\n        normalize (bool, optional):\n            If True, divide by $log2(order!)$ to normalize the entropy between $0$ and $1$.&lt;br&gt;\n            Defaults to `False`.\n\n    Returns:\n        (float):\n            The SVD entropy of the data set.\n\n    !!! Success \"Credit\"\n        All credit goes to the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ??? Tip \"See Also\"\n        - [`antropy.svd_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.svd_entropy.html)\n        - [`ts_stat_tests.algorithms.approx_entropy`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`ts_stat_tests.algorithms.sample_entropy`][ts_stat_tests.algorithms.regularity.sample_entropy]\n        - [`ts_stat_tests.algorithms.perm_entropy`][ts_stat_tests.algorithms.regularity.permutation_entropy]\n        - [`ts_stat_tests.algorithms.spectral_entropy`][ts_stat_tests.algorithms.regularity.spectral_entropy]\n    \"\"\"\n    return a_svd_entropy(\n        x=x,\n        order=order,\n        delay=delay,\n        normalize=normalize,\n    )\n</code></pre>"},{"location":"usage/changelog/","title":"Change Log","text":"<p>v0.3.1</p> <p>v0.2.0</p> <p>v0.1.0</p>"},{"location":"usage/changelog/#v031-add-normality-module","title":"v0.3.1 - Add Normality Module","text":"<p><code>v0.3.1</code> <code>2026-01-08</code> data-science-extensions/ts-stat-tests/releases/v0.3.1</p> Release Notes Updates <ul> <li><code>90cd570</code>: Fix grammar error     (by chrimaho)        * <code>b708fe1</code>: Remove documentation     Duplicate dictionary key \"entropy\" in the return type documentation. The second entry on line 341 should be removed as it's identical to the one on line 340.     (by chrimaho)        * <code>4f554e5</code>: Incomplete documentation     The text \"because REASONS\" is a placeholder that should be replaced with actual reasoning for why the AntroPy package was selected.     (by chrimaho)        * <code>1427c5e</code>: Document additional entropy algorithms in regularity tests<ul> <li>Expand documentation to include <code>permutation_entropy()</code> and <code>svd_entropy()</code> functions as supported algorithms</li> <li>Clarify algorithm selection options and aliases for improved user guidance</li> <li>Ensure consistency across docstrings and abstract sections for easier reference (by chrimaho)        * <code>1782d7b</code>: Update progress docs for completed regularity and normality</li> <li>Mark completed <code>antropy</code> regularity algorithms and add <code>svd_entropy()</code> function to documentation</li> <li>Mark all normality algorithms as implemented and tested</li> <li>Reflect full test and unit test coverage for correlation, regularity, and normality modules in progress tables</li> <li>Improve accuracy of documentation for current feature and test status (by chrimaho)        * <code>d006b1c</code>: Add documentation for normality tests and algorithms</li> <li>Provide an overview of normality testing in time-series analysis, including rationale and references</li> <li>Detail the use of <code>scipy.stats</code> and <code>statsmodels</code> libraries for statistical tests</li> <li>Link to relevant source modules for implementation details</li> <li>Outline available tests and algorithms via code documentation structure</li> <li>Help users understand when and why to perform normality checks on residuals (by chrimaho)        * <code>4584b3f</code>: Add unit test for fallback branch in normality check</li> <li>Improve test coverage by introducing a scenario that triggers the fallback path in the <code>is_normal()</code> function</li> <li>Ensure behaviour when the <code>normality()</code> function returns an object that is neither a tuple/list nor has a <code>pvalue</code> attribute</li> <li>Confirm that the <code>is_normal()</code> function handles unexpected return types gracefully and returns expected results (by chrimaho)        * <code>ec8d327</code>: Introduce unified normality test interface and refactor docs</li> <li>Add <code>normality()</code> and <code>is_normal()</code> functions to standardise access to multiple normality test algorithms</li> <li>Refactor and expand documentation for all normality algorithms, improving clarity and consistency of usage examples and equations</li> <li>Replace scattered summary/info/example blocks with a unified doc structure and imperative notes, using Australian English spelling</li> <li>Update and extend test coverage for new interfaces, ensuring comprehensive behaviour for all supported normality tests</li> <li>Improve parameter handling and error messaging for invalid algorithm selection using <code>generate_error_message()</code></li> <li>Align with latest <code>scipy</code> and <code>statsmodels</code> result object conventions for type safety and compatibility (by chrimaho)        * <code>ad67e9d</code>: Add detailed docstrings for normality test functions</li> <li>Improve documentation by adding comprehensive docstrings to all normality test functions</li> <li>Include summaries, parameter descriptions, return types, example usages, equations, references, notes, and related function links</li> <li>Standardise documentation style and formatting for clarity and consistency</li> <li>Enhance usability for end users by providing practical guidance and mathematical context, referencing the relevant statistical literature and library sources (by chrimaho)        * <code>25c4823</code>: Add module-level summary and structured docstrings</li> <li>Provide a clear overview and description for the normality testing algorithms</li> <li>Improve future maintainability by standardising documentation structure</li> <li>Clarify module purpose for statistical analysis and forecasting workflows</li> <li>Facilitate easier onboarding for new contributors (by chrimaho)        * <code>8f21f94</code>: Add statistical normality test algorithm implementations</li> <li>Provide comprehensive implementations for assessing data normality via <code>jb()</code>, <code>ob()</code>, <code>sw()</code>, <code>dp()</code>, and <code>ad()</code> functions.</li> <li>Include detailed docstrings, equations, practical examples, and references to support correct use and interpretation.</li> <li>Standardise parameter types, outputs, and documentation for consistent usage across different normality tests.</li> <li>Facilitate robust statistical validation in time series forecasting workflows by exposing multiple established tests. (by chrimaho)        * <code>a4cfa2a</code>: Add normality test documentation and stubs</li> <li>Create initial documentation for normality tests</li> <li>Add normality test file stubs for future implementation</li> <li>Update navigation to include normality section</li> <li>Prepare codebase for standardisation of normality test approach (by chrimaho)        * <code>c0e61c5</code>: Fix docs reference bugs (by chrimaho)        * <code>1083496</code>: Add regularity documentation</li> <li>Introduce comprehensive overview and rationale for regularity testing using entropy-based algorithms</li> <li>Standardise documentation with references, examples, and usage guidelines for <code>ts_stat_tests</code> modules</li> <li>Clarify differences between approximate entropy and sample entropy in forecasting context</li> <li>Link to further resources on time-series analysis methodology and data quality</li> <li>Detail regularity algorithms and tests, including filtering logic for code navigation (by chrimaho)</li> </ul> </li> </ul>"},{"location":"usage/changelog/#overview","title":"\ud83d\ude80 Overview","text":"<p>Introduce the <code>normality</code> module, providing a suite of statistical tests to assess the distribution of time series data. Significantly expand the documentation and test coverage for the <code>regularity</code> and <code>normality</code> modules, update the implementation progress, and improve the clarity and accuracy of references across the project. Notably, introduce new documentation pages, update the <code>README.md</code> and progress tables to reflect completed work, and add a comprehensive test suite for normality algorithms.</p>"},{"location":"usage/changelog/#implementation-details","title":"\ud83d\udee0\ufe0f Implementation details","text":""},{"location":"usage/changelog/#normality-module-implementation","title":"Normality module implementation","text":"<ul> <li>Introduce the <code>src/ts_stat_tests/algorithms/normality.py</code> module, implementing five core normality tests: <code>jb()</code>, <code>ob()</code>, <code>sw()</code>, <code>dp()</code>, and <code>ad()</code>.</li> <li>Implement a unified interface in <code>src/ts_stat_tests/tests/normality.py</code> with the following standardised functions:<ul> <li><code>normality()</code>: Act as a dispatcher to run a selected normality test.</li> <li><code>is_normal()</code>: Return a standardised dictionary containing the test result (<code>True</code>/<code>False</code>), statistic, and p-value.</li> </ul> </li> <li>Support various <code>nan_policy</code> options (\"propagate\", \"raise\", \"omit\") across algorithms to handle missing data gracefully.</li> <li>Provide detailed mathematical documentation, including LaTeX equations and bibliographic references, in the docstrings of all normality test functions.</li> </ul>"},{"location":"usage/changelog/#regularity-module-enhancements","title":"Regularity module enhancements","text":"<ul> <li>Expand the <code>src/ts_stat_tests/algorithms/regularity.py</code> module by exposing additional entropy measures: <code>permutation_entropy()</code> and <code>svd_entropy()</code>.</li> <li>Update the <code>entropy()</code>, <code>regularity()</code>, and <code>is_regular()</code> functions in <code>src/ts_stat_tests/tests/regularity.py</code> to support these new algorithms via intuitive string aliases.</li> <li>Standardise parameter handling and improve error messaging using <code>generate_error_message()</code>.</li> <li>Enhance docstrings with comprehensive examples, mathematical context, and internal cross-references.</li> </ul>"},{"location":"usage/changelog/#documentation-and-progress-tracking","title":"Documentation and progress tracking","text":"<ul> <li>Create new documentation pages to provide detailed guidance:<ul> <li><code>docs/code/normality.md</code>: Provide an overview of normality testing, rationale, algorithm details, and API references.</li> <li><code>docs/code/regularity.md</code>: Detail entropy-based regularity testing and provide a comprehensive API guide.</li> </ul> </li> <li>Update <code>docs/code/index.md</code> and <code>README.md</code> progress tables to mark the <code>normality</code> and <code>regularity</code> modules as 100% complete.</li> <li>Refactor <code>docs/code/correlation.md</code> to correct and clarify module references and formatting.</li> <li>Register the <code>normality</code> section in <code>mkdocs.yml</code> to improve site navigation and information architecture.</li> </ul>"},{"location":"usage/changelog/#testing-and-quality-assurance","title":"Testing and quality assurance","text":"<ul> <li>Add a comprehensive unit test suite in <code>src/tests/test_normality.py</code>, covering all implemented normality algorithms and edge cases.</li> <li>Implement a specific mock-based scenario to trigger the fallback path in <code>is_normal()</code>, ensuring 100% code coverage for the normality module.</li> <li>Adhere to Australian English spelling (e.g. <code>standardise</code>, <code>optimise</code>, <code>recognise</code>) and imperative coding style throughout the codebase.</li> <li>Ensure all code changes pass linting and type checks to maintain high standards for reliability, usability, and transparency.</li> </ul>"},{"location":"usage/changelog/#checklist","title":"\u2705 Checklist","text":"<ul> <li> Implement core normality test algorithms in <code>src/ts_stat_tests/algorithms/normality.py</code>.</li> <li> Create unified normality test interfaces in <code>src/ts_stat_tests/tests/normality.py</code>.</li> <li> Add comprehensive unit tests in <code>src/tests/test_normality.py</code> and achieve 100% coverage.</li> <li> Expand regularity algorithms and update convenience wrappers.</li> <li> Generate detailed documentation for both normality and regularity modules.</li> <li> Update project-wide progress tables in <code>README.md</code> and <code>docs/code/index.md</code>.</li> <li> Standardise spelling to Australian English and ensure consistent formatting.</li> </ul>"},{"location":"usage/changelog/#pull-requests","title":"\ud83d\udcaa Pull Requests","text":"<ul> <li>Implement normality module and enhance regularity documentation by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/26</li> </ul> <p>Full Changelog: https://github.com/data-science-extensions/ts-stat-tests/compare/v0.2.0...v0.3.0</p>"},{"location":"usage/changelog/#v020-add-regularity-algorithms","title":"v0.2.0 - Add Regularity Algorithms","text":"<p><code>v0.2.0</code> <code>2026-01-06</code> data-science-extensions/ts-stat-tests/releases/v0.2.0</p> Release Notes Updates <ul> <li><code>df1172b</code>: Fix f-string syntax in documentation     The example code contains syntax errors with mismatched quotes. Lines 665, 670, and other locations use straight double quotes inside the f-string which will cause a Python syntax error. The method parameter value should use single quotes or escaped double quotes.     (by chrimaho)        * <code>388713e</code>: Fix documentation typo     The examples show calls to <code>\"sample_entropy\"</code>, <code>\"approx_entropy\"</code>, and <code>\"spectral_entropy\"</code> functions, but the surrounding documentation indicates these should be calls to <code>\"is_regular()\"</code> with different algorithm parameters. The function names in the examples should be corrected to match the function being documented.     (by chrimaho)        * <code>49ecb5a</code>: Fix documentation consistency     There's an inconsistency in the documentation examples. The function calls in the examples use <code>\"approx_entropy\"</code>, <code>\"sample_entropy\"</code>, and <code>\"spectral_entropy\"</code> directly, but the surrounding text indicates these should be calls to the <code>\"regularity()\"</code> function with different algorithm parameters. The examples should match the function being documented.     (by chrimaho)        * <code>579c51d</code>: Fix redundant docstring comment     The docstring contains duplicate quote marks in the <code>\"Summary\"</code> admonition. It should be either <code>!!! note \"Summary\"</code> or just <code>!!! note Summary</code> without the extra quotes.     (by chrimaho)        * <code>9b125fb</code>: Fix another duplication     The documentation lists <code>\"approx\"</code> twice in line 337 which appears to be a typo. It should likely list the valid string options as: <code>[\"app\", \"approx\"]</code> based on the implementation in the entropy function.     (by chrimaho)        * <code>b8461aa</code>: Fix duplication     The documentation lists <code>\"approx\"</code> twice in line 99 which appears to be a typo. It should likely list the valid string options as: <code>[\"app\", \"approx\"]</code> based on the implementation in the options dictionary on line 181.     (by chrimaho)        * <code>90ce8c2</code>: Update src/ts_stat_tests/algorithms/regularity.py     The example output shows a raw array representation <code>\"array([0.9426, 0.9383, 0.9411, 0.9375])\"</code> instead of formatted output. Since this is in an f-string print statement, the output format is inconsistent with the other examples which show formatted floats. Consider using <code>.tolist()</code> or formatting the array elements consistently.     (by chrimaho)        * <code>43954ef</code>: Remove redundant docs comment     The docstring contains duplicate quote marks in the <code>\"Summary\"</code> admonition. It should be either <code>!!! note \"Summary\"</code> or just <code>!!! note Summary</code> without the extra quotes. It should not have duplicate <code>\"Summary\"</code> sections.     (by chrimaho)        * <code>81defae</code>: Remove duplicates     The documentation lists <code>\"approx\"</code> twice in line 227 which appears to be a typo. It should likely list the valid string options as: <code>[\"app\", \"approx\"]</code> based on the implementation in the entropy function.     (by chrimaho)        * <code>b2c8d65</code>: Fix <code>numpy</code> cast process     The cast to <code>np.ndarray</code> on line 429 may be incorrect if the input <code>x</code> is actually an <code>ArrayLike</code> that is not a <code>numpy</code> array (e.g., a <code>list</code> or <code>pd.Series</code>). The <code>np.std()</code> function already accepts <code>ArrayLike</code> inputs, so this cast is both unnecessary and potentially misleading. Consider removing the <code>cast()</code> or verifying that <code>x</code> has been converted to a <code>numpy</code> array first.     (by chrimaho)        * <code>a344cca</code>: Fix <code>numpy</code> version constraint     The <code>numpy</code> version constraint <code>\"numpy&lt;2.4\"</code> lacks a lower bound, which could allow installation of very old <code>numpy</code> versions (including 1.x) that may not be compatible with the codebase. Consider adding a lower bound like <code>\"numpy&gt;=2.0.0,&lt;2.4\"</code> to ensure compatibility, matching the override-dependencies specification on line 45.     (by chrimaho)        * <code>541a996</code>: Expand test coverage for utils and entropy logic<ul> <li>Include unit tests for the <code>load_airline()</code> function to verify data type validation and error handling.</li> <li>Add coverage for the <code>is_almost_equal()</code> and <code>assert_almost_equal()</code> utility functions to testing parameter validation and failure messages.</li> <li>Minimise gaps in coverage for the <code>svd_entropy()</code> and <code>entropy()</code> functions by testing direct calls and algorithm selections.</li> <li>Verify tolerance logic in the <code>is_regular()</code> function for <code>None</code> and string-based input values.</li> <li>Bring test coverage up to 100% (by chrimaho)        * <code>63ebd14</code>: Add permutation and SVD entropy options</li> <li>Expose the <code>permutation_entropy()</code> and <code>svd_entropy()</code> functions within the regularity testing module.</li> <li>Extend the <code>entropy()</code> function logic to include support for permutation and SVD algorithm types.</li> <li>Update the options mapping to recognise new aliases and standardise selection of these entropy measures. (by chrimaho)        * <code>369a321</code>: Optimise file discovery performance</li> <li>Use the <code>find</code> system command within the <code>get_all_files()</code> function to accelerate directory traversal.</li> <li>Implement a fallback mechanism to the <code>.glob()</code> method on the <code>Path()</code> class if the system command is unavailable.</li> <li>Prune <code>.venv</code> and hidden directories in the search path to minimise processing time.</li> <li>Standardise the output by applying the <code>sorted()</code> function to the list of discovered file paths.</li> <li>Add a docstring to the <code>get_all_files()</code> function to document the dual-method execution logic. (by chrimaho)        * <code>4dc7cad</code>: Update the Pylint configuration to suppress <code>R0801</code> duplicate-code check</li> <li>Ignore duplicate-code warnings to reduce linting noise</li> <li>Allow code repetition where refactoring for deduplication is not desirable (by chrimaho)        * <code>5f6b9e5</code>: Refine documentation and update test baselines</li> <li>Update baseline numerical values for <code>approx_entropy()</code>, <code>sample_entropy()</code>, <code>permutation_entropy()</code>, and <code>spectral_entropy()</code> functions in the test suite to match revised calculations.</li> <li>Standardise the type hint for the <code>metric</code> parameter in <code>entropy()</code>, <code>regularity()</code>, and <code>is_regular()</code> functions to use <code>VALID_KDTREE_METRIC_OPTIONS</code>.</li> <li>Refine docstrings by removing redundant admonition titles in <code>svd_entropy()</code>, <code>entropy()</code>, <code>regularity()</code>, and <code>is_regular()</code> functions. (by chrimaho)        * <code>89ce566</code>: Improve regularity test type safety and documentation</li> <li>Standardise docstring admonition blocks to use consistent <code>note</code> and <code>abstract</code> labels.</li> <li>Add a comprehensive docstring for the <code>svd_entropy()</code> function including parameters and return types.</li> <li>Enhance type safety by using the <code>cast()</code> function and specific type aliases for <code>metric</code> and <code>method</code> arguments.</li> <li>Refactor logic in the <code>entropy()</code> function to replace nested branches with early returns.</li> <li>Update parameter documentation for <code>tolerance</code> and <code>metric</code> in the <code>approx_entropy()</code> and <code>sample_entropy()</code> functions.</li> <li>Ensure the <code>is_regular()</code> function returns predictable types through explicit dictionary value casting.</li> <li>Add module-level headers and summary documentation to the regularity test utility source code. (by chrimaho)        * <code>3e13d54</code>: Restrict NumPy version for Numba compatibility</li> <li>Limit <code>numpy</code> version to less than <code>2.4</code> to ensure compatibility with Numba</li> <li>Synchronise the version cap across project dependencies and <code>uv</code> overrides (by chrimaho)        * <code>c0d065d</code>: Add unit tests for regularity and entropy metrics</li> <li>Implement the <code>TestRegularity()</code> class to provide comprehensive unit testing for regularity and entropy algorithms.</li> <li>Add test cases for the <code>approx_entropy()</code>, <code>sample_entropy()</code>, <code>permutation_entropy()</code>, and <code>spectral_entropy()</code> functions across multiple data types including noise, sine waves, and linear trends.</li> <li>Verify that the <code>is_regular()</code> function validates return keys and value types while handling invalid algorithm parameters.</li> <li>Validate numerical accuracy for various scenarios such as multidimensional arrays and different distance metrics used within the algorithms.</li> <li>Utilise the <code>assert_almost_equal()</code> function to ensure precise verification of calculated entropy values. (by chrimaho)        * <code>f71614b</code>: Standardise documentation and fix docstring typos</li> <li>Update code block labels to <code>pycon</code> to support interactive example rendering</li> <li>Fix a typographical error in the <code>import</code> statement for the <code>spectral_entropy()</code> function</li> <li>Standardise spacing in arithmetic expressions within the <code>sample_entropy()</code> and <code>permutation_entropy()</code> functions</li> <li>Improve docstring layout by adding vertical spacing before example blocks in the <code>approx_entropy()</code> and <code>spectral_entropy()</code> functions (by chrimaho)        * <code>700a063</code>: Add regularity and entropy statistical tests</li> <li>Introduce <code>entropy()</code> function as a unified interface for several entropy calculation algorithms</li> <li>Add <code>regularity()</code> function to provide a pass-through wrapper for assessing time series regularity</li> <li>Implement <code>is_regular()</code> function to determine if a dataset is regular by comparing entropy against a threshold</li> <li>Apply <code>typechecked</code> decorator to ensure robust parameter validation for all new functions</li> <li>Include helper logic to calculate a default <code>tolerance</code> based on the standard deviation of the input data</li> <li>Support <code>approx_entropy()</code>, <code>sample_entropy()</code>, and <code>spectral_entropy()</code> functions via a central entry point</li> <li>Provide internal parameter validation to normalise algorithm selection strings (by chrimaho)        * <code>50b5405</code>: Add regularity test file</li> <li>Initialise the <code>test_regularity.py</code> file to provide a structure for upcoming test cases (by chrimaho)        * <code>f33cbf7</code>: Add documentation for regularity functions</li> <li>Provide extensive docstrings for <code>approx_entropy()</code>, <code>sample_entropy()</code>, <code>permutation_entropy()</code>, and <code>spectral_entropy()</code> functions.</li> <li>Include mathematical formulations such as \\(ApEn(m, r, N) = \\phi_m(r) - \\phi_{m+1}(r)\\) to describe regularity logic.</li> <li>Add practical examples demonstrating how to apply entropy functions to various datasets.</li> <li>Document parameter configurations such as the <code>order</code> and <code>metric</code> arguments for the <code>sample_entropy()</code> function.</li> <li>Detail the steps required to normalise results within the <code>permutation_entropy()</code> and <code>spectral_entropy()</code> functions.</li> <li>Incorporate academic references and credit the <code>AntroPy</code> library for the underlying implementations.ts (by chrimaho)        * <code>759edd6</code>: Refine typing and expand entropy functions</li> <li>Introduce <code>Literal</code> type hints to restrict valid options for <code>metric</code> and <code>method</code> parameters.</li> <li>Add <code>tolerance</code> parameter to <code>approx_entropy()</code> and <code>sample_entropy()</code> functions.</li> <li>Standardise parameter definitions in <code>approx_entropy()</code>, <code>sample_entropy()</code>, and <code>spectral_entropy()</code> functions using <code>VALID_KDTREE_METRIC_OPTIONS</code> and <code>VALID_SPECTRAL_ENTROPY_METHOD_OPTIONS</code> constants. (by chrimaho)        * <code>5c48940</code>: Add <code>svd_entropy()</code> function for regularity testing</li> <li>Include <code>svd_entropy</code> in the <code>__all__</code> list to expose the new algorithm.</li> <li>Implement the <code>svd_entropy()</code> function as a type-checked wrapper for the <code>antropy</code> implementation to provide Singular Value Decomposition entropy calculations. (by chrimaho)        * <code>a51404c</code>: Add documentation header and module docstring</li> <li>Add a descriptive header to the regularity module to clarify its purpose.</li> <li>Include a module-level docstring that summarises functionality for computing regularity measures. (by chrimaho)        * <code>1796788</code>: Add regularity entropy algorithms</li> <li>Implement <code>approx_entropy()</code>, <code>sample_entropy()</code>, <code>permutation_entropy()</code>, and <code>spectral_entropy()</code> functions</li> <li>Utilise the <code>antropy</code> library for underlying entropy calculations</li> <li>Apply the <code>typechecked</code> decorator to ensure parameter type safety</li> <li>Define the <code>__all__</code> list to expose the public API of the module (by chrimaho)        * <code>ffb08f7</code>: Add regularity module and documentation</li> <li>Provide regularity statistical tests within the algorithms library</li> <li>Ensure code quality with a new test suite</li> <li>Initialise the documentation and update the site navigation (by chrimaho)        * <code>dd00a96</code>: Drop Python 3.9 support from CD workflow</li> <li>Remove <code>3.9</code> from the environment matrix to align with modern support standards.</li> <li>Optimise the deployment pipeline by focusing on more recent releases. (by chrimaho)</li> </ul> </li> </ul>"},{"location":"usage/changelog/#overview_1","title":"\ud83d\ude80 Overview","text":"<p>This release introduces a suite of regularity algorithms for time series analysis, including several entropy-based measures. It also features significant performance optimisations for project maintenance utilities and ensures the package achieves 100% unit test coverage. These enhancements stabilise the codebase and provide a robust foundation for assessing the complexity and regularity of time series data.</p>"},{"location":"usage/changelog/#implementation-details_1","title":"\ud83d\udee0\ufe0f Implementation details","text":""},{"location":"usage/changelog/#regularity-and-entropy-algorithms","title":"Regularity and entropy algorithms","text":"<ul> <li>Implement the <code>entropy()</code> function as a unified dispatcher for multiple entropy calculation algorithms, supporting Approximate, Sample, Permutation, Spectral, and SVD entropy.</li> <li>Introduce the <code>regularity()</code> function to provide a standardised wrapper for assessing time series regularity.</li> <li>Add the <code>is_regular()</code> function to determine if a time series meets a defined regularity threshold.</li> <li>Provide comprehensive wrappers for the <code>approx_entropy()</code>, <code>sample_entropy()</code>, <code>permutation_entropy()</code>, <code>spectral_entropy()</code>, and <code>svd_entropy()</code> functions.</li> <li>Utilise the <code>antropy</code> library for underlying calculations and apply the <code>@typechecked</code> decorator for rigorous runtime parameter validation.</li> <li>Integrate mathematical documentation within docstrings to describe the logic behind regularity measures.</li> </ul>"},{"location":"usage/changelog/#performance-and-maintenance-utilities","title":"Performance and maintenance utilities","text":"<ul> <li>Refactor the <code>get_all_files()</code> function in the <code>scripts.py</code> module to use system-level <code>git ls-files</code> and <code>find</code> commands.</li> <li>Optimise directory traversal for project maintenance tasks, reducing execution time from 41 seconds to near-instantaneous.</li> <li>Update the <code>Pylint</code> configuration to ignore <code>R0801</code> (duplicate code) warnings, facilitating specialised test case implementation.</li> <li>Fix documentation build failures by resolving <code>mkdocs-material</code> extension inconsistencies in the development environment.</li> </ul>"},{"location":"usage/changelog/#quality-assurance-and-environment","title":"Quality assurance and environment","text":"<ul> <li>Achieve 100% unit test coverage across the library by adding comprehensive test cases for regularity algorithms, data loading, and error handling.</li> <li>Enhance the <code>assert_almost_equal()</code> and <code>is_almost_equal()</code> utility functions to support precise verification of statistical results.</li> <li>Update the <code>load_airline()</code> function to include stricter data type validation and error reporting.</li> <li>Restrict the <code>numpy</code> version to <code>&lt; 2.4</code> in <code>pyproject.toml</code> to maintain compatibility with <code>Numba()</code> and <code>antropy</code>.</li> <li>Drop support for Python 3.9 in the CI/CD workflows to align with modern Python lifecycle standards.</li> </ul>"},{"location":"usage/changelog/#checklist_1","title":"\u2705 Checklist","text":"<ul> <li> Implement core regularity algorithms and entropy measures.</li> <li> Optimise file discovery performance in maintenance scripts.</li> <li> Reach 100% unit test coverage across all modules.</li> <li> Update package dependencies and Python support.</li> <li> Standardise documentation and verify build success.</li> </ul>"},{"location":"usage/changelog/#pull-requests_1","title":"\ud83d\udcaa Pull Requests","text":"<ul> <li>Implement regularity algorithms by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/25</li> </ul> <p>Full Changelog: https://github.com/data-science-extensions/ts-stat-tests/compare/v0.1.0...v0.2.0</p>"},{"location":"usage/changelog/#v010-initial-release-of-time-series-statistical-tests","title":"v0.1.0 - Initial release of Time Series Statistical Tests","text":"<p><code>v0.1.0</code> <code>2026-01-05</code> data-science-extensions/ts-stat-tests/releases/v0.1.0</p> Release Notes Updates <ul> <li><code>085a4c0</code>: Re-enable key components in the <code>cd</code> workflow, ready for first deployment     (by chrimaho)        * <code>a722504</code>: Ensure coverage reports are staged even when directory paths are ignored.<ul> <li>Use the <code>--force</code> flag in the <code>git_add_coverage_report()</code> function.</li> <li>Bypass <code>.gitignore</code> restrictions to guarantee documentation updates. (by chrimaho)        * <code>85f68e6</code>: Hide some lines in <code>cd</code> to check workflow during release (by chrimaho)        * <code>16c9288</code>: Standardise type hints and remove dependencies</li> <li>Replace custom type hints with native Python equivalents for the <code>__all__</code> variable</li> <li>Remove the unnecessary import from the <code>toolbox_python.collection_types</code> module</li> <li>Update the <code>replace()</code> function to use the <code>re.Match()</code> class directly (by chrimaho)        * <code>8e76143</code>: Fix missing justifications in docs (by chrimaho)        * <code>a3085ff</code>: Fix typos (by chrimaho)        * <code>aef58db</code>: Fix typo (by chrimaho)        * <code>90c953b</code>: Fix incorrect percentages (by chrimaho)        * <code>15202ad</code>: Re-enable <code>mkdocs</code> checks (by chrimaho)        * <code>728c24a</code>: Add overview of module progress (by chrimaho)        * <code>02d36d1</code>: Add <code>correlation</code> module docs page (by chrimaho)        * <code>60c7127</code>: Fix some typos on the <code>correlation</code> module docs (by chrimaho)        * <code>fe9003d</code>: Add docs landing page and home page structures (by chrimaho)        * <code>37fab51</code>: Add custom hooks for docs to utilise (by chrimaho)        * <code>83dd55f</code>: Initial commit of package icons (by chrimaho)        * <code>8c3f0cf</code>: Initial commit of package stylesheets (by chrimaho)        * <code>950c1fb</code>: Fill in README file (by chrimaho)        * <code>e2c62bd</code>: Initial commit of standard package docs (by chrimaho)        * <code>1f1a9d6</code>: Initial commit of <code>mkdocs</code> config file (by chrimaho)        * <code>c7839a4</code>: Set UTF-8 encoding for Python I/O in CI</li> <li>Define the <code>PYTHONIOENCODING</code> environment variable to ensure consistent character handling across CI environments (by chrimaho)        * <code>cc368ff</code>: Fix typo (by chrimaho)        * <code>0b2d590</code>: Standardise variable naming convention</li> <li>Rename the repository variable to lower-case to follow standard naming conventions for local instances within the <code>main()</code> function.</li> <li>Update references to the variable when calling the <code>.get_releases()</code> method and <code>.get_commits()</code> method.</li> <li>The variable name <code>REPO</code> is defined in uppercase following constant naming conventions, but it's not a constant - it's a dynamically created repository object. Consider using lowercase <code>repo</code> to follow Python naming conventions where uppercase names are reserved for constants. (by chrimaho)        * <code>ed5404d</code>: Initialise the output file before starting the main process</li> <li>Call the <code>prepare_output_file()</code> function to ensure the destination is ready for writing.</li> <li>Set up the file environment prior to opening the GitHub and file context managers. (by chrimaho)        * <code>6060627</code>: Define constant for short SHA length</li> <li>Introduce <code>SHORT_SHA_LENGTH</code> constant to replace hardcoded magic numbers</li> <li>Update <code>add_commit_info()</code> function to use defined constant</li> <li>Standardise representation of short commit identifiers for improved maintainability (by chrimaho)        * <code>aa243b7</code>: FixPotential bug If <code>commit.author</code> is <code>None</code>, the code constructs an incomplete markdown link <code>[]()</code> with empty values. This would result in broken links in the changelog. Consider providing a default fallback text like <code>\"Unknown\"</code> for the author name and omitting the link altogether when the author is unavailable. (by chrimaho)        * <code>d1e0800</code>: Refactor correlation tests to use pytest</li> <li>Replace <code>.assertRaises()</code> method with <code>raises()</code> function to standardise test assertions</li> <li>Update <code>.test_is_correlated()</code> method to expect <code>NotImplementedError()</code> class from <code>is_correlated()</code> function (by chrimaho)        * <code>01d3544</code>: Create script to automate changelog generation</li> <li>Introduce the <code>src/utils/changelog.py</code> script to automate the creation of <code>CHANGELOG.md</code> by retrieving data via the GitHub API.</li> <li>Implement the <code>prepare_output_file()</code> function to ensure the document is recreated from scratch on each run.</li> <li>Define the <code>add_page_styling()</code> function to embed CSS for better navigation in Markdown viewers.</li> <li>Utilise the <code>main()</code> function to fetch releases via the <code>.get_releases()</code> method and filter out irrelevant commits to maintain a clean history.</li> <li>Use the <code>Github()</code> class to authenticate and manage repository interactions.</li> <li>Standardise the output format for release notes and commit details to improve readability. (by chrimaho)        * <code>702d902</code>: Add continuous delivery workflow for releases</li> <li>Introduce <code>cd.yml</code> to automate the release cycle triggered by published GitHub releases.</li> <li>Orchestrate a multi-stage pipeline including testing, building, PyPI deployment, and documentation generation.</li> <li>Utilise <code>uv</code> to synchronise dependencies and manage the build process for improved performance.</li> <li>Execute the <code>git_update_version_cli()</code> function to handle versioning during the build stage.</li> <li>Verify package integrity using an installation matrix across multiple operating systems and Python versions.</li> <li>Automate changelog generation via the <code>changelog.py</code> script and push updates to the repository.</li> <li>Upload distribution assets to GitHub releases and coverage data to Codecov. (by chrimaho)        * <code>3e014f6</code>: Increase CI job parallelism</li> <li>Increase the <code>max-parallel</code> limit to 30 to allow for more concurrent jobs</li> <li>Optimise the CI pipeline performance by utilising more available runners for the matrix build (by chrimaho)        * <code>cd1f7ad</code>: Add CI workflow for automated code validation</li> <li>Implement GitHub Actions to automate validation on push and pull request events.</li> <li>Configure a check job for non-main branches to facilitate early issue detection.</li> <li>Define a matrix strategy to ensure cross-platform compatibility across various OS types.</li> <li>Support Python versions 3.9 through 3.14 to maintain broad environment stability.</li> <li>Utilise <code>uv</code> for efficient dependency management and execution of the validation script. (by chrimaho)        * <code>1f68794</code>: Add Dependabot for GitHub Actions</li> <li>Initialise the configuration file to manage <code>github-actions</code> dependencies.</li> <li>Schedule weekly updates to ensure actions remain current.</li> <li>Assign a default reviewer and label to simplify the pull request review process. (by chrimaho)        * <code>c059c7c</code>: Raise error for unimplemented correlation test</li> <li>Replace the <code>None</code> return value in the <code>is_correlated()</code> function with a <code>NotImplementedError</code> to explicitly signal that this logic is a placeholder and has not yet been implemented. (by chrimaho)        * <code>470fbfa</code>: Clarify type ignore reasons</li> <li>Document rationale for <code># type: ignore</code> on calls to the <code>acorr_lm()</code> function and <code>acorr_breusch_godfrey()</code> function</li> <li>Note that <code>statsmodels</code> type hints are incomplete or incompatible with internal <code>RegressionResults()</code> class types (by chrimaho)        * <code>8ea4c9d</code>: Standardise equality check utility functions</li> <li>Set default value for <code>places</code> parameter in <code>is_almost_equal()</code> function and <code>assert_almost_equal()</code> function overloads</li> <li>Refactor <code>is_almost_equal()</code> function to prioritise argument validation over value equality checks</li> <li>Standardise type annotations for the <code>params</code> dictionary in <code>assert_almost_equal()</code> function</li> <li>Simplify logic for error messages in <code>assert_almost_equal()</code> function by defaulting to precision-based comparison (by chrimaho)        * <code>00a6029</code>: Initialise parent classes in test suites</li> <li>Call the <code>.setUpClass()</code> and <code>.tearDownClass()</code> methods of the superclass in the <code>BaseTester()</code> class</li> <li>Implement the <code>.setUp()</code>, <code>.tearDown()</code>, and <code>.tearDownClass()</code> methods in the <code>TestCorrelation()</code> class to ensure proper lifecycle management (by chrimaho)        * <code>3b018b4</code>: Remove redundant lines in config file (by chrimaho)        * <code>58163f0</code>: Update documentation headers and module summaries</li> <li>Add header comments to the <code>setup.py</code> script explaining its role in unit tests.</li> <li>Standardise documentation in the <code>errors.py</code> module regarding error generation and data equality checks. (by chrimaho)        * <code>9c7206a</code>: Optimise data generation and loading via caching</li> <li>Apply the <code>@lru_cache</code> decorator to the <code>get_random_generator()</code> function and various <code>data_*()</code> functions to reduce redundant test overhead.</li> <li>Remove the global <code>seed()</code> function call to favour the use of independent random generators.</li> <li>Enhance the <code>load_airline()</code> function with result caching to improve data retrieval performance. (by chrimaho)        * <code>891edd2</code>: Refactor correlation logic and remove redundant code</li> <li>Remove commented-out code from the <code>correlation()</code> function to clean up the source.</li> <li>Update the <code>is_correlated()</code> function to return <code>None</code> instead of raising a <code>NotImplementedError</code> class to provide a neutral placeholder. (by chrimaho)        * <code>0d078f3</code>: Add implementation error to the <code>is_correlated</code> function The function <code>is_correlated</code> is defined but returns <code>None</code> and serves no purpose. It's marked as a placeholder in the docstring, but placeholder functions should either be removed or raise <code>NotImplementedError</code> to indicate they're not yet implemented. Returning <code>None</code> with no implementation can lead to confusion. (by chrimaho)        * <code>335045d</code>: Refine correlation APIs and improve type safety</li> <li>Introduce <code>@overload()</code> function signatures for <code>acf()</code>, <code>pacf()</code>, <code>ccf()</code>, <code>lm()</code>, <code>bglm()</code>, and <code>correlation()</code> functions to improve static analysis and developer experience.</li> <li>Standardise parameter validation using <code>Literal()</code> class aliases for algorithm options within the <code>acf()</code> and <code>pacf()</code> functions.</li> <li>Update the <code>ccf()</code> function to support <code>nlags</code> and <code>alpha</code> parameters for better consistency with other correlation tools.</li> <li>Enforce keyword-only arguments for the <code>store</code> parameter in <code>lm()</code> and <code>bglm()</code> functions and the <code>alpha</code> and <code>qstat</code> parameters in the <code>acf()</code> function.</li> <li>Refine the <code>correlation()</code> function to act as a unified interface with specific return type hints based on the provided algorithm string.</li> <li>Ensure the <code>lb()</code> function return type is correctly annotated as a <code>pd.DataFrame()</code> class. (by chrimaho)        * <code>6176186</code>: Validate data type in <code>load_airline()</code></li> <li>Ensure the <code>load_airline()</code> function returns a <code>pd.Series()</code> class by introducing a type check</li> <li>Raise a <code>TypeError()</code> class if the result of the <code>.squeeze()</code> method is not a series to prevent errors when initialising the <code>pd.PeriodIndex()</code> class</li> <li>Resolve PyRight errors (by chrimaho)        * <code>d668fa7</code>: Organise and clean up check function calls</li> <li>Remove the redundant commented out <code>check_mypy()</code> function call from the <code>check()</code> function</li> <li>Move and disable the <code>check_mkdocs()</code> function call to execute after the <code>check_build()</code> function call within the <code>check()</code> function (by chrimaho)        * <code>b9a9c1a</code>: Add <code>pyright</code> for static type analysis</li> <li>Include <code>pyright</code> in the development dependencies</li> <li>Define the <code>check_pyright()</code> function to analyse code types</li> <li>Integrate the <code>check_pyright()</code> function into the validation <code>check()</code> function (by chrimaho)        * <code>d7b0669</code>: Ignore complexipy cache directory</li> <li>Prevent <code>.complexipy_cache/*</code> from being tracked in the repository (by chrimaho)        * <code>15b4f17</code>: Expand correlation tests and organise imports</li> <li>Import <code>correlation()</code> and <code>is_correlated()</code> functions to expand test coverage for the correlation module.</li> <li>Add <code>.test_correlation_ccf_raises()</code> and <code>.test_correlation_bglm()</code> methods to validate algorithm error handling and behaviour.</li> <li>Simplify the <code>.setUpClass()</code> method by removing redundant type annotations from class attributes and local variables.</li> <li>Reformat import statements to improve code structure and maintainability. (by chrimaho)        * <code>f61bbf8</code>: Update type hints to use abstract base classes</li> <li>Replace <code>Dict()</code> and <code>List()</code> with <code>Mapping()</code> and <code>Collection()</code> classes in the <code>generate_error_message()</code> function to support broader input types</li> <li>Standardise the use of the built-in <code>dict()</code> class within the <code>assert_almost_equal()</code> function</li> <li>Simplify complex <code>Union</code> type annotations to improve code maintainability (by chrimaho)        * <code>c77a089</code>: Introduce correlation test dispatcher</li> <li>Implement the <code>correlation()</code> function to provide a unified interface for various statistical correlation algorithms.</li> <li>Map string aliases to internal implementations such as the <code>_acf()</code>, <code>_pacf()</code>, and <code>_ccf()</code> functions.</li> <li>Utilise the <code>generate_error_message()</code> function to handle unsupported algorithm selections with descriptive feedback.</li> <li>Ensure the <code>y</code> parameter is provided when the <code>ccf</code> algorithm is requested via the <code>correlation()</code> function.</li> <li>Add a placeholder <code>is_correlated()</code> function to define the intended package structure. (by chrimaho)        * <code>47ff459</code>: Refine type annotations for correlation functions</li> <li>Standardise return type signatures for the <code>lm()</code> function and the <code>bglm()</code> function.</li> <li>Introduce <code>overload()</code> definitions for the <code>lm()</code> function to improve static analysis and type safety.</li> <li>Replace <code>np.float64</code>, <code>np.ndarray</code>, and complex <code>Union</code> types with specific <code>float</code> tuples to ensure consistency. (by chrimaho)        * <code>4f783a0</code>: Add error handling and float comparison utilities</li> <li>Add <code>generate_error_message()</code> function to standardise error reporting for invalid parameter options.</li> <li>Implement <code>is_almost_equal()</code> function to provide flexible float comparison using either decimal places or a specific delta.</li> <li>Include <code>assert_almost_equal()</code> function to facilitate testing by raising descriptive <code>AssertionError</code> messages when float values deviate beyond tolerances. (by chrimaho)        * <code>aa0c816</code>: Disable invalid-name linting rule</li> <li>Disable the <code>C0103</code> rule to standardise the project's naming convention exceptions. (by chrimaho)        * <code>8b1424e</code>: Standardise docstrings and improve type hinting</li> <li>Define <code>VALID_PACF_METHODS</code> constant to centralise allowed methods for the <code>pacf()</code> function.</li> <li>Update docstring callouts to use <code>!!! note</code> and <code>???+ abstract</code> admonitions for improved documentation consistency.</li> <li>Refine parameter type descriptions in docstrings to use <code>ArrayLike</code> and <code>Optional</code> types for the <code>acf()</code>, <code>pacf()</code>, <code>ccf()</code>, <code>lb()</code>, and <code>lm()</code> functions.</li> <li>Reformat code examples to use <code>pycon</code> syntax and ensure correct line continuation for better rendering.</li> <li>Clean up <code>!!! deprecation</code> blocks and standardise internal formatting. (by chrimaho)        * <code>02fd5d2</code>: Document and standardise data utility module</li> <li>Add module-level docstrings and structural headers to improve navigation</li> <li>Enhance the <code>load_airline()</code> function with comprehensive documentation and academic references</li> <li>Remove unused <code>numpy</code> import to tidy the <code>imports</code> section</li> <li>Standardise the file layout using consistent section markers (by chrimaho)        * <code>f6824ad</code>: Suppress specific Pylint linting warnings</li> <li>Introduce configuration to relax linting constraints</li> <li>Minimise noise by disabling checks for line length and file length</li> <li>Disable warnings for argument counts and redefined built-ins (by chrimaho)        * <code>716db6d</code>: Expand and standardise docstring sections</li> <li>Add <code>credit</code>, <code>references</code>, <code>see also</code>, and <code>deprecation</code> sections to the configuration</li> <li>Standardise the internal key order for all section definitions to maintain consistency</li> <li>Align configuration properties to improve visual clarity and maintenance (by chrimaho)        * <code>48abfe1</code>: Update package name and add script documentation</li> <li>Add a header comment block to the utility script to document usage and purpose.</li> <li>Update the <code>PACKAGE_NAME</code> constant to reflect the project identity. (by chrimaho)        * <code>9ba6915</code>: Introduce a centralised scripts module</li> <li>Provide a centralised suite of automation utilities for project maintenance.</li> <li>Implement a <code>run_command()</code> function to handle shell execution and argument expansion.</li> <li>Include a <code>lint()</code> function to standardise code formatting across the repository.</li> <li>Define a <code>check()</code> function to aggregate quality assurance tests and build verification.</li> <li>Automate git workflows for versioning, tagging, and documentation deployment.</li> <li>Expose utilities via a CLI entry point to simplify development and CI/CD pipelines. (by chrimaho)        * <code>c6021f8</code>: Introduce pre-commit hooks for code quality</li> <li>Standardise repository-wide formatting and linting rules</li> <li>Automate file-level sanitisation for whitespace and line endings</li> <li>Enforce Python style consistency using <code>black</code>, <code>isort</code>, and <code>pyupgrade</code></li> <li>Prevent direct commits to the <code>main</code> branch via <code>no-commit-to-branch</code></li> <li>Validate configuration files with <code>check-json</code>, <code>check-toml</code>, and <code>check-yaml</code></li> <li>Integrate <code>uv</code> lockfile and synchronisation checks to maintain environment integrity</li> <li>Sanitise documentation and check spelling with <code>blacken-docs</code> and <code>codespell</code></li> <li>Implement a local <code>ty-check</code> hook for type safety (by chrimaho)        * <code>ccc7aed</code>: Add unit tests for the <code>correlation</code> module</li> <li>Update <code>llvmlite</code> and <code>numba</code> packages to stable versions in <code>pyproject.toml</code></li> <li>Add <code>stochastic</code> package to the test dependencies</li> <li>Introduce <code>BaseTester()</code> class to standardise test data setup via the <code>.setUpClass()</code> method</li> <li>Implement <code>load_airline()</code> function to retrieve sample datasets for time series analysis</li> <li>Add unit tests for correlation functions including <code>acf()</code> and <code>pacf()</code></li> <li>Configure <code>uv</code> dependency overrides to maintain <code>numpy</code> package version consistency (by chrimaho)        * <code>f18bc49</code>: Fix the PACF formula clarity The PACF formula is incorrectly formatted and unclear. The notation <code>\"Corr(Y_t, Y_{t-k} / Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1})\"</code> uses division where conditional notation should be used. It should be <code>\"Corr(Y_t, Y_{t-k} | Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1})\"</code> with a vertical bar (<code>|</code>) to denote conditioning, not a division symbol. (by chrimaho)        * <code>9de39de</code>: Correct ACF formula in <code>acf()</code> function docstring</li> <li>Update the mathematical expression in the <code>acf()</code> function docstring to correctly show square roots in the denominator.</li> <li>Add the simplified version of the ACF formula for stationary time series to the <code>acf()</code> function documentation. (by chrimaho)        * <code>984fb1d</code>: Fix typo issue Inconsistent terminology: The documentation states \"Ljung-Box and Box-Pierce statistic differ\" but should use \"statistics\" (plural) since there are two statistics being discussed. (by chrimaho)        * <code>04dbd4b</code>: Fix <code>p-value</code> formatting issue (by chrimaho)        * <code>9631202</code>: Fix self-referential documentation issue The text \"See <code>q_stat</code> for more information\" is circular and unhelpful since this IS the documentation for the <code>qstat</code> parameter. This reference should either be removed or point to relevant external documentation or the Returns section. (by chrimaho)        * <code>044f8a0</code>: Fix typo (by chrimaho)        * <code>cbf0fb7</code>: Fix duplicate word (by chrimaho)        * <code>feba34c</code>: Standardise formatting of <code>p-value</code> in docs (by chrimaho)        * <code>ac283f5</code>: Correct the formula provided in the <code>lm()</code> function documentation</li> <li>Clarify the relationship between the LM test, Engle's ARCH test, and the Breusch-Godfrey test within the <code>lm()</code> function</li> <li>Refine the mathematical definition of the test statistic to focus on the auxiliary regression <code>$R^2$</code>, the number of observations, and degrees of freedom</li> <li>Provide a clearer step-by-step procedure for fitting the time series model and running the auxiliary regression to obtain the test statistic</li> <li>Improve the explanation of the null hypothesis and the resulting asymptotic chi-squared distribution of the LM statistic</li> <li>Standardise document formatting by using KaTeX for math equations and back-ticks for parameters like <code>resid</code> and <code>nlags</code> (by chrimaho)        * <code>1d32293</code>: Add correlation algorithms for time series analysis</li> <li>Introduce <code>acf()</code> function, <code>pacf()</code> function, and <code>ccf()</code> function to compute temporal dependencies.</li> <li>Implement <code>lb()</code> function, <code>lm()</code> function, and <code>bglm()</code> function for residual autocorrelation testing.</li> <li>Utilise <code>statsmodels</code> library to ensure robust and standardised statistical results.</li> <li>Include detailed docstrings with mathematical formulas and usage examples.</li> <li>Apply <code>@typechecked</code> decorator to ensure runtime type safety for all algorithm inputs. (by chrimaho)        * <code>fa3e3fd</code>: Remove duplicate function declaration (by chrimaho)        * <code>94efa8b</code>: Initialise project structure and configuration</li> <li>Define project metadata and dependencies within <code>pyproject.toml</code> using <code>uv_build</code>.</li> <li>Configure linting, formatting, and testing tools to standardise development.</li> <li>Implement <code>strip_ansi_codes()</code> function in <code>setup.py</code> to facilitate environment-agnostic CLI output verification.</li> <li>Add test naming helper functions like <code>name_func_flat_list()</code> function to <code>setup.py</code>.</li> <li>Configure <code>sys.path</code> in the <code>tests</code> <code>__init__.py</code> file to ensure local module resolution.</li> <li>Extend <code>.gitignore</code> to exclude <code>uv.lock</code>, <code>.vscode/</code>, and other local environment directories. (by chrimaho)        * <code>b963f05</code>: Initial commit (by chrimaho)</li> </ul> </li> </ul>"},{"location":"usage/changelog/#overview_2","title":"\ud83d\ude80 Overview","text":"<p>This is the initial release of <code>ts-stat-tests</code>, a Python package dedicated to providing a unified and standardised interface for time series statistical testing. This release establishes the foundational project infrastructure, implements a comprehensive suite of correlation algorithms, and provides extensive documentation and automated CI/CD pipelines. It aims to bridge the gap between R and Python for time series analysis by offering a single, robust library for standard time series statistical tests.</p>"},{"location":"usage/changelog/#implementation-details_2","title":"\ud83d\udee0\ufe0f Implementation details","text":""},{"location":"usage/changelog/#core-correlation-algorithms","title":"Core correlation algorithms","text":"<ul> <li>Introduce the <code>acf()</code> function, <code>pacf()</code> function, and <code>ccf()</code> function to estimate autocorrelation, partial autocorrelation, and cross-correlation functions.</li> <li>Implement residual diagnostic tests including the <code>lb()</code> function for Ljung-Box testing, the <code>lm()</code> function for Lagrange Multiplier tests, and the <code>bglm()</code> function for Breusch-Godfrey tests.</li> <li>Provide a unified <code>correlation()</code> dispatcher function to simplify algorithm selection via string aliases.</li> <li>Utilise the <code>statsmodels</code> library as the underlying engine to ensure robust and standardised statistical results.</li> <li>Include detailed docstrings for all algorithms, featuring mathematical definitions, parameter details, and practical usage examples.</li> </ul>"},{"location":"usage/changelog/#project-infrastructure-and-automation","title":"Project infrastructure and automation","text":"<ul> <li>Initialise the project structure using <code>uv</code> for efficient dependency management and build orchestration.</li> <li>Implement a centralised <code>scripts.py</code> module to provide a suite of automation utilities for linting, checking, and maintenance.</li> <li>Configure pre-commit hooks to enforce code quality, standardise formatting, and prevent direct commits to the <code>main</code> branch.</li> <li>Establish GitHub Actions workflows for continuous integration (<code>ci.yml</code>) and continuous delivery (<code>cd.yml</code>) to automate testing and releases.</li> <li>Integrate Dependabot to automate updates for GitHub Actions dependencies on a weekly schedule.</li> </ul>"},{"location":"usage/changelog/#documentation-and-presentation","title":"Documentation and presentation","text":"<ul> <li>Launch a comprehensive documentation site using <code>mkdocs</code> with the <code>material</code> theme and custom stylesheets.</li> <li>Expand the <code>README.md</code> with project badges, a motivation section, and a detailed feature implementation table.</li> <li>Implement custom documentation hooks in <code>shortcodes.py</code> to support dynamic content and enhanced rendering.</li> <li>Add placeholder pages and \"To Do\" notes for <code>CHANGELOG.md</code> and <code>CONTRIBUTING.md</code> to guide future documentation efforts.</li> </ul>"},{"location":"usage/changelog/#type-safety-and-quality-assurance","title":"Type safety and quality assurance","text":"<ul> <li>Enforce runtime type safety by applying the <code>@typechecked</code> decorator to all core algorithm functions.</li> <li>Integrate <code>pyright</code> for rigorous static type analysis and resolve complex type-hinting issues.</li> <li>Implement utility functions for error reporting and float comparison, including <code>generate_error_message()</code>, <code>is_almost_equal()</code>, and <code>assert_almost_equal()</code>.</li> <li>Achieve 100% unit test coverage for the correlation module to ensure reliability and correctness.</li> </ul>"},{"location":"usage/changelog/#checklist_2","title":"\u2705 Checklist","text":"<ul> <li> Establish foundational project structure and configuration.</li> <li> Implement core correlation algorithms and diagnostic tests.</li> <li> Create a centralised automation and maintenance script.</li> <li> Launch a comprehensive documentation site and expand <code>README.md</code>.</li> <li> Configure CI/CD pipelines and pre-commit hooks.</li> <li> Enforce runtime and static type safety across the package.</li> <li> Achieve high unit test coverage for the initial release.</li> </ul>"},{"location":"usage/changelog/#pull-requests_2","title":"\ud83d\udcaa Pull Requests","text":"<ul> <li>Initialise project structure and configuration by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/15</li> <li>Add correlation algorithms for time series analysis by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/16</li> <li>Set up unit tests and enhance development infrastructure by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/17</li> <li>Establish CI/CD pipelines and automate release workflows by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/18</li> <li>Bump actions/checkout from 5 to 6 by @dependabot[bot] in https://github.com/data-science-extensions/ts-stat-tests/pull/19</li> <li>Bump actions/setup-python from 5 to 6 by @dependabot[bot] in https://github.com/data-science-extensions/ts-stat-tests/pull/23</li> <li>Bump astral-sh/setup-uv from 6 to 7 by @dependabot[bot] in https://github.com/data-science-extensions/ts-stat-tests/pull/22</li> <li>Bump actions/upload-artifact from 4 to 6 by @dependabot[bot] in https://github.com/data-science-extensions/ts-stat-tests/pull/21</li> <li>Bump actions/download-artifact from 5 to 7 by @dependabot[bot] in https://github.com/data-science-extensions/ts-stat-tests/pull/20</li> <li>Enhance project documentation and presentation by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/24</li> </ul>"},{"location":"usage/changelog/#new-contributors","title":"\ud83c\udd95 New Contributors","text":"<ul> <li>@chrimaho made their first contribution in https://github.com/data-science-extensions/ts-stat-tests/pull/15</li> <li>@dependabot[bot] made their first contribution in https://github.com/data-science-extensions/ts-stat-tests/pull/19</li> </ul> <p>Full Changelog: https://github.com/data-science-extensions/ts-stat-tests/commits/v0.1.0</p>"},{"location":"usage/contributing/","title":"Contributing","text":"<p>To Do</p> <p>Add docs for contributing to this package.</p>"},{"location":"usage/overview/","title":"Overview","text":"# Time Series Statistical Tests  ### `ts-stat-tests`  [![PyPI version](https://img.shields.io/pypi/v/ts-stat-tests?label=version&amp;logo=git&amp;color=blue)](https://pypi.org/project/ts-stat-tests/) [![Released](https://img.shields.io/github/release-date/data-science-extensions/ts-stat-tests?label=released&amp;color=blue&amp;logo=google-calendar&amp;logoColor=FF7143)](https://pypi.org/project/ts-stat-tests/#history) [![Python](https://img.shields.io/pypi/pyversions/ts-stat-tests.svg?style=flat&amp;logo=python&amp;logoColor=FFDE50&amp;color=blue)](https://pypi.org/project/ts-stat-tests/) [![OS](https://img.shields.io/static/v1?label=os&amp;message=ubuntu+|+macos+|+windows&amp;color=blue&amp;logo=ubuntu&amp;logoColor=green)](https://pypi.org/project/ts-stat-tests/) [![Build Tests](https://img.shields.io/github/actions/workflow/status/data-science-extensions/ts-stat-tests/ci-build-package.yml?logo=github&amp;logoColor=white&amp;label=build+tests)](https://github.com/data-science-extensions/ts-stat-tests/actions/workflows/ci-build-package.yml) [![MyPy Tests](https://img.shields.io/github/actions/workflow/status/data-science-extensions/ts-stat-tests/ci-mypy-tests.yml?logo=github&amp;logoColor=white&amp;label=mypy+tests)](https://github.com/data-science-extensions/ts-stat-tests/actions/workflows/ci-mypy-tests.yml) [![Unit Tests](https://img.shields.io/github/actions/workflow/status/data-science-extensions/ts-stat-tests/ci-unit-tests.yml?logo=github&amp;logoColor=white&amp;label=unit+tests)](https://github.com/data-science-extensions/ts-stat-tests/actions/workflows/ci-unit-tests.yml) [![codecov](https://codecov.io/gh/data-science-extensions/ts-stat-tests/branch/main/graph/badge.svg)](https://codecov.io/gh/data-science-extensions/ts-stat-tests) [![Deploy Docs](https://img.shields.io/github/actions/workflow/status/data-science-extensions/ts-stat-tests/cd-deploy-docs.yml?logo=github&amp;logoColor=white&amp;label=deploy+docs)](https://github.com/data-science-extensions/ts-stat-tests/actions/workflows/cd-deploy-docs.yml) [![Publish Package](https://img.shields.io/github/actions/workflow/status/data-science-extensions/ts-stat-tests/cd-publish-package.yml?logo=github&amp;logoColor=white&amp;label=publish+package)](https://github.com/data-science-extensions/ts-stat-tests/actions/workflows/ci-publish-package.yml) [![CodeQL](https://github.com/data-science-extensions/ts-stat-tests/actions/workflows/github-code-scanning/codeql/badge.svg?branch=main&amp;label=code+ql)](https://github.com/data-science-extensions/ts-stat-tests/actions/workflows/github-code-scanning/codeql) [![License][badge-license]](https://github.com/data-science-extensions/ts-stat-tests/blob/master/LICENSE) [![Downloads][badge-downloads]](https://piptrends.com/package/ts-stat-tests) [![Code Style][badge-style]](https://github.com/psf/black) [![Contributions](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/data-science-extensions/ts-stat-tests/issues)"},{"location":"usage/overview/#motivation","title":"Motivation","text":"<p>Time Series Analysis has been around for a long time, especially for doing Statistical Testing. Some Python packages are going a long way to make this even easier than it has ever been before. Such as <code>sktime</code> and <code>pycaret</code> and <code>pmdarima</code> and <code>statsmodels</code>.</p> <p>There are some typical Statistical Tests which are accessible in these Python (QS, Normality, Stability, etc). However, there are still some statistical tests which are not yet ported over to Python, but which have been written in R and are quite stable.</p> <p>Moreover, there is no one single library package for doing time-series statistical tests in Python.</p> <p>That's exactly what this package aims to achieve.</p> <p>A single package for doing all the standard time-series statistical tests.</p>"},{"location":"usage/overview/#tests","title":"Tests","text":"<p>Full credit goes to the packages listed in this table.</p> Type Name Source Package Source Language Implemented Correlation Auto-Correlation function (ACF) <code>statsmodels</code> Python \u2705 Correlation Partial Auto-Correlation function (PACF) <code>statsmodels</code> Python \u2705 Correlation Cross-Correlation function (CCF) <code>statsmodels</code> Python \u2705 Correlation Ljung-Box test of autocorrelation in residuals (LB) <code>statsmodels</code> Python \u2705 Correlation Lagrange Multiplier tests for autocorrelation (LM) <code>statsmodels</code> Python \u2705 Correlation Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation (BGLM) <code>statsmodels</code> Python \u2705 Regularity Approximate Entropy <code>antropy</code> python \u2705 Regularity Sample Entropy <code>antropy</code> python \u2705 Regularity Permutation Entropy <code>antropy</code> python \u2705 Regularity Spectral Entropy <code>antropy</code> python \u2705 Regularity SVD Entropy <code>antropy</code> python \u2705 Seasonality QS <code>seastests</code> R \ud83d\udd32 Seasonality Osborn-Chui-Smith-Birchenhall test of seasonality (OCSB) <code>pmdarima</code> Python \ud83d\udd32 Seasonality Canova-Hansen test for seasonal differences (CH) <code>pmdarima</code> Python \ud83d\udd32 Seasonality Seasonal Strength <code>tsfeatures</code> Python \ud83d\udd32 Seasonality Trend Strength <code>tsfeatures</code> Python \ud83d\udd32 Seasonality Spikiness <code>tsfeatures</code> Python \ud83d\udd32 Stability Stability <code>tsfeatures</code> Python \ud83d\udd32 Stability Lumpiness <code>tsfeatures</code> Python \ud83d\udd32 Stationarity Augmented Dickey-Fuller test for stationarity (ADF) <code>statsmodels</code> Python \ud83d\udd32 Stationarity Kwiatkowski-Phillips-Schmidt-Shin test for stationarity (KPSS) <code>statsmodels</code> Python \ud83d\udd32 Stationarity Range unit-root test for stationarity (RUR) <code>statsmodels</code> Python \ud83d\udd32 Stationarity Zivot-Andrews structural-break unit-root test (ZA) <code>statsmodels</code> Python \ud83d\udd32 Stationarity Phillips-Peron test for stationarity (PP) <code>pmdarima</code> Python \ud83d\udd32 Stationarity Elliott-Rothenberg-Stock (ERS) de-trended Dickey-Fuller test <code>arch</code> Python \ud83d\udd32 Stationarity Variance Ratio (VR) test for a random walk <code>arch</code> Python \ud83d\udd32 Normality Jarque-Bera test of normality (JB) <code>statsmodels</code> Python \u2705 Normality Omnibus test for normality (OB) <code>statsmodels</code> Python \u2705 Normality Shapiro-Wilk test for normality (SW) <code>scipy</code> Python \u2705 Normality D'Agostino &amp; Pearson's test for normality <code>scipy</code> Python \u2705 Normality Anderson-Darling test for normality <code>scipy</code> Python \u2705 Linearity Harvey Collier test for linearity (HC) <code>statsmodels</code> Python \ud83d\udd32 Linearity Lagrange Multiplier test for linearity (LM) <code>statsmodels</code> Python \ud83d\udd32 Linearity Rainbow test for linearity (RB) <code>statsmodels</code> Python \ud83d\udd32 Linearity Ramsey's RESET test for neglected nonlinearity (RR) <code>statsmodels</code> Python \ud83d\udd32 Heteroscedasticity Engle's Test for Autoregressive Conditional Heteroscedasticity (ARCH) <code>statsmodels</code> Python \ud83d\udd32 Heteroscedasticity Breusch-Pagan Lagrange Multiplier test for heteroscedasticity (BPL) <code>statsmodels</code> Python \ud83d\udd32 Heteroscedasticity Goldfeld-Quandt test for homoskedasticity (GQ) <code>statsmodels</code> Python \ud83d\udd32 Heteroscedasticity White's Lagrange Multiplier Test for Heteroscedasticity (WLM) <code>statsmodels</code> Python \ud83d\udd32"},{"location":"usage/overview/#known-limitations","title":"Known limitations","text":"<ul> <li>These listed tests is not exhaustive, and there is probably some more that could be added. Therefore, we encourage you to raise issues or pull requests to add more statistical tests to this suite.</li> <li>This package does not re-invent any of these tests. It merely calls the underlying packages, and calls the functions which are already written elsewhere.</li> </ul>"}]}