{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"code/","title":"Details about the tests","text":""},{"location":"code/#intro","title":"Intro","text":"<p>TL;DR</p> <p>There are a number of other really good libraries which implements these tests individually:</p> <ul> <li><code>pmdarima</code></li> <li><code>statsmodels</code></li> <li><code>arch</code></li> <li><code>tsfeatures</code></li> <li><code>antropy</code></li> <li><code>scipy</code></li> </ul> <p>These packages all implement the statistical tests in a slightly different way.  However, no one library contains all of the required tests, all in one place.</p>"},{"location":"code/#implementation-progress","title":"Implementation Progress","text":"module algorithms tests unit-tests Correlation <p>6/6   =  100%</p> <p>2/2   = 100%</p> <p>19/19    = 100%</p> Regularity <p>5/5   =  100%</p> <p>2/2   = 100%</p> <p>34/34    = 100%</p> Seasonality <p>6/6   =  100%</p> <p>2/2   = 100%</p> <p>10/10    = 100%</p> Stability <p>0/2   =    0%</p> <p>0/2   =   0%</p> <p>0/4      =   0%</p> Stationarity <p>7/7   =  100%</p> <p>2/2   = 100%</p> <p>44/44    = 100%</p> Normality <p>5/5   =  100%</p> <p>2/2   = 100%</p> <p>12/12    = 100%</p> Linearity <p>0/4   =    0%</p> <p>0/2   =   0%</p> <p>0/0      =   0%</p> Heteroscedasticity <p>0/4   =    0%</p> <p>0/2   =   0%</p> <p>0/0      =   0%</p> Overall <p>29/41 =   71%</p> <p>10/16 =  63%</p> <p>119/123 =  89%</p>"},{"location":"code/#tests","title":"Tests","text":"<p>Details</p> <p>Legend:</p> icon description \u2705 Already implemented in this package \ud83d\udd32 To be developed and implemented \u274e Will not be implemented as it is covered by a function from a different package Test InfoPython Import category algorithm library:test Correlation Auto-Correlation function (ACF) \u2705<code>statsmodels</code>:<code>acf()</code>\u274e<code>pmdarima</code>:<code>acf()</code> Correlation Partial Auto-Correlation function (PACF) \u2705<code>statsmodels</code>:<code>pacf()</code>\u274e<code>pmdarima</code>:<code>pacf()</code> Correlation Cross-Correlation function (CCF) \u2705<code>statsmodels</code>:<code>ccf()</code> Correlation Ljung-Box test of autocorrelation in residuals (LB) \u2705<code>statsmodels</code>:<code>acorr_ljungbox()</code> Correlation Lagrange Multiplier tests for autocorrelation (LM) \u2705<code>statsmodels</code>:<code>acorr_lm()</code> Correlation Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation (BGLM) \u2705<code>statsmodels</code>:<code>acorr_breusch_godfrey()</code> Regularity Approximate Entropy \u2705<code>antropy</code>:<code>app_entropy()</code> Regularity Sample Entropy \u2705<code>antropy</code>:<code>sample_entropy()</code> Regularity Permutation Entropy \u2705<code>antropy</code>:<code>perm_entropy()</code> Regularity Spectral Entropy \u2705<code>antropy</code>:<code>spectral_entropy()</code> Regularity SVD Entropy \u2705<code>antropy</code>:<code>svd_entropy()</code> Seasonality QS \u2705<code>seastests</code>:<code>qs()</code> Seasonality Osborn-Chui-Smith-Birchenhall test of seasonality (OCSB) \u2705<code>pmdarima</code>:<code>OCSBTest()</code> Seasonality Canova-Hansen test for seasonal differences (CH) \u2705<code>pmdarima</code>:<code>CHTest()</code> Seasonality Seasonal Strength \u2705<code>tsfeatures</code>:<code>stl_features()</code> Seasonality Trend Strength \u2705<code>tsfeatures</code>:<code>stl_features()</code> Seasonality Spikiness \u2705<code>tsfeatures</code>:<code>stl_features()</code> Stability Stability \ud83d\udd32<code>tsfeatures</code>:<code>stability()</code> Stability Lumpiness \ud83d\udd32<code>tsfeatures</code>:<code>lumpiness()</code> Stationarity Augmented Dickey-Fuller test for stationarity (ADF) \u2705<code>statsmodels</code>:<code>adfuller()</code>\u274e<code>pmdarima</code>:<code>ADFTest()</code>\u274e<code>arch</code>:<code>ADF()</code> Stationarity Kwiatkowski-Phillips-Schmidt-Shin test for stationarity (KPSS) \u2705<code>statsmodels</code>:<code>kpss()</code>\u274e<code>pmdarima</code>:<code>KPSSTest()</code>\u274e<code>arch</code>:<code>KPSS()</code> Stationarity Range unit-root test for stationarity (RUR) \u2705<code>statsmodels</code>:<code>range_unit_root_test()</code> Stationarity Zivot-Andrews structural-break unit-root test (ZA) \u2705<code>statsmodels</code>:<code>zivot_andrews()</code>\u274e<code>arch</code>:<code>ZivotAndrews()</code> Stationarity Phillips-Peron test for stationarity (PP) \u2705<code>pmdarima</code>:<code>PPTest()</code>\u274e<code>arch</code>:<code>PhillipsPerron()</code> Stationarity Elliott-Rothenberg-Stock (ERS) de-trended Dickey-Fuller test \u2705<code>arch</code>:<code>DFGLS()</code> Stationarity Variance Ratio (VR) test for a random walk \u2705<code>arch</code>:<code>VarianceRatio()</code> Normality Jarque-Bera test of normality (JB) \u2705<code>statsmodels</code>:<code>jarque_bera()</code> Normality Omnibus test for normality (OB) \u2705<code>statsmodels</code>:<code>omni_normtest()</code> Normality Shapiro-Wilk test for normality (SW) \u2705<code>scipy</code>:<code>shapiro()</code> Normality D'Agostino &amp; Pearson's test for normality (DP) \u2705<code>scipy</code>:<code>normaltest()</code> Normality Anderson-Darling test for normality (AD) \u2705<code>scipy</code>:<code>anderson()</code> Linearity Harvey Collier test for linearity (HC) \ud83d\udd32<code>statsmodels</code>:<code>linear_harvey_collier()</code> Linearity Lagrange Multiplier test for linearity (LM) \ud83d\udd32<code>statsmodels</code>:<code>linear_lm()</code> Linearity Rainbow test for linearity (RB) \ud83d\udd32<code>statsmodels</code>:<code>linear_rainbow()</code> Linearity Ramsey's RESET test for neglected nonlinearity (RR) \ud83d\udd32<code>statsmodels</code>:<code>linear_reset()</code> Heteroscedasticity Engle's Test for Autoregressive Conditional Heteroscedasticity (ARCH) \ud83d\udd32<code>statsmodels</code>:<code>het_arch()</code> Heteroscedasticity Breusch-Pagan Lagrange Multiplier test for heteroscedasticity (BPL) \ud83d\udd32<code>statsmodels</code>:<code>het_breuschpagan()</code> Heteroscedasticity Goldfeld-Quandt test for homoskedasticity (GQ) \ud83d\udd32<code>statsmodels</code>:<code>het_goldfeldquandt()</code> Heteroscedasticity White's Lagrange Multiplier Test for Heteroscedasticity (WLM) \ud83d\udd32<code>statsmodels</code>:<code>het_white()</code> Covariance ... test library:import ADF pmdarima: <code>from pmdarima.arima import ADFTest</code>statsmodels: <code>from statsmodels.tsa.stattools import adfuller</code>arch: <code>from arch.unitroot import ADF</code> KPSS pmdarima: <code>from pmdarima.arima import KPSSTest</code>statsmodels: <code>from statsmodels.tsa.stattools import kpss</code>arch: <code>from arch.unitroot import KPSS</code> PP pmdarima: <code>from pmdarima.arima import PPTest</code>arch: <code>from arch.unitroot import PhillipsPerron</code> RUR pmdarima: <code>from statsmodels.tsa.stattools import range_unit_root_test</code> ZA pmdarima: <code>from statsmodels.tsa.stattools import zivot_andrews</code> arch: <code>from arch.unitroot import ZivotAndrews</code> OCSB pmdarima: <code>from pmdarima.arima import OCSBTest</code> CH pmdarima: <code>from pmdarima.arima import CHTest</code> ACF pmdarima: <code>from pmdarima.utils import acf</code>statsmodels: <code>from statsmodels.tsa.stattools import acf</code> PACF pmdarima: <code>from pmdarima.utils import pacf</code>statsmodels: <code>from statsmodels.tsa.stattools import pacf</code> CCF statsmodels: <code>from statsmodels.tsa.stattools import ccf</code> ALB statsmodels: <code>from statsmodels.stats.diagnostic import acorr_ljungbox</code> ALM statsmodels: <code>from statsmodels.stats.diagnostic import acorr_lm</code> ABG statsmodels: <code>from statsmodels.stats.diagnostic import acorr_breusch_godfrey</code> JB statsmodels: <code>from statsmodels.stats.stattools import jarque_bera</code> OB statsmodels: <code>from statsmodels.stats.stattools import omni_normtest</code> HC statsmodels: <code>from statsmodels.stats.diagnostic import linear_harvey_collier</code> LM statsmodels: <code>from statsmodels.stats.diagnostic import linear_lm</code> RB statsmodels: <code>from statsmodels.stats.diagnostic import linear_rainbow</code> RR statsmodels: <code>from statsmodels.stats.diagnostic import linear_reset</code> ARCH statsmodels: <code>from statsmodels.stats.diagnostic import het_arch</code> BPL statsmodels: <code>from statsmodels.stats.diagnostic import het_breuschpagan</code> GQ statsmodels: <code>from statsmodels.stats.diagnostic import het_goldfeldquandt</code> WLM statsmodels: <code>from statsmodels.stats.diagnostic import het_white</code>"},{"location":"code/correlation/","title":"Test the <code>correlation</code> of a given Time-Series Dataset","text":""},{"location":"code/correlation/#introduction","title":"Introduction","text":"<p>Summary</p> <p>As stated by Anais Dotis-Georgiou:</p> <p>The term autocorrelation refers to the degree of similarity between A) a given time series, and B) a lagged version of itself, over C) successive time intervals. In other words, autocorrelation is intended to measure the relationship between a variable's present value and any past values that you may have access to.</p> <p>Therefore, a time series autocorrelation attempts to measure the current values of a variable against the historical data of that variable. It ultimately plots one series over the other, and determines the degree of similarity between the two.</p> <p>For the sake of comparison, autocorrelation is essentially the exact same process that you would go through when calculating the correlation between two different sets of time series values on your own. The major difference here is that autocorrelation uses the same time series two times: once in its original values, and then again once a few different time periods have occurred.</p> <p>Autocorrelation is also known as serial correlation, time series correlation and lagged correlation. Regardless of how it's being used, autocorrelation is an ideal method for uncovering trends and patterns in time series data that would have otherwise gone undiscovered.</p> <p> For more info, see: InfluxData: Autocorrelation in Time Series Data.</p> <p>Info</p> <p>An important test to do on Time-Series data is to measure it's level of Auto-Correlation (McMurry &amp; Politis, 2010; Hyndman, nd.(b)). While 'correlation' refers to how two variables change based on the other's value, 'auto-correlation' is how a variable changes based on it's own value over time (the phrase \"auto\" refers to \"self\"). For the Auto-Correlation Function, it uses a '<code>lag</code>' function. For example, a lag value of <code>0</code> is 100% correlated, which is logical, because that is it's own value; whereas a lag value of <code>1</code> or greater, the level of auto-correlation decreases as it gets further away from <code>lag0</code>.</p> <p>For well-structured time-series data sets, it would be expected to see a conical-shaped Auto-Correlation plot. If it were not a well-structured time-series data set, then this Auto-Correlation plot would look more like white noise, and there would not be any logical shape. The blue dotted lines are included as a reference point for determining if any of the observations are significantly different from zero.</p> <p>Moreover, analysis of the data's Auto-Correlation (ACF) should be combined with analysis of its Partial Auto-Correlation (PACF). While the ACF is the \"direct\" relationship between an observation and it's relevant lag observation, the PACF removes the \"indirect\" relationship between these observations. Effectively, the Partial Auto-Correlation between <code>lag1</code> and <code>lag5</code> is the \"actual\" correlation between these two observations, after removing the influence that <code>lag2</code>, <code>lag3</code>, and <code>lag4</code> has on <code>lag5</code>.</p> <p>What this means is that the Partial Auto-Correlation plot would have a very high value at <code>lag0</code>, which will drop very quickly at <code>lag1</code>, and should remain below the blue reference lines for the remainder of the Correlogram. The observations of <code>lag&gt;0</code> should resemble white noise data points. If it does not resemble white noise, and there is a distinct pattern occurring, then the data is not suitable for time-series forecasting.</p> library category algorithm short import script url statsmodels Correlation Autocorrelation Function ACF <code>from statsmodels.tsa.stattools import acf</code> https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.acf.html Correlation Partial Autocorrelation Function PACF <code>from statsmodels.tsa.stattools import pacf</code> https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf.html Correlation Cross-Correlation Function CCF <code>from statsmodels.tsa.stattools import ccf</code> https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.ccf.html Correlation Ljung-Box Test LB <code>from statsmodels.stats.diagnostic import acorr_ljungbox</code> https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_ljungbox.html Correlation Lagrange Multiplier Test LM <code>from statsmodels.stats.diagnostic import acorr_lm</code> https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_lm.html Correlation Breusch-Godfrey LM Test BGLM <code>from statsmodels.stats.diagnostic import acorr_breusch_godfrey</code> https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_breusch_godfrey.html <p> For more info, see: Statsmodels Diagnostic.</p> <p>Source Library</p> <p>The <code>statsmodels</code> package was chosen because it provides mature, well-tested implementations of core time-series tools (such as ACF, PACF, and correlograms), integrates seamlessly with <code>numpy</code> and <code>pandas</code> data structures, and offers a comprehensive suite of statistical tests that align closely with the methods demonstrated in this project.</p> <p>Source Module</p> <p>All of the source code can be found within these modules:</p> <ul> <li><code>ts_stat_tests.algorithms.correlation</code>.</li> <li><code>ts_stat_tests.tests.correlation</code>.</li> </ul>"},{"location":"code/correlation/#correlation-tests","title":"Correlation Tests","text":""},{"location":"code/correlation/#ts_stat_tests.tests.correlation","title":"ts_stat_tests.tests.correlation","text":"<p>Summary</p> <p>This module contains tests for the correlation functions defined in the <code>ts_stat_tests.algorithms.correlation</code> module.</p>"},{"location":"code/correlation/#ts_stat_tests.tests.correlation.correlation","title":"correlation","text":"<pre><code>correlation(\n    x: ArrayLike,\n    algorithm: Literal[\"acf\", \"auto\", \"ac\"],\n    **kwargs: Union[float, int, str, bool, ArrayLike, None]\n) -&gt; Union[\n    NDArray[np.float64], tuple[NDArray[np.float64], ...]\n]\n</code></pre><pre><code>correlation(\n    x: ArrayLike1D,\n    algorithm: Literal[\"pacf\", \"partial\", \"pc\"],\n    **kwargs: Union[float, int, str, bool, ArrayLike, None]\n) -&gt; Union[\n    NDArray[np.float64], tuple[NDArray[np.float64], ...]\n]\n</code></pre><pre><code>correlation(\n    x: ArrayLike,\n    algorithm: Literal[\n        \"ccf\", \"cross\", \"cross-correlation\", \"cc\"\n    ],\n    **kwargs: Union[float, int, str, bool, ArrayLike, None]\n) -&gt; Union[\n    NDArray[np.float64], tuple[NDArray[np.float64], ...]\n]\n</code></pre><pre><code>correlation(\n    x: ArrayLike,\n    algorithm: Literal[\n        \"lb\",\n        \"alb\",\n        \"acorr_ljungbox\",\n        \"acor_lb\",\n        \"a_lb\",\n        \"ljungbox\",\n    ],\n    **kwargs: Union[float, int, str, bool, ArrayLike, None]\n) -&gt; pd.DataFrame\n</code></pre><pre><code>correlation(\n    x: ArrayLike,\n    algorithm: Literal[\"lm\", \"alm\", \"acorr_lm\", \"a_lm\"],\n    **kwargs: Union[float, int, str, bool, ArrayLike, None]\n) -&gt; Union[\n    tuple[float, float, float, float],\n    tuple[float, float, float, float, ResultsStore],\n]\n</code></pre><pre><code>correlation(\n    x: Union[RegressionResults, RegressionResultsWrapper],\n    algorithm: Literal[\"bglm\", \"breusch_godfrey\", \"bg\"],\n    **kwargs: Union[float, int, str, bool, ArrayLike, None]\n) -&gt; Union[\n    tuple[float, float, float, float],\n    tuple[float, float, float, float, ResultsStore],\n]\n</code></pre> <pre><code>correlation(\n    x: Union[\n        ArrayLike,\n        ArrayLike1D,\n        RegressionResults,\n        RegressionResultsWrapper,\n    ],\n    algorithm: str = \"acf\",\n    **kwargs: Union[float, int, str, bool, ArrayLike, None]\n) -&gt; Union[\n    NDArray[np.float64],\n    tuple[NDArray[np.float64], ...],\n    pd.DataFrame,\n    tuple[float, float, float, float],\n    tuple[float, float, float, float, ResultsStore],\n]\n</code></pre> <p>Summary</p> <p>A unified interface for various correlation tests.</p> Details <p>This function acts as a dispatcher for several correlation measures and tests, allowing users to access them through a single, consistent API. Depending on the <code>algorithm</code> parameter, it routes the call to the appropriate implementation in <code>ts_stat_tests.algorithms.correlation</code>.</p> <p>The supported algorithms include:</p> <ul> <li>Autocorrelation Function (ACF): Measures the correlation of a signal with a delayed copy of itself.</li> <li>Partial Autocorrelation Function (PACF): Measures the correlation between a signal and its lagged values after removing the effects of intermediate lags.</li> <li>Cross-Correlation Function (CCF): Measures the correlation between two signals at different lags.</li> <li>Ljung-Box Test: Tests for the presence of autocorrelation in the residuals of a model.</li> <li>Lagrange Multiplier (LM) Test: A generic test for autocorrelation, often used for ARCH effects.</li> <li>Breusch-Godfrey Test: A more general version of the LM test for serial correlation in residuals.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[ArrayLike, ArrayLike1D, RegressionResults, RegressionResultsWrapper]</code> <p>The input time series data or regression results.</p> required <code>algorithm</code> <code>str</code> <p>The correlation algorithm to use. Options include: - \"acf\", \"auto\", \"ac\": Autocorrelation Function - \"pacf\", \"partial\", \"pc\": Partial Autocorrelation Function - \"ccf\", \"cross\", \"cross-correlation\", \"cc\": Cross-Correlation Function - \"lb\", \"alb\", \"acorr_ljungbox\", \"acor_lb\", \"a_lb\", \"ljungbox\": Ljung-Box Test - \"lm\", \"alm\", \"acorr_lm\", \"a_lm\": Lagrange Multiplier Test - \"bglm\", \"breusch_godfrey\", \"bg\": Breusch-Godfrey Test</p> <code>'acf'</code> <code>kwargs</code> <code>Union[float, int, str, bool, ArrayLike, None]</code> <p>Additional keyword arguments specific to the chosen algorithm.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported algorithm is specified.</p> <p>Returns:</p> Type Description <code>Union[NDArray[float64], tuple[NDArray[float64], ...], DataFrame, tuple[float, float, float, float], tuple[float, float, float, float, ResultsStore]]</code> <p>Returns the result of the specified correlation test.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.tests.correlation import correlation\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n&gt;&gt;&gt; normal = data_normal\n</code></pre> Example 1: Autocorrelation (ACF)<pre><code>&gt;&gt;&gt; res = correlation(normal, algorithm=\"acf\", nlags=10)\n&gt;&gt;&gt; print(f\"Lag 1 ACF: {res[1]:.4f}\")\nLag 1 ACF: 0.0236\n</code></pre> Example 2: Ljung-Box test<pre><code>&gt;&gt;&gt; res = correlation(normal, algorithm=\"lb\", lags=[5])\n&gt;&gt;&gt; print(res)\n    lb_stat  lb_pvalue\n5  7.882362   0.162839\n</code></pre> See Also <ul> <li><code>ts_stat_tests.algorithms.correlation.acf</code>: Autocorrelation Function algorithm.</li> <li><code>ts_stat_tests.algorithms.correlation.pacf</code>: Partial Autocorrelation Function algorithm.</li> <li><code>ts_stat_tests.algorithms.correlation.ccf</code>: Cross-Correlation Function algorithm.</li> <li><code>ts_stat_tests.algorithms.correlation.lb</code>: Ljung-Box Test algorithm.</li> <li><code>ts_stat_tests.algorithms.correlation.lm</code>: Lagrange Multiplier Test algorithm.</li> <li><code>ts_stat_tests.algorithms.correlation.bglm</code>: Breusch-Godfrey Test algorithm.</li> </ul> Source code in <code>src/ts_stat_tests/tests/correlation.py</code> <pre><code>@typechecked\ndef correlation(\n    x: Union[ArrayLike, ArrayLike1D, RegressionResults, RegressionResultsWrapper],\n    algorithm: str = \"acf\",\n    **kwargs: Union[float, int, str, bool, ArrayLike, None],\n) -&gt; Union[\n    NDArray[np.float64],\n    tuple[NDArray[np.float64], ...],\n    pd.DataFrame,\n    tuple[float, float, float, float],\n    tuple[float, float, float, float, ResultsStore],\n]:\n    \"\"\"\n    !!! note \"Summary\"\n        A unified interface for various correlation tests.\n\n    ???+ abstract \"Details\"\n        This function acts as a dispatcher for several correlation measures and tests, allowing users to access them through a single, consistent API. Depending on the `algorithm` parameter, it routes the call to the appropriate implementation in `ts_stat_tests.algorithms.correlation`.\n\n        The supported algorithms include:\n\n        - **Autocorrelation Function (ACF)**: Measures the correlation of a signal with a delayed copy of itself.\n        - **Partial Autocorrelation Function (PACF)**: Measures the correlation between a signal and its lagged values after removing the effects of intermediate lags.\n        - **Cross-Correlation Function (CCF)**: Measures the correlation between two signals at different lags.\n        - **Ljung-Box Test**: Tests for the presence of autocorrelation in the residuals of a model.\n        - **Lagrange Multiplier (LM) Test**: A generic test for autocorrelation, often used for ARCH effects.\n        - **Breusch-Godfrey Test**: A more general version of the LM test for serial correlation in residuals.\n\n    Params:\n        x (Union[ArrayLike, ArrayLike1D, RegressionResults, RegressionResultsWrapper]):\n            The input time series data or regression results.\n        algorithm (str):\n            The correlation algorithm to use. Options include:\n            - \"acf\", \"auto\", \"ac\": Autocorrelation Function\n            - \"pacf\", \"partial\", \"pc\": Partial Autocorrelation Function\n            - \"ccf\", \"cross\", \"cross-correlation\", \"cc\": Cross-Correlation Function\n            - \"lb\", \"alb\", \"acorr_ljungbox\", \"acor_lb\", \"a_lb\", \"ljungbox\": Ljung-Box Test\n            - \"lm\", \"alm\", \"acorr_lm\", \"a_lm\": Lagrange Multiplier Test\n            - \"bglm\", \"breusch_godfrey\", \"bg\": Breusch-Godfrey Test\n        kwargs (Union[float, int, str, bool, ArrayLike, None]):\n            Additional keyword arguments specific to the chosen algorithm.\n\n    Raises:\n        (ValueError):\n            If an unsupported algorithm is specified.\n\n    Returns:\n        (Union[NDArray[np.float64], tuple[NDArray[np.float64], ...], pd.DataFrame, tuple[float, float, float, float], tuple[float, float, float, float, ResultsStore]]):\n            Returns the result of the specified correlation test.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.tests.correlation import correlation\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n        &gt;&gt;&gt; normal = data_normal\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Autocorrelation (ACF)\"}\n        &gt;&gt;&gt; res = correlation(normal, algorithm=\"acf\", nlags=10)\n        &gt;&gt;&gt; print(f\"Lag 1 ACF: {res[1]:.4f}\")\n        Lag 1 ACF: 0.0236\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Ljung-Box test\"}\n        &gt;&gt;&gt; res = correlation(normal, algorithm=\"lb\", lags=[5])\n        &gt;&gt;&gt; print(res)\n            lb_stat  lb_pvalue\n        5  7.882362   0.162839\n\n        ```\n\n    ??? tip \"See Also\"\n        - [`ts_stat_tests.algorithms.correlation.acf`][ts_stat_tests.algorithms.correlation.acf]: Autocorrelation Function algorithm.\n        - [`ts_stat_tests.algorithms.correlation.pacf`][ts_stat_tests.algorithms.correlation.pacf]: Partial Autocorrelation Function algorithm.\n        - [`ts_stat_tests.algorithms.correlation.ccf`][ts_stat_tests.algorithms.correlation.ccf]: Cross-Correlation Function algorithm.\n        - [`ts_stat_tests.algorithms.correlation.lb`][ts_stat_tests.algorithms.correlation.lb]: Ljung-Box Test algorithm.\n        - [`ts_stat_tests.algorithms.correlation.lm`][ts_stat_tests.algorithms.correlation.lm]: Lagrange Multiplier Test algorithm.\n        - [`ts_stat_tests.algorithms.correlation.bglm`][ts_stat_tests.algorithms.correlation.bglm]: Breusch-Godfrey Test algorithm.\n    \"\"\"\n\n    options: dict[str, tuple[str, ...]] = {\n        \"acf\": (\"acf\", \"auto\", \"ac\"),\n        \"pacf\": (\"pacf\", \"partial\", \"pc\"),\n        \"ccf\": (\"ccf\", \"cross\", \"cross-correlation\", \"cc\"),\n        \"lb\": (\"alb\", \"acorr_ljungbox\", \"acor_lb\", \"a_lb\", \"lb\", \"ljungbox\"),\n        \"lm\": (\"alm\", \"acorr_lm\", \"a_lm\", \"lm\"),\n        \"bglm\": (\"bglm\", \"breusch_godfrey\", \"bg\"),\n    }\n\n    if algorithm in options[\"acf\"]:\n        return _acf(x=x, **kwargs)  # type: ignore\n\n    if algorithm in options[\"pacf\"]:\n        return _pacf(x=x, **kwargs)  # type: ignore\n\n    if algorithm in options[\"lb\"]:\n        return _lb(x=x, **kwargs)  # type: ignore\n\n    if algorithm in options[\"lm\"]:\n        return _lm(resid=x, **kwargs)  # type: ignore\n\n    if algorithm in options[\"ccf\"]:\n        if \"y\" not in kwargs or kwargs[\"y\"] is None:\n            raise ValueError(\"The 'ccf' algorithm requires a 'y' parameter.\")\n        return _ccf(x=x, **kwargs)  # type: ignore\n\n    if algorithm in options[\"bglm\"]:\n        return _bglm(res=x, **kwargs)  # type: ignore\n\n    raise ValueError(\n        generate_error_message(\n            parameter_name=\"algorithm\",\n            value_parsed=algorithm,\n            options=options,\n        )\n    )\n</code></pre>"},{"location":"code/correlation/#ts_stat_tests.tests.correlation.is_correlated","title":"is_correlated","text":"<pre><code>is_correlated(\n    x: Union[\n        ArrayLike,\n        ArrayLike1D,\n        RegressionResults,\n        RegressionResultsWrapper,\n    ],\n    algorithm: str = \"lb\",\n    alpha: float = 0.05,\n    **kwargs: Union[float, int, str, bool, ArrayLike, None]\n) -&gt; dict[str, Union[str, float, bool, None]]\n</code></pre> <p>Summary</p> <p>Test whether a given data set is <code>correlated</code> or not.</p> Details <p>This function checks for autocorrelation in the given data using various tests. By default, it uses the Ljung-Box test.</p> <ul> <li>Ljung-Box (<code>lb</code>): Tests the null hypothesis that the data are independently distributed (i.e. no autocorrelation). If the p-value is less than <code>alpha</code>, the null hypothesis is rejected, and the series is considered <code>correlated</code>. If multiple lags are provided, it checks if any of the p-values are below <code>alpha</code>.</li> <li>LM Test (<code>lm</code>): Tests for serial correlation. If the LMP-value is less than <code>alpha</code>, it is considered <code>correlated</code>.</li> <li>Breusch-Godfrey (<code>bglm</code>): Tests for serial correlation in residuals. If the LMP-value is less than <code>alpha</code>, it is considered <code>correlated</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[ArrayLike, ArrayLike1D, RegressionResults, RegressionResultsWrapper]</code> <p>The input time series data or regression results.</p> required <code>algorithm</code> <code>str</code> <p>The correlation algorithm to use. Options include: - <code>\"lb\"</code>, <code>\"alb\"</code>, <code>\"acorr_ljungbox\"</code>, <code>\"acor_lb\"</code>, <code>\"a_lb\"</code>, <code>\"ljungbox\"</code>: Ljung-Box Test (default) - <code>\"lm\"</code>, <code>\"alm\"</code>, <code>\"acorr_lm\"</code>, <code>\"a_lm\"</code>: Lagrange Multiplier Test - <code>\"bglm\"</code>, <code>\"breusch_godfrey\"</code>, <code>\"bg\"</code>: Breusch-Godfrey Test</p> <code>'lb'</code> <code>alpha</code> <code>float</code> <p>The significance level for the test. Default: <code>0.05</code>.</p> <code>0.05</code> <code>kwargs</code> <code>Union[float, int, str, bool, ArrayLike, None]</code> <p>Additional arguments to pass to the underlying algorithm.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unsupported algorithm is specified.</p> <p>Returns:</p> Type Description <code>dict[str, Union[str, float, bool, None]]</code> <p>A dictionary containing: - <code>\"result\"</code> (bool): <code>True</code> if the series is significantly correlated. - <code>\"statistic\"</code> (float): The test statistic. - <code>\"pvalue\"</code> (float): The p-value of the test. - <code>\"alpha\"</code> (float): The significance level used. - <code>\"algorithm\"</code> (str): The algorithm name used.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.tests.correlation import is_correlated\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n&gt;&gt;&gt; normal = data_normal\n</code></pre> Example 1: Ljung-Box test on random data<pre><code>&gt;&gt;&gt; res = is_correlated(normal, algorithm=\"lb\", lags=[5])\n&gt;&gt;&gt; res[\"result\"]\nFalse\n&gt;&gt;&gt; print(f\"p-value: {res['pvalue']:.4f}\")\np-value: 0.1628\n</code></pre> Example 2: LM test<pre><code>&gt;&gt;&gt; res = is_correlated(normal, algorithm=\"lm\", nlags=5)\n&gt;&gt;&gt; res[\"result\"]\nFalse\n</code></pre> See Also <ul> <li><code>correlation()</code>: Dispatcher for correlation measures and tests.</li> <li><code>ts_stat_tests.algorithms.correlation.lb</code>: Ljung-Box Test algorithm.</li> <li><code>ts_stat_tests.algorithms.correlation.lm</code>: Lagrange Multiplier Test algorithm.</li> <li><code>ts_stat_tests.algorithms.correlation.bglm</code>: Breusch-Godfrey Test algorithm.</li> </ul> Source code in <code>src/ts_stat_tests/tests/correlation.py</code> <pre><code>@typechecked\ndef is_correlated(\n    x: Union[ArrayLike, ArrayLike1D, RegressionResults, RegressionResultsWrapper],\n    algorithm: str = \"lb\",\n    alpha: float = 0.05,\n    **kwargs: Union[float, int, str, bool, ArrayLike, None],\n) -&gt; dict[str, Union[str, float, bool, None]]:\n    \"\"\"\n    !!! note \"Summary\"\n        Test whether a given data set is `correlated` or not.\n\n    ???+ abstract \"Details\"\n        This function checks for autocorrelation in the given data using various tests. By default, it uses the Ljung-Box test.\n\n        - **Ljung-Box (`lb`)**: Tests the null hypothesis that the data are independently distributed (i.e. no autocorrelation). If the p-value is less than `alpha`, the null hypothesis is rejected, and the series is considered `correlated`. If multiple lags are provided, it checks if any of the p-values are below `alpha`.\n        - **LM Test (`lm`)**: Tests for serial correlation. If the LMP-value is less than `alpha`, it is considered `correlated`.\n        - **Breusch-Godfrey (`bglm`)**: Tests for serial correlation in residuals. If the LMP-value is less than `alpha`, it is considered `correlated`.\n\n    Params:\n        x (Union[ArrayLike, ArrayLike1D, RegressionResults, RegressionResultsWrapper]):\n            The input time series data or regression results.\n        algorithm (str):\n            The correlation algorithm to use. Options include:\n            - `\"lb\"`, `\"alb\"`, `\"acorr_ljungbox\"`, `\"acor_lb\"`, `\"a_lb\"`, `\"ljungbox\"`: Ljung-Box Test (default)\n            - `\"lm\"`, `\"alm\"`, `\"acorr_lm\"`, `\"a_lm\"`: Lagrange Multiplier Test\n            - `\"bglm\"`, `\"breusch_godfrey\"`, `\"bg\"`: Breusch-Godfrey Test\n        alpha (float, optional):\n            The significance level for the test. Default: `0.05`.\n        kwargs (Union[float, int, str, bool, ArrayLike, None]):\n            Additional arguments to pass to the underlying algorithm.\n\n    Raises:\n        (ValueError):\n            If an unsupported algorithm is specified.\n\n    Returns:\n        (dict[str, Union[str, float, bool, None]]):\n            A dictionary containing:\n            - `\"result\"` (bool): `True` if the series is significantly correlated.\n            - `\"statistic\"` (float): The test statistic.\n            - `\"pvalue\"` (float): The p-value of the test.\n            - `\"alpha\"` (float): The significance level used.\n            - `\"algorithm\"` (str): The algorithm name used.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.tests.correlation import is_correlated\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n        &gt;&gt;&gt; normal = data_normal\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Ljung-Box test on random data\"}\n        &gt;&gt;&gt; res = is_correlated(normal, algorithm=\"lb\", lags=[5])\n        &gt;&gt;&gt; res[\"result\"]\n        False\n        &gt;&gt;&gt; print(f\"p-value: {res['pvalue']:.4f}\")\n        p-value: 0.1628\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: LM test\"}\n        &gt;&gt;&gt; res = is_correlated(normal, algorithm=\"lm\", nlags=5)\n        &gt;&gt;&gt; res[\"result\"]\n        False\n\n        ```\n\n    ??? tip \"See Also\"\n        - [`correlation()`][ts_stat_tests.tests.correlation.correlation]: Dispatcher for correlation measures and tests.\n        - [`ts_stat_tests.algorithms.correlation.lb`][ts_stat_tests.algorithms.correlation.lb]: Ljung-Box Test algorithm.\n        - [`ts_stat_tests.algorithms.correlation.lm`][ts_stat_tests.algorithms.correlation.lm]: Lagrange Multiplier Test algorithm.\n        - [`ts_stat_tests.algorithms.correlation.bglm`][ts_stat_tests.algorithms.correlation.bglm]: Breusch-Godfrey Test algorithm.\n    \"\"\"\n    options: dict[str, tuple[str, ...]] = {\n        \"lb\": (\"alb\", \"acorr_ljungbox\", \"acor_lb\", \"a_lb\", \"lb\", \"ljungbox\"),\n        \"lm\": (\"alm\", \"acorr_lm\", \"a_lm\", \"lm\"),\n        \"bglm\": (\"bglm\", \"breusch_godfrey\", \"bg\"),\n    }\n\n    res = correlation(x=x, algorithm=algorithm, **kwargs)  # type: ignore\n\n    is_corr: bool = False\n    stat: float = 0.0\n    pval: Union[float, None] = None\n\n    if algorithm in options[\"lb\"]:\n        df = res\n        # Check if any p-value is significant\n        pval = float(df[\"lb_pvalue\"].min())\n        # Metric: if any lag shows correlation, the series is correlated\n        is_corr = bool(pval &lt; alpha)\n        # Return the statistic for the most significant lag\n        idx = df[\"lb_pvalue\"].idxmin()\n        stat = float(df.loc[idx, \"lb_stat\"])\n\n    elif algorithm in options[\"lm\"] or algorithm in options[\"bglm\"]:\n        # returns (lm, lmpval, fval, fpval)\n        res_tuple = res\n        stat = float(res_tuple[0])\n        pval = float(res_tuple[1])\n        is_corr = bool(pval &lt; alpha)\n\n    else:\n        raise ValueError(\n            f\"Algorithm '{algorithm}' is not supported for 'is_correlated'. \"\n            f\"Supported algorithms for boolean check are: 'lb', 'lm', 'bglm'.\"\n        )\n\n    return {\n        \"result\": is_corr,\n        \"statistic\": stat,\n        \"pvalue\": pval,\n        \"alpha\": alpha,\n        \"algorithm\": algorithm,\n    }\n</code></pre>"},{"location":"code/correlation/#correlation-algorithms","title":"Correlation Algorithms","text":""},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation","title":"ts_stat_tests.algorithms.correlation","text":"<p>Summary</p> <p>The correlation algorithms module provides functions to compute correlation measures for time series data, including the autocorrelation function (ACF), partial autocorrelation function (PACF), and cross-correlation function (CCF). These measures help identify relationships and dependencies between time series variables, which are essential for time series analysis and forecasting.</p> <p>This module leverages the <code>statsmodels</code> library to implement these correlation measures, ensuring robust and efficient computations. The functions are designed to handle various input scenarios and provide options for customization, such as specifying the number of lags, confidence intervals, and handling missing data.</p> <p>By using these correlation algorithms, users can gain insights into the temporal dependencies within their time series data, aiding in model selection and improving forecasting accuracy.</p>"},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation.acf","title":"acf","text":"<pre><code>acf(\n    x: ArrayLike,\n    adjusted: bool = False,\n    nlags: Optional[int] = None,\n    fft: bool = True,\n    bartlett_confint: bool = True,\n    missing: VALID_ACF_MISSING_OPTIONS = \"none\",\n    *,\n    qstat: Literal[False] = False,\n    alpha: None = None\n) -&gt; NDArray[np.float64]\n</code></pre><pre><code>acf(\n    x: ArrayLike,\n    adjusted: bool = False,\n    nlags: Optional[int] = None,\n    fft: bool = True,\n    bartlett_confint: bool = True,\n    missing: VALID_ACF_MISSING_OPTIONS = \"none\",\n    *,\n    qstat: Literal[False] = False,\n    alpha: float\n) -&gt; tuple[NDArray[np.float64], NDArray[np.float64]]\n</code></pre><pre><code>acf(\n    x: ArrayLike,\n    adjusted: bool = False,\n    nlags: Optional[int] = None,\n    fft: bool = True,\n    bartlett_confint: bool = True,\n    missing: VALID_ACF_MISSING_OPTIONS = \"none\",\n    *,\n    qstat: Literal[True],\n    alpha: None = None\n) -&gt; tuple[\n    NDArray[np.float64],\n    NDArray[np.float64],\n    NDArray[np.float64],\n]\n</code></pre><pre><code>acf(\n    x: ArrayLike,\n    adjusted: bool = False,\n    nlags: Optional[int] = None,\n    fft: bool = True,\n    bartlett_confint: bool = True,\n    missing: VALID_ACF_MISSING_OPTIONS = \"none\",\n    *,\n    qstat: Literal[True],\n    alpha: float\n) -&gt; tuple[\n    NDArray[np.float64],\n    NDArray[np.float64],\n    NDArray[np.float64],\n    NDArray[np.float64],\n]\n</code></pre> <pre><code>acf(\n    x: ArrayLike,\n    adjusted: bool = False,\n    nlags: Optional[int] = None,\n    fft: bool = True,\n    bartlett_confint: bool = True,\n    missing: VALID_ACF_MISSING_OPTIONS = \"none\",\n    *,\n    qstat: bool = False,\n    alpha: Optional[float] = None\n) -&gt; Union[\n    NDArray[np.float64],\n    tuple[NDArray[np.float64], NDArray[np.float64]],\n    tuple[\n        NDArray[np.float64],\n        NDArray[np.float64],\n        NDArray[np.float64],\n    ],\n    tuple[\n        NDArray[np.float64],\n        NDArray[np.float64],\n        NDArray[np.float64],\n        NDArray[np.float64],\n    ],\n]\n</code></pre> <p>Summary</p> <p>The autocorrelation function (ACF) is a statistical tool used to study the correlation between a time series and its lagged values. In time series forecasting, the ACF is used to identify patterns and relationships between values in a time series at different lags, which can then be used to make predictions about future values.</p> <p>This function will implement the <code>acf()</code> function from the <code>statsmodels</code> library.</p> Details <p>The acf at lag <code>0</code> (ie., <code>1</code>) is returned.</p> <p>For very long time series it is recommended to use <code>fft</code> convolution instead. When <code>fft</code> is <code>False</code> uses a simple, direct estimator of the autocovariances that only computes the first \\(nlags + 1\\) values. This can be much faster when the time series is long and only a small number of autocovariances are needed.</p> <p>If <code>adjusted</code> is <code>True</code>, the denominator for the autocovariance is adjusted for the loss of data.</p> <p>The ACF measures the correlation between a time series and its lagged values at different lags. The correlation is calculated as the ratio of the covariance between the series and its lagged values to the product of their standard deviations. The ACF is typically plotted as a graph, with the lag on the <code>x</code>-axis and the correlation coefficient on the <code>y</code>-axis.</p> <p>If the ACF shows a strong positive correlation at lag \\(k\\), this means that values in the time series at time \\(t\\) and time \\(t-k\\) are strongly related. This can be useful in forecasting, as it suggests that past values can be used to predict future values. If the ACF shows a strong negative correlation at lag \\(k\\), this means that values at time \\(t\\) and time \\(t-k\\) are strongly inversely related, which can also be useful in forecasting.</p> <p>The ACF can be used to identify the order of an autoregressive (AR) model, which is a type of model used in time series forecasting. The order of an AR model is the number of lags that are used to predict future values. The ACF can also be used to diagnose the presence of seasonality in a time series.</p> <p>Overall, the autocorrelation function is a valuable tool in time series forecasting, as it helps to identify patterns and relationships between values in a time series that can be used to make predictions about future values.</p> <p>The ACF can be calculated using the <code>acf()</code> function in the <code>statsmodels</code> package in Python. The function takes a time series array as input and returns an array of autocorrelation coefficients at different lags. The significance of the autocorrelation coefficients can be tested using the Ljung-Box test, which tests the null hypothesis that the autocorrelation coefficients are zero up to a certain lag. The Ljung-Box test can be performed using the <code>acorr_ljungbox()</code> function in the <code>statsmodels</code> package. If the p-value of the test is less than a certain significance level (e.g. \\(0.05\\)), then there is evidence of significant autocorrelation in the time series up to the specified lag.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The time series data.</p> required <code>adjusted</code> <code>bool</code> <p>If <code>True</code>, then denominators for auto-covariance are \\(n-k\\), otherwise \\(n\\). Defaults to <code>False</code>.</p> <code>False</code> <code>nlags</code> <code>Optional[int]</code> <p>Number of lags to return autocorrelation for. If not provided, uses \\(\\min(10 \\times \\log_{10}(n_{obs}), n_{obs}-1)\\) (calculated with: <code>min(int(10 * np.log10(nobs)), nobs - 1)</code>). The returned value includes lag \\(0\\) (ie., \\(1\\)) so size of the acf vector is \\((nlags + 1,)\\). Defaults to <code>None</code>.</p> <code>None</code> <code>qstat</code> <code>bool</code> <p>If <code>True</code>, also returns the Ljung-Box \\(Q\\) statistic and corresponding p-values for each autocorrelation coefficient; see the Returns section for details. Defaults to <code>False</code>.</p> <code>False</code> <code>fft</code> <code>bool</code> <p>If <code>True</code>, computes the ACF via FFT. Defaults to <code>True</code>.</p> <code>True</code> <code>alpha</code> <code>Optional[float]</code> <p>If a number is given, the confidence intervals for the given level are returned. For instance if <code>alpha=0.05</code>, a \\(95\\%\\) confidence intervals are returned where the standard deviation is computed according to Bartlett's formula. Defaults to <code>None</code>.</p> <code>None</code> <code>bartlett_confint</code> <code>bool</code> <p>Confidence intervals for ACF values are generally placed at 2 standard errors around \\(r_k\\). The formula used for standard error depends upon the situation. If the autocorrelations are being used to test for randomness of residuals as part of the ARIMA routine, the standard errors are determined assuming the residuals are white noise. The approximate formula for any lag is that standard error of each \\(r_k = \\frac{1}{\\sqrt{N}}\\). See section 9.4 of [2] for more details on the \\(\\frac{1}{\\sqrt{N}}\\) result. For more elementary discussion, see section 5.3.2 in [3]. For the ACF of raw data, the standard error at a lag \\(k\\) is found as if the right model was an \\(\\text{MA}(k-1)\\). This allows the possible interpretation that if all autocorrelations past a certain lag are within the limits, the model might be an \\(\\text{MA}\\) of order defined by the last significant autocorrelation. In this case, a moving average model is assumed for the data and the standard errors for the confidence intervals should be generated using Bartlett's formula. For more details on Bartlett formula result, see section 7.2 in [2]. Defaults to <code>True</code>.</p> <code>True</code> <code>missing</code> <code>VALID_ACF_MISSING_OPTIONS</code> <p>A string in <code>[\"none\", \"raise\", \"conservative\", \"drop\"]</code> specifying how the <code>NaN</code>'s are to be treated.</p> <ul> <li><code>\"none\"</code> performs no checks.</li> <li><code>\"raise\"</code> raises an exception if NaN values are found.</li> <li><code>\"drop\"</code> removes the missing observations and then estimates the autocovariances treating the non-missing as contiguous.</li> <li><code>\"conservative\"</code> computes the autocovariance using nan-ops so that nans are removed when computing the mean and cross-products that are used to estimate the autocovariance.</li> </ul> <p>When using <code>\"conservative\"</code>, \\(n\\) is set to the number of non-missing observations. Defaults to <code>\"none\"</code>.</p> <code>'none'</code> <p>Returns:</p> Type Description <code>Union[NDArray[float64], tuple[NDArray[float64], NDArray[float64]], tuple[NDArray[float64], NDArray[float64], NDArray[float64]], tuple[NDArray[float64], NDArray[float64], NDArray[float64], NDArray[float64]]]</code> <p>Depending on <code>qstat</code> and <code>alpha</code>, returns the following values: - <code>acf</code> (NDArray[np.float64]): The autocorrelation function for lags <code>0, 1, ..., nlags</code>. - <code>confint</code> (NDArray[np.float64], optional): Confidence intervals for the ACF (returned if <code>alpha</code> is not <code>None</code>). - <code>qstat</code> (NDArray[np.float64], optional): The Ljung-Box Q-Statistic (returned if <code>qstat</code> is <code>True</code>). - <code>pvalues</code> (NDArray[np.float64], optional): P-values associated with the Q-statistics (returned if <code>qstat</code> is <code>True</code>).</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import acf\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_macrodata\n&gt;&gt;&gt; data_macro = data_macrodata.realgdp.values\n&gt;&gt;&gt; data_airline = data_airline.values\n</code></pre> Example 1: Basic ACF<pre><code>&gt;&gt;&gt; res_acf = acf(data_macro, nlags=5)\n&gt;&gt;&gt; print(res_acf[1:6])\n[0.98685781 0.97371846 0.96014366 0.94568545 0.93054425]\n</code></pre> Example 2: ACF with Confidence Intervals and Q-Statistics<pre><code>&gt;&gt;&gt; res_acf, res_confint, res_qstat, res_pvalues = acf(\n...     data_macro, nlags=5, qstat=True, alpha=0.05\n... )\n&gt;&gt;&gt; print(res_acf[1:6])\n[0.98685781 0.97371846 0.96014366 0.94568545 0.93054425]\n&gt;&gt;&gt; print(res_confint[1:6])\n[[0.84929531 1.12442032]\n [0.73753616 1.20990077]\n [0.65738012 1.2629072 ]\n [0.5899385  1.30143239]\n [0.53004062 1.33104787]]\n&gt;&gt;&gt; print(res_qstat[:5])\n[200.63546275 396.93562234 588.75493948 775.77587865 957.77058934]\n&gt;&gt;&gt; print(res_pvalues[:5])\n[1.51761209e-045 6.40508316e-087 2.76141970e-127 1.35591614e-166\n 8.33354393e-205]\n</code></pre> Example 3: ACF without FFT<pre><code>&gt;&gt;&gt; res_acf, res_confint, res_qstat, res_pvalues = acf(\n...     data_macro, nlags=5, qstat=True, alpha=0.05, fft=False\n... )\n&gt;&gt;&gt; print(res_acf[1:6])\n[0.98685781 0.97371846 0.96014366 0.94568545 0.93054425]\n&gt;&gt;&gt; print(res_qstat[:5])\n[200.63546275 396.93562234 588.75493948 775.77587865 957.77058934]\n</code></pre> Example 4: ACF with Adjusted Denominator<pre><code>&gt;&gt;&gt; res_acf, res_confint = acf(data_macro, nlags=5, adjusted=True, alpha=0.05)\n&gt;&gt;&gt; print(res_acf[1:6])\n[0.99174325 0.98340721 0.97454582 0.9646942  0.95404284]\n&gt;&gt;&gt; print(res_confint[1:6])\n[[0.85418074 1.12930575]\n [0.74645168 1.22036273]\n [0.66999819 1.27909344]\n [0.60595482 1.32343358]\n [0.54917796 1.35890772]]\n</code></pre> Calculation <p>The ACF at lag \\(k\\) is defined as:</p> \\[ ACF(k) = \\frac{ Cov(Y_t, Y_{t-k}) }{ \\sqrt{Var(Y_t) \\times Var(Y_{t-k})} } \\] <p>where:</p> <ul> <li>\\(Y_t\\) and \\(Y_{t-k}\\) are the values of the time series at time \\(t\\) and time \\(t-k\\), respectively,</li> <li>\\(Cov(Y_t, Y_{t-k})\\) is the covariance between the two values, and</li> <li>\\(Var(Y_t)\\) and \\(Var(Y_{t-k})\\) are the variances of the two values.</li> </ul> <p>For a stationary series, this simplifies to:</p> \\[ ACF(k) = \\frac{ Cov(Y_t, Y_{t-k}) }{ Var(Y_t) } \\] Credit <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> References <ol> <li>Parzen, E., 1963. On spectral analysis with missing observations and amplitude modulation. Sankhya: The Indian Journal of Statistics, Series A, pp.383-392.</li> <li>Brockwell and Davis, 1987. Time Series Theory and Methods.</li> <li>Brockwell and Davis, 2010. Introduction to Time Series and Forecasting, 2nd edition.</li> </ol> See Also <ul> <li><code>statsmodels.tsa.stattools.acf</code>: Estimate the autocorrelation function.</li> <li><code>statsmodels.tsa.stattools.pacf</code>: Partial autocorrelation estimation.</li> <li><code>statsmodels.tsa.stattools.ccf</code>: The cross-correlation function.</li> <li><code>ts_stat_tests.algorithms.correlation.acf</code>: Estimate the autocorrelation function</li> <li><code>ts_stat_tests.algorithms.correlation.pacf</code>: Partial autocorrelation estimate.</li> <li><code>ts_stat_tests.algorithms.correlation.ccf</code>: The cross-correlation function.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/correlation.py</code> <pre><code>@typechecked\ndef acf(\n    x: ArrayLike,\n    adjusted: bool = False,\n    nlags: Optional[int] = None,\n    fft: bool = True,\n    bartlett_confint: bool = True,\n    missing: VALID_ACF_MISSING_OPTIONS = \"none\",\n    *,\n    qstat: bool = False,\n    alpha: Optional[float] = None,\n) -&gt; Union[\n    NDArray[np.float64],\n    tuple[NDArray[np.float64], NDArray[np.float64]],\n    tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.float64]],\n    tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.float64], NDArray[np.float64]],\n]:\n    r\"\"\"\n    !!! note \"Summary\"\n\n        The autocorrelation function (ACF) is a statistical tool used to study the correlation between a time series and its lagged values. In time series forecasting, the ACF is used to identify patterns and relationships between values in a time series at different lags, which can then be used to make predictions about future values.\n\n        This function will implement the [`acf()`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.acf.html) function from the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ???+ abstract \"Details\"\n\n        The acf at lag `0` (ie., `1`) is returned.\n\n        For very long time series it is recommended to use `fft` convolution instead. When `fft` is `False` uses a simple, direct estimator of the autocovariances that only computes the first $nlags + 1$ values. This can be much faster when the time series is long and only a small number of autocovariances are needed.\n\n        If `adjusted` is `True`, the denominator for the autocovariance is adjusted for the loss of data.\n\n        The ACF measures the correlation between a time series and its lagged values at different lags. The correlation is calculated as the ratio of the covariance between the series and its lagged values to the product of their standard deviations. The ACF is typically plotted as a graph, with the lag on the `x`-axis and the correlation coefficient on the `y`-axis.\n\n        If the ACF shows a strong positive correlation at lag $k$, this means that values in the time series at time $t$ and time $t-k$ are strongly related. This can be useful in forecasting, as it suggests that past values can be used to predict future values. If the ACF shows a strong negative correlation at lag $k$, this means that values at time $t$ and time $t-k$ are strongly inversely related, which can also be useful in forecasting.\n\n        The ACF can be used to identify the order of an autoregressive (AR) model, which is a type of model used in time series forecasting. The order of an AR model is the number of lags that are used to predict future values. The ACF can also be used to diagnose the presence of seasonality in a time series.\n\n        Overall, the autocorrelation function is a valuable tool in time series forecasting, as it helps to identify patterns and relationships between values in a time series that can be used to make predictions about future values.\n\n        The ACF can be calculated using the `acf()` function in the `statsmodels` package in Python. The function takes a time series array as input and returns an array of autocorrelation coefficients at different lags. The significance of the autocorrelation coefficients can be tested using the Ljung-Box test, which tests the null hypothesis that the autocorrelation coefficients are zero up to a certain lag. The Ljung-Box test can be performed using the `acorr_ljungbox()` function in the `statsmodels` package. If the p-value of the test is less than a certain significance level (e.g. $0.05$), then there is evidence of significant autocorrelation in the time series up to the specified lag.\n\n    Params:\n        x (ArrayLike):\n            The time series data.\n        adjusted (bool, optional):\n            If `True`, then denominators for auto-covariance are $n-k$, otherwise $n$.&lt;br&gt;\n            Defaults to `False`.\n        nlags (Optional[int], optional):\n            Number of lags to return autocorrelation for. If not provided, uses $\\min(10 \\times \\log_{10}(n_{obs}), n_{obs}-1)$ (calculated with: `min(int(10 * np.log10(nobs)), nobs - 1)`). The returned value includes lag $0$ (ie., $1$) so size of the acf vector is $(nlags + 1,)$.&lt;br&gt;\n            Defaults to `None`.\n        qstat (bool, optional):\n            If `True`, also returns the Ljung-Box $Q$ statistic and corresponding p-values for each autocorrelation coefficient; see the *Returns* section for details.&lt;br&gt;\n            Defaults to `False`.\n        fft (bool, optional):\n            If `True`, computes the ACF via FFT.&lt;br&gt;\n            Defaults to `True`.\n        alpha (Optional[float], optional):\n            If a number is given, the confidence intervals for the given level are returned. For instance if `alpha=0.05`, a $95\\%$ confidence intervals are returned where the standard deviation is computed according to Bartlett's formula.&lt;br&gt;\n            Defaults to `None`.\n        bartlett_confint (bool, optional):\n            Confidence intervals for ACF values are generally placed at 2 standard errors around $r_k$. The formula used for standard error depends upon the situation. If the autocorrelations are being used to test for randomness of residuals as part of the ARIMA routine, the standard errors are determined assuming the residuals are white noise. The approximate formula for any lag is that standard error of each $r_k = \\frac{1}{\\sqrt{N}}$. See section 9.4 of [2] for more details on the $\\frac{1}{\\sqrt{N}}$ result. For more elementary discussion, see section 5.3.2 in [3]. For the ACF of raw data, the standard error at a lag $k$ is found as if the right model was an $\\text{MA}(k-1)$. This allows the possible interpretation that if all autocorrelations past a certain lag are within the limits, the model might be an $\\text{MA}$ of order defined by the last significant autocorrelation. In this case, a moving average model is assumed for the data and the standard errors for the confidence intervals should be generated using Bartlett's formula. For more details on Bartlett formula result, see section 7.2 in [2].&lt;br&gt;\n            Defaults to `True`.\n        missing (VALID_ACF_MISSING_OPTIONS, optional):\n            A string in `[\"none\", \"raise\", \"conservative\", \"drop\"]` specifying how the `NaN`'s are to be treated.\n\n            - `\"none\"` performs no checks.\n            - `\"raise\"` raises an exception if NaN values are found.\n            - `\"drop\"` removes the missing observations and then estimates the autocovariances treating the non-missing as contiguous.\n            - `\"conservative\"` computes the autocovariance using nan-ops so that nans are removed when computing the mean and cross-products that are used to estimate the autocovariance.\n\n            When using `\"conservative\"`, $n$ is set to the number of non-missing observations.&lt;br&gt;\n            Defaults to `\"none\"`.\n\n    Returns:\n        (Union[NDArray[np.float64], tuple[NDArray[np.float64], NDArray[np.float64]], tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.float64]], tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.float64], NDArray[np.float64]]]):\n            Depending on `qstat` and `alpha`, returns the following values:\n            - `acf` (NDArray[np.float64]): The autocorrelation function for lags `0, 1, ..., nlags`.\n            - `confint` (NDArray[np.float64], optional): Confidence intervals for the ACF (returned if `alpha` is not `None`).\n            - `qstat` (NDArray[np.float64], optional): The Ljung-Box Q-Statistic (returned if `qstat` is `True`).\n            - `pvalues` (NDArray[np.float64], optional): P-values associated with the Q-statistics (returned if `qstat` is `True`).\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import acf\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_macrodata\n        &gt;&gt;&gt; data_macro = data_macrodata.realgdp.values\n        &gt;&gt;&gt; data_airline = data_airline.values\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Basic ACF\"}\n        &gt;&gt;&gt; res_acf = acf(data_macro, nlags=5)\n        &gt;&gt;&gt; print(res_acf[1:6])\n        [0.98685781 0.97371846 0.96014366 0.94568545 0.93054425]\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: ACF with Confidence Intervals and Q-Statistics\"}\n        &gt;&gt;&gt; res_acf, res_confint, res_qstat, res_pvalues = acf(\n        ...     data_macro, nlags=5, qstat=True, alpha=0.05\n        ... )\n        &gt;&gt;&gt; print(res_acf[1:6])\n        [0.98685781 0.97371846 0.96014366 0.94568545 0.93054425]\n        &gt;&gt;&gt; print(res_confint[1:6])\n        [[0.84929531 1.12442032]\n         [0.73753616 1.20990077]\n         [0.65738012 1.2629072 ]\n         [0.5899385  1.30143239]\n         [0.53004062 1.33104787]]\n        &gt;&gt;&gt; print(res_qstat[:5])\n        [200.63546275 396.93562234 588.75493948 775.77587865 957.77058934]\n        &gt;&gt;&gt; print(res_pvalues[:5])\n        [1.51761209e-045 6.40508316e-087 2.76141970e-127 1.35591614e-166\n         8.33354393e-205]\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 3: ACF without FFT\"}\n        &gt;&gt;&gt; res_acf, res_confint, res_qstat, res_pvalues = acf(\n        ...     data_macro, nlags=5, qstat=True, alpha=0.05, fft=False\n        ... )\n        &gt;&gt;&gt; print(res_acf[1:6])\n        [0.98685781 0.97371846 0.96014366 0.94568545 0.93054425]\n        &gt;&gt;&gt; print(res_qstat[:5])\n        [200.63546275 396.93562234 588.75493948 775.77587865 957.77058934]\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 4: ACF with Adjusted Denominator\"}\n        &gt;&gt;&gt; res_acf, res_confint = acf(data_macro, nlags=5, adjusted=True, alpha=0.05)\n        &gt;&gt;&gt; print(res_acf[1:6])\n        [0.99174325 0.98340721 0.97454582 0.9646942  0.95404284]\n        &gt;&gt;&gt; print(res_confint[1:6])\n        [[0.85418074 1.12930575]\n         [0.74645168 1.22036273]\n         [0.66999819 1.27909344]\n         [0.60595482 1.32343358]\n         [0.54917796 1.35890772]]\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The ACF at lag $k$ is defined as:\n\n        $$\n        ACF(k) = \\frac{ Cov(Y_t, Y_{t-k}) }{ \\sqrt{Var(Y_t) \\times Var(Y_{t-k})} }\n        $$\n\n        where:\n\n        - $Y_t$ and $Y_{t-k}$ are the values of the time series at time $t$ and time $t-k$, respectively,\n        - $Cov(Y_t, Y_{t-k})$ is the covariance between the two values, and\n        - $Var(Y_t)$ and $Var(Y_{t-k})$ are the variances of the two values.\n\n        For a stationary series, this simplifies to:\n\n        $$\n        ACF(k) = \\frac{ Cov(Y_t, Y_{t-k}) }{ Var(Y_t) }\n        $$\n\n    ??? success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/) library.\n\n    ??? question \"References\"\n        1. Parzen, E., 1963. On spectral analysis with missing observations and amplitude modulation. Sankhya: The Indian Journal of Statistics, Series A, pp.383-392.\n        2. Brockwell and Davis, 1987. Time Series Theory and Methods.\n        3. Brockwell and Davis, 2010. Introduction to Time Series and Forecasting, 2nd edition.\n\n    ??? tip \"See Also\"\n        - [`statsmodels.tsa.stattools.acf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.acf.html): Estimate the autocorrelation function.\n        - [`statsmodels.tsa.stattools.pacf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf.html): Partial autocorrelation estimation.\n        - [`statsmodels.tsa.stattools.ccf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.ccf.html): The cross-correlation function.\n        - [`ts_stat_tests.algorithms.correlation.acf`][ts_stat_tests.algorithms.correlation.acf]: Estimate the autocorrelation function\n        - [`ts_stat_tests.algorithms.correlation.pacf`][ts_stat_tests.algorithms.correlation.pacf]: Partial autocorrelation estimate.\n        - [`ts_stat_tests.algorithms.correlation.ccf`][ts_stat_tests.algorithms.correlation.ccf]: The cross-correlation function.\n    \"\"\"\n    return st_acf(\n        x=x,\n        adjusted=adjusted,\n        nlags=nlags,\n        qstat=qstat,\n        fft=fft,\n        alpha=alpha,\n        bartlett_confint=bartlett_confint,\n        missing=missing,\n    )\n</code></pre>"},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation.pacf","title":"pacf","text":"<pre><code>pacf(\n    x: ArrayLike1D,\n    nlags: Optional[int] = None,\n    method: VALID_PACF_METHOD_OPTIONS = \"ywadjusted\",\n    *,\n    alpha: None = None\n) -&gt; NDArray[np.float64]\n</code></pre><pre><code>pacf(\n    x: ArrayLike1D,\n    nlags: Optional[int] = None,\n    method: VALID_PACF_METHOD_OPTIONS = \"ywadjusted\",\n    *,\n    alpha: float\n) -&gt; tuple[NDArray[np.float64], NDArray[np.float64]]\n</code></pre> <pre><code>pacf(\n    x: ArrayLike1D,\n    nlags: Optional[int] = None,\n    method: VALID_PACF_METHOD_OPTIONS = \"ywadjusted\",\n    *,\n    alpha: Optional[float] = None\n) -&gt; Union[\n    NDArray[np.float64],\n    tuple[NDArray[np.float64], NDArray[np.float64]],\n]\n</code></pre> <p>Summary</p> <p>The partial autocorrelation function (PACF) is a statistical tool used in time series forecasting to identify the direct relationship between two variables, controlling for the effect of the other variables in the time series. In other words, the PACF measures the correlation between a time series and its lagged values, while controlling for the effects of other intermediate lags.</p> <p>This function will implement the <code>pacf()</code> function from the <code>statsmodels</code> library.</p> Details <p>Based on simulation evidence across a range of low-order ARMA models, the best methods based on root MSE are Yule-Walker (MLW), Levinson-Durbin (MLE) and Burg, respectively. The estimators with the lowest bias included these three in addition to OLS and OLS-adjusted. Yule-Walker (adjusted) and Levinson-Durbin (adjusted) performed consistently worse than the other options.</p> <p>The PACF is a plot of the correlation between a time series and its lagged values, controlling for the effect of other lags. The PACF is useful for identifying the order of an autoregressive (AR) model, which is a type of model used in time series forecasting. The order of an AR model is the number of lags that are used to predict future values.</p> <p>The PACF is calculated using the Yule-Walker equations, which are a set of linear equations that describe the relationship between a time series and its lagged values. The PACF is calculated as the difference between the correlation coefficient at lag \\(k\\) and the correlation coefficient at lag \\(k-1\\), controlling for the effects of intermediate lags.</p> <p>The PACF is typically plotted as a graph, with the lag on the <code>x</code>-axis and the correlation coefficient on the <code>y</code>-axis. If the PACF shows a strong positive correlation at lag \\(k\\), this means that values in the time series at time \\(t\\) and time \\(t-k\\) are strongly related, after controlling for the effects of intermediate lags. This suggests that past values can be used to predict future values using an AR model with an order of \\(k\\).</p> <p>Overall, the partial autocorrelation function is a valuable tool in time series forecasting, as it helps to identify the order of an autoregressive model and to control for the effects of intermediate lags. By identifying the direct relationship between two variables, the PACF can help to improve the accuracy of time series forecasting models.</p> <p>The PACF can be calculated using the <code>pacf()</code> function in the <code>statsmodels</code> package in Python. The function takes a time series array as input and returns an array of partial autocorrelation coefficients at different lags. The significance of the partial autocorrelation coefficients can be tested using the same Ljung-Box test as for the ACF. If the p-value of the test is less than a certain significance level (e.g. \\(0.05\\)), then there is evidence of significant partial autocorrelation in the time series up to the specified lag.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike1D</code> <p>Observations of time series for which pacf is calculated.</p> required <code>nlags</code> <code>Optional[int]</code> <p>Number of lags to return autocorrelation for. If not provided, uses \\(\\min(10 \\times \\log_{10}(n_{obs}), \\lfloor \\frac{n_{obs}}{2} \\rfloor - 1)\\) (calculated with: <code>min(int(10*np.log10(nobs)), nobs // 2 - 1)</code>). The returned value includes lag <code>0</code> (ie., <code>1</code>) so size of the pacf vector is \\((nlags + 1,)\\). Defaults to <code>None</code>.</p> <code>None</code> <code>method</code> <code>VALID_PACF_METHOD_OPTIONS</code> <p>Specifies which method for the calculations to use.</p> <ul> <li><code>\"yw\"</code> or <code>\"ywadjusted\"</code>: Yule-Walker with sample-size adjustment in denominator for acovf. Default.</li> <li><code>\"ywm\"</code> or <code>\"ywmle\"</code>: Yule-Walker without adjustment.</li> <li><code>\"ols\"</code>: regression of time series on lags of it and on constant.</li> <li><code>\"ols-inefficient\"</code>: regression of time series on lags using a single common sample to estimate all pacf coefficients.</li> <li><code>\"ols-adjusted\"</code>: regression of time series on lags with a bias adjustment.</li> <li><code>\"ld\"</code> or <code>\"ldadjusted\"</code>: Levinson-Durbin recursion with bias correction.</li> <li><code>\"ldb\"</code> or <code>\"ldbiased\"</code>: Levinson-Durbin recursion without bias correction.</li> </ul> <p>Defaults to <code>\"ywadjusted\"</code>.</p> <code>'ywadjusted'</code> <code>alpha</code> <code>Optional[float]</code> <p>If a number is given, the confidence intervals for the given level are returned. For instance if <code>alpha=.05</code>, \\(95\\%\\) confidence intervals are returned where the standard deviation is computed according to \\(\\frac{1}{\\sqrt{len(x)}}\\). Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[NDArray[float64], tuple[NDArray[float64], NDArray[float64]]]</code> <p>Depending on <code>alpha</code>, returns the following values: - <code>pacf</code> (NDArray[np.float64]): The partial autocorrelations for lags <code>0, 1, ..., nlags</code>. - <code>confint</code> (NDArray[np.float64], optional): Confidence intervals for the PACF (returned if <code>alpha</code> is not <code>None</code>).</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline\n&gt;&gt;&gt; data = data_airline.values\n</code></pre> Example 1: Basic PACF using Yule-Walker adjusted<pre><code>&gt;&gt;&gt; res_pacf = pacf(data, nlags=5)\n&gt;&gt;&gt; print(res_pacf[1:6])\n[ 0.95467704 -0.26527732  0.05546955  0.10885622  0.08112579]\n</code></pre> Example 2: PACF with confidence intervals<pre><code>&gt;&gt;&gt; res_pacf, res_confint = pacf(data, nlags=5, alpha=0.05)\n&gt;&gt;&gt; print(res_confint[1:3])\n[[ 0.79134671  1.11800737]\n [-0.42860765 -0.10194698]]\n</code></pre> Example 3: PACF using OLS method<pre><code>&gt;&gt;&gt; res_pacf_ols = pacf(data, nlags=5, method=\"ols\")\n&gt;&gt;&gt; print(res_pacf_ols[1:6])\n[ 0.95893198 -0.32983096  0.2018249   0.14500798  0.25848232]\n</code></pre> Example 4: PACF using Levinson-Durbin recursion with bias correction<pre><code>&gt;&gt;&gt; res_pacf_ld = pacf(data, nlags=5, method=\"ldadjusted\")\n&gt;&gt;&gt; print(res_pacf_ld[1:6])\n[ 0.95467704 -0.26527732  0.05546955  0.10885622  0.08112579]\n</code></pre> Calculation <p>The PACF at lag \\(k\\) is defined as:</p> \\[ PACF(k) = \\text{Corr}\\left( Y_t, Y_{t-k} \\mid Y_{t-1}, Y_{t-2}, \\dots, Y_{t-k+1} \\right) \\] <p>where:</p> <ul> <li>\\(Y_t\\) and \\(Y_{t-k}\\) are the values of the time series at time \\(t\\) and time \\(t-k\\), respectively, and</li> <li>\\(Y_{t-1}, Y_{t-2}, \\dots, Y_{t-k+1}\\) are the values of the time series at intervening lags.</li> <li>\\(\\text{Corr}()\\) denotes the correlation coefficient between two variables.</li> </ul> Credit <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> References <ol> <li>Box, G. E., Jenkins, G. M., Reinsel, G. C., &amp; Ljung, G. M. (2015). Time series analysis: forecasting and control. John Wiley &amp; Sons, p. 66.</li> <li>Brockwell, P.J. and Davis, R.A., 2016. Introduction to time series and forecasting. Springer.</li> </ol> See Also <ul> <li><code>statsmodels.tsa.stattools.acf</code>: Estimate the autocorrelation function.</li> <li><code>statsmodels.tsa.stattools.pacf</code>: Partial autocorrelation estimation.</li> <li><code>statsmodels.tsa.stattools.ccf</code>: The cross-correlation function.</li> <li><code>statsmodels.tsa.stattools.pacf_yw</code>: Partial autocorrelation estimation using Yule-Walker.</li> <li><code>statsmodels.tsa.stattools.pacf_ols</code>: Partial autocorrelation estimation using OLS.</li> <li><code>statsmodels.tsa.stattools.pacf_burg</code>: Partial autocorrelation estimation using Burg's method.</li> <li><code>ts_stat_tests.algorithms.correlation.acf</code>: Estimate the autocorrelation function</li> <li><code>ts_stat_tests.algorithms.correlation.pacf</code>: Partial autocorrelation estimate.</li> <li><code>ts_stat_tests.algorithms.correlation.ccf</code>: The cross-correlation function.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/correlation.py</code> <pre><code>@typechecked\ndef pacf(\n    x: ArrayLike1D,\n    nlags: Optional[int] = None,\n    method: VALID_PACF_METHOD_OPTIONS = \"ywadjusted\",\n    *,\n    alpha: Optional[float] = None,\n) -&gt; Union[NDArray[np.float64], tuple[NDArray[np.float64], NDArray[np.float64]]]:\n    r\"\"\"\n    !!! note \"Summary\"\n\n        The partial autocorrelation function (PACF) is a statistical tool used in time series forecasting to identify the direct relationship between two variables, controlling for the effect of the other variables in the time series. In other words, the PACF measures the correlation between a time series and its lagged values, while controlling for the effects of other intermediate lags.\n\n        This function will implement the [`pacf()`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf.html) function from the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ???+ abstract \"Details\"\n\n        Based on simulation evidence across a range of low-order ARMA models, the best methods based on root MSE are Yule-Walker (MLW), Levinson-Durbin (MLE) and Burg, respectively. The estimators with the lowest bias included these three in addition to OLS and OLS-adjusted. Yule-Walker (adjusted) and Levinson-Durbin (adjusted) performed consistently worse than the other options.\n\n        The PACF is a plot of the correlation between a time series and its lagged values, controlling for the effect of other lags. The PACF is useful for identifying the order of an autoregressive (AR) model, which is a type of model used in time series forecasting. The order of an AR model is the number of lags that are used to predict future values.\n\n        The PACF is calculated using the Yule-Walker equations, which are a set of linear equations that describe the relationship between a time series and its lagged values. The PACF is calculated as the difference between the correlation coefficient at lag $k$ and the correlation coefficient at lag $k-1$, controlling for the effects of intermediate lags.\n\n        The PACF is typically plotted as a graph, with the lag on the `x`-axis and the correlation coefficient on the `y`-axis. If the PACF shows a strong positive correlation at lag $k$, this means that values in the time series at time $t$ and time $t-k$ are strongly related, after controlling for the effects of intermediate lags. This suggests that past values can be used to predict future values using an AR model with an order of $k$.\n\n        Overall, the partial autocorrelation function is a valuable tool in time series forecasting, as it helps to identify the order of an autoregressive model and to control for the effects of intermediate lags. By identifying the direct relationship between two variables, the PACF can help to improve the accuracy of time series forecasting models.\n\n        The PACF can be calculated using the `pacf()` function in the `statsmodels` package in Python. The function takes a time series array as input and returns an array of partial autocorrelation coefficients at different lags. The significance of the partial autocorrelation coefficients can be tested using the same Ljung-Box test as for the ACF. If the p-value of the test is less than a certain significance level (e.g. $0.05$), then there is evidence of significant partial autocorrelation in the time series up to the specified lag.\n\n    Params:\n        x (ArrayLike1D):\n            Observations of time series for which pacf is calculated.\n        nlags (Optional[int], optional):\n            Number of lags to return autocorrelation for. If not provided, uses $\\min(10 \\times \\log_{10}(n_{obs}), \\lfloor \\frac{n_{obs}}{2} \\rfloor - 1)$ (calculated with: `min(int(10*np.log10(nobs)), nobs // 2 - 1)`). The returned value includes lag `0` (ie., `1`) so size of the pacf vector is $(nlags + 1,)$.&lt;br&gt;\n            Defaults to `None`.\n        method (VALID_PACF_METHOD_OPTIONS, optional):\n            Specifies which method for the calculations to use.\n\n            - `\"yw\"` or `\"ywadjusted\"`: Yule-Walker with sample-size adjustment in denominator for acovf. Default.\n            - `\"ywm\"` or `\"ywmle\"`: Yule-Walker without adjustment.\n            - `\"ols\"`: regression of time series on lags of it and on constant.\n            - `\"ols-inefficient\"`: regression of time series on lags using a single common sample to estimate all pacf coefficients.\n            - `\"ols-adjusted\"`: regression of time series on lags with a bias adjustment.\n            - `\"ld\"` or `\"ldadjusted\"`: Levinson-Durbin recursion with bias correction.\n            - `\"ldb\"` or `\"ldbiased\"`: Levinson-Durbin recursion without bias correction.&lt;br&gt;\n\n            Defaults to `\"ywadjusted\"`.\n        alpha (Optional[float], optional):\n            If a number is given, the confidence intervals for the given level are returned. For instance if `alpha=.05`, $95\\%$ confidence intervals are returned where the standard deviation is computed according to $\\frac{1}{\\sqrt{len(x)}}$.&lt;br&gt;\n            Defaults to `None`.\n\n    Returns:\n        (Union[NDArray[np.float64], tuple[NDArray[np.float64], NDArray[np.float64]]]):\n            Depending on `alpha`, returns the following values:\n            - `pacf` (NDArray[np.float64]): The partial autocorrelations for lags `0, 1, ..., nlags`.\n            - `confint` (NDArray[np.float64], optional): Confidence intervals for the PACF (returned if `alpha` is not `None`).\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline\n        &gt;&gt;&gt; data = data_airline.values\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Basic PACF using Yule-Walker adjusted\"}\n        &gt;&gt;&gt; res_pacf = pacf(data, nlags=5)\n        &gt;&gt;&gt; print(res_pacf[1:6])\n        [ 0.95467704 -0.26527732  0.05546955  0.10885622  0.08112579]\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: PACF with confidence intervals\"}\n        &gt;&gt;&gt; res_pacf, res_confint = pacf(data, nlags=5, alpha=0.05)\n        &gt;&gt;&gt; print(res_confint[1:3])\n        [[ 0.79134671  1.11800737]\n         [-0.42860765 -0.10194698]]\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 3: PACF using OLS method\"}\n        &gt;&gt;&gt; res_pacf_ols = pacf(data, nlags=5, method=\"ols\")\n        &gt;&gt;&gt; print(res_pacf_ols[1:6])\n        [ 0.95893198 -0.32983096  0.2018249   0.14500798  0.25848232]\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 4: PACF using Levinson-Durbin recursion with bias correction\"}\n        &gt;&gt;&gt; res_pacf_ld = pacf(data, nlags=5, method=\"ldadjusted\")\n        &gt;&gt;&gt; print(res_pacf_ld[1:6])\n        [ 0.95467704 -0.26527732  0.05546955  0.10885622  0.08112579]\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The PACF at lag $k$ is defined as:\n\n        $$\n        PACF(k) = \\text{Corr}\\left( Y_t, Y_{t-k} \\mid Y_{t-1}, Y_{t-2}, \\dots, Y_{t-k+1} \\right)\n        $$\n\n        where:\n\n        - $Y_t$ and $Y_{t-k}$ are the values of the time series at time $t$ and time $t-k$, respectively, and\n        - $Y_{t-1}, Y_{t-2}, \\dots, Y_{t-k+1}$ are the values of the time series at intervening lags.\n        - $\\text{Corr}()$ denotes the correlation coefficient between two variables.\n\n    ??? success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/) library.\n\n    ??? question \"References\"\n        1. Box, G. E., Jenkins, G. M., Reinsel, G. C., &amp; Ljung, G. M. (2015). Time series analysis: forecasting and control. John Wiley &amp; Sons, p. 66.\n        2. Brockwell, P.J. and Davis, R.A., 2016. Introduction to time series and forecasting. Springer.\n\n    ??? tip \"See Also\"\n        - [`statsmodels.tsa.stattools.acf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.acf.html): Estimate the autocorrelation function.\n        - [`statsmodels.tsa.stattools.pacf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf.html): Partial autocorrelation estimation.\n        - [`statsmodels.tsa.stattools.ccf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.ccf.html): The cross-correlation function.\n        - [`statsmodels.tsa.stattools.pacf_yw`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf_yw.html): Partial autocorrelation estimation using Yule-Walker.\n        - [`statsmodels.tsa.stattools.pacf_ols`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf_ols.html): Partial autocorrelation estimation using OLS.\n        - [`statsmodels.tsa.stattools.pacf_burg`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf_burg.html): Partial autocorrelation estimation using Burg's method.\n        - [`ts_stat_tests.algorithms.correlation.acf`][ts_stat_tests.algorithms.correlation.acf]: Estimate the autocorrelation function\n        - [`ts_stat_tests.algorithms.correlation.pacf`][ts_stat_tests.algorithms.correlation.pacf]: Partial autocorrelation estimate.\n        - [`ts_stat_tests.algorithms.correlation.ccf`][ts_stat_tests.algorithms.correlation.ccf]: The cross-correlation function.\n    \"\"\"\n    return st_pacf(\n        x=x,\n        nlags=nlags,\n        method=method,\n        alpha=alpha,\n    )\n</code></pre>"},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation.ccf","title":"ccf","text":"<pre><code>ccf(\n    x: ArrayLike,\n    y: ArrayLike,\n    adjusted: bool = True,\n    fft: bool = True,\n    *,\n    nlags: Optional[int] = None,\n    alpha: None = None\n) -&gt; NDArray[np.float64]\n</code></pre><pre><code>ccf(\n    x: ArrayLike,\n    y: ArrayLike,\n    adjusted: bool = True,\n    fft: bool = True,\n    *,\n    nlags: Optional[int] = None,\n    alpha: float\n) -&gt; tuple[NDArray[np.float64], NDArray[np.float64]]\n</code></pre> <pre><code>ccf(\n    x: ArrayLike,\n    y: ArrayLike,\n    adjusted: bool = True,\n    fft: bool = True,\n    *,\n    nlags: Optional[int] = None,\n    alpha: Optional[float] = None\n) -&gt; Union[\n    NDArray[np.float64],\n    tuple[NDArray[np.float64], NDArray[np.float64]],\n]\n</code></pre> <p>Summary</p> <p>The cross-correlation function (CCF) is a statistical tool used in time series forecasting to measure the correlation between two time series at different lags. It is used to study the relationship between two time series, and can help to identify lead-lag relationships and causal effects.</p> <p>This function will implement the <code>ccf()</code> function from the <code>statsmodels</code> library.</p> Details <p>If <code>adjusted</code> is <code>True</code>, the denominator for the autocovariance is adjusted.</p> <p>The CCF measures the correlation between two time series at different lags. It is calculated as the ratio of the covariance between the two series at lag \\(k\\) to the product of their standard deviations. The CCF is typically plotted as a graph, with the lag on the <code>x</code>-axis and the correlation coefficient on the <code>y</code>-axis.</p> <p>If the CCF shows a strong positive correlation at lag \\(k\\), this means that changes in one time series at time \\(t\\) are strongly related to changes in the other time series at time \\(t-k\\). This suggests a lead-lag relationship between the two time series, where changes in one series lead changes in the other series by a certain number of periods. The CCF can be used to estimate the time lag between the two time series.</p> <p>The CCF can also help to identify causal relationships between two time series. If the CCF shows a strong positive correlation at lag \\(k\\), and the lag is consistent with a causal relationship between the two time series, this suggests that changes in one time series are causing changes in the other time series.</p> <p>Overall, the cross-correlation function is a valuable tool in time series forecasting, as it helps to study the relationship between two time series and to identify lead-lag relationships and causal effects. By identifying the relationship between two time series, the CCF can help to improve the accuracy of time series forecasting models.</p> <p>The CCF can be calculated using the <code>ccf()</code> function in the <code>statsmodels</code> package in Python. The function takes two time series arrays as input and returns an array of cross-correlation coefficients at different lags. The significance of the cross-correlation coefficients can be tested using a similar test to the Ljung-Box test, such as the Box-Pierce test or the Breusch-Godfrey test. These tests can be performed using the <code>boxpierce()</code> and <code>lm()</code> functions in the <code>statsmodels</code> package, respectively. If the p-value of the test is less than a certain significance level (e.g. \\(0.05\\)), then there is evidence of significant cross-correlation between the two time series at the specified lag.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The time series data to use in the calculation.</p> required <code>y</code> <code>ArrayLike</code> <p>The time series data to use in the calculation.</p> required <code>adjusted</code> <code>bool</code> <p>If <code>True</code>, then denominators for cross-correlation is \\(n-k\\), otherwise \\(n\\). Defaults to <code>True</code>.</p> <code>True</code> <code>fft</code> <code>bool</code> <p>If <code>True</code>, use FFT convolution. This method should be preferred for long time series. Defaults to <code>True</code>.</p> <code>True</code> <code>nlags</code> <code>Optional[int]</code> <p>Number of lags to return cross-correlations for. If not provided, the number of lags equals len(x). Defaults to <code>None</code>.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>If a number is given, the confidence intervals for the given level are returned. For instance if <code>alpha=.05</code>, 95% confidence intervals are returned where the standard deviation is computed according to <code>1/sqrt(len(x))</code>. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[NDArray[float64], tuple[NDArray[float64], NDArray[float64]]]</code> <p>Depending on <code>alpha</code>, returns the following values: - <code>ccf</code> (NDArray[np.float64]): The cross-correlation function of <code>x</code> and <code>y</code> for lags <code>0, 1, ..., nlags</code>. - <code>confint</code> (NDArray[np.float64], optional): Confidence intervals for the CCF (returned if <code>alpha</code> is not <code>None</code>).</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import ccf\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline\n&gt;&gt;&gt; data = data_airline.values\n</code></pre> Example 1: Basic CCF<pre><code>&gt;&gt;&gt; res = ccf(data, data + 1, adjusted=True)\n&gt;&gt;&gt; print(res[:5])\n[1.         0.95467704 0.88790688 0.82384458 0.774129  ]\n</code></pre> Example 2: CCF with confidence intervals<pre><code>&gt;&gt;&gt; res_ccf, res_conf = ccf(data, data + 1, alpha=0.05)\n&gt;&gt;&gt; print(res_ccf[:5])\n[1.         0.95467704 0.88790688 0.82384458 0.774129  ]\n&gt;&gt;&gt; print(res_conf[:5])\n[[0.83666967 1.16333033]\n [0.79134671 1.11800737]\n [0.72457654 1.05123721]\n [0.66051425 0.98717492]\n [0.61079867 0.93745933]]\n</code></pre> Example 3: CCF without adjustment<pre><code>&gt;&gt;&gt; res_ccf_no_adj = ccf(data, data + 1, adjusted=False)\n&gt;&gt;&gt; print(res_ccf_no_adj[:5])\n[1.         0.94804734 0.87557484 0.80668116 0.75262542]\n</code></pre> Example 4: CCF without FFT<pre><code>&gt;&gt;&gt; res_ccf_no_fft = ccf(data, data + 1, fft=False)\n&gt;&gt;&gt; print(res_ccf_no_fft[:5])\n[1.         0.95467704 0.88790688 0.82384458 0.774129  ]\n</code></pre> Calculation <p>The CCF at lag \\(k\\) is defined as:</p> \\[ CCF(k) = \\text{Corr}(X_t, Y_{t-k}) \\] <p>where:</p> <ul> <li>\\(X_t\\) and \\(Y_{t-k}\\) are the values of the two time series at time \\(t\\) and time \\(t-k\\), respectively.</li> <li>\\(\\text{Corr}()\\) denotes the correlation coefficient between two variables.</li> </ul> Credit <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> See Also <ul> <li><code>statsmodels.tsa.stattools.acf</code>: Estimate the autocorrelation function.</li> <li><code>statsmodels.tsa.stattools.pacf</code>: Partial autocorrelation estimation.</li> <li><code>statsmodels.tsa.stattools.ccf</code>: The cross-correlation function.</li> <li><code>ts_stat_tests.algorithms.correlation.acf</code>: Estimate the autocorrelation function</li> <li><code>ts_stat_tests.algorithms.correlation.pacf</code>: Partial autocorrelation estimate.</li> <li><code>ts_stat_tests.algorithms.correlation.ccf</code>: The cross-correlation function.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/correlation.py</code> <pre><code>@typechecked\ndef ccf(\n    x: ArrayLike,\n    y: ArrayLike,\n    adjusted: bool = True,\n    fft: bool = True,\n    *,\n    nlags: Optional[int] = None,\n    alpha: Optional[float] = None,\n) -&gt; Union[NDArray[np.float64], tuple[NDArray[np.float64], NDArray[np.float64]]]:\n    r\"\"\"\n    !!! note \"Summary\"\n\n        The cross-correlation function (CCF) is a statistical tool used in time series forecasting to measure the correlation between two time series at different lags. It is used to study the relationship between two time series, and can help to identify lead-lag relationships and causal effects.\n\n        This function will implement the [`ccf()`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.ccf.html) function from the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ???+ abstract \"Details\"\n\n        If `adjusted` is `True`, the denominator for the autocovariance is adjusted.\n\n        The CCF measures the correlation between two time series at different lags. It is calculated as the ratio of the covariance between the two series at lag $k$ to the product of their standard deviations. The CCF is typically plotted as a graph, with the lag on the `x`-axis and the correlation coefficient on the `y`-axis.\n\n        If the CCF shows a strong positive correlation at lag $k$, this means that changes in one time series at time $t$ are strongly related to changes in the other time series at time $t-k$. This suggests a lead-lag relationship between the two time series, where changes in one series lead changes in the other series by a certain number of periods. The CCF can be used to estimate the time lag between the two time series.\n\n        The CCF can also help to identify causal relationships between two time series. If the CCF shows a strong positive correlation at lag $k$, and the lag is consistent with a causal relationship between the two time series, this suggests that changes in one time series are causing changes in the other time series.\n\n        Overall, the cross-correlation function is a valuable tool in time series forecasting, as it helps to study the relationship between two time series and to identify lead-lag relationships and causal effects. By identifying the relationship between two time series, the CCF can help to improve the accuracy of time series forecasting models.\n\n        The CCF can be calculated using the `ccf()` function in the `statsmodels` package in Python. The function takes two time series arrays as input and returns an array of cross-correlation coefficients at different lags. The significance of the cross-correlation coefficients can be tested using a similar test to the Ljung-Box test, such as the Box-Pierce test or the Breusch-Godfrey test. These tests can be performed using the `boxpierce()` and `lm()` functions in the `statsmodels` package, respectively. If the p-value of the test is less than a certain significance level (e.g. $0.05$), then there is evidence of significant cross-correlation between the two time series at the specified lag.\n\n    Params:\n        x (ArrayLike):\n            The time series data to use in the calculation.\n        y (ArrayLike):\n            The time series data to use in the calculation.\n        adjusted (bool, optional):\n            If `True`, then denominators for cross-correlation is $n-k$, otherwise $n$.&lt;br&gt;\n            Defaults to `True`.\n        fft (bool, optional):\n            If `True`, use FFT convolution. This method should be preferred for long time series.&lt;br&gt;\n            Defaults to `True`.\n        nlags (Optional[int], optional):\n            Number of lags to return cross-correlations for. If not provided, the number of lags equals len(x).\n            Defaults to `None`.\n        alpha (Optional[float], optional):\n            If a number is given, the confidence intervals for the given level are returned. For instance if `alpha=.05`, 95% confidence intervals are returned where the standard deviation is computed according to `1/sqrt(len(x))`.\n            Defaults to `None`.\n\n    Returns:\n        (Union[NDArray[np.float64], tuple[NDArray[np.float64], NDArray[np.float64]]]):\n            Depending on `alpha`, returns the following values:\n            - `ccf` (NDArray[np.float64]): The cross-correlation function of `x` and `y` for lags `0, 1, ..., nlags`.\n            - `confint` (NDArray[np.float64], optional): Confidence intervals for the CCF (returned if `alpha` is not `None`).\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import ccf\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline\n        &gt;&gt;&gt; data = data_airline.values\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Basic CCF\"}\n        &gt;&gt;&gt; res = ccf(data, data + 1, adjusted=True)\n        &gt;&gt;&gt; print(res[:5])\n        [1.         0.95467704 0.88790688 0.82384458 0.774129  ]\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: CCF with confidence intervals\"}\n        &gt;&gt;&gt; res_ccf, res_conf = ccf(data, data + 1, alpha=0.05)\n        &gt;&gt;&gt; print(res_ccf[:5])\n        [1.         0.95467704 0.88790688 0.82384458 0.774129  ]\n        &gt;&gt;&gt; print(res_conf[:5])\n        [[0.83666967 1.16333033]\n         [0.79134671 1.11800737]\n         [0.72457654 1.05123721]\n         [0.66051425 0.98717492]\n         [0.61079867 0.93745933]]\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 3: CCF without adjustment\"}\n        &gt;&gt;&gt; res_ccf_no_adj = ccf(data, data + 1, adjusted=False)\n        &gt;&gt;&gt; print(res_ccf_no_adj[:5])\n        [1.         0.94804734 0.87557484 0.80668116 0.75262542]\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 4: CCF without FFT\"}\n        &gt;&gt;&gt; res_ccf_no_fft = ccf(data, data + 1, fft=False)\n        &gt;&gt;&gt; print(res_ccf_no_fft[:5])\n        [1.         0.95467704 0.88790688 0.82384458 0.774129  ]\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The CCF at lag $k$ is defined as:\n\n        $$\n        CCF(k) = \\text{Corr}(X_t, Y_{t-k})\n        $$\n\n        where:\n\n        - $X_t$ and $Y_{t-k}$ are the values of the two time series at time $t$ and time $t-k$, respectively.\n        - $\\text{Corr}()$ denotes the correlation coefficient between two variables.\n\n    ??? success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/) library.\n\n    ??? tip \"See Also\"\n        - [`statsmodels.tsa.stattools.acf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.acf.html): Estimate the autocorrelation function.\n        - [`statsmodels.tsa.stattools.pacf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf.html): Partial autocorrelation estimation.\n        - [`statsmodels.tsa.stattools.ccf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.ccf.html): The cross-correlation function.\n        - [`ts_stat_tests.algorithms.correlation.acf`][ts_stat_tests.algorithms.correlation.acf]: Estimate the autocorrelation function\n        - [`ts_stat_tests.algorithms.correlation.pacf`][ts_stat_tests.algorithms.correlation.pacf]: Partial autocorrelation estimate.\n        - [`ts_stat_tests.algorithms.correlation.ccf`][ts_stat_tests.algorithms.correlation.ccf]: The cross-correlation function.\n    \"\"\"\n    return st_ccf(\n        x=x,\n        y=y,\n        adjusted=adjusted,\n        fft=fft,\n        nlags=nlags,\n        alpha=alpha,\n    )\n</code></pre>"},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation.lb","title":"lb","text":"<pre><code>lb(\n    x: ArrayLike,\n    lags: Optional[Union[int, ArrayLike]] = None,\n    boxpierce: bool = False,\n    model_df: int = 0,\n    period: Optional[int] = None,\n    return_df: bool = True,\n    auto_lag: bool = False,\n) -&gt; pd.DataFrame\n</code></pre> <p>Summary</p> <p>The Ljung-Box test is a statistical test used in time series forecasting to test for the presence of autocorrelation in the residuals of a model. The test is based on the autocorrelation function (ACF) of the residuals, and can be used to assess the adequacy of a time series model and to identify areas for improvement.</p> <p>This function will implement the <code>acorr_ljungbox()</code> function from the <code>statsmodels</code> library.</p> Details <p>The Ljung-Box and Box-Pierce statistics differ in how they scale the autocorrelation function; the Ljung-Box test has better finite-sample properties.</p> <p>Under the null hypothesis, the test statistic follows a chi-squared distribution with degrees of freedom equal to \\(m-p\\), where \\(p\\) is the number of parameters estimated in fitting the time series model.</p> <p>The Ljung-Box test is performed by calculating the autocorrelation function (ACF) of the residuals from a time series model, and then comparing the ACF values to the expected values under the null hypothesis of no autocorrelation. The test statistic is calculated as the sum of the squared autocorrelations up to a given lag, and is compared to a chi-squared distribution with degrees of freedom equal to the number of lags tested.</p> <p>If the test statistic is greater than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is rejected, indicating that there is evidence of autocorrelation in the residuals. This suggests that the time series model is inadequate, and that additional terms may need to be added to the model to account for the remaining autocorrelation.</p> <p>If the test statistic is less than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is not rejected, indicating that there is no evidence of autocorrelation in the residuals. This suggests that the time series model is adequate, and that no further improvements are needed.</p> <p>Overall, the Ljung-Box test is a valuable tool in time series forecasting, as it helps to assess the adequacy of a time series model and to identify areas for improvement. By testing for autocorrelation in the residuals, the test helps to ensure that the model is accurately capturing the underlying patterns in the time series data.</p> <p>The Ljung-Box test can be calculated using the <code>acorr_ljungbox()</code> function in the <code>statsmodels</code> package in Python. The function takes a time series array and the maximum lag \\(m\\) as input, and returns an array of \\(Q\\)-statistics and associated p-values for each lag up to \\(m\\). If the p-value of the test is less than a certain significance level (e.g. \\(0.05\\)), then there is evidence of significant autocorrelation in the time series up to the specified lag.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data series. The data is demeaned before the test statistic is computed.</p> required <code>lags</code> <code>Optional[Union[int, ArrayLike]]</code> <p>If lags is an integer (<code>int</code>) then this is taken to be the largest lag that is included, the test result is reported for all smaller lag length. If lags is a list or array, then all lags are included up to the largest lag in the list, however only the tests for the lags in the list are reported. If lags is <code>None</code>, then the default maxlag is currently \\(\\min(\\lfloor \\frac{n_{obs}}{2} \\rfloor - 2, 40)\\) (calculated with: <code>min(nobs // 2 - 2, 40)</code>). The default number of <code>lags</code> changes if <code>period</code> is set.</p> <p>Deprecation</p> <p>After <code>statsmodels</code> version <code>0.12</code>, this calculation will change from</p> \\[ \\min\\left(\\lfloor \\frac{n_{obs}}{2} \\rfloor - 2, 40\\right) \\] <p>to</p> \\[ \\min\\left(10, \\frac{n_{obs}}{5}\\right) \\] <p>Defaults to <code>None</code>.</p> <code>None</code> <code>boxpierce</code> <code>bool</code> <p>If <code>True</code>, then additional to the results of the Ljung-Box test also the Box-Pierce test results are returned. Defaults to <code>False</code>.</p> <code>False</code> <code>model_df</code> <code>int</code> <p>Number of degrees of freedom consumed by the model. In an ARMA model, this value is usually \\(p+q\\) where \\(p\\) is the AR order and \\(q\\) is the MA order. This value is subtracted from the degrees-of-freedom used in the test so that the adjusted dof for the statistics are \\(lags - model_df\\). If \\(lags - model_df \\le 0\\), then <code>NaN</code> is returned. Defaults to <code>0</code>.</p> <code>0</code> <code>period</code> <code>Optional[int]</code> <p>The period of a Seasonal time series. Used to compute the max lag for seasonal data which uses \\(\\min(2 \\times period, \\lfloor \\frac{n_{obs}}{5} \\rfloor)\\) (calculated with: <code>min(2*period,nobs//5)</code>) if set. If <code>None</code>, then the default rule is used to set the number of lags. When set, must be \\(\\ge 2\\). Defaults to <code>None</code>.</p> <code>None</code> <code>return_df</code> <code>bool</code> <p>Flag indicating whether to return the result as a single DataFrame with columns <code>lb_stat</code>, <code>lb_pvalue</code>, and optionally <code>bp_stat</code> and <code>bp_pvalue</code>. Set to <code>True</code> to return the DataFrame or <code>False</code> to continue returning the \\(2-4\\) output. If <code>None</code> (the default), a warning is raised.</p> <p>Deprecation</p> <p>After <code>statsmodels</code> version <code>0.12</code>, this will become the only return method.</p> <p>Defaults to <code>True</code>.</p> <code>True</code> <code>auto_lag</code> <code>bool</code> <p>Flag indicating whether to automatically determine the optimal lag length based on threshold of maximum correlation value. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[DataFrame, tuple[NDArray[float64], NDArray[float64]], tuple[NDArray[float64], NDArray[float64], NDArray[float64], NDArray[float64]]]</code> <p>Depending on <code>return_df</code> and <code>boxpierce</code>, returns the following values: - <code>lb_stat</code> (NDArray[np.float64]): The Ljung-Box test statistic. - <code>lb_pvalue</code> (NDArray[np.float64]): The p-value for the Ljung-Box test. - <code>bp_stat</code> (NDArray[np.float64], optional): The Box-Pierce test statistic (returned if <code>boxpierce</code> is <code>True</code>). - <code>bp_pvalue</code> (NDArray[np.float64], optional): The p-value for the Box-Pierce test (returned if <code>boxpierce</code> is <code>True</code>).</p> Examples Setup<pre><code>&gt;&gt;&gt; from statsmodels import api as sm\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import lb\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline\n&gt;&gt;&gt; data = data_airline.values\n&gt;&gt;&gt; res = sm.tsa.ARIMA(data, order=(1, 0, 1)).fit()\n</code></pre> Example 1: Ljung-Box test on ARIMA residuals<pre><code>&gt;&gt;&gt; results = lb(res.resid, lags=[10], return_df=True)\n&gt;&gt;&gt; print(results)\n      lb_stat  lb_pvalue\n10  13.844361    0.18021\n</code></pre> Example 2: Ljung-Box and Box-Pierce tests with multiple lags<pre><code>&gt;&gt;&gt; results = lb(res.resid, lags=[5, 10, 15], boxpierce=True, return_df=True)\n&gt;&gt;&gt; print(results)\n      lb_stat     lb_pvalue    bp_stat     bp_pvalue\n5    6.274986  2.803736e-01   6.019794  3.042976e-01\n10  13.844361  1.802099e-01  13.080554  2.192028e-01\n15  86.182531  5.083111e-12  78.463124  1.332482e-10\n</code></pre> Example 3: Ljung-Box test with specific lag<pre><code>&gt;&gt;&gt; results = lb(res.resid, lags=[5], return_df=True)\n&gt;&gt;&gt; print(results)\n    lb_stat  lb_pvalue\n5  6.274986   0.280374\n</code></pre> Calculation <p>The Ljung-Box test statistic is calculated as:</p> \\[ Q(m) = n(n+2) \\sum_{k=1}^m \\frac{r_k^2}{n-k} \\] <p>where:</p> <ul> <li>\\(n\\) is the sample size,</li> <li>\\(m\\) is the maximum lag being tested,</li> <li>\\(r_k\\) is the sample autocorrelation at lag \\(k\\), and</li> <li>\\(\\sum\\) denotes the sum over \\(k\\) from \\(1\\) to \\(m\\).</li> </ul> Credit <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> References <ol> <li>Green, W. \"Econometric Analysis,\" 5th ed., Pearson, 2003.</li> <li>J. Carlos Escanciano, Ignacio N. Lobato \"An automatic Portmanteau test for serial correlation\"., Volume 151, 2009.</li> </ol> See Also <ul> <li><code>statsmodels.regression.linear_model.OLS.fit</code>: Fit a linear model.</li> <li><code>statsmodels.regression.linear_model.RegressionResults</code>: The output results of a linear regression model.</li> <li><code>statsmodels.stats.diagnostic.acorr_ljungbox</code>: Ljung-Box test for serial correlation.</li> <li><code>statsmodels.stats.diagnostic.acorr_lm</code>: Lagrange Multiplier tests for autocorrelation.</li> <li><code>statsmodels.stats.diagnostic.acorr_breusch_godfrey</code>: Breusch-Godfrey test for serial correlation.</li> <li><code>ts_stat_tests.algorithms.correlation.lb</code>: Ljung-Box test of autocorrelation in residuals.</li> <li><code>ts_stat_tests.algorithms.correlation.lm</code>: Lagrange Multiplier tests for autocorrelation.</li> <li><code>ts_stat_tests.algorithms.correlation.bglm</code>: Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/correlation.py</code> <pre><code>@typechecked\ndef lb(\n    x: ArrayLike,\n    lags: Optional[Union[int, ArrayLike]] = None,\n    boxpierce: bool = False,\n    model_df: int = 0,\n    period: Optional[int] = None,\n    return_df: bool = True,\n    auto_lag: bool = False,\n) -&gt; pd.DataFrame:\n    r\"\"\"\n    !!! note \"Summary\"\n\n        The Ljung-Box test is a statistical test used in time series forecasting to test for the presence of autocorrelation in the residuals of a model. The test is based on the autocorrelation function (ACF) of the residuals, and can be used to assess the adequacy of a time series model and to identify areas for improvement.\n\n        This function will implement the [`acorr_ljungbox()`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_ljungbox.html) function from the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ???+ abstract \"Details\"\n\n        The Ljung-Box and Box-Pierce statistics differ in how they scale the autocorrelation function; the Ljung-Box test has better finite-sample properties.\n\n        Under the null hypothesis, the test statistic follows a chi-squared distribution with degrees of freedom equal to $m-p$, where $p$ is the number of parameters estimated in fitting the time series model.\n\n        The Ljung-Box test is performed by calculating the autocorrelation function (ACF) of the residuals from a time series model, and then comparing the ACF values to the expected values under the null hypothesis of no autocorrelation. The test statistic is calculated as the sum of the squared autocorrelations up to a given lag, and is compared to a chi-squared distribution with degrees of freedom equal to the number of lags tested.\n\n        If the test statistic is greater than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is rejected, indicating that there is evidence of autocorrelation in the residuals. This suggests that the time series model is inadequate, and that additional terms may need to be added to the model to account for the remaining autocorrelation.\n\n        If the test statistic is less than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is not rejected, indicating that there is no evidence of autocorrelation in the residuals. This suggests that the time series model is adequate, and that no further improvements are needed.\n\n        Overall, the Ljung-Box test is a valuable tool in time series forecasting, as it helps to assess the adequacy of a time series model and to identify areas for improvement. By testing for autocorrelation in the residuals, the test helps to ensure that the model is accurately capturing the underlying patterns in the time series data.\n\n        The Ljung-Box test can be calculated using the `acorr_ljungbox()` function in the `statsmodels` package in Python. The function takes a time series array and the maximum lag $m$ as input, and returns an array of $Q$-statistics and associated p-values for each lag up to $m$. If the p-value of the test is less than a certain significance level (e.g. $0.05$), then there is evidence of significant autocorrelation in the time series up to the specified lag.\n\n    Params:\n        x (ArrayLike):\n            The data series. The data is demeaned before the test statistic is computed.\n        lags (Optional[Union[int, ArrayLike]], optional):\n            If lags is an integer (`int`) then this is taken to be the largest lag that is included, the test result is reported for all smaller lag length. If lags is a list or array, then all lags are included up to the largest lag in the list, however only the tests for the lags in the list are reported. If lags is `None`, then the default maxlag is currently $\\min(\\lfloor \\frac{n_{obs}}{2} \\rfloor - 2, 40)$ (calculated with: `min(nobs // 2 - 2, 40)`). The default number of `lags` changes if `period` is set.\n            !!! deprecation \"Deprecation\"\n                After `statsmodels` version `0.12`, this calculation will change from\n\n                $$\n                \\min\\left(\\lfloor \\frac{n_{obs}}{2} \\rfloor - 2, 40\\right)\n                $$\n\n                to\n\n                $$\n                \\min\\left(10, \\frac{n_{obs}}{5}\\right)\n                $$\n\n            Defaults to `None`.\n        boxpierce (bool, optional):\n            If `True`, then additional to the results of the Ljung-Box test also the Box-Pierce test results are returned.&lt;br&gt;\n            Defaults to `False`.\n        model_df (int, optional):\n            Number of degrees of freedom consumed by the model. In an ARMA model, this value is usually $p+q$ where $p$ is the AR order and $q$ is the MA order. This value is subtracted from the degrees-of-freedom used in the test so that the adjusted dof for the statistics are $lags - model_df$. If $lags - model_df \\le 0$, then `NaN` is returned.&lt;br&gt;\n            Defaults to `0`.\n        period (Optional[int], optional):\n            The period of a Seasonal time series. Used to compute the max lag for seasonal data which uses $\\min(2 \\times period, \\lfloor \\frac{n_{obs}}{5} \\rfloor)$ (calculated with: `min(2*period,nobs//5)`) if set. If `None`, then the default rule is used to set the number of lags. When set, must be $\\ge 2$.&lt;br&gt;\n            Defaults to `None`.\n        return_df (bool, optional):\n            Flag indicating whether to return the result as a single DataFrame with columns `lb_stat`, `lb_pvalue`, and optionally `bp_stat` and `bp_pvalue`. Set to `True` to return the DataFrame or `False` to continue returning the $2-4$ output. If `None` (the default), a warning is raised.\n\n            !!! deprecation \"Deprecation\"\n                After `statsmodels` version `0.12`, this will become the only return method.\n\n            Defaults to `True`.\n        auto_lag (bool, optional):\n            Flag indicating whether to automatically determine the optimal lag length based on threshold of maximum correlation value.&lt;br&gt;\n            Defaults to `False`.\n\n    Returns:\n        (Union[pd.DataFrame, tuple[NDArray[np.float64], NDArray[np.float64]], tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.float64], NDArray[np.float64]]]):\n            Depending on `return_df` and `boxpierce`, returns the following values:\n            - `lb_stat` (NDArray[np.float64]): The Ljung-Box test statistic.\n            - `lb_pvalue` (NDArray[np.float64]): The p-value for the Ljung-Box test.\n            - `bp_stat` (NDArray[np.float64], optional): The Box-Pierce test statistic (returned if `boxpierce` is `True`).\n            - `bp_pvalue` (NDArray[np.float64], optional): The p-value for the Box-Pierce test (returned if `boxpierce` is `True`).\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from statsmodels import api as sm\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import lb\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline\n        &gt;&gt;&gt; data = data_airline.values\n        &gt;&gt;&gt; res = sm.tsa.ARIMA(data, order=(1, 0, 1)).fit()\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Ljung-Box test on ARIMA residuals\"}\n        &gt;&gt;&gt; results = lb(res.resid, lags=[10], return_df=True)\n        &gt;&gt;&gt; print(results)\n              lb_stat  lb_pvalue\n        10  13.844361    0.18021\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Ljung-Box and Box-Pierce tests with multiple lags\"}\n        &gt;&gt;&gt; results = lb(res.resid, lags=[5, 10, 15], boxpierce=True, return_df=True)\n        &gt;&gt;&gt; print(results)\n              lb_stat     lb_pvalue    bp_stat     bp_pvalue\n        5    6.274986  2.803736e-01   6.019794  3.042976e-01\n        10  13.844361  1.802099e-01  13.080554  2.192028e-01\n        15  86.182531  5.083111e-12  78.463124  1.332482e-10\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 3: Ljung-Box test with specific lag\"}\n        &gt;&gt;&gt; results = lb(res.resid, lags=[5], return_df=True)\n        &gt;&gt;&gt; print(results)\n            lb_stat  lb_pvalue\n        5  6.274986   0.280374\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The Ljung-Box test statistic is calculated as:\n\n        $$\n        Q(m) = n(n+2) \\sum_{k=1}^m \\frac{r_k^2}{n-k}\n        $$\n\n        where:\n\n        - $n$ is the sample size,\n        - $m$ is the maximum lag being tested,\n        - $r_k$ is the sample autocorrelation at lag $k$, and\n        - $\\sum$ denotes the sum over $k$ from $1$ to $m$.\n\n    ??? success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/) library.\n\n    ??? question \"References\"\n        1. Green, W. \"Econometric Analysis,\" 5th ed., Pearson, 2003.\n        2. J. Carlos Escanciano, Ignacio N. Lobato \"An automatic Portmanteau test for serial correlation\"., Volume 151, 2009.\n\n    ??? tip \"See Also\"\n        - [`statsmodels.regression.linear_model.OLS.fit`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.fit.html): Fit a linear model.\n        - [`statsmodels.regression.linear_model.RegressionResults`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.RegressionResults.html): The output results of a linear regression model.\n        - [`statsmodels.stats.diagnostic.acorr_ljungbox`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_ljungbox.html): Ljung-Box test for serial correlation.\n        - [`statsmodels.stats.diagnostic.acorr_lm`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_lm.html): Lagrange Multiplier tests for autocorrelation.\n        - [`statsmodels.stats.diagnostic.acorr_breusch_godfrey`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_breusch_godfrey.html): Breusch-Godfrey test for serial correlation.\n        - [`ts_stat_tests.algorithms.correlation.lb`][ts_stat_tests.algorithms.correlation.lb]: Ljung-Box test of autocorrelation in residuals.\n        - [`ts_stat_tests.algorithms.correlation.lm`][ts_stat_tests.algorithms.correlation.lm]: Lagrange Multiplier tests for autocorrelation.\n        - [`ts_stat_tests.algorithms.correlation.bglm`][ts_stat_tests.algorithms.correlation.bglm]: Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation.\n    \"\"\"\n    return acorr_ljungbox(\n        x=x,\n        lags=lags,\n        boxpierce=boxpierce,\n        model_df=model_df,\n        period=period,\n        return_df=return_df,\n        auto_lag=auto_lag,\n    )\n</code></pre>"},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation.lm","title":"lm","text":"<pre><code>lm(\n    resid: ArrayLike,\n    nlags: Optional[int] = None,\n    *,\n    store: Literal[False] = False,\n    period: Optional[int] = None,\n    ddof: int = 0,\n    cov_type: VALID_LM_COV_TYPE_OPTIONS = \"nonrobust\",\n    cov_kwargs: Optional[dict] = None\n) -&gt; tuple[float, NDArray[np.float64], float, float]\n</code></pre><pre><code>lm(\n    resid: ArrayLike,\n    nlags: Optional[int] = None,\n    *,\n    store: Literal[True],\n    period: Optional[int] = None,\n    ddof: int = 0,\n    cov_type: VALID_LM_COV_TYPE_OPTIONS = \"nonrobust\",\n    cov_kwargs: Optional[dict] = None\n) -&gt; tuple[float, float, float, float, ResultsStore]\n</code></pre> <pre><code>lm(\n    resid: ArrayLike,\n    nlags: Optional[int] = None,\n    *,\n    store: bool = False,\n    period: Optional[int] = None,\n    ddof: int = 0,\n    cov_type: VALID_LM_COV_TYPE_OPTIONS = \"nonrobust\",\n    cov_kwargs: Optional[dict] = None\n) -&gt; Union[\n    tuple[\n        float,\n        Union[float, NDArray[np.float64]],\n        float,\n        float,\n    ],\n    tuple[\n        float,\n        Union[float, NDArray[np.float64]],\n        float,\n        float,\n        ResultsStore,\n    ],\n]\n</code></pre> <p>Summary</p> <p>The Lagrange Multiplier (LM) test is a statistical test used in time series forecasting to test for the presence of autocorrelation in a model. The test is based on the residual sum of squares (RSS) of a time series model, and can be used to assess the adequacy of the model and to identify areas for improvement.</p> <p>This function implements the <code>acorr_lm()</code> function from the <code>statsmodels</code> library.</p> Details <p>This is a generic Lagrange Multiplier (LM) test for autocorrelation. It returns Engle's ARCH test if <code>resid</code> is the squared residual array. The Breusch-Godfrey test is a variation on this LM test with additional exogenous variables in the auxiliary regression.</p> <p>The LM test proceeds by:</p> <ul> <li>Fitting a time series model to the data and obtaining the residuals.</li> <li>Running an auxiliary regression of these residuals on their past <code>nlags</code> values (and any relevant exogenous variables).</li> <li>Computing the LM statistic as \\((n_{obs} - ddof) \\times R^2\\) from this auxiliary regression.</li> </ul> <p>Under the null hypothesis that the autocorrelations up to the specified lag are zero (no serial correlation in the residuals), the LM statistic is asymptotically distributed as a chi-squared random variable with degrees of freedom equal to the number of lagged residual terms included in the auxiliary regression (i.e. the number of lags being tested, adjusted for any restrictions implied by the model).</p> <p>If the test statistic is greater than the critical value from the chi-squared distribution (or equivalently, if the p-value is less than a chosen significance level such as \\(0.05\\)), then the null hypothesis of no autocorrelation is rejected, indicating that there is evidence of autocorrelation in the residuals.</p> <p>The LM test is a generalization of the Durbin-Watson test, which is a simpler test that only tests for first-order autocorrelation.</p> <p>Parameters:</p> Name Type Description Default <code>resid</code> <code>ArrayLike</code> <p>Time series to test.</p> required <code>nlags</code> <code>Optional[int]</code> <p>Highest lag to use. Defaults to <code>None</code>.</p> <p>Deprecation</p> <p>The behavior of this parameter will change after <code>statsmodels</code> version <code>0.12</code>.</p> <code>None</code> <code>store</code> <code>bool</code> <p>If <code>True</code> then the intermediate results are also returned. Defaults to <code>False</code>.</p> <code>False</code> <code>period</code> <code>Optional[int]</code> <p>The period of a Seasonal time series. Used to compute the max lag for seasonal data which uses \\(\\min(2 \\times period, \\lfloor \\frac{n_{obs}}{5} \\rfloor)\\) (calculated with: <code>min(2*period,nobs//5)</code>) if set. If <code>None</code>, then the default rule is used to set the number of lags. When set, must be \\(\\ge 2\\). Defaults to <code>None</code>.</p> <code>None</code> <code>ddof</code> <code>int</code> <p>The number of degrees of freedom consumed by the model used to produce <code>resid</code>. Defaults to <code>0</code>.</p> <code>0</code> <code>cov_type</code> <code>VALID_LM_COV_TYPE_OPTIONS</code> <p>Covariance type. The default is <code>\"nonrobust\"</code> which uses the classic OLS covariance estimator. Specify one of <code>\"HC0\"</code>, <code>\"HC1\"</code>, <code>\"HC2\"</code>, <code>\"HC3\"</code> to use White's covariance estimator. All covariance types supported by <code>OLS.fit</code> are accepted. Defaults to <code>\"nonrobust\"</code>.</p> <code>'nonrobust'</code> <code>cov_kwargs</code> <code>Optional[dict]</code> <p>Dictionary of covariance options passed to <code>OLS.fit</code>. See <code>OLS.fit</code> for more details. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[tuple[float, Union[float, NDArray[float64]], float, float], tuple[float, Union[float, NDArray[float64]], float, float, ResultsStore]]</code> <p>Returns the following values: - <code>lm</code> (float): Lagrange multiplier test statistic. - <code>lmpval</code> (Union[float, NDArray[np.float64]]): The p-value for the Lagrange multiplier test. - <code>fval</code> (float): The f-statistic of the F test. - <code>fpval</code> (float): The p-value of the F test. - <code>res_store</code> (ResultsStore, optional): Intermediate results (returned if <code>store</code> is <code>True</code>).</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import lm\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline\n&gt;&gt;&gt; data = data_airline.values\n</code></pre> Example 1: Lagrange Multiplier test<pre><code>&gt;&gt;&gt; res_lm, res_p, res_f, res_fp = lm(data)\n&gt;&gt;&gt; print(f\"LM statistic: {res_lm:.4f}\")\nLM statistic: 128.0966\n&gt;&gt;&gt; print(f\"p-value: {res_p:.4e}\")\np-value: 1.1417e-22\n</code></pre> Example 2: Lagrange Multiplier test with intermediate results<pre><code>&gt;&gt;&gt; res_lm, res_p, res_f, res_fp, res_store = lm(data, store=True)\n&gt;&gt;&gt; print(f\"LM statistic: {res_lm:.4f}\")\nLM statistic: 128.0966\n&gt;&gt;&gt; print(f\"p-value: {res_p:.4e}\")\np-value: 1.1417e-22\n</code></pre> Example 3: Lagrange Multiplier test with robust covariance<pre><code>&gt;&gt;&gt; res_lm, res_p, res_f, res_fp = lm(data, cov_type=\"HC3\")\n&gt;&gt;&gt; print(f\"LM statistic: {res_lm:.4f}\")\nLM statistic: 2063.3981\n&gt;&gt;&gt; print(f\"p-value: {res_p:.1f}\")\np-value: 0.0\n</code></pre> Example 4: Lagrange Multiplier test with seasonal period<pre><code>&gt;&gt;&gt; res_lm, res_p, res_f, res_fp = lm(data, period=12)\n&gt;&gt;&gt; print(f\"LM statistic: {res_lm:.4f}\")\nLM statistic: 119.1109\n&gt;&gt;&gt; print(f\"p-value: {res_p:.4e}\")\np-value: 1.3968e-14\n</code></pre> Example 5: Lagrange Multiplier test with specified degrees of freedom<pre><code>&gt;&gt;&gt; res_lm, res_p, res_f, res_fp = lm(data, ddof=2)\n&gt;&gt;&gt; print(f\"LM statistic: {res_lm:.4f}\")\nLM statistic: 126.1847\n&gt;&gt;&gt; print(f\"p-value: {res_p:.4e}\")\np-value: 2.7990e-22\n</code></pre> Calculation <p>The LM test statistic is computed as:</p> \\[ LM = (n_{obs} - ddof) \\times R^2 \\] <p>where:</p> <ul> <li>\\(R^2\\) is the coefficient of determination from the auxiliary regression of the residuals on their own <code>nlags</code> lags,</li> <li>\\(n_{obs}\\) is the number of observations, and</li> <li>\\(ddof\\) is the model degrees of freedom lost due to parameter estimation.</li> </ul> Credit <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> See Also <ul> <li><code>statsmodels.stats.diagnostic.acorr_lm</code>: Lagrange Multiplier tests for autocorrelation.</li> <li><code>ts_stat_tests.algorithms.correlation.lb</code>: Ljung-Box test of autocorrelation in residuals.</li> <li><code>ts_stat_tests.algorithms.correlation.bglm</code>: Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/correlation.py</code> <pre><code>@typechecked\ndef lm(\n    resid: ArrayLike,\n    nlags: Optional[int] = None,\n    *,\n    store: bool = False,\n    period: Optional[int] = None,\n    ddof: int = 0,\n    cov_type: VALID_LM_COV_TYPE_OPTIONS = \"nonrobust\",\n    cov_kwargs: Optional[dict] = None,\n) -&gt; Union[\n    tuple[float, Union[float, NDArray[np.float64]], float, float],\n    tuple[float, Union[float, NDArray[np.float64]], float, float, ResultsStore],\n]:\n    r\"\"\"\n    !!! note \"Summary\"\n\n        The Lagrange Multiplier (LM) test is a statistical test used in time series forecasting to test for the presence of autocorrelation in a model. The test is based on the residual sum of squares (RSS) of a time series model, and can be used to assess the adequacy of the model and to identify areas for improvement.\n\n        This function implements the [`acorr_lm()`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_lm.html) function from the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ???+ abstract \"Details\"\n\n        This is a generic Lagrange Multiplier (LM) test for autocorrelation. It returns Engle's ARCH test if `resid` is the squared residual array. The Breusch-Godfrey test is a variation on this LM test with additional exogenous variables in the auxiliary regression.\n\n        The LM test proceeds by:\n\n        - Fitting a time series model to the data and obtaining the residuals.\n        - Running an auxiliary regression of these residuals on their past `nlags` values (and any relevant exogenous variables).\n        - Computing the LM statistic as $(n_{obs} - ddof) \\times R^2$ from this auxiliary regression.\n\n        Under the null hypothesis that the autocorrelations up to the specified lag are zero (no serial correlation in the residuals), the LM statistic is asymptotically distributed as a chi-squared random variable with degrees of freedom equal to the number of lagged residual terms included in the auxiliary regression (i.e. the number of lags being tested, adjusted for any restrictions implied by the model).\n\n        If the test statistic is greater than the critical value from the chi-squared distribution (or equivalently, if the p-value is less than a chosen significance level such as $0.05$), then the null hypothesis of no autocorrelation is rejected, indicating that there is evidence of autocorrelation in the residuals.\n\n        The LM test is a generalization of the Durbin-Watson test, which is a simpler test that only tests for first-order autocorrelation.\n\n    Params:\n        resid (ArrayLike):\n            Time series to test.\n        nlags (Optional[int], optional):\n            Highest lag to use. Defaults to `None`.\n            !!! deprecation \"Deprecation\"\n                The behavior of this parameter will change after `statsmodels` version `0.12`.\n        store (bool, optional):\n            If `True` then the intermediate results are also returned. Defaults to `False`.\n        period (Optional[int], optional):\n            The period of a Seasonal time series. Used to compute the max lag for seasonal data which uses $\\min(2 \\times period, \\lfloor \\frac{n_{obs}}{5} \\rfloor)$ (calculated with: `min(2*period,nobs//5)`) if set. If `None`, then the default rule is used to set the number of lags. When set, must be $\\ge 2$. Defaults to `None`.\n        ddof (int, optional):\n            The number of degrees of freedom consumed by the model used to produce `resid`. Defaults to `0`.\n        cov_type (VALID_LM_COV_TYPE_OPTIONS, optional):\n            Covariance type. The default is `\"nonrobust\"` which uses the classic OLS covariance estimator. Specify one of `\"HC0\"`, `\"HC1\"`, `\"HC2\"`, `\"HC3\"` to use White's covariance estimator. All covariance types supported by `OLS.fit` are accepted. Defaults to `\"nonrobust\"`.\n        cov_kwargs (Optional[dict], optional):\n            Dictionary of covariance options passed to `OLS.fit`. See [`OLS.fit`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.fit.html) for more details. Defaults to `None`.\n\n    Returns:\n        (Union[tuple[float, Union[float, NDArray[np.float64]], float, float], tuple[float, Union[float, NDArray[np.float64]], float, float, ResultsStore]]):\n            Returns the following values:\n            - `lm` (float): Lagrange multiplier test statistic.\n            - `lmpval` (Union[float, NDArray[np.float64]]): The p-value for the Lagrange multiplier test.\n            - `fval` (float): The f-statistic of the F test.\n            - `fpval` (float): The p-value of the F test.\n            - `res_store` (ResultsStore, optional): Intermediate results (returned if `store` is `True`).\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import lm\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline\n        &gt;&gt;&gt; data = data_airline.values\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Lagrange Multiplier test\"}\n        &gt;&gt;&gt; res_lm, res_p, res_f, res_fp = lm(data)\n        &gt;&gt;&gt; print(f\"LM statistic: {res_lm:.4f}\")\n        LM statistic: 128.0966\n        &gt;&gt;&gt; print(f\"p-value: {res_p:.4e}\")\n        p-value: 1.1417e-22\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Lagrange Multiplier test with intermediate results\"}\n        &gt;&gt;&gt; res_lm, res_p, res_f, res_fp, res_store = lm(data, store=True)\n        &gt;&gt;&gt; print(f\"LM statistic: {res_lm:.4f}\")\n        LM statistic: 128.0966\n        &gt;&gt;&gt; print(f\"p-value: {res_p:.4e}\")\n        p-value: 1.1417e-22\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 3: Lagrange Multiplier test with robust covariance\"}\n        &gt;&gt;&gt; res_lm, res_p, res_f, res_fp = lm(data, cov_type=\"HC3\")\n        &gt;&gt;&gt; print(f\"LM statistic: {res_lm:.4f}\")\n        LM statistic: 2063.3981\n        &gt;&gt;&gt; print(f\"p-value: {res_p:.1f}\")\n        p-value: 0.0\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 4: Lagrange Multiplier test with seasonal period\"}\n        &gt;&gt;&gt; res_lm, res_p, res_f, res_fp = lm(data, period=12)\n        &gt;&gt;&gt; print(f\"LM statistic: {res_lm:.4f}\")\n        LM statistic: 119.1109\n        &gt;&gt;&gt; print(f\"p-value: {res_p:.4e}\")\n        p-value: 1.3968e-14\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 5: Lagrange Multiplier test with specified degrees of freedom\"}\n        &gt;&gt;&gt; res_lm, res_p, res_f, res_fp = lm(data, ddof=2)\n        &gt;&gt;&gt; print(f\"LM statistic: {res_lm:.4f}\")\n        LM statistic: 126.1847\n        &gt;&gt;&gt; print(f\"p-value: {res_p:.4e}\")\n        p-value: 2.7990e-22\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The LM test statistic is computed as:\n\n        $$\n        LM = (n_{obs} - ddof) \\times R^2\n        $$\n\n        where:\n\n        - $R^2$ is the coefficient of determination from the auxiliary regression of the residuals on their own `nlags` lags,\n        - $n_{obs}$ is the number of observations, and\n        - $ddof$ is the model degrees of freedom lost due to parameter estimation.\n\n    ??? success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/) library.\n\n    ??? tip \"See Also\"\n        - [`statsmodels.stats.diagnostic.acorr_lm`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_lm.html): Lagrange Multiplier tests for autocorrelation.\n        - [`ts_stat_tests.algorithms.correlation.lb`][ts_stat_tests.algorithms.correlation.lb]: Ljung-Box test of autocorrelation in residuals.\n        - [`ts_stat_tests.algorithms.correlation.bglm`][ts_stat_tests.algorithms.correlation.bglm]: Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation.\n    \"\"\"\n    return acorr_lm(\n        resid=resid,\n        nlags=nlags,\n        store=store,\n        period=period,\n        ddof=ddof,\n        cov_type=cov_type,\n        cov_kwargs=cov_kwargs,\n    )\n</code></pre>"},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation.bglm","title":"bglm","text":"<pre><code>bglm(\n    res: Union[RegressionResults, RegressionResultsWrapper],\n    nlags: Optional[int] = None,\n    *,\n    store: Literal[False] = False\n) -&gt; tuple[\n    float, Union[float, NDArray[np.float64]], float, float\n]\n</code></pre><pre><code>bglm(\n    res: Union[RegressionResults, RegressionResultsWrapper],\n    nlags: Optional[int] = None,\n    *,\n    store: Literal[True]\n) -&gt; tuple[\n    float,\n    Union[float, NDArray[np.float64]],\n    float,\n    float,\n    ResultsStore,\n]\n</code></pre> <pre><code>bglm(\n    res: Union[RegressionResults, RegressionResultsWrapper],\n    nlags: Optional[int] = None,\n    *,\n    store: bool = False\n) -&gt; Union[\n    tuple[\n        float,\n        Union[float, NDArray[np.float64]],\n        float,\n        float,\n    ],\n    tuple[\n        float,\n        Union[float, NDArray[np.float64]],\n        float,\n        float,\n        ResultsStore,\n    ],\n]\n</code></pre> <p>Summary</p> <p>The Breusch-Godfrey Lagrange Multiplier (BGLM) test is a statistical test used in time series forecasting to test for the presence of autocorrelation in the residuals of a model. The test is a generalization of the LM test and can be used to test for autocorrelation up to a specified order.</p> <p>This function implements the <code>acorr_breusch_godfrey()</code> function from the <code>statsmodels</code> library.</p> Details <p>BG adds lags of residual to exog in the design matrix for the auxiliary regression with residuals as endog. See Greene (2002), section 12.7.1.</p> <p>The BGLM test is performed by first fitting a time series model to the data and then obtaining the residuals from the model. The residuals are then used to estimate the autocorrelation function (ACF) up to a specified order.</p> <p>Under the null hypothesis that there is no autocorrelation in the residuals of the regression model, the BGLM test statistic follows a chi-squared distribution with degrees of freedom equal to the number of lags included in the model.</p> <p>If the test statistic is greater than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is rejected, indicating that there is evidence of autocorrelation in the residuals.</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>Union[RegressionResults, RegressionResultsWrapper]</code> <p>Estimation results for which the residuals are tested for serial correlation.</p> required <code>nlags</code> <code>Optional[int]</code> <p>Number of lags to include in the auxiliary regression. (<code>nlags</code> is highest lag). Defaults to <code>None</code>.</p> <code>None</code> <code>store</code> <code>bool</code> <p>If <code>store</code> is <code>True</code>, then an additional class instance that contains intermediate results is returned. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[tuple[float, float, float, float], tuple[float, float, float, float, ResultsStore]]</code> <p>Returns the following values: - <code>lm</code> (float): Lagrange multiplier test statistic. - <code>lmpval</code> (float): The p-value for the Lagrange multiplier test. - <code>fval</code> (float): The value of the f-statistic for the F test. - <code>fpval</code> (float): The p-value of the F test. - <code>res_store</code> (ResultsStore, optional): Intermediate results (returned if <code>store</code> is <code>True</code>).</p> Examples Setup<pre><code>&gt;&gt;&gt; from statsmodels import api as sm\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import bglm\n&gt;&gt;&gt; y = sm.datasets.longley.load_pandas().endog\n&gt;&gt;&gt; X = sm.datasets.longley.load_pandas().exog\n&gt;&gt;&gt; X = sm.add_constant(X)\n&gt;&gt;&gt; model = sm.OLS(y, X).fit()\n</code></pre> Example 1: Breusch-Godfrey test<pre><code>&gt;&gt;&gt; res_lm, res_p, res_f, res_fp = bglm(model)\n&gt;&gt;&gt; print(f\"LM statistic: {res_lm:.4f}\")\nLM statistic: 5.1409\n&gt;&gt;&gt; print(f\"p-value: {res_p:.4f}\")\np-value: 0.1618\n</code></pre> Example 2: Breusch-Godfrey test with intermediate results<pre><code>&gt;&gt;&gt; res_lm, res_p, res_f, res_fp, res_store = bglm(model, store=True)\n&gt;&gt;&gt; print(f\"LM statistic: {res_lm:.4f}\")\nLM statistic: 5.1409\n&gt;&gt;&gt; print(f\"p-value: {res_p:.4f}\")\np-value: 0.1618\n</code></pre> Example 3: Breusch-Godfrey test with specified lags<pre><code>&gt;&gt;&gt; res_lm, res_p, res_f, res_fp = bglm(model, nlags=2)\n&gt;&gt;&gt; print(f\"LM statistic: {res_lm:.4f}\")\nLM statistic: 2.8762\n&gt;&gt;&gt; print(f\"p-value: {res_p:.4f}\")\np-value: 0.2374\n</code></pre> Calculation <p>The BGLM test statistic is calculated as:</p> \\[ BGLM = n \\times R^2 \\] <p>where:</p> <ul> <li>\\(n\\) is the sample size and</li> <li>\\(R^2\\) is the coefficient of determination from a regression of the residuals on the lagged values of the residuals and the lagged values of the predictor variable.</li> </ul> Credit <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> References <ol> <li>Greene, W. H. Econometric Analysis. New Jersey. Prentice Hall; 5th edition. (2002).</li> </ol> See Also <ul> <li><code>statsmodels.stats.diagnostic.acorr_breusch_godfrey</code>: Breusch-Godfrey test for serial correlation.</li> <li><code>ts_stat_tests.algorithms.correlation.lb</code>: Ljung-Box test of autocorrelation in residuals.</li> <li><code>ts_stat_tests.algorithms.correlation.lm</code>: Lagrange Multiplier tests for autocorrelation.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/correlation.py</code> <pre><code>@typechecked\ndef bglm(\n    res: Union[RegressionResults, RegressionResultsWrapper],\n    nlags: Optional[int] = None,\n    *,\n    store: bool = False,\n) -&gt; Union[\n    tuple[float, Union[float, NDArray[np.float64]], float, float],\n    tuple[float, Union[float, NDArray[np.float64]], float, float, ResultsStore],\n]:\n    r\"\"\"\n    !!! note \"Summary\"\n\n        The Breusch-Godfrey Lagrange Multiplier (BGLM) test is a statistical test used in time series forecasting to test for the presence of autocorrelation in the residuals of a model. The test is a generalization of the LM test and can be used to test for autocorrelation up to a specified order.\n\n        This function implements the [`acorr_breusch_godfrey()`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_breusch_godfrey.html) function from the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ???+ abstract \"Details\"\n\n        BG adds lags of residual to exog in the design matrix for the auxiliary regression with residuals as endog. See Greene (2002), section 12.7.1.\n\n        The BGLM test is performed by first fitting a time series model to the data and then obtaining the residuals from the model. The residuals are then used to estimate the autocorrelation function (ACF) up to a specified order.\n\n        Under the null hypothesis that there is no autocorrelation in the residuals of the regression model, the BGLM test statistic follows a chi-squared distribution with degrees of freedom equal to the number of lags included in the model.\n\n        If the test statistic is greater than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is rejected, indicating that there is evidence of autocorrelation in the residuals.\n\n    Params:\n        res (Union[RegressionResults, RegressionResultsWrapper]):\n            Estimation results for which the residuals are tested for serial correlation.\n        nlags (Optional[int], optional):\n            Number of lags to include in the auxiliary regression. (`nlags` is highest lag). Defaults to `None`.\n        store (bool, optional):\n            If `store` is `True`, then an additional class instance that contains intermediate results is returned. Defaults to `False`.\n\n    Returns:\n        (Union[tuple[float, float, float, float], tuple[float, float, float, float, ResultsStore]]):\n            Returns the following values:\n            - `lm` (float): Lagrange multiplier test statistic.\n            - `lmpval` (float): The p-value for the Lagrange multiplier test.\n            - `fval` (float): The value of the f-statistic for the F test.\n            - `fpval` (float): The p-value of the F test.\n            - `res_store` (ResultsStore, optional): Intermediate results (returned if `store` is `True`).\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from statsmodels import api as sm\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import bglm\n        &gt;&gt;&gt; y = sm.datasets.longley.load_pandas().endog\n        &gt;&gt;&gt; X = sm.datasets.longley.load_pandas().exog\n        &gt;&gt;&gt; X = sm.add_constant(X)\n        &gt;&gt;&gt; model = sm.OLS(y, X).fit()\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Breusch-Godfrey test\"}\n        &gt;&gt;&gt; res_lm, res_p, res_f, res_fp = bglm(model)\n        &gt;&gt;&gt; print(f\"LM statistic: {res_lm:.4f}\")\n        LM statistic: 5.1409\n        &gt;&gt;&gt; print(f\"p-value: {res_p:.4f}\")\n        p-value: 0.1618\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Breusch-Godfrey test with intermediate results\"}\n        &gt;&gt;&gt; res_lm, res_p, res_f, res_fp, res_store = bglm(model, store=True)\n        &gt;&gt;&gt; print(f\"LM statistic: {res_lm:.4f}\")\n        LM statistic: 5.1409\n        &gt;&gt;&gt; print(f\"p-value: {res_p:.4f}\")\n        p-value: 0.1618\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 3: Breusch-Godfrey test with specified lags\"}\n        &gt;&gt;&gt; res_lm, res_p, res_f, res_fp = bglm(model, nlags=2)\n        &gt;&gt;&gt; print(f\"LM statistic: {res_lm:.4f}\")\n        LM statistic: 2.8762\n        &gt;&gt;&gt; print(f\"p-value: {res_p:.4f}\")\n        p-value: 0.2374\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The BGLM test statistic is calculated as:\n\n        $$\n        BGLM = n \\times R^2\n        $$\n\n        where:\n\n        - $n$ is the sample size and\n        - $R^2$ is the coefficient of determination from a regression of the residuals on the lagged values of the residuals and the lagged values of the predictor variable.\n\n    ??? success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/) library.\n\n    ??? question \"References\"\n        1. Greene, W. H. Econometric Analysis. New Jersey. Prentice Hall; 5th edition. (2002).\n\n    ??? tip \"See Also\"\n        - [`statsmodels.stats.diagnostic.acorr_breusch_godfrey`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_breusch_godfrey.html): Breusch-Godfrey test for serial correlation.\n        - [`ts_stat_tests.algorithms.correlation.lb`][ts_stat_tests.algorithms.correlation.lb]: Ljung-Box test of autocorrelation in residuals.\n        - [`ts_stat_tests.algorithms.correlation.lm`][ts_stat_tests.algorithms.correlation.lm]: Lagrange Multiplier tests for autocorrelation.\n    \"\"\"\n    return acorr_breusch_godfrey(\n        res=res,\n        nlags=nlags,\n        store=store,\n    )\n</code></pre>"},{"location":"code/data/","title":"Data","text":""},{"location":"code/data/#ts_stat_tests.utils.data","title":"ts_stat_tests.utils.data","text":"<p>Summary</p> <p>This module contains utility functions to load classic time series datasets for testing and demonstration purposes.</p> <p>It provides interfaces for both synthetic data generation (random numbers, sine waves, trends) and external data loading from common benchmarks.</p>"},{"location":"code/data/#ts_stat_tests.utils.data.get_random_generator","title":"get_random_generator  <code>cached</code>","text":"<pre><code>get_random_generator(seed: int) -&gt; RandomGenerator\n</code></pre> <p>Summary</p> <p>Generates a NumPy random number generator with a specified seed for reproducibility.</p> Details <p>This function returns a <code>numpy.random.Generator</code> instance using <code>default_rng</code>. This is the recommended way to generate random numbers in modern NumPy (v1.17+).</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>The seed value for the random number generator.</p> required <p>Returns:</p> Type Description <code>Generator</code> <p>A NumPy random number generator initialized with the given seed.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import get_random_generator\n</code></pre> Example 1: Creating a RandomGenerator<pre><code>&gt;&gt;&gt; rng = get_random_generator(42)\n&gt;&gt;&gt; print(rng is not None)\nTrue\n&gt;&gt;&gt; print(type(rng))\n&lt;class 'numpy.random._generator.Generator'&gt;\n</code></pre> References <ol> <li>NumPy Random Generator</li> </ol> Source code in <code>src/ts_stat_tests/utils/data.py</code> <pre><code>@lru_cache\n@typechecked\ndef get_random_generator(seed: int) -&gt; RandomGenerator:\n    r\"\"\"\n    !!! note \"Summary\"\n        Generates a NumPy random number generator with a specified seed for reproducibility.\n\n    ???+ abstract \"Details\"\n        This function returns a `numpy.random.Generator` instance using `default_rng`. This is the recommended way to generate random numbers in modern NumPy (v1.17+).\n\n    Params:\n        seed (int):\n            The seed value for the random number generator.\n\n    Returns:\n        (RandomGenerator):\n            A NumPy random number generator initialized with the given seed.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import get_random_generator\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Creating a RandomGenerator\"}\n        &gt;&gt;&gt; rng = get_random_generator(42)\n        &gt;&gt;&gt; print(rng is not None)\n        True\n        &gt;&gt;&gt; print(type(rng))\n        &lt;class 'numpy.random._generator.Generator'&gt;\n\n        ```\n\n    ??? question \"References\"\n        1. [NumPy Random Generator](https://numpy.org/doc/stable/reference/random/generator.html)\n\n    \"\"\"\n    return np.random.default_rng(seed)\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.get_random_numbers","title":"get_random_numbers  <code>cached</code>","text":"<pre><code>get_random_numbers(seed: int) -&gt; NDArray[np.float64]\n</code></pre> <p>Summary</p> <p>Generates an array of random numbers with a specified seed for reproducibility.</p> Details <p>Generates a 1D array of 1000 random floating-point numbers distributed uniformly in the half-open interval \\([0.0, 1.0)\\).</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>The seed value for the random number generator.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>An array of random numbers with shape (1000,).</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import get_random_numbers\n&gt;&gt;&gt; data = get_random_numbers(42)\n</code></pre> Example 1: Generating Random Numbers<pre><code>&gt;&gt;&gt; print(data.shape)\n(1000,)\n&gt;&gt;&gt; print(type(data))\n&lt;class 'numpy.ndarray'&gt;\n&gt;&gt;&gt; print(data.dtype)\nfloat64\n&gt;&gt;&gt; print(data[:5])\n[0.77395605 0.43887844 0.85859792 0.69736803 0.09417735]\n</code></pre> References <ol> <li>NumPy Random Generator</li> </ol> Source code in <code>src/ts_stat_tests/utils/data.py</code> <pre><code>@lru_cache\n@typechecked\ndef get_random_numbers(seed: int) -&gt; NDArray[np.float64]:\n    r\"\"\"\n    !!! note \"Summary\"\n        Generates an array of random numbers with a specified seed for reproducibility.\n\n    ???+ abstract \"Details\"\n        Generates a 1D array of 1000 random floating-point numbers distributed uniformly in the half-open interval $[0.0, 1.0)$.\n\n    Params:\n        seed (int):\n            The seed value for the random number generator.\n\n    Returns:\n        (NDArray[np.float64]):\n            An array of random numbers with shape (1000,).\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import get_random_numbers\n        &gt;&gt;&gt; data = get_random_numbers(42)\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Generating Random Numbers\"}\n        &gt;&gt;&gt; print(data.shape)\n        (1000,)\n        &gt;&gt;&gt; print(type(data))\n        &lt;class 'numpy.ndarray'&gt;\n        &gt;&gt;&gt; print(data.dtype)\n        float64\n        &gt;&gt;&gt; print(data[:5])\n        [0.77395605 0.43887844 0.85859792 0.69736803 0.09417735]\n\n        ```\n\n    ??? question \"References\"\n        1. [NumPy Random Generator](https://numpy.org/doc/stable/reference/random/generator.html)\n\n    \"\"\"\n    rng: RandomGenerator = get_random_generator(seed)\n    return rng.random(size=1000)\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.get_random_numbers_2d","title":"get_random_numbers_2d  <code>cached</code>","text":"<pre><code>get_random_numbers_2d(seed: int) -&gt; NDArray[np.float64]\n</code></pre> <p>Summary</p> <p>Generates a 2D array of random numbers with a specified seed for reproducibility.</p> Details <p>Produces a 2D matrix of shape \\((4, 3000)\\) containing uniform random values.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>The seed value for the random number generator.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>A 2D array of random numbers with shape (4, 3000).</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import get_random_numbers_2d\n&gt;&gt;&gt; data = get_random_numbers_2d(42)\n</code></pre> Example 1: Generating 2D Random Numbers<pre><code>&gt;&gt;&gt; print(data.shape)\n(4, 3000)\n&gt;&gt;&gt; print(type(data))\n&lt;class 'numpy.ndarray'&gt;\n&gt;&gt;&gt; print(data.dtype)\nfloat64\n&gt;&gt;&gt; print(data[:, :5])\n[[0.06206311 0.45826204 0.12903006 0.15232671 0.63228281]\n [0.71609997 0.3571156  0.85186786 0.24097716 0.53839349]\n [0.74315144 0.90157433 0.59866347 0.52857443 0.89016256]\n [0.72072839 0.71123776 0.20269503 0.0366554  0.30379952]]\n</code></pre> References <ol> <li>NumPy Random Generator</li> </ol> Source code in <code>src/ts_stat_tests/utils/data.py</code> <pre><code>@lru_cache\n@typechecked\ndef get_random_numbers_2d(seed: int) -&gt; NDArray[np.float64]:\n    r\"\"\"\n    !!! note \"Summary\"\n        Generates a 2D array of random numbers with a specified seed for reproducibility.\n\n    ???+ abstract \"Details\"\n        Produces a 2D matrix of shape $(4, 3000)$ containing uniform random values.\n\n    Params:\n        seed (int):\n            The seed value for the random number generator.\n\n    Returns:\n        (NDArray[np.float64]):\n            A 2D array of random numbers with shape (4, 3000).\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import get_random_numbers_2d\n        &gt;&gt;&gt; data = get_random_numbers_2d(42)\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Generating 2D Random Numbers\"}\n        &gt;&gt;&gt; print(data.shape)\n        (4, 3000)\n        &gt;&gt;&gt; print(type(data))\n        &lt;class 'numpy.ndarray'&gt;\n        &gt;&gt;&gt; print(data.dtype)\n        float64\n        &gt;&gt;&gt; print(data[:, :5])\n        [[0.06206311 0.45826204 0.12903006 0.15232671 0.63228281]\n         [0.71609997 0.3571156  0.85186786 0.24097716 0.53839349]\n         [0.74315144 0.90157433 0.59866347 0.52857443 0.89016256]\n         [0.72072839 0.71123776 0.20269503 0.0366554  0.30379952]]\n\n        ```\n\n    ??? question \"References\"\n        1. [NumPy Random Generator](https://numpy.org/doc/stable/reference/random/generator.html)\n\n    \"\"\"\n    rng: RandomGenerator = get_random_generator(seed)\n    return rng.random(size=(4, 3000))\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.get_sine_wave","title":"get_sine_wave  <code>cached</code>","text":"<pre><code>get_sine_wave() -&gt; NDArray[np.float64]\n</code></pre> <p>Summary</p> <p>Generates a sine wave dataset.</p> Details <p>Produces a 1D array of 1000 samples of a sine wave with amplitude 1.0 and period 1000.</p> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>An array representing a sine wave with shape (3000,).</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import get_sine_wave\n&gt;&gt;&gt; data = get_sine_wave()\n</code></pre> Example 1: Generating a Sine Wave<pre><code>&gt;&gt;&gt; print(data.shape)\n(3000,)\n&gt;&gt;&gt; print(type(data))\n&lt;class 'numpy.ndarray'&gt;\n&gt;&gt;&gt; print(data.dtype)\nfloat64\n&gt;&gt;&gt; print(data[:5])\n[0.         0.06279052 0.12533323 0.18738131 0.24868989]\n</code></pre> References <ol> <li>NumPy Trigonometric Functions</li> </ol> Source code in <code>src/ts_stat_tests/utils/data.py</code> <pre><code>@lru_cache\n@typechecked\ndef get_sine_wave() -&gt; NDArray[np.float64]:\n    r\"\"\"\n    !!! note \"Summary\"\n        Generates a sine wave dataset.\n\n    ???+ abstract \"Details\"\n        Produces a 1D array of 1000 samples of a sine wave with amplitude 1.0 and period 1000.\n\n    Returns:\n        (NDArray[np.float64]):\n            An array representing a sine wave with shape (3000,).\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import get_sine_wave\n        &gt;&gt;&gt; data = get_sine_wave()\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Generating a Sine Wave\"}\n        &gt;&gt;&gt; print(data.shape)\n        (3000,)\n        &gt;&gt;&gt; print(type(data))\n        &lt;class 'numpy.ndarray'&gt;\n        &gt;&gt;&gt; print(data.dtype)\n        float64\n        &gt;&gt;&gt; print(data[:5])\n        [0.         0.06279052 0.12533323 0.18738131 0.24868989]\n\n        ```\n\n    ??? question \"References\"\n        1. [NumPy Trigonometric Functions](https://numpy.org/doc/stable/reference/routines.math.html#trigonometric-functions)\n\n    \"\"\"\n    return np.sin(2 * np.pi * 1 * np.arange(3000) / 100)\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.get_normal_curve","title":"get_normal_curve  <code>cached</code>","text":"<pre><code>get_normal_curve(seed: int) -&gt; NDArray[np.float64]\n</code></pre> <p>Summary</p> <p>Generates a normal distribution curve dataset.</p> Details <p>Draws 1000 samples from a standard Gaussian distribution.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>The seed value for the random number generator.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>An array representing a normal distribution curve with shape (1000,).</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import get_normal_curve\n&gt;&gt;&gt; data = get_normal_curve(42)\n</code></pre> Example 1: Generating Normal Curve Data<pre><code>&gt;&gt;&gt; print(data.shape)\n(1000,)\n&gt;&gt;&gt; print(type(data))\n&lt;class 'numpy.ndarray'&gt;\n&gt;&gt;&gt; print(data.dtype)\nfloat64\n&gt;&gt;&gt; print(data[:5])\n[ 0.12993113 -0.75691222 -0.33007356 -1.88579735 -0.37064992]\n</code></pre> References <ol> <li>NumPy Random Normal</li> </ol> Source code in <code>src/ts_stat_tests/utils/data.py</code> <pre><code>@lru_cache\n@typechecked\ndef get_normal_curve(seed: int) -&gt; NDArray[np.float64]:\n    r\"\"\"\n    !!! note \"Summary\"\n        Generates a normal distribution curve dataset.\n\n    ???+ abstract \"Details\"\n        Draws 1000 samples from a standard Gaussian distribution.\n\n    Params:\n        seed (int):\n            The seed value for the random number generator.\n\n    Returns:\n        (NDArray[np.float64]):\n            An array representing a normal distribution curve with shape (1000,).\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import get_normal_curve\n        &gt;&gt;&gt; data = get_normal_curve(42)\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Generating Normal Curve Data\"}\n        &gt;&gt;&gt; print(data.shape)\n        (1000,)\n        &gt;&gt;&gt; print(type(data))\n        &lt;class 'numpy.ndarray'&gt;\n        &gt;&gt;&gt; print(data.dtype)\n        float64\n        &gt;&gt;&gt; print(data[:5])\n        [ 0.12993113 -0.75691222 -0.33007356 -1.88579735 -0.37064992]\n\n        ```\n\n    ??? question \"References\"\n        1. [NumPy Random Normal](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.normal.html)\n\n    \"\"\"\n    rng: RandomGenerator = get_random_generator(seed)\n    return rng.normal(loc=0.0, scale=1.0, size=1000)\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.get_straight_line","title":"get_straight_line  <code>cached</code>","text":"<pre><code>get_straight_line() -&gt; NDArray[np.float64]\n</code></pre> <p>Summary</p> <p>Generates a straight line dataset.</p> Details <p>Returns a sequence of integers from 0 to 999.</p> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>An array representing a straight line with shape (1000,).</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import get_straight_line\n&gt;&gt;&gt; data = get_straight_line()\n</code></pre> Example 1: Generating Straight Line Data<pre><code>&gt;&gt;&gt; print(data.shape)\n(1000,)\n&gt;&gt;&gt; print(type(data))\n&lt;class 'numpy.ndarray'&gt;\n&gt;&gt;&gt; print(data.dtype)\nfloat64\n&gt;&gt;&gt; print(data[:5])\n[0. 1. 2. 3. 4.]\n</code></pre> References <ol> <li>NumPy Arange</li> </ol> Source code in <code>src/ts_stat_tests/utils/data.py</code> <pre><code>@lru_cache\n@typechecked\ndef get_straight_line() -&gt; NDArray[np.float64]:\n    r\"\"\"\n    !!! note \"Summary\"\n        Generates a straight line dataset.\n\n    ???+ abstract \"Details\"\n        Returns a sequence of integers from 0 to 999.\n\n    Returns:\n        (NDArray[np.float64]):\n            An array representing a straight line with shape (1000,).\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import get_straight_line\n        &gt;&gt;&gt; data = get_straight_line()\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Generating Straight Line Data\"}\n        &gt;&gt;&gt; print(data.shape)\n        (1000,)\n        &gt;&gt;&gt; print(type(data))\n        &lt;class 'numpy.ndarray'&gt;\n        &gt;&gt;&gt; print(data.dtype)\n        float64\n        &gt;&gt;&gt; print(data[:5])\n        [0. 1. 2. 3. 4.]\n\n        ```\n\n    ??? question \"References\"\n        1. [NumPy Arange](https://numpy.org/doc/stable/reference/generated/numpy.arange.html)\n\n    \"\"\"\n    return np.arange(1000).astype(np.float64)\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.get_trend_data","title":"get_trend_data  <code>cached</code>","text":"<pre><code>get_trend_data() -&gt; NDArray[np.float64]\n</code></pre> <p>Summary</p> <p>Generates trend data.</p> Details <p>Generates an array with a linear trend by combining two ramp functions.</p> <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>An array representing trend data with shape (1000,).</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import get_trend_data\n&gt;&gt;&gt; data = get_trend_data()\n</code></pre> Example 1: Generating Trend Data<pre><code>&gt;&gt;&gt; print(data.shape)\n(1000,)\n&gt;&gt;&gt; print(type(data))\n&lt;class 'numpy.ndarray'&gt;\n&gt;&gt;&gt; print(data.dtype)\nfloat64\n&gt;&gt;&gt; print(data[:5])\n[0.  1.5 3.  4.5 6. ]\n</code></pre> References <ol> <li>NumPy Arange</li> </ol> Source code in <code>src/ts_stat_tests/utils/data.py</code> <pre><code>@lru_cache\n@typechecked\ndef get_trend_data() -&gt; NDArray[np.float64]:\n    r\"\"\"\n    !!! note \"Summary\"\n        Generates trend data.\n\n    ???+ abstract \"Details\"\n        Generates an array with a linear trend by combining two ramp functions.\n\n    Returns:\n        (NDArray[np.float64]):\n            An array representing trend data with shape (1000,).\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import get_trend_data\n        &gt;&gt;&gt; data = get_trend_data()\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Generating Trend Data\"}\n        &gt;&gt;&gt; print(data.shape)\n        (1000,)\n        &gt;&gt;&gt; print(type(data))\n        &lt;class 'numpy.ndarray'&gt;\n        &gt;&gt;&gt; print(data.dtype)\n        float64\n        &gt;&gt;&gt; print(data[:5])\n        [0.  1.5 3.  4.5 6. ]\n\n        ```\n\n    ??? question \"References\"\n        1. [NumPy Arange](https://numpy.org/doc/stable/reference/generated/numpy.arange.html)\n\n    \"\"\"\n    return np.arange(1000) + 0.5 * np.arange(1000)\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.get_uniform_data","title":"get_uniform_data  <code>cached</code>","text":"<pre><code>get_uniform_data(seed: int) -&gt; NDArray[np.float64]\n</code></pre> <p>Summary</p> <p>Generates uniform random data with a specified seed for reproducibility.</p> Details <p>Returns a 1D array of 1000 samples from a uniform distribution.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>The seed value for the random number generator.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>An array of uniform random data with shape (1000,).</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import get_uniform_data\n&gt;&gt;&gt; data = get_uniform_data(42)\n</code></pre> Example 1: Generating Uniform Data<pre><code>&gt;&gt;&gt; print(data.shape)\n(1000,)\n&gt;&gt;&gt; print(type(data))\n&lt;class 'numpy.ndarray'&gt;\n&gt;&gt;&gt; print(data.dtype)\nfloat64\n&gt;&gt;&gt; print(data[:5])\n[0.80227457 0.81857128 0.87962986 0.11378193 0.29263938]\n</code></pre> References <ol> <li>NumPy Random Uniform</li> </ol> Source code in <code>src/ts_stat_tests/utils/data.py</code> <pre><code>@lru_cache\n@typechecked\ndef get_uniform_data(seed: int) -&gt; NDArray[np.float64]:\n    r\"\"\"\n    !!! note \"Summary\"\n        Generates uniform random data with a specified seed for reproducibility.\n\n    ???+ abstract \"Details\"\n        Returns a 1D array of 1000 samples from a uniform distribution.\n\n    Params:\n        seed (int):\n            The seed value for the random number generator.\n\n    Returns:\n        (NDArray[np.float64]):\n            An array of uniform random data with shape (1000,).\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import get_uniform_data\n        &gt;&gt;&gt; data = get_uniform_data(42)\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Generating Uniform Data\"}\n        &gt;&gt;&gt; print(data.shape)\n        (1000,)\n        &gt;&gt;&gt; print(type(data))\n        &lt;class 'numpy.ndarray'&gt;\n        &gt;&gt;&gt; print(data.dtype)\n        float64\n        &gt;&gt;&gt; print(data[:5])\n        [0.80227457 0.81857128 0.87962986 0.11378193 0.29263938]\n\n        ```\n\n    ??? question \"References\"\n        1. [NumPy Random Uniform](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.uniform.html)\n\n    \"\"\"\n    rng: RandomGenerator = get_random_generator(seed)\n    return rng.uniform(low=0.0, high=1.0, size=1000)\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.get_noise_data","title":"get_noise_data  <code>cached</code>","text":"<pre><code>get_noise_data(seed: int) -&gt; NDArray[np.float64]\n</code></pre> <p>Summary</p> <p>Generates fractional Gaussian noise data with a specified seed for reproducibility.</p> Details <p>Uses the <code>stochastic</code> library to sample fractional Gaussian noise with a Hurst exponent of 0.5, effectively producing white noise.</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int</code> <p>The seed value for the random number generator.</p> required <p>Returns:</p> Type Description <code>NDArray[float64]</code> <p>An array of fractional Gaussian noise data with shape (1000,).</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import get_noise_data\n&gt;&gt;&gt; data = get_noise_data(42)\n</code></pre> Example 1: Generating Noise Data<pre><code>&gt;&gt;&gt; print(data.shape)\n(1000,)\n&gt;&gt;&gt; print(type(data))\n&lt;class 'numpy.ndarray'&gt;\n&gt;&gt;&gt; print(data.dtype)\nfloat64\n&gt;&gt;&gt; print(data[:5])\n[-0.05413957 -0.0007609  -0.00177524  0.00909899 -0.03044404]\n</code></pre> References <ol> <li>Stochastic Library</li> </ol> Source code in <code>src/ts_stat_tests/utils/data.py</code> <pre><code>@lru_cache\n@typechecked\ndef get_noise_data(seed: int) -&gt; NDArray[np.float64]:\n    r\"\"\"\n    !!! note \"Summary\"\n        Generates fractional Gaussian noise data with a specified seed for reproducibility.\n\n    ???+ abstract \"Details\"\n        Uses the `stochastic` library to sample fractional Gaussian noise with a Hurst exponent of 0.5, effectively producing white noise.\n\n    Params:\n        seed (int):\n            The seed value for the random number generator.\n\n    Returns:\n        (NDArray[np.float64]):\n            An array of fractional Gaussian noise data with shape (1000,).\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import get_noise_data\n        &gt;&gt;&gt; data = get_noise_data(42)\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Generating Noise Data\"}\n        &gt;&gt;&gt; print(data.shape)\n        (1000,)\n        &gt;&gt;&gt; print(type(data))\n        &lt;class 'numpy.ndarray'&gt;\n        &gt;&gt;&gt; print(data.dtype)\n        float64\n        &gt;&gt;&gt; print(data[:5])\n        [-0.05413957 -0.0007609  -0.00177524  0.00909899 -0.03044404]\n\n        ```\n\n    ??? question \"References\"\n        1. [Stochastic Library](https://github.com/crflynn/stochastic)\n\n    \"\"\"\n    rng: RandomGenerator = get_random_generator(seed)\n    return FractionalGaussianNoise(hurst=0.5, rng=rng).sample(1000)\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.load_airline","title":"load_airline  <code>cached</code>","text":"<pre><code>load_airline() -&gt; pd.Series\n</code></pre> <p>Summary</p> <p>Loads the classic Airline Passengers dataset as a pandas Series.</p> Details <p>The Air Passengers dataset provides monthly totals of a US airline's international passengers from 1949 to 1960. It is a classic dataset for time series analysis, exhibiting both trend and seasonality.</p> <p>Returns:</p> Type Description <code>Series</code> <p>The Airline Passengers dataset.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; data = load_airline()\n</code></pre> Example 1: Loading Airline Data<pre><code>&gt;&gt;&gt; print(len(data))\n144\n&gt;&gt;&gt; print(type(data))\n&lt;class 'pandas.core.series.Series'&gt;\n&gt;&gt;&gt; print(data.head())\nPeriod\n1949-01    112.0\n1949-02    118.0\n1949-03    132.0\n1949-04    129.0\n1949-05    121.0\nFreq: M, Name: Number of airline passengers, dtype: float64\n</code></pre> Credit <p>Inspiration from: <code>sktime.datasets.load_airline()</code></p> References <ol> <li>Box, G. E. P., Jenkins, G. M., Reinsel, G. C., &amp; Ljung, G. M. (2015). Time series analysis: forecasting and control. John Wiley &amp; Sons.</li> </ol> Source code in <code>src/ts_stat_tests/utils/data.py</code> <pre><code>@lru_cache\n@typechecked\ndef load_airline() -&gt; pd.Series:\n    r\"\"\"\n    !!! note \"Summary\"\n        Loads the classic Airline Passengers dataset as a pandas Series.\n\n    ???+ abstract \"Details\"\n        The Air Passengers dataset provides monthly totals of a US airline's international passengers from 1949 to 1960. It is a classic dataset for time series analysis, exhibiting both trend and seasonality.\n\n    Returns:\n        (pd.Series):\n            The Airline Passengers dataset.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; data = load_airline()\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Loading Airline Data\"}\n        &gt;&gt;&gt; print(len(data))\n        144\n        &gt;&gt;&gt; print(type(data))\n        &lt;class 'pandas.core.series.Series'&gt;\n        &gt;&gt;&gt; print(data.head())\n        Period\n        1949-01    112.0\n        1949-02    118.0\n        1949-03    132.0\n        1949-04    129.0\n        1949-05    121.0\n        Freq: M, Name: Number of airline passengers, dtype: float64\n\n        ```\n\n    ??? success \"Credit\"\n        Inspiration from: [`sktime.datasets.load_airline()`](https://www.sktime.net/en/stable/api_reference/generated/sktime.datasets.load_airline.html)\n\n    ??? question \"References\"\n        1. Box, G. E. P., Jenkins, G. M., Reinsel, G. C., &amp; Ljung, G. M. (2015). Time series analysis: forecasting and control. John Wiley &amp; Sons.\n\n    \"\"\"\n    data_source = \"https://raw.githubusercontent.com/sktime/sktime/main/sktime/datasets/data/Airline/Airline.csv\"\n    _data = pd.read_csv(data_source, index_col=0, dtype={1: float}).squeeze(\"columns\")\n    if not isinstance(_data, pd.Series):\n        raise TypeError(\"Expected a pandas Series from the data source.\")\n    data: pd.Series = _data\n    data.index = pd.PeriodIndex(data.index, freq=\"M\", name=\"Period\")\n    data.name = \"Number of airline passengers\"\n    return data\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.load_macrodata","title":"load_macrodata  <code>cached</code>","text":"<pre><code>load_macrodata() -&gt; pd.DataFrame\n</code></pre> <p>Summary</p> <p>Loads the classic Macrodata dataset as a pandas DataFrame.</p> Details <p>This dataset contains various US macroeconomic time series from 1959Q1 to 2009Q3. It includes variables such as real GDP, consumption, investment, etc.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The Macrodata dataset.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import load_macrodata\n&gt;&gt;&gt; data = load_macrodata()\n</code></pre> Example 1: Loading Macrodata<pre><code>&gt;&gt;&gt; print(data.shape)\n(203, 14)\n&gt;&gt;&gt; print(type(data))\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n&gt;&gt;&gt; print(data[[\"year\", \"quarter\", \"realgdp\"]].head())\n        year  quarter   realgdp\nPeriod\n1959Q1  1959        1  2710.349\n1959Q2  1959        2  2778.801\n1959Q3  1959        3  2775.488\n1959Q4  1959        4  2785.204\n1960Q1  1960        1  2847.699\n</code></pre> Credit <p>Inspiration from: <code>statsmodels.datasets.macrodata.load_pandas()</code></p> References <ol> <li>R. F. Engle, D. F. Hendry, and J. F. Richard (1983). Exogeneity. Econometrica, 51(2):277\u2013304.</li> </ol> Source code in <code>src/ts_stat_tests/utils/data.py</code> <pre><code>@lru_cache\n@typechecked\ndef load_macrodata() -&gt; pd.DataFrame:\n    r\"\"\"\n    !!! note \"Summary\"\n        Loads the classic Macrodata dataset as a pandas DataFrame.\n\n    ???+ abstract \"Details\"\n        This dataset contains various US macroeconomic time series from 1959Q1 to 2009Q3. It includes variables such as real GDP, consumption, investment, etc.\n\n    Returns:\n        (pd.DataFrame):\n            The Macrodata dataset.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_macrodata\n        &gt;&gt;&gt; data = load_macrodata()\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Loading Macrodata\"}\n        &gt;&gt;&gt; print(data.shape)\n        (203, 14)\n        &gt;&gt;&gt; print(type(data))\n        &lt;class 'pandas.core.frame.DataFrame'&gt;\n        &gt;&gt;&gt; print(data[[\"year\", \"quarter\", \"realgdp\"]].head())\n                year  quarter   realgdp\n        Period\n        1959Q1  1959        1  2710.349\n        1959Q2  1959        2  2778.801\n        1959Q3  1959        3  2775.488\n        1959Q4  1959        4  2785.204\n        1960Q1  1960        1  2847.699\n\n        ```\n\n    ??? success \"Credit\"\n        Inspiration from: [`statsmodels.datasets.macrodata.load_pandas()`](https://www.statsmodels.org/stable/datasets/generated/statsmodels.datasets.macrodata.macrodata.load_pandas.html)\n\n    ??? question \"References\"\n        1. R. F. Engle, D. F. Hendry, and J. F. Richard (1983). Exogeneity. Econometrica, 51(2):277\u2013304.\n\n    \"\"\"\n    data_source = (\n        \"https://raw.githubusercontent.com/statsmodels/statsmodels/main/statsmodels/datasets/macrodata/macrodata.csv\"\n    )\n    data: pd.DataFrame = pd.read_csv(\n        data_source,\n        index_col=None,\n        dtype={\n            \"year\": int,\n            \"quarter\": int,\n        },\n    )\n    data.index = pd.PeriodIndex(\n        data=data.year.astype(str) + \"Q\" + data.quarter.astype(str),\n        freq=\"Q\",\n        name=\"Period\",\n    )\n    return data\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.data_airline","title":"data_airline  <code>module-attribute</code>","text":"<pre><code>data_airline: Series = load_airline()\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.data_macrodata","title":"data_macrodata  <code>module-attribute</code>","text":"<pre><code>data_macrodata: DataFrame = load_macrodata()\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.data_random","title":"data_random  <code>module-attribute</code>","text":"<pre><code>data_random: NDArray[float64] = get_random_numbers(SEED)\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.data_random_2d","title":"data_random_2d  <code>module-attribute</code>","text":"<pre><code>data_random_2d: NDArray[float64] = get_random_numbers_2d(\n    SEED\n)\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.data_sine","title":"data_sine  <code>module-attribute</code>","text":"<pre><code>data_sine: NDArray[float64] = get_sine_wave()\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.data_normal","title":"data_normal  <code>module-attribute</code>","text":"<pre><code>data_normal: NDArray[float64] = get_normal_curve(SEED)\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.data_line","title":"data_line  <code>module-attribute</code>","text":"<pre><code>data_line: NDArray[float64] = get_straight_line()\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.data_trend","title":"data_trend  <code>module-attribute</code>","text":"<pre><code>data_trend: NDArray[float64] = get_trend_data()\n</code></pre>"},{"location":"code/data/#ts_stat_tests.utils.data.data_noise","title":"data_noise  <code>module-attribute</code>","text":"<pre><code>data_noise: NDArray[float64] = get_noise_data(SEED)\n</code></pre>"},{"location":"code/errors/","title":"Errors","text":""},{"location":"code/errors/#ts_stat_tests.utils.errors","title":"ts_stat_tests.utils.errors","text":"<p>Summary</p> <p>Provides utility functions for generating standardized error messages and performing numeric assertions.</p> <p>This module includes functions to format error messages consistently and check if numeric values are within a specified tolerance, which is useful for testing and validation purposes.</p>"},{"location":"code/errors/#ts_stat_tests.utils.errors.generate_error_message","title":"generate_error_message","text":"<pre><code>generate_error_message(\n    parameter_name: str,\n    value_parsed: str,\n    options: Union[\n        Mapping[\n            str, Union[tuple[str, ...], list[str], str]\n        ],\n        tuple[str, ...],\n        list[str],\n    ],\n) -&gt; str\n</code></pre> <p>Summary</p> <p>Generates a formatted error message for mismatched values or invalid options.</p> Details <p>This function constructs a standardized string that describes a mismatch between a provided value and the allowed options for a given parameter. It is primarily used to provide clear, consistent feedback in <code>ValueError</code> exceptions within dispatchers.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_name</code> <code>str</code> <p>The name of the parameter or variable being checked.</p> required <code>value_parsed</code> <code>str</code> <p>The actual value that was received.</p> required <code>options</code> <code>Union[Mapping[str, Union[tuple[str, ...], list[str], str]], tuple[str, ...], list[str]]</code> <p>The set of valid options or a dictionary mapping categories to valid options.</p> required <p>Returns:</p> Type Description <code>str</code> <p>A formatted error message string.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.errors import generate_error_message\n</code></pre> Example 1: Simple Options<pre><code>&gt;&gt;&gt; msg = generate_error_message(\"param\", \"invalid\", [\"opt1\", \"opt2\"])\n&gt;&gt;&gt; print(msg)\nInvalid 'param': invalid. Options: ['opt1', 'opt2']\n</code></pre> References <ol> <li>Python F-Strings documentation</li> </ol> Source code in <code>src/ts_stat_tests/utils/errors.py</code> <pre><code>@typechecked\ndef generate_error_message(\n    parameter_name: str,\n    value_parsed: str,\n    options: Union[Mapping[str, Union[tuple[str, ...], list[str], str]], tuple[str, ...], list[str]],\n) -&gt; str:\n    r\"\"\"\n    !!! note \"Summary\"\n        Generates a formatted error message for mismatched values or invalid options.\n\n    ???+ abstract \"Details\"\n        This function constructs a standardized string that describes a mismatch between a provided value and the allowed options for a given parameter. It is primarily used to provide clear, consistent feedback in `ValueError` exceptions within dispatchers.\n\n    Params:\n        parameter_name (str):\n            The name of the parameter or variable being checked.\n        value_parsed (str):\n            The actual value that was received.\n        options (Union[Mapping[str, Union[tuple[str, ...], list[str], str]], tuple[str, ...], list[str]]):\n            The set of valid options or a dictionary mapping categories to valid options.\n\n    Returns:\n        (str):\n            A formatted error message string.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.errors import generate_error_message\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Simple Options\"}\n        &gt;&gt;&gt; msg = generate_error_message(\"param\", \"invalid\", [\"opt1\", \"opt2\"])\n        &gt;&gt;&gt; print(msg)\n        Invalid 'param': invalid. Options: ['opt1', 'opt2']\n\n        ```\n\n    ??? question \"References\"\n        1. [Python F-Strings documentation](https://docs.python.org/3/reference/lexical_analysis.html#f-strings)\n\n    \"\"\"\n    return f\"Invalid '{parameter_name}': {value_parsed}. Options: {options}\"\n</code></pre>"},{"location":"code/errors/#ts_stat_tests.utils.errors.is_almost_equal","title":"is_almost_equal","text":"<pre><code>is_almost_equal(\n    first: float, second: float, *, places: int = 7\n) -&gt; bool\n</code></pre><pre><code>is_almost_equal(\n    first: float, second: float, *, delta: float\n) -&gt; bool\n</code></pre> <pre><code>is_almost_equal(\n    first: float,\n    second: float,\n    *,\n    places: Optional[int] = None,\n    delta: Optional[float] = None\n) -&gt; bool\n</code></pre> <p>Summary</p> <p>Checks if two float values are almost equal within a specified precision.</p> Details <p>Determines the equality of two floating-point numbers within a tolerance. This is necessary because floating-point arithmetic can introduce small errors that make direct equality comparisons (e.g., <code>a == b</code>) unreliable.</p> <p>The user can specify tolerance either by <code>places</code> (decimal places) or by an absolute <code>delta</code>.</p> <p>Parameters:</p> Name Type Description Default <code>first</code> <code>float</code> <p>The first float value.</p> required <code>second</code> <code>float</code> <p>The second float value.</p> required <code>places</code> <code>Optional[int]</code> <p>The number of decimal places for comparison. Defaults to 7 if not provided.</p> <code>None</code> <code>delta</code> <code>Optional[float]</code> <p>An optional delta value for comparison.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>places</code> and <code>delta</code> are provided.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the values are almost equal, <code>False</code> otherwise.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.errors import is_almost_equal\n</code></pre> Example 1: Using `places`<pre><code>&gt;&gt;&gt; res_places = is_almost_equal(1.0, 1.00000001, places=7)\n&gt;&gt;&gt; print(res_places)\nTrue\n</code></pre> Example 2: Using `delta`<pre><code>&gt;&gt;&gt; res_delta = is_almost_equal(1.0, 1.1, delta=0.2)\n&gt;&gt;&gt; print(res_delta)\nTrue\n</code></pre> Example 3: Not Almost Equal<pre><code>&gt;&gt;&gt; res_not_equal = is_almost_equal(1.0, 1.1, places=3)\n&gt;&gt;&gt; print(res_not_equal)\nFalse\n</code></pre> Calculation <p>The comparison depends on whether <code>delta</code> or <code>places</code> is provided.</p> <p>If <code>delta</code> is specified:</p> \\[ |first - second| \\le \\text{delta} \\] <p>If <code>places</code> is specified (defaults to 7):</p> \\[ \\text{round}(|first - second|, \\text{places}) = 0 \\] Credit <p>Inspiration from Python's UnitTest function <code>assertAlmostEqual</code>.</p> References <ol> <li>Python unittest source code</li> </ol> Source code in <code>src/ts_stat_tests/utils/errors.py</code> <pre><code>@typechecked\ndef is_almost_equal(\n    first: float,\n    second: float,\n    *,\n    places: Optional[int] = None,\n    delta: Optional[float] = None,\n) -&gt; bool:\n    r\"\"\"\n    !!! note \"Summary\"\n        Checks if two float values are almost equal within a specified precision.\n\n    ???+ abstract \"Details\"\n        Determines the equality of two floating-point numbers within a tolerance. This is necessary because floating-point arithmetic can introduce small errors that make direct equality comparisons (e.g., `a == b`) unreliable.\n\n        The user can specify tolerance either by `places` (decimal places) or by an absolute `delta`.\n\n    Params:\n        first (float):\n            The first float value.\n        second (float):\n            The second float value.\n        places (Optional[int]):\n            The number of decimal places for comparison. Defaults to 7 if not provided.\n        delta (Optional[float]):\n            An optional delta value for comparison.\n\n    Raises:\n        (ValueError):\n            If both `places` and `delta` are provided.\n\n    Returns:\n        (bool):\n            `True` if the values are almost equal, `False` otherwise.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.errors import is_almost_equal\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Using `places`\"}\n        &gt;&gt;&gt; res_places = is_almost_equal(1.0, 1.00000001, places=7)\n        &gt;&gt;&gt; print(res_places)\n        True\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Using `delta`\"}\n        &gt;&gt;&gt; res_delta = is_almost_equal(1.0, 1.1, delta=0.2)\n        &gt;&gt;&gt; print(res_delta)\n        True\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 3: Not Almost Equal\"}\n        &gt;&gt;&gt; res_not_equal = is_almost_equal(1.0, 1.1, places=3)\n        &gt;&gt;&gt; print(res_not_equal)\n        False\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The comparison depends on whether `delta` or `places` is provided.\n\n        If `delta` is specified:\n\n        $$\n        |first - second| \\le \\text{delta}\n        $$\n\n        If `places` is specified (defaults to 7):\n\n        $$\n        \\text{round}(|first - second|, \\text{places}) = 0\n        $$\n\n    ??? success \"Credit\"\n        Inspiration from Python's UnitTest function [`assertAlmostEqual`](https://github.com/python/cpython/blob/3.11/Lib/unittest/case.py).\n\n    ??? question \"References\"\n        1. [Python unittest source code](https://github.com/python/cpython/blob/main/Lib/unittest/case.py)\n\n    \"\"\"\n    if places is not None and delta is not None:\n        raise ValueError(\"Specify `delta` or `places`, not both.\")\n    if first == second:\n        return True\n    diff: float = abs(first - second)\n    if delta is not None:\n        if diff &lt;= delta:\n            return True\n    else:\n        places_val: int = places if places is not None else 7\n        if round(diff, places_val) == 0:\n            return True\n    return False\n</code></pre>"},{"location":"code/errors/#ts_stat_tests.utils.errors.assert_almost_equal","title":"assert_almost_equal","text":"<pre><code>assert_almost_equal(\n    first: float,\n    second: float,\n    msg: Optional[str] = None,\n    *,\n    places: int = 7\n) -&gt; None\n</code></pre><pre><code>assert_almost_equal(\n    first: float,\n    second: float,\n    msg: Optional[str] = None,\n    *,\n    delta: float\n) -&gt; None\n</code></pre> <pre><code>assert_almost_equal(\n    first: float,\n    second: float,\n    msg: Optional[str] = None,\n    *,\n    places: Optional[int] = None,\n    delta: Optional[float] = None\n) -&gt; None\n</code></pre> <p>Summary</p> <p>Asserts that two float values are almost equal within a specified precision.</p> Details <p>Performs a floating-point comparison using is_almost_equal. If the comparison fails, an <code>AssertionError</code> is raised with a descriptive message.</p> <p>Parameters:</p> Name Type Description Default <code>first</code> <code>float</code> <p>The first float value.</p> required <code>second</code> <code>float</code> <p>The second float value.</p> required <code>msg</code> <code>Optional[str]</code> <p>An optional message to include in the exception if the values are not almost equal.</p> <code>None</code> <code>places</code> <code>Optional[int]</code> <p>The number of decimal places for comparison. Defaults to 7 if not provided.</p> <code>None</code> <code>delta</code> <code>Optional[float]</code> <p>An optional delta value for comparison.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If both <code>places</code> and <code>delta</code> are provided.</p> <code>AssertionError</code> <p>If the two float values are not almost equal.</p> <p>Returns:</p> Type Description <code>None</code> <p>None. Raises an <code>AssertionError</code> if the values are not almost equal to within the tolerances specified.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.errors import assert_almost_equal\n</code></pre> Example 1: Using `places`<pre><code>&gt;&gt;&gt; res_places = assert_almost_equal(1.0, 1.0, places=7)\n&gt;&gt;&gt; print(res_places is None)\nTrue\n</code></pre> Example 2: Using `delta`<pre><code>&gt;&gt;&gt; res_delta = assert_almost_equal(1.0, 1.1, delta=0.2)\n&gt;&gt;&gt; print(res_delta is None)\nTrue\n</code></pre> Example 3: AssertionError Raised<pre><code>&gt;&gt;&gt; assert_almost_equal(1.0, 1.1, places=3)\nTraceback (most recent call last):\n    ...\nAssertionError: Assertion failed: 1.0 != 1.1 (places=3, delta=None)\n</code></pre> Calculation <p>Refer to is_almost_equal for the underlying logic.</p> Credit <p>Inspiration from Python's UnitTest function <code>assertAlmostEqual</code>.</p> References <ol> <li>Python unittest source code</li> </ol> Source code in <code>src/ts_stat_tests/utils/errors.py</code> <pre><code>@typechecked\ndef assert_almost_equal(\n    first: float,\n    second: float,\n    msg: Optional[str] = None,\n    *,\n    places: Optional[int] = None,\n    delta: Optional[float] = None,\n) -&gt; None:\n    r\"\"\"\n    !!! note \"Summary\"\n        Asserts that two float values are almost equal within a specified precision.\n\n    ???+ abstract \"Details\"\n        Performs a floating-point comparison using [is_almost_equal][ts_stat_tests.utils.errors.is_almost_equal]. If the comparison fails, an `AssertionError` is raised with a descriptive message.\n\n    Params:\n        first (float):\n            The first float value.\n        second (float):\n            The second float value.\n        msg (Optional[str]):\n            An optional message to include in the exception if the values are not almost equal.\n        places (Optional[int]):\n            The number of decimal places for comparison. Defaults to 7 if not provided.\n        delta (Optional[float]):\n            An optional delta value for comparison.\n\n    Raises:\n        (ValueError):\n            If both `places` and `delta` are provided.\n        (AssertionError):\n            If the two float values are not almost equal.\n\n    Returns:\n        (None):\n            None. Raises an `AssertionError` if the values are not almost equal to within the tolerances specified.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.errors import assert_almost_equal\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Using `places`\"}\n        &gt;&gt;&gt; res_places = assert_almost_equal(1.0, 1.0, places=7)\n        &gt;&gt;&gt; print(res_places is None)\n        True\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Using `delta`\"}\n        &gt;&gt;&gt; res_delta = assert_almost_equal(1.0, 1.1, delta=0.2)\n        &gt;&gt;&gt; print(res_delta is None)\n        True\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 3: AssertionError Raised\"}\n        &gt;&gt;&gt; assert_almost_equal(1.0, 1.1, places=3)\n        Traceback (most recent call last):\n            ...\n        AssertionError: Assertion failed: 1.0 != 1.1 (places=3, delta=None)\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        Refer to [is_almost_equal][ts_stat_tests.utils.errors.is_almost_equal] for the underlying logic.\n\n    ??? success \"Credit\"\n        Inspiration from Python's UnitTest function [`assertAlmostEqual`](https://github.com/python/cpython/blob/3.11/Lib/unittest/case.py).\n\n    ??? question \"References\"\n        1. [Python unittest source code](https://github.com/python/cpython/blob/main/Lib/unittest/case.py)\n\n    \"\"\"\n    is_equal: bool = False\n    if delta is not None:\n        is_equal = is_almost_equal(first, second, delta=delta)\n    else:\n        places_val: int = places if places is not None else 7\n        is_equal = is_almost_equal(first, second, places=places_val)\n\n    if not is_equal:\n        error_msg: str = msg if msg is not None else f\"Assertion failed: {first} != {second} ({places=}, {delta=})\"\n        raise AssertionError(error_msg)\n</code></pre>"},{"location":"code/normality/","title":"Test the <code>normality</code> of a given Time-Series Dataset","text":""},{"location":"code/normality/#introduction","title":"Introduction","text":"<p>Summary</p> <p>As stated by the NIST/SEMATECH e-Handbook of Statistical Methods:</p> <p>Normal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known. Their importance is partly due to the central limit theorem.</p> <p> For more info, see: Engineering Statistics Handbook: Measures of Skewness and Kurtosis.</p> <p>Info</p> <p>The normality test is used to determine whether a data set is well-modeled by a normal distribution. In time series forecasting, we primarily test the residuals (errors) of a model for normality. If the residuals follow a normal distribution, it suggests that the model has successfully captured the systematic patterns in the data, and the remaining errors are random white noise.</p> <p>If the residuals are not normally distributed, it may indicate that the model is missing important features, such as seasonal patterns or long-term trends, or that a transformation of the data (e.g., Log or Box-Cox) is required before modeling.</p> library category algorithm short import script url scipy Normality Shapiro-Wilk Test SW <code>from scipy.stats import shapiro</code> https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html scipy Normality D'Agostino &amp; Pearson's Test DP <code>from scipy.stats import normaltest</code> https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.normaltest.html scipy Normality Anderson-Darling Test AD <code>from scipy.stats import anderson</code> https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.anderson.html statsmodels Normality Jarque-Bera Test JB <code>from statsmodels.stats.stattools import jarque_bera</code> https://www.statsmodels.org/stable/generated/statsmodels.stats.stattools.jarque_bera.html statsmodels Normality Omnibus Test OB <code>from statsmodels.stats.diagnostic import omni_normtest</code> https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.omni_normtest.html <p> For more info, see: Hyndman &amp; Athanasopoulos: Forecasting: Principles and Practice.</p> <p>Source Library</p> <p>The <code>scipy</code> and <code>statsmodels</code> packages were chosen because they provide standard, reliable implementations of classical statistical tests. <code>scipy.stats</code> provides implementations for Shapiro-Wilk, D'Agostino-Pearson, and Anderson-Darling tests, while <code>statsmodels</code> provides the Jarque-Bera and Omnibus tests.</p> <p>Source Module</p> <p>All of the source code can be found within these modules:</p> <ul> <li><code>ts_stat_tests.algorithms.normality</code>.</li> <li><code>ts_stat_tests.tests.normality</code>.</li> </ul>"},{"location":"code/normality/#normality-tests","title":"Normality Tests","text":""},{"location":"code/normality/#ts_stat_tests.tests.normality","title":"ts_stat_tests.tests.normality","text":"<p>Summary</p> <p>This module contains convenience functions and tests for normality measures, allowing for easy access to different normality algorithms.</p>"},{"location":"code/normality/#ts_stat_tests.tests.normality.normality","title":"normality","text":"<pre><code>normality(\n    x: ArrayLike,\n    algorithm: str = \"dp\",\n    axis: int = 0,\n    nan_policy: VALID_DP_NAN_POLICY_OPTIONS = \"propagate\",\n    dist: VALID_AD_DIST_OPTIONS = \"norm\",\n) -&gt; Union[\n    tuple[float, ...],\n    NormaltestResult,\n    ShapiroResult,\n    AndersonResult,\n]\n</code></pre> <p>Summary</p> <p>Perform a normality test on the given data.</p> Details <p>This function is a convenience wrapper around the five underlying algorithms: - <code>jb()</code> - <code>ob()</code> - <code>sw()</code> - <code>dp()</code> - <code>ad()</code></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked. Should be a <code>1-D</code> or <code>N-D</code> data array.</p> required <code>algorithm</code> <code>str</code> <p>Which normality algorithm to use. - <code>jb()</code>: <code>[\"jb\", \"jarque\", \"jarque-bera\"]</code> - <code>ob()</code>: <code>[\"ob\", \"omni\", \"omnibus\"]</code> - <code>sw()</code>: <code>[\"sw\", \"shapiro\", \"shapiro-wilk\"]</code> - <code>dp()</code>: <code>[\"dp\", \"dagostino\", \"dagostino-pearson\"]</code> - <code>ad()</code>: <code>[\"ad\", \"anderson\", \"anderson-darling\"]</code> Default: <code>\"dp\"</code></p> <code>'dp'</code> <code>axis</code> <code>int</code> <p>Axis along which to compute the test. Default: <code>0</code></p> <code>0</code> <code>nan_policy</code> <code>VALID_DP_NAN_POLICY_OPTIONS</code> <p>Defines how to handle when input contains <code>NaN</code>. - <code>propagate</code>: returns <code>NaN</code> - <code>raise</code>: throws an error - <code>omit</code>: performs the calculations ignoring <code>NaN</code> values Default: <code>\"propagate\"</code></p> <code>'propagate'</code> <code>dist</code> <code>VALID_AD_DIST_OPTIONS</code> <p>The type of distribution to test against. Only relevant when <code>algorithm=anderson</code>. Default: <code>\"norm\"</code></p> <code>'norm'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the given value for <code>algorithm</code> is not valid.</p> <p>Returns:</p> Type Description <code>Union[tuple[float, float], tuple[float, list[float], list[float]]]</code> <p>If not <code>\"ad\"</code>, returns a <code>tuple</code> of <code>(stat, pvalue)</code>. If <code>\"ad\"</code>, returns a <code>tuple</code> of <code>(stat, critical_values, significance_level)</code>.</p> <p>Credit</p> <p>Calculations are performed by <code>scipy.stats</code> and <code>statsmodels.stats</code>.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.tests.normality import normality\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n&gt;&gt;&gt; normal = data_normal\n</code></pre> Example 1: D'Agostino-Pearson test<pre><code>&gt;&gt;&gt; stat, pvalue = normality(normal, algorithm=\"dp\")\n&gt;&gt;&gt; print(f\"DP statistic: {stat:.4f}\")\nDP statistic: 1.3537\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.5082\n</code></pre> Example 2: Jarque-Bera test<pre><code>&gt;&gt;&gt; stat, pvalue = normality(normal, algorithm=\"jb\")\n&gt;&gt;&gt; print(f\"JB statistic: {stat:.4f}\")\nJB statistic: 1.4168\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.4924\n</code></pre> Source code in <code>src/ts_stat_tests/tests/normality.py</code> <pre><code>@typechecked\ndef normality(\n    x: ArrayLike,\n    algorithm: str = \"dp\",\n    axis: int = 0,\n    nan_policy: VALID_DP_NAN_POLICY_OPTIONS = \"propagate\",\n    dist: VALID_AD_DIST_OPTIONS = \"norm\",\n) -&gt; Union[tuple[float, ...], NormaltestResult, ShapiroResult, AndersonResult]:\n    \"\"\"\n    !!! note \"Summary\"\n        Perform a normality test on the given data.\n\n    ???+ abstract \"Details\"\n        This function is a convenience wrapper around the five underlying algorithms:&lt;br&gt;\n        - [`jb()`][ts_stat_tests.algorithms.normality.jb]&lt;br&gt;\n        - [`ob()`][ts_stat_tests.algorithms.normality.ob]&lt;br&gt;\n        - [`sw()`][ts_stat_tests.algorithms.normality.sw]&lt;br&gt;\n        - [`dp()`][ts_stat_tests.algorithms.normality.dp]&lt;br&gt;\n        - [`ad()`][ts_stat_tests.algorithms.normality.ad]\n\n    Params:\n        x (ArrayLike):\n            The data to be checked. Should be a `1-D` or `N-D` data array.\n        algorithm (str):\n            Which normality algorithm to use.&lt;br&gt;\n            - `jb()`: `[\"jb\", \"jarque\", \"jarque-bera\"]`&lt;br&gt;\n            - `ob()`: `[\"ob\", \"omni\", \"omnibus\"]`&lt;br&gt;\n            - `sw()`: `[\"sw\", \"shapiro\", \"shapiro-wilk\"]`&lt;br&gt;\n            - `dp()`: `[\"dp\", \"dagostino\", \"dagostino-pearson\"]`&lt;br&gt;\n            - `ad()`: `[\"ad\", \"anderson\", \"anderson-darling\"]`&lt;br&gt;\n            Default: `\"dp\"`\n        axis (int):\n            Axis along which to compute the test.\n            Default: `0`\n        nan_policy (VALID_DP_NAN_POLICY_OPTIONS):\n            Defines how to handle when input contains `NaN`.&lt;br&gt;\n            - `propagate`: returns `NaN`&lt;br&gt;\n            - `raise`: throws an error&lt;br&gt;\n            - `omit`: performs the calculations ignoring `NaN` values&lt;br&gt;\n            Default: `\"propagate\"`\n        dist (VALID_AD_DIST_OPTIONS):\n            The type of distribution to test against.&lt;br&gt;\n            Only relevant when `algorithm=anderson`.&lt;br&gt;\n            Default: `\"norm\"`\n\n    Raises:\n        (ValueError):\n            When the given value for `algorithm` is not valid.\n\n    Returns:\n        (Union[tuple[float, float], tuple[float, list[float], list[float]]]):\n            If not `\"ad\"`, returns a `tuple` of `(stat, pvalue)`.\n            If `\"ad\"`, returns a `tuple` of `(stat, critical_values, significance_level)`.\n\n    !!! success \"Credit\"\n        Calculations are performed by `scipy.stats` and `statsmodels.stats`.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.tests.normality import normality\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n        &gt;&gt;&gt; normal = data_normal\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: D'Agostino-Pearson test\"}\n        &gt;&gt;&gt; stat, pvalue = normality(normal, algorithm=\"dp\")\n        &gt;&gt;&gt; print(f\"DP statistic: {stat:.4f}\")\n        DP statistic: 1.3537\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.5082\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Jarque-Bera test\"}\n        &gt;&gt;&gt; stat, pvalue = normality(normal, algorithm=\"jb\")\n        &gt;&gt;&gt; print(f\"JB statistic: {stat:.4f}\")\n        JB statistic: 1.4168\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.4924\n\n        ```\n    \"\"\"\n    options: dict[str, tuple[str, ...]] = {\n        \"jb\": (\"jb\", \"jarque\", \"jarque-bera\"),\n        \"ob\": (\"ob\", \"omni\", \"omnibus\"),\n        \"sw\": (\"sw\", \"shapiro\", \"shapiro-wilk\"),\n        \"dp\": (\"dp\", \"dagostino\", \"dagostino-pearson\"),\n        \"ad\": (\"ad\", \"anderson\", \"anderson-darling\"),\n    }\n    if algorithm in options[\"jb\"]:\n        res_jb = _jb(x=x, axis=axis)\n        return (res_jb[0], res_jb[1])\n    if algorithm in options[\"ob\"]:\n        return _ob(x=x, axis=axis)\n    if algorithm in options[\"sw\"]:\n        return _sw(x=x)\n    if algorithm in options[\"dp\"]:\n        return _dp(x=x, axis=axis, nan_policy=nan_policy)\n    if algorithm in options[\"ad\"]:\n        return _ad(x=x, dist=dist)\n\n    raise ValueError(\n        generate_error_message(\n            parameter_name=\"algorithm\",\n            value_parsed=algorithm,\n            options=options,\n        )\n    )\n</code></pre>"},{"location":"code/normality/#ts_stat_tests.tests.normality.is_normal","title":"is_normal","text":"<pre><code>is_normal(\n    x: ArrayLike,\n    algorithm: str = \"dp\",\n    alpha: float = 0.05,\n    axis: int = 0,\n    nan_policy: VALID_DP_NAN_POLICY_OPTIONS = \"propagate\",\n    dist: VALID_AD_DIST_OPTIONS = \"norm\",\n) -&gt; dict[str, Union[str, float, bool, None]]\n</code></pre> <p>Summary</p> <p>Test whether a given data set is <code>normal</code> or not.</p> Details <p>This function implements the given algorithm (defined in the parameter <code>algorithm</code>), and returns a dictionary containing the relevant data: <pre><code>{\n    \"result\": ...,  # The result of the test. Will be `True` if `p-value &gt;= alpha`, and `False` otherwise\n    \"statistic\": ...,  # The test statistic\n    \"p_value\": ...,  # The p-value of the test (if applicable)\n    \"alpha\": ...,  # The significance level used\n}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked. Should be a <code>1-D</code> or <code>N-D</code> data array.</p> required <code>algorithm</code> <code>str</code> <p>Which normality algorithm to use. - <code>jb()</code>: <code>[\"jb\", \"jarque\", \"jarque-bera\"]</code> - <code>ob()</code>: <code>[\"ob\", \"omni\", \"omnibus\"]</code> - <code>sw()</code>: <code>[\"sw\", \"shapiro\", \"shapiro-wilk\"]</code> - <code>dp()</code>: <code>[\"dp\", \"dagostino\", \"dagostino-pearson\"]</code> - <code>ad()</code>: <code>[\"ad\", \"anderson\", \"anderson-darling\"]</code> Default: <code>\"dp\"</code></p> <code>'dp'</code> <code>alpha</code> <code>float</code> <p>Significance level. Default: <code>0.05</code></p> <code>0.05</code> <code>axis</code> <code>int</code> <p>Axis along which to compute the test. Default: <code>0</code></p> <code>0</code> <code>nan_policy</code> <code>VALID_DP_NAN_POLICY_OPTIONS</code> <p>Defines how to handle when input contains <code>NaN</code>. - <code>propagate</code>: returns <code>NaN</code> - <code>raise</code>: throws an error - <code>omit</code>: performs the calculations ignoring <code>NaN</code> values Default: <code>\"propagate\"</code></p> <code>'propagate'</code> <code>dist</code> <code>VALID_AD_DIST_OPTIONS</code> <p>The type of distribution to test against. Only relevant when <code>algorithm=anderson</code>. Default: <code>\"norm\"</code></p> <code>'norm'</code> <p>Returns:</p> Type Description <code>dict[str, Union[str, float, bool, None]]</code> <p>A dictionary containing: - <code>\"result\"</code> (bool): Indicator if the series is normal. - <code>\"statistic\"</code> (float): The test statistic. - <code>\"p_value\"</code> (float): The p-value of the test (if applicable). - <code>\"alpha\"</code> (float): The significance level used.</p> <p>Credit</p> <p>Calculations are performed by <code>scipy.stats</code> and <code>statsmodels.stats</code>.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.tests.normality import is_normal\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal, data_random\n&gt;&gt;&gt; normal = data_normal\n&gt;&gt;&gt; random = data_random\n</code></pre> Example 1: Test normal data<pre><code>&gt;&gt;&gt; res = is_normal(normal, algorithm=\"dp\")\n&gt;&gt;&gt; res[\"result\"]\nTrue\n&gt;&gt;&gt; print(f\"p-value: {res['p_value']:.4f}\")\np-value: 0.5082\n</code></pre> Example 2: Test non-normal (random) data<pre><code>&gt;&gt;&gt; res = is_normal(random, algorithm=\"sw\")\n&gt;&gt;&gt; res[\"result\"]\nFalse\n</code></pre> Source code in <code>src/ts_stat_tests/tests/normality.py</code> <pre><code>@typechecked\ndef is_normal(\n    x: ArrayLike,\n    algorithm: str = \"dp\",\n    alpha: float = 0.05,\n    axis: int = 0,\n    nan_policy: VALID_DP_NAN_POLICY_OPTIONS = \"propagate\",\n    dist: VALID_AD_DIST_OPTIONS = \"norm\",\n) -&gt; dict[str, Union[str, float, bool, None]]:\n    \"\"\"\n    !!! note \"Summary\"\n        Test whether a given data set is `normal` or not.\n\n    ???+ abstract \"Details\"\n        This function implements the given algorithm (defined in the parameter `algorithm`), and returns a dictionary containing the relevant data:\n        ```python\n        {\n            \"result\": ...,  # The result of the test. Will be `True` if `p-value &gt;= alpha`, and `False` otherwise\n            \"statistic\": ...,  # The test statistic\n            \"p_value\": ...,  # The p-value of the test (if applicable)\n            \"alpha\": ...,  # The significance level used\n        }\n        ```\n\n    Params:\n        x (ArrayLike):\n            The data to be checked. Should be a `1-D` or `N-D` data array.\n        algorithm (str):\n            Which normality algorithm to use.&lt;br&gt;\n            - `jb()`: `[\"jb\", \"jarque\", \"jarque-bera\"]`&lt;br&gt;\n            - `ob()`: `[\"ob\", \"omni\", \"omnibus\"]`&lt;br&gt;\n            - `sw()`: `[\"sw\", \"shapiro\", \"shapiro-wilk\"]`&lt;br&gt;\n            - `dp()`: `[\"dp\", \"dagostino\", \"dagostino-pearson\"]`&lt;br&gt;\n            - `ad()`: `[\"ad\", \"anderson\", \"anderson-darling\"]`&lt;br&gt;\n            Default: `\"dp\"`\n        alpha (float):\n            Significance level.\n            Default: `0.05`\n        axis (int):\n            Axis along which to compute the test.\n            Default: `0`\n        nan_policy (VALID_DP_NAN_POLICY_OPTIONS):\n            Defines how to handle when input contains `NaN`.&lt;br&gt;\n            - `propagate`: returns `NaN`&lt;br&gt;\n            - `raise`: throws an error&lt;br&gt;\n            - `omit`: performs the calculations ignoring `NaN` values&lt;br&gt;\n            Default: `\"propagate\"`\n        dist (VALID_AD_DIST_OPTIONS):\n            The type of distribution to test against.&lt;br&gt;\n            Only relevant when `algorithm=anderson`.&lt;br&gt;\n            Default: `\"norm\"`\n\n    Returns:\n        (dict[str, Union[str, float, bool, None]]):\n            A dictionary containing:\n            - `\"result\"` (bool): Indicator if the series is normal.\n            - `\"statistic\"` (float): The test statistic.\n            - `\"p_value\"` (float): The p-value of the test (if applicable).\n            - `\"alpha\"` (float): The significance level used.\n\n    !!! success \"Credit\"\n        Calculations are performed by `scipy.stats` and `statsmodels.stats`.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.tests.normality import is_normal\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal, data_random\n        &gt;&gt;&gt; normal = data_normal\n        &gt;&gt;&gt; random = data_random\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Test normal data\"}\n        &gt;&gt;&gt; res = is_normal(normal, algorithm=\"dp\")\n        &gt;&gt;&gt; res[\"result\"]\n        True\n        &gt;&gt;&gt; print(f\"p-value: {res['p_value']:.4f}\")\n        p-value: 0.5082\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Test non-normal (random) data\"}\n        &gt;&gt;&gt; res = is_normal(random, algorithm=\"sw\")\n        &gt;&gt;&gt; res[\"result\"]\n        False\n\n        ```\n    \"\"\"\n    res: Any = normality(x=x, algorithm=algorithm, axis=axis, nan_policy=nan_policy, dist=dist)\n\n    if algorithm in (\"ad\", \"anderson\", \"anderson-darling\"):\n        # res is AndersonResult(statistic, critical_values, significance_level, fit_result)\n        # indexing only gives the first 3 elements\n        res_list: list[Any] = list(res) if isinstance(res, (tuple, list)) else []\n        if len(res_list) &gt;= 3:\n            v0: Any = res_list[0]\n            v1: Any = res_list[1]\n            v2: Any = res_list[2]\n            stat = v0\n            crit = v1\n            sig = v2\n\n            # sig is something like [15. , 10. ,  5. ,  2.5,  1. ]\n            # alpha is something like 0.05 (which is 5%)\n            sig_arr = np.asarray(sig)\n            crit_arr = np.asarray(crit)\n            idx = np.argmin(np.abs(sig_arr - (alpha * 100)))\n            critical_value = crit_arr[idx]\n            is_norm = stat &lt; critical_value\n            return {\n                \"result\": bool(is_norm),\n                \"statistic\": float(stat),\n                \"critical_value\": float(critical_value),\n                \"significance_level\": float(sig_arr[idx]),\n                \"alpha\": float(alpha),\n            }\n        # Fallback for unexpected return format\n        return {\n            \"result\": False,\n            \"statistic\": 0.0,\n            \"alpha\": float(alpha),\n        }\n\n    # For others, they return (statistic, pvalue) or similar\n    p_val: Union[float, None] = None\n    stat_val: Union[float, None] = None\n\n    # Use getattr to avoid type checker attribute issues\n    p_val_attr = getattr(res, \"pvalue\", None)\n    stat_val_attr = getattr(res, \"statistic\", None)\n\n    if p_val_attr is not None and stat_val_attr is not None:\n        p_val = float(p_val_attr)\n        stat_val = float(stat_val_attr)\n    elif isinstance(res, (tuple, list)) and len(res) &gt;= 2:\n        res_tuple: Any = res\n        stat_val = float(res_tuple[0])\n        p_val = float(res_tuple[1])\n    else:\n        # Fallback\n        if isinstance(res, (float, int)):\n            stat_val = float(res)\n        p_val = None\n\n    is_norm_val = p_val &gt;= alpha if p_val is not None else False\n\n    return {\n        \"result\": bool(is_norm_val),\n        \"statistic\": stat_val,\n        \"p_value\": p_val,\n        \"alpha\": float(alpha),\n    }\n</code></pre>"},{"location":"code/normality/#normality-algorithms","title":"Normality Algorithms","text":""},{"location":"code/normality/#ts_stat_tests.algorithms.normality","title":"ts_stat_tests.algorithms.normality","text":"<p>Summary</p> <p>This module provides implementations of various statistical tests to assess the normality of data distributions. These tests are essential in statistical analysis and time series forecasting, as many models assume that the underlying data follows a normal distribution.</p>"},{"location":"code/normality/#ts_stat_tests.algorithms.normality.jb","title":"jb","text":"<pre><code>jb(\n    x: ArrayLike, axis: int = 0\n) -&gt; tuple[np.float64, np.float64, np.float64, np.float64]\n</code></pre> <p>Summary</p> <p>The Jarque-Bera test is a statistical test used to determine whether a dataset follows a normal distribution. In time series forecasting, the test can be used to evaluate whether the residuals of a model follow a normal distribution.</p> Details <p>To apply the Jarque-Bera test to time series data, we first need to estimate the residuals of the forecasting model. The residuals represent the difference between the actual values of the time series and the values predicted by the model. We can then use the Jarque-Bera test to evaluate whether the residuals follow a normal distribution.</p> <p>The Jarque-Bera test is based on two statistics, skewness and kurtosis, which measure the degree of asymmetry and peakedness in the distribution of the residuals. The test compares the observed skewness and kurtosis of the residuals to the expected values for a normal distribution. If the observed values are significantly different from the expected values, the test rejects the null hypothesis that the residuals follow a normal distribution.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>Data to test for normality. Usually regression model residuals that are mean 0.</p> required <code>axis</code> <code>int</code> <p>Axis to use if data has more than 1 dimension. Default: <code>0</code></p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input data <code>x</code> is invalid.</p> <p>Returns:</p> Name Type Description <code>JB</code> <code>float</code> <p>The Jarque-Bera test statistic.</p> <code>JBpv</code> <code>float</code> <p>The pvalue of the test statistic.</p> <code>skew</code> <code>float</code> <p>Estimated skewness of the data.</p> <code>kurtosis</code> <code>float</code> <p>Estimated kurtosis of the data.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.normality import jb\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_noise\n&gt;&gt;&gt; airline = data_airline.values\n&gt;&gt;&gt; noise = data_noise\n</code></pre> Example 1: Using the airline dataset<pre><code>&gt;&gt;&gt; jb_value, p_value, skew, kurt = jb(airline)\n&gt;&gt;&gt; print(f\"{jb_value:.4f}\")\n8.9225\n</code></pre> Example 2: Using random noise<pre><code>&gt;&gt;&gt; jb_value, p_value, skew, kurt = jb(noise)\n&gt;&gt;&gt; print(f\"{jb_value:.4f}\")\n0.7478\n&gt;&gt;&gt; print(f\"{p_value:.4f}\")\n0.6881\n&gt;&gt;&gt; print(f\"{skew:.4f}\")\n-0.0554\n&gt;&gt;&gt; print(f\"{kurt:.4f}\")\n3.0753\n</code></pre> Calculation <p>The Jarque-Bera test statistic is defined as:</p> \\[ JB = \\frac{n}{6} \\left( S^2 + \\frac{(K-3)^2}{4} \\right) \\] <p>where:</p> <ul> <li>\\(n\\) is the sample size,</li> <li>\\(S\\) is the sample skewness, and</li> <li>\\(K\\) is the sample kurtosis.</li> </ul> Notes <p>Each output returned has 1 dimension fewer than data. The Jarque-Bera test statistic tests the null that the data is normally distributed against an alternative that the data follow some other distribution. It has an asymptotic \\(\\chi_2^2\\) distribution.</p> Credit <p>All credit goes to the <code>statsmodels</code> library.</p> References <ul> <li>Jarque, C. and Bera, A. (1980) \"Efficient tests for normality, homoscedasticity and serial independence of regression residuals\", 6 Econometric Letters 255-259.</li> </ul> See Also <ul> <li><code>ob()</code></li> <li><code>sw()</code></li> <li><code>dp()</code></li> <li><code>ad()</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/normality.py</code> <pre><code>@typechecked\ndef jb(x: ArrayLike, axis: int = 0) -&gt; tuple[np.float64, np.float64, np.float64, np.float64]:\n    r\"\"\"\n    !!! note \"Summary\"\n        The Jarque-Bera test is a statistical test used to determine whether a dataset follows a normal distribution. In time series forecasting, the test can be used to evaluate whether the residuals of a model follow a normal distribution.\n\n    ???+ abstract \"Details\"\n        To apply the Jarque-Bera test to time series data, we first need to estimate the residuals of the forecasting model. The residuals represent the difference between the actual values of the time series and the values predicted by the model. We can then use the Jarque-Bera test to evaluate whether the residuals follow a normal distribution.\n\n        The Jarque-Bera test is based on two statistics, skewness and kurtosis, which measure the degree of asymmetry and peakedness in the distribution of the residuals. The test compares the observed skewness and kurtosis of the residuals to the expected values for a normal distribution. If the observed values are significantly different from the expected values, the test rejects the null hypothesis that the residuals follow a normal distribution.\n\n    Params:\n        x (ArrayLike):\n            Data to test for normality. Usually regression model residuals that are mean 0.\n        axis (int):\n            Axis to use if data has more than 1 dimension.\n            Default: `0`\n\n    Raises:\n        (ValueError):\n            If the input data `x` is invalid.\n\n    Returns:\n        JB (float):\n            The Jarque-Bera test statistic.\n        JBpv (float):\n            The pvalue of the test statistic.\n        skew (float):\n            Estimated skewness of the data.\n        kurtosis (float):\n            Estimated kurtosis of the data.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.normality import jb\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_noise\n        &gt;&gt;&gt; airline = data_airline.values\n        &gt;&gt;&gt; noise = data_noise\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Using the airline dataset\"}\n        &gt;&gt;&gt; jb_value, p_value, skew, kurt = jb(airline)\n        &gt;&gt;&gt; print(f\"{jb_value:.4f}\")\n        8.9225\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Using random noise\"}\n        &gt;&gt;&gt; jb_value, p_value, skew, kurt = jb(noise)\n        &gt;&gt;&gt; print(f\"{jb_value:.4f}\")\n        0.7478\n        &gt;&gt;&gt; print(f\"{p_value:.4f}\")\n        0.6881\n        &gt;&gt;&gt; print(f\"{skew:.4f}\")\n        -0.0554\n        &gt;&gt;&gt; print(f\"{kurt:.4f}\")\n        3.0753\n\n        ```\n\n    ??? equation \"Calculation\"\n        The Jarque-Bera test statistic is defined as:\n\n        $$\n        JB = \\frac{n}{6} \\left( S^2 + \\frac{(K-3)^2}{4} \\right)\n        $$\n\n        where:\n\n        - $n$ is the sample size,\n        - $S$ is the sample skewness, and\n        - $K$ is the sample kurtosis.\n\n    ??? note \"Notes\"\n        Each output returned has 1 dimension fewer than data.\n        The Jarque-Bera test statistic tests the null that the data is normally distributed against an alternative that the data follow some other distribution. It has an asymptotic $\\chi_2^2$ distribution.\n\n    ??? success \"Credit\"\n        All credit goes to the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ??? question \"References\"\n        - Jarque, C. and Bera, A. (1980) \"Efficient tests for normality, homoscedasticity and serial independence of regression residuals\", 6 Econometric Letters 255-259.\n\n    ??? tip \"See Also\"\n        - [`ob()`][ts_stat_tests.algorithms.normality.ob]\n        - [`sw()`][ts_stat_tests.algorithms.normality.sw]\n        - [`dp()`][ts_stat_tests.algorithms.normality.dp]\n        - [`ad()`][ts_stat_tests.algorithms.normality.ad]\n    \"\"\"\n    return _jb(resids=x, axis=axis)  # type: ignore[return-value]\n</code></pre>"},{"location":"code/normality/#ts_stat_tests.algorithms.normality.ob","title":"ob","text":"<pre><code>ob(x: ArrayLike, axis: int = 0) -&gt; tuple[float, float]\n</code></pre> <p>Summary</p> <p>The Omnibus test is a statistical test used to evaluate the normality of a dataset, including time series data. In time series forecasting, the Omnibus test can be used to assess whether the residuals of a model follow a normal distribution.</p> Details <p>The Omnibus test uses a combination of skewness and kurtosis measures to assess whether the residuals follow a normal distribution. Skewness measures the degree of asymmetry in the distribution of the residuals, while kurtosis measures the degree of peakedness or flatness. If the residuals follow a normal distribution, their skewness and kurtosis should be close to zero.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>Data to test for normality. Usually regression model residuals that are mean 0.</p> required <code>axis</code> <code>int</code> <p>Axis to use if data has more than 1 dimension. Default: <code>0</code></p> <code>0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input data <code>x</code> is invalid.</p> <p>Returns:</p> Name Type Description <code>statistic</code> <code>float</code> <p>The Omnibus test statistic.</p> <code>pvalue</code> <code>float</code> <p>The p-value for the hypothesis test.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.normality import ob\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_noise\n&gt;&gt;&gt; airline = data_airline.values\n&gt;&gt;&gt; noise = data_noise\n</code></pre> Example 1: Using the airline dataset<pre><code>&gt;&gt;&gt; stat, p_val = ob(airline)\n&gt;&gt;&gt; print(f\"{stat:.4f}\")\n8.6554\n</code></pre> Example 2: Using random noise<pre><code>&gt;&gt;&gt; stat, p_val = ob(noise)\n&gt;&gt;&gt; print(f\"{stat:.4f}\")\n0.8637\n</code></pre> Calculation <p>The D'Agostino's \\(K^2\\) test statistic is defined as:</p> \\[ K^2 = Z_1(g_1)^2 + Z_2(g_2)^2 \\] <p>where:</p> <ul> <li>\\(Z_1(g_1)\\) is the standard normal transformation of skewness, and</li> <li>\\(Z_2(g_2)\\) is the standard normal transformation of kurtosis.</li> </ul> Notes <p>The Omnibus test statistic tests the null that the data is normally distributed against an alternative that the data follow some other distribution. It is based on D'Agostino's \\(K^2\\) test statistic.</p> Credit <p>All credit goes to the <code>statsmodels</code> library.</p> References <ul> <li>D'Agostino, R. B. and Pearson, E. S. (1973), \"Tests for departure from normality,\" Biometrika, 60, 613-622.</li> <li>D'Agostino, R. B. and Stephens, M. A. (1986), \"Goodness-of-fit techniques,\" New York: Marcel Dekker.</li> </ul> See Also <ul> <li><code>jb()</code></li> <li><code>sw()</code></li> <li><code>dp()</code></li> <li><code>ad()</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/normality.py</code> <pre><code>@typechecked\ndef ob(x: ArrayLike, axis: int = 0) -&gt; tuple[float, float]:\n    r\"\"\"\n    !!! note \"Summary\"\n        The Omnibus test is a statistical test used to evaluate the normality of a dataset, including time series data. In time series forecasting, the Omnibus test can be used to assess whether the residuals of a model follow a normal distribution.\n\n    ???+ abstract \"Details\"\n        The Omnibus test uses a combination of skewness and kurtosis measures to assess whether the residuals follow a normal distribution. Skewness measures the degree of asymmetry in the distribution of the residuals, while kurtosis measures the degree of peakedness or flatness. If the residuals follow a normal distribution, their skewness and kurtosis should be close to zero.\n\n    Params:\n        x (ArrayLike):\n            Data to test for normality. Usually regression model residuals that are mean 0.\n        axis (int):\n            Axis to use if data has more than 1 dimension.\n            Default: `0`\n\n    Raises:\n        (ValueError):\n            If the input data `x` is invalid.\n\n    Returns:\n        statistic (float):\n            The Omnibus test statistic.\n        pvalue (float):\n            The p-value for the hypothesis test.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.normality import ob\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_noise\n        &gt;&gt;&gt; airline = data_airline.values\n        &gt;&gt;&gt; noise = data_noise\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Using the airline dataset\"}\n        &gt;&gt;&gt; stat, p_val = ob(airline)\n        &gt;&gt;&gt; print(f\"{stat:.4f}\")\n        8.6554\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Using random noise\"}\n        &gt;&gt;&gt; stat, p_val = ob(noise)\n        &gt;&gt;&gt; print(f\"{stat:.4f}\")\n        0.8637\n\n        ```\n\n    ??? equation \"Calculation\"\n        The D'Agostino's $K^2$ test statistic is defined as:\n\n        $$\n        K^2 = Z_1(g_1)^2 + Z_2(g_2)^2\n        $$\n\n        where:\n\n        - $Z_1(g_1)$ is the standard normal transformation of skewness, and\n        - $Z_2(g_2)$ is the standard normal transformation of kurtosis.\n\n    ??? note \"Notes\"\n        The Omnibus test statistic tests the null that the data is normally distributed against an alternative that the data follow some other distribution. It is based on D'Agostino's $K^2$ test statistic.\n\n    ??? success \"Credit\"\n        All credit goes to the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ??? question \"References\"\n        - D'Agostino, R. B. and Pearson, E. S. (1973), \"Tests for departure from normality,\" Biometrika, 60, 613-622.\n        - D'Agostino, R. B. and Stephens, M. A. (1986), \"Goodness-of-fit techniques,\" New York: Marcel Dekker.\n\n    ??? tip \"See Also\"\n        - [`jb()`][ts_stat_tests.algorithms.normality.jb]\n        - [`sw()`][ts_stat_tests.algorithms.normality.sw]\n        - [`dp()`][ts_stat_tests.algorithms.normality.dp]\n        - [`ad()`][ts_stat_tests.algorithms.normality.ad]\n    \"\"\"\n    return _ob(resids=x, axis=axis)\n</code></pre>"},{"location":"code/normality/#ts_stat_tests.algorithms.normality.sw","title":"sw","text":"<pre><code>sw(x: ArrayLike) -&gt; ShapiroResult\n</code></pre> <p>Summary</p> <p>The Shapiro-Wilk test is a statistical test used to determine whether a dataset follows a normal distribution.</p> Details <p>The Shapiro-Wilk test is based on the null hypothesis that the residuals of the forecasting model are normally distributed. The test calculates a test statistic that compares the observed distribution of the residuals to the expected distribution under the null hypothesis of normality.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>Array of sample data.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input data <code>x</code> is invalid.</p> <p>Returns:</p> Type Description <code>ShapiroResult</code> <p>A named tuple containing the test statistic and p-value: - statistic (float): The test statistic. - pvalue (float): The p-value for the hypothesis test.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.normality import sw\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_noise\n&gt;&gt;&gt; airline = data_airline.values\n&gt;&gt;&gt; noise = data_noise\n</code></pre> Example 1: Using the airline dataset<pre><code>&gt;&gt;&gt; stat, p_val = sw(airline)\n&gt;&gt;&gt; print(f\"{stat:.4f}\")\n0.9520\n</code></pre> Example 2: Using random noise<pre><code>&gt;&gt;&gt; stat, p_val = sw(noise)\n&gt;&gt;&gt; print(f\"{stat:.4f}\")\n0.9985\n</code></pre> Calculation <p>The Shapiro-Wilk test statistic is defined as:</p> \\[ W = \\frac{\\left( \\sum_{i=1}^n a_i x_{(i)} \\right)^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\] <p>where:</p> <ul> <li>\\(x_{(i)}\\) are the ordered sample values,</li> <li>\\(\\bar{x}\\) is the sample mean, and</li> <li>\\(a_i\\) are constants generated from the covariances, variances and means of the order statistics of a sample of size \\(n\\) from a normal distribution.</li> </ul> Notes <p>The algorithm used is described in (Algorithm as R94 Appl. Statist. (1995)) but censoring parameters as described are not implemented. For \\(N &gt; 5000\\) the \\(W\\) test statistic is accurate but the \\(p-value\\) may not be.</p> Credit <p>All credit goes to the <code>scipy</code> library.</p> References <ul> <li>Shapiro, S. S. &amp; Wilk, M.B (1965). An analysis of variance test for normality (complete samples), Biometrika, Vol. 52, pp. 591-611.</li> <li>Algorithm as R94 Appl. Statist. (1995) VOL. 44, NO. 4.</li> </ul> See Also <ul> <li><code>jb()</code></li> <li><code>ob()</code></li> <li><code>dp()</code></li> <li><code>ad()</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/normality.py</code> <pre><code>@typechecked\ndef sw(x: ArrayLike) -&gt; ShapiroResult:\n    r\"\"\"\n    !!! note \"Summary\"\n        The Shapiro-Wilk test is a statistical test used to determine whether a dataset follows a normal distribution.\n\n    ???+ abstract \"Details\"\n        The Shapiro-Wilk test is based on the null hypothesis that the residuals of the forecasting model are normally distributed. The test calculates a test statistic that compares the observed distribution of the residuals to the expected distribution under the null hypothesis of normality.\n\n    Params:\n        x (ArrayLike):\n            Array of sample data.\n\n    Raises:\n        (ValueError):\n            If the input data `x` is invalid.\n\n    Returns:\n        (ShapiroResult):\n            A named tuple containing the test statistic and p-value:\n            - statistic (float): The test statistic.\n            - pvalue (float): The p-value for the hypothesis test.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.normality import sw\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_noise\n        &gt;&gt;&gt; airline = data_airline.values\n        &gt;&gt;&gt; noise = data_noise\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Using the airline dataset\"}\n        &gt;&gt;&gt; stat, p_val = sw(airline)\n        &gt;&gt;&gt; print(f\"{stat:.4f}\")\n        0.9520\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Using random noise\"}\n        &gt;&gt;&gt; stat, p_val = sw(noise)\n        &gt;&gt;&gt; print(f\"{stat:.4f}\")\n        0.9985\n\n        ```\n\n    ??? equation \"Calculation\"\n        The Shapiro-Wilk test statistic is defined as:\n\n        $$\n        W = \\frac{\\left( \\sum_{i=1}^n a_i x_{(i)} \\right)^2}{\\sum_{i=1}^n (x_i - \\bar{x})^2}\n        $$\n\n        where:\n\n        - $x_{(i)}$ are the ordered sample values,\n        - $\\bar{x}$ is the sample mean, and\n        - $a_i$ are constants generated from the covariances, variances and means of the order statistics of a sample of size $n$ from a normal distribution.\n\n    ??? note \"Notes\"\n        The algorithm used is described in (Algorithm as R94 Appl. Statist. (1995)) but censoring parameters as described are not implemented. For $N &gt; 5000$ the $W$ test statistic is accurate but the $p-value$ may not be.\n\n    ??? success \"Credit\"\n        All credit goes to the [`scipy`](https://docs.scipy.org/) library.\n\n    ??? question \"References\"\n        - Shapiro, S. S. &amp; Wilk, M.B (1965). An analysis of variance test for normality (complete samples), Biometrika, Vol. 52, pp. 591-611.\n        - Algorithm as R94 Appl. Statist. (1995) VOL. 44, NO. 4.\n\n    ??? tip \"See Also\"\n        - [`jb()`][ts_stat_tests.algorithms.normality.jb]\n        - [`ob()`][ts_stat_tests.algorithms.normality.ob]\n        - [`dp()`][ts_stat_tests.algorithms.normality.dp]\n        - [`ad()`][ts_stat_tests.algorithms.normality.ad]\n    \"\"\"\n    return _sw(x=x)\n</code></pre>"},{"location":"code/normality/#ts_stat_tests.algorithms.normality.dp","title":"dp","text":"<pre><code>dp(\n    x: ArrayLike,\n    axis: int = 0,\n    nan_policy: VALID_DP_NAN_POLICY_OPTIONS = \"propagate\",\n) -&gt; NormaltestResult\n</code></pre> <p>Summary</p> <p>The D'Agostino and Pearson's test is a statistical test used to evaluate whether a dataset follows a normal distribution.</p> Details <p>The D'Agostino and Pearson's test uses a combination of skewness and kurtosis measures to assess whether the residuals follow a normal distribution. Skewness measures the degree of asymmetry in the distribution of the residuals, while kurtosis measures the degree of peakedness or flatness.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The array containing the sample to be tested.</p> required <code>axis</code> <code>int</code> <p>Axis along which to compute test. If <code>None</code>, compute over the whole array <code>a</code>. Default: <code>0</code></p> <code>0</code> <code>nan_policy</code> <code>VALID_DP_NAN_POLICY_OPTIONS</code> <p>Defines how to handle when input contains nan.</p> <ul> <li><code>\"propagate\"</code>: returns nan</li> <li><code>\"raise\"</code>: throws an error</li> <li><code>\"omit\"</code>: performs the calculations ignoring nan values</li> </ul> <p>Default: <code>\"propagate\"</code></p> <code>'propagate'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input data <code>x</code> is invalid.</p> <p>Returns:</p> Type Description <code>NormaltestResult</code> <p>A named tuple containing the test statistic and p-value: - statistic (float): The test statistic (\\(K^2\\)). - pvalue (float): A 2-sided chi-squared probability for the hypothesis test.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.normality import dp\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_noise\n&gt;&gt;&gt; airline = data_airline.values\n&gt;&gt;&gt; noise = data_noise\n</code></pre> Example 1: Using the airline dataset<pre><code>&gt;&gt;&gt; stat, p_val = dp(airline)\n&gt;&gt;&gt; print(f\"{stat:.4f}\")\n8.6554\n</code></pre> Example 2: Using random noise<pre><code>&gt;&gt;&gt; stat, p_val = dp(noise)\n&gt;&gt;&gt; print(f\"{stat:.4f}\")\n0.8637\n</code></pre> Calculation <p>The D'Agostino's \\(K^2\\) test statistic is defined as:</p> \\[ K^2 = Z_1(g_1)^2 + Z_2(g_2)^2 \\] <p>where:</p> <ul> <li>\\(Z_1(g_1)\\) is the standard normal transformation of skewness, and</li> <li>\\(Z_2(g_2)\\) is the standard normal transformation of kurtosis.</li> </ul> Notes <p>This function is a wrapper for the <code>scipy.stats.normaltest</code> function.</p> Credit <p>All credit goes to the <code>scipy</code> library.</p> References <ul> <li>D'Agostino, R. B. (1971), \"An omnibus test of normality for moderate and large sample size\", Biometrika, 58, 341-348</li> <li>D'Agostino, R. and Pearson, E. S. (1973), \"Tests for departure from normality\", Biometrika, 60, 613-622</li> </ul> See Also <ul> <li><code>jb()</code></li> <li><code>ob()</code></li> <li><code>sw()</code></li> <li><code>ad()</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/normality.py</code> <pre><code>@typechecked\ndef dp(\n    x: ArrayLike,\n    axis: int = 0,\n    nan_policy: VALID_DP_NAN_POLICY_OPTIONS = \"propagate\",\n) -&gt; NormaltestResult:\n    r\"\"\"\n    !!! note \"Summary\"\n        The D'Agostino and Pearson's test is a statistical test used to evaluate whether a dataset follows a normal distribution.\n\n    ???+ abstract \"Details\"\n        The D'Agostino and Pearson's test uses a combination of skewness and kurtosis measures to assess whether the residuals follow a normal distribution. Skewness measures the degree of asymmetry in the distribution of the residuals, while kurtosis measures the degree of peakedness or flatness.\n\n    Params:\n        x (ArrayLike):\n            The array containing the sample to be tested.\n        axis (int):\n            Axis along which to compute test. If `None`, compute over the whole array `a`.\n            Default: `0`\n        nan_policy (VALID_DP_NAN_POLICY_OPTIONS):\n            Defines how to handle when input contains nan.\n\n            - `\"propagate\"`: returns nan\n            - `\"raise\"`: throws an error\n            - `\"omit\"`: performs the calculations ignoring nan values\n\n            Default: `\"propagate\"`\n\n    Raises:\n        (ValueError):\n            If the input data `x` is invalid.\n\n    Returns:\n        (NormaltestResult):\n            A named tuple containing the test statistic and p-value:\n            - statistic (float): The test statistic ($K^2$).\n            - pvalue (float): A 2-sided chi-squared probability for the hypothesis test.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.normality import dp\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_noise\n        &gt;&gt;&gt; airline = data_airline.values\n        &gt;&gt;&gt; noise = data_noise\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Using the airline dataset\"}\n        &gt;&gt;&gt; stat, p_val = dp(airline)\n        &gt;&gt;&gt; print(f\"{stat:.4f}\")\n        8.6554\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Using random noise\"}\n        &gt;&gt;&gt; stat, p_val = dp(noise)\n        &gt;&gt;&gt; print(f\"{stat:.4f}\")\n        0.8637\n\n        ```\n\n    ??? equation \"Calculation\"\n        The D'Agostino's $K^2$ test statistic is defined as:\n\n        $$\n        K^2 = Z_1(g_1)^2 + Z_2(g_2)^2\n        $$\n\n        where:\n\n        - $Z_1(g_1)$ is the standard normal transformation of skewness, and\n        - $Z_2(g_2)$ is the standard normal transformation of kurtosis.\n\n    ??? note \"Notes\"\n        This function is a wrapper for the `scipy.stats.normaltest` function.\n\n    ??? success \"Credit\"\n        All credit goes to the [`scipy`](https://docs.scipy.org/) library.\n\n    ??? question \"References\"\n        - D'Agostino, R. B. (1971), \"An omnibus test of normality for moderate and large sample size\", Biometrika, 58, 341-348\n        - D'Agostino, R. and Pearson, E. S. (1973), \"Tests for departure from normality\", Biometrika, 60, 613-622\n\n    ??? tip \"See Also\"\n        - [`jb()`][ts_stat_tests.algorithms.normality.jb]\n        - [`ob()`][ts_stat_tests.algorithms.normality.ob]\n        - [`sw()`][ts_stat_tests.algorithms.normality.sw]\n        - [`ad()`][ts_stat_tests.algorithms.normality.ad]\n    \"\"\"\n    return _dp(a=x, axis=axis, nan_policy=nan_policy)\n</code></pre>"},{"location":"code/normality/#ts_stat_tests.algorithms.normality.ad","title":"ad","text":"<pre><code>ad(\n    x: ArrayLike, dist: VALID_AD_DIST_OPTIONS = \"norm\"\n) -&gt; AndersonResult\n</code></pre> <p>Summary</p> <p>The Anderson-Darling test is a statistical test used to evaluate whether a dataset follows a normal distribution.</p> Details <p>The Anderson-Darling test tests the null hypothesis that a sample is drawn from a population that follows a particular distribution. For the Anderson-Darling test, the critical values depend on which distribution is being tested against.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>Array of sample data.</p> required <code>dist</code> <code>VALID_AD_DIST_OPTIONS</code> <p>The type of distribution to test against. Default: <code>\"norm\"</code></p> <code>'norm'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input data <code>x</code> is invalid.</p> <p>Returns:</p> Type Description <code>AndersonResult</code> <p>A named tuple containing the test statistic, critical values, and significance levels: - statistic (float): The Anderson-Darling test statistic. - critical_values (list[float]): The critical values for this distribution. - significance_level (list[float]): The significance levels for the corresponding critical values in percents.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.normality import ad\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_noise\n&gt;&gt;&gt; airline = data_airline.values\n&gt;&gt;&gt; noise = data_noise\n</code></pre> Example 1: Using the airline dataset<pre><code>&gt;&gt;&gt; stat, cv, sl = ad(airline)\n&gt;&gt;&gt; print(f\"{stat:.4f}\")\n1.8185\n</code></pre> Example 2: Using random normal data<pre><code>&gt;&gt;&gt; stat, cv, sl = ad(noise)\n&gt;&gt;&gt; print(f\"{stat:.4f}\")\n0.2325\n</code></pre> Calculation <p>The Anderson-Darling test statistic \\(A^2\\) is defined as:</p> \\[ A^2 = -n - \\sum_{i=1}^n \\frac{2i-1}{n} \\left[ \\ln(F(x_i)) + \\ln(1 - F(x_{n-i+1})) \\right] \\] <p>where:</p> <ul> <li>\\(n\\) is the sample size,</li> <li>\\(F\\) is the cumulative distribution function of the specified distribution, and</li> <li>\\(x_i\\) are the ordered sample values.</li> </ul> Notes <p>Critical values provided are for the following significance levels: - normal/exponential: 15%, 10%, 5%, 2.5%, 1% - logistic: 25%, 10%, 5%, 2.5%, 1%, 0.5% - Gumbel: 25%, 10%, 5%, 2.5%, 1%</p> Credit <p>All credit goes to the <code>scipy</code> library.</p> References <ul> <li>Stephens, M. A. (1974). EDF Statistics for Goodness of Fit and Some Comparisons, Journal of the American Statistical Association, Vol. 69, pp. 730-737.</li> </ul> See Also <ul> <li><code>jb()</code></li> <li><code>ob()</code></li> <li><code>sw()</code></li> <li><code>dp()</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/normality.py</code> <pre><code>@typechecked\ndef ad(\n    x: ArrayLike,\n    dist: VALID_AD_DIST_OPTIONS = \"norm\",\n) -&gt; AndersonResult:\n    r\"\"\"\n    !!! note \"Summary\"\n        The Anderson-Darling test is a statistical test used to evaluate whether a dataset follows a normal distribution.\n\n    ???+ abstract \"Details\"\n        The Anderson-Darling test tests the null hypothesis that a sample is drawn from a population that follows a particular distribution. For the Anderson-Darling test, the critical values depend on which distribution is being tested against.\n\n    Params:\n        x (ArrayLike):\n            Array of sample data.\n        dist (VALID_AD_DIST_OPTIONS):\n            The type of distribution to test against.\n            Default: `\"norm\"`\n\n    Raises:\n        (ValueError):\n            If the input data `x` is invalid.\n\n    Returns:\n        (AndersonResult):\n            A named tuple containing the test statistic, critical values, and significance levels:\n            - statistic (float): The Anderson-Darling test statistic.\n            - critical_values (list[float]): The critical values for this distribution.\n            - significance_level (list[float]): The significance levels for the corresponding critical values in percents.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.normality import ad\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_noise\n        &gt;&gt;&gt; airline = data_airline.values\n        &gt;&gt;&gt; noise = data_noise\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Using the airline dataset\"}\n        &gt;&gt;&gt; stat, cv, sl = ad(airline)\n        &gt;&gt;&gt; print(f\"{stat:.4f}\")\n        1.8185\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Using random normal data\"}\n        &gt;&gt;&gt; stat, cv, sl = ad(noise)\n        &gt;&gt;&gt; print(f\"{stat:.4f}\")\n        0.2325\n\n        ```\n\n    ??? equation \"Calculation\"\n        The Anderson-Darling test statistic $A^2$ is defined as:\n\n        $$\n        A^2 = -n - \\sum_{i=1}^n \\frac{2i-1}{n} \\left[ \\ln(F(x_i)) + \\ln(1 - F(x_{n-i+1})) \\right]\n        $$\n\n        where:\n\n        - $n$ is the sample size,\n        - $F$ is the cumulative distribution function of the specified distribution, and\n        - $x_i$ are the ordered sample values.\n\n    ??? note \"Notes\"\n        Critical values provided are for the following significance levels:\n        - normal/exponential: 15%, 10%, 5%, 2.5%, 1%\n        - logistic: 25%, 10%, 5%, 2.5%, 1%, 0.5%\n        - Gumbel: 25%, 10%, 5%, 2.5%, 1%\n\n    ??? success \"Credit\"\n        All credit goes to the [`scipy`](https://docs.scipy.org/) library.\n\n    ??? question \"References\"\n        - Stephens, M. A. (1974). EDF Statistics for Goodness of Fit and Some Comparisons, Journal of the American Statistical Association, Vol. 69, pp. 730-737.\n\n    ??? tip \"See Also\"\n        - [`jb()`][ts_stat_tests.algorithms.normality.jb]\n        - [`ob()`][ts_stat_tests.algorithms.normality.ob]\n        - [`sw()`][ts_stat_tests.algorithms.normality.sw]\n        - [`dp()`][ts_stat_tests.algorithms.normality.dp]\n    \"\"\"\n    return _ad(x=x, dist=dist)\n</code></pre>"},{"location":"code/regularity/","title":"Test the <code>regularity</code> of a given Time-Series Dataset","text":""},{"location":"code/regularity/#introduction","title":"Introduction","text":"<p>Summary</p> <p>As stated by Selva Prabhakaran:</p> <p>The more regular and repeatable patterns a time series has, the easier it is to forecast.</p> <p>The 'Approximate Entropy' algorithm can be used to quantify the regularity and unpredictability of fluctuations in a time series.</p> <p>The higher the approximate entropy, the more difficult it is to forecast it.</p> <p>Another better alternate is the 'Sample Entropy'.</p> <p>Sample Entropy is similar to approximate entropy but is more consistent in estimating the complexity even for smaller time series.</p> <p>For example, a random time series with fewer data points can have a lower 'approximate entropy' than a more 'regular' time series, whereas, a longer random time series will have a higher 'approximate entropy'.</p> <p> For more info, see: Time Series Analysis in Python: A Comprehensive Guide with Examples.</p> <p>Info</p> <p>To state that the data is 'regular' is to say that the data points are evenly spaced, regularly collected, and not missing data points (ie. do not contain excessive <code>NA</code> values). Logically, it is not always necessary to conduct the Test for Regularity on automatically collected data (like for example with Energy Prices, or Daily Temperature), however if this data was collected manually then it is highly recommended. If the data does not meet the requirements of Regularity, then it is necessary to return to the data collection plan, and revise the methodology used.</p> library category algorithm short import script url antropy Regularity Approximate Entropy AppEn <code>from antropy import app_entropy</code> https://raphaelvallat.com/antropy/build/html/generated/antropy.app_entropy.html antropy Regularity Sample Entropy SampEn <code>from antropy import sample_entropy</code> https://raphaelvallat.com/antropy/build/html/generated/antropy.sample_entropy.html antropy Regularity Permutation Entropy PermEn <code>from antropy import perm_entropy</code> https://raphaelvallat.com/antropy/build/html/generated/antropy.perm_entropy.html antropy Regularity Spectral Entropy SpecEn <code>from antropy import spectral_entropy</code> https://raphaelvallat.com/antropy/build/html/generated/antropy.spectral_entropy.html antropy Regularity SVD Entropy SvdEn <code>from antropy import svd_entropy</code> https://raphaelvallat.com/antropy/build/html/generated/antropy.svd_entropy.html <p> For more info, see: The Future of Australian Energy Prices: Time-Series Analysis of Historic Prices and Forecast for Future Prices.</p> <p>Source Library</p> <p>The <code>AntroPy</code> package was chosen because it provides well-tested and efficient implementations of approximate entropy, sample entropy, and related complexity measures for time-series data, is built on top of the scientific Python stack (NumPy/SciPy), and is actively maintained and open source, making it a reliable choice for reproducible statistical analysis.</p> <p>Source Module</p> <p>All of the source code can be found within the modules:</p> <ul> <li><code>ts_stat_tests.algorithms.regularity</code>.</li> <li><code>ts_stat_tests.tests.regularity</code>.</li> </ul>"},{"location":"code/regularity/#regularity-tests","title":"Regularity Tests","text":""},{"location":"code/regularity/#ts_stat_tests.tests.regularity","title":"ts_stat_tests.tests.regularity","text":"<p>Summary</p> <p>This module contains convenience functions and tests for regularity measures, allowing for easy access to different entropy algorithms.</p>"},{"location":"code/regularity/#ts_stat_tests.tests.regularity.entropy","title":"entropy","text":"<pre><code>entropy(\n    x: ArrayLike,\n    algorithm: str = \"sample\",\n    order: int = 2,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n    sf: float = 1,\n    normalize: bool = True,\n) -&gt; Union[float, NDArray[np.float64]]\n</code></pre> <p>Summary</p> <p>Test for the entropy of a given data set.</p> Details <p>This function is a convenience wrapper around the five underlying algorithms: - <code>approx_entropy()</code> - <code>sample_entropy()</code> - <code>spectral_entropy()</code> - <code>permutation_entropy()</code> - <code>svd_entropy()</code></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked. Should be a <code>1-D</code> or <code>N-D</code> data array.</p> required <code>algorithm</code> <code>str</code> <p>Which entropy algorithm to use. - <code>sample_entropy()</code>: <code>[\"sample\", \"sampl\", \"samp\"]</code> - <code>approx_entropy()</code>: <code>[\"app\", \"approx\"]</code> - <code>spectral_entropy()</code>: <code>[\"spec\", \"spect\", \"spectral\"]</code> - <code>permutation_entropy()</code>: <code>[\"perm\", \"permutation\"]</code> - <code>svd_entropy()</code>: <code>[\"svd\", \"svd_entropy\"]</code> Defaults to <code>\"sample\"</code>.</p> <code>'sample'</code> <code>order</code> <code>int</code> <p>Embedding dimension. Only relevant when <code>algorithm=sample</code> or <code>algorithm=approx</code>. Defaults to <code>2</code>.</p> <code>2</code> <code>metric</code> <code>VALID_KDTREE_METRIC_OPTIONS</code> <p>Name of the distance metric function used with <code>sklearn.neighbors.KDTree</code>. Default is to use the Chebyshev distance. Only relevant when <code>algorithm=sample</code> or <code>algorithm=approx</code>. Defaults to <code>\"chebyshev\"</code>.</p> <code>'chebyshev'</code> <code>sf</code> <code>float</code> <p>Sampling frequency, in Hz. Only relevant when <code>algorithm=spectral</code>. Defaults to <code>1</code>.</p> <code>1</code> <code>normalize</code> <code>bool</code> <p>If <code>True</code>, divide by \\(log2(psd.size)\\) to normalize the spectral entropy to be between \\(0\\) and \\(1\\). Otherwise, return the spectral entropy in bit. Only relevant when <code>algorithm=spectral</code>. Defaults to <code>True</code>.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the given value for <code>algorithm</code> is not valid.</p> <p>Returns:</p> Type Description <code>Union[float, NDArray[float64]]</code> <p>The calculated entropy value.</p> Credit <p>All credit goes to the <code>AntroPy</code> library.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.tests.regularity import entropy\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n&gt;&gt;&gt; normal = data_normal\n</code></pre> Example 1: Sample Entropy<pre><code>&gt;&gt;&gt; print(entropy(x=normal, algorithm=\"sample\"))\n2.2374...\n</code></pre> Example 2: Approx Entropy<pre><code>&gt;&gt;&gt; print(entropy(x=normal, algorithm=\"approx\"))\n1.6643...\n</code></pre> Example 3: Spectral Entropy<pre><code>&gt;&gt;&gt; print(entropy(x=normal, algorithm=\"spectral\", sf=1))\n0.9329...\n</code></pre> References <ul> <li>Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.</li> <li>https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html</li> <li>Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.</li> <li>https://en.wikipedia.org/wiki/Spectral_density</li> <li>https://en.wikipedia.org/wiki/Welch%27s_method</li> </ul> See Also <ul> <li><code>regularity()</code></li> <li><code>approx_entropy()</code></li> <li><code>sample_entropy()</code></li> <li><code>spectral_entropy()</code></li> <li><code>permutation_entropy()</code></li> <li><code>svd_entropy()</code></li> </ul> Source code in <code>src/ts_stat_tests/tests/regularity.py</code> <pre><code>@typechecked\ndef entropy(\n    x: ArrayLike,\n    algorithm: str = \"sample\",\n    order: int = 2,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n    sf: float = 1,\n    normalize: bool = True,\n) -&gt; Union[float, NDArray[np.float64]]:\n    \"\"\"\n    !!! note \"Summary\"\n        Test for the entropy of a given data set.\n\n    ???+ abstract \"Details\"\n        This function is a convenience wrapper around the five underlying algorithms:&lt;br&gt;\n        - [`approx_entropy()`][ts_stat_tests.algorithms.regularity.approx_entropy]&lt;br&gt;\n        - [`sample_entropy()`][ts_stat_tests.algorithms.regularity.sample_entropy]&lt;br&gt;\n        - [`spectral_entropy()`][ts_stat_tests.algorithms.regularity.spectral_entropy]&lt;br&gt;\n        - [`permutation_entropy()`][ts_stat_tests.algorithms.regularity.permutation_entropy]&lt;br&gt;\n        - [`svd_entropy()`][ts_stat_tests.algorithms.regularity.svd_entropy]\n\n    Params:\n        x (ArrayLike):\n            The data to be checked. Should be a `1-D` or `N-D` data array.\n        algorithm (str, optional):\n            Which entropy algorithm to use.&lt;br&gt;\n            - `sample_entropy()`: `[\"sample\", \"sampl\", \"samp\"]`&lt;br&gt;\n            - `approx_entropy()`: `[\"app\", \"approx\"]`&lt;br&gt;\n            - `spectral_entropy()`: `[\"spec\", \"spect\", \"spectral\"]`&lt;br&gt;\n            - `permutation_entropy()`: `[\"perm\", \"permutation\"]`&lt;br&gt;\n            - `svd_entropy()`: `[\"svd\", \"svd_entropy\"]`&lt;br&gt;\n            Defaults to `\"sample\"`.\n        order (int, optional):\n            Embedding dimension.&lt;br&gt;\n            Only relevant when `algorithm=sample` or `algorithm=approx`.&lt;br&gt;\n            Defaults to `2`.\n        metric (VALID_KDTREE_METRIC_OPTIONS):\n            Name of the distance metric function used with [`sklearn.neighbors.KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree). Default is to use the [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance).&lt;br&gt;\n            Only relevant when `algorithm=sample` or `algorithm=approx`.&lt;br&gt;\n            Defaults to `\"chebyshev\"`.\n        sf (float, optional):\n            Sampling frequency, in Hz.&lt;br&gt;\n            Only relevant when `algorithm=spectral`.&lt;br&gt;\n            Defaults to `1`.\n        normalize (bool, optional):\n            If `True`, divide by $log2(psd.size)$ to normalize the spectral entropy to be between $0$ and $1$. Otherwise, return the spectral entropy in bit.&lt;br&gt;\n            Only relevant when `algorithm=spectral`.&lt;br&gt;\n            Defaults to `True`.\n\n    Raises:\n        (ValueError):\n            When the given value for `algorithm` is not valid.\n\n    Returns:\n        (Union[float, NDArray[np.float64]]):\n            The calculated entropy value.\n\n    ??? success \"Credit\"\n        All credit goes to the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.tests.regularity import entropy\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n        &gt;&gt;&gt; normal = data_normal\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Sample Entropy\"}\n        &gt;&gt;&gt; print(entropy(x=normal, algorithm=\"sample\"))\n        2.2374...\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Approx Entropy\"}\n        &gt;&gt;&gt; print(entropy(x=normal, algorithm=\"approx\"))\n        1.6643...\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 3: Spectral Entropy\"}\n        &gt;&gt;&gt; print(entropy(x=normal, algorithm=\"spectral\", sf=1))\n        0.9329...\n\n        ```\n\n    ??? question \"References\"\n        - Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n        - https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n        - Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.\n        - https://en.wikipedia.org/wiki/Spectral_density\n        - https://en.wikipedia.org/wiki/Welch%27s_method\n\n    ??? tip \"See Also\"\n        - [`regularity()`][ts_stat_tests.tests.regularity.regularity]\n        - [`approx_entropy()`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`sample_entropy()`][ts_stat_tests.algorithms.regularity.sample_entropy]\n        - [`spectral_entropy()`][ts_stat_tests.algorithms.regularity.spectral_entropy]\n        - [`permutation_entropy()`][ts_stat_tests.algorithms.regularity.permutation_entropy]\n        - [`svd_entropy()`][ts_stat_tests.algorithms.regularity.svd_entropy]\n    \"\"\"\n    options: dict[str, tuple[str, ...]] = {\n        \"sampl\": (\"sample\", \"sampl\", \"samp\"),\n        \"approx\": (\"app\", \"approx\"),\n        \"spect\": (\"spec\", \"spect\", \"spectral\"),\n        \"perm\": (\"perm\", \"permutation\"),\n        \"svd\": (\"svd\", \"svd_entropy\"),\n    }\n    if algorithm in options[\"sampl\"]:\n        return sample_entropy(x=x, order=order, metric=metric)\n    if algorithm in options[\"approx\"]:\n        return approx_entropy(x=x, order=order, metric=metric)\n    if algorithm in options[\"spect\"]:\n        return spectral_entropy(x=x, sf=sf, normalize=normalize)\n    if algorithm in options[\"perm\"]:\n        return permutation_entropy(x=x, order=order, normalize=normalize)\n    if algorithm in options[\"svd\"]:\n        return svd_entropy(x=x, order=order, normalize=normalize)\n    raise ValueError(\n        generate_error_message(\n            parameter_name=\"algorithm\",\n            value_parsed=algorithm,\n            options=options,\n        )\n    )\n</code></pre>"},{"location":"code/regularity/#ts_stat_tests.tests.regularity.regularity","title":"regularity","text":"<pre><code>regularity(\n    x: ArrayLike,\n    algorithm: str = \"sample\",\n    order: int = 2,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n    sf: float = 1,\n    normalize: bool = True,\n) -&gt; Union[float, NDArray[np.float64]]\n</code></pre> <p>Summary</p> <p>Test for the regularity of a given data set.</p> Details <p>This is a pass-through, convenience wrapper around the <code>entropy()</code> function.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked. Should be a <code>1-D</code> or <code>N-D</code> data array.</p> required <code>algorithm</code> <code>str</code> <p>Which entropy algorithm to use. - <code>sample_entropy()</code>: <code>[\"sample\", \"sampl\", \"samp\"]</code> - <code>approx_entropy()</code>: <code>[\"app\", \"approx\"]</code> - <code>spectral_entropy()</code>: <code>[\"spec\", \"spect\", \"spectral\"]</code> - <code>permutation_entropy()</code>: <code>[\"perm\", \"permutation\"]</code> - <code>svd_entropy()</code>: <code>[\"svd\", \"svd_entropy\"]</code> Defaults to <code>\"sample\"</code>.</p> <code>'sample'</code> <code>order</code> <code>int</code> <p>Embedding dimension. Only relevant when <code>algorithm=sample</code> or <code>algorithm=approx</code>. Defaults to <code>2</code>.</p> <code>2</code> <code>metric</code> <code>VALID_KDTREE_METRIC_OPTIONS</code> <p>Name of the distance metric function used with <code>sklearn.neighbors.KDTree</code>. Default is to use the Chebyshev distance. Only relevant when <code>algorithm=sample</code> or <code>algorithm=approx</code>. Defaults to <code>\"chebyshev\"</code>.</p> <code>'chebyshev'</code> <code>sf</code> <code>float</code> <p>Sampling frequency, in Hz. Only relevant when <code>algorithm=spectral</code>. Defaults to <code>1</code>.</p> <code>1</code> <code>normalize</code> <code>bool</code> <p>If <code>True</code>, divide by \\(log2(psd.size)\\) to normalize the spectral entropy to be between \\(0\\) and \\(1\\). Otherwise, return the spectral entropy in bit. Only relevant when <code>algorithm=spectral</code>. Defaults to <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[float, NDArray[float64]]</code> <p>The calculated regularity (entropy) value.</p> Credit <p>All credit goes to the <code>AntroPy</code> library.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.tests.regularity import regularity\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n&gt;&gt;&gt; normal = data_normal\n</code></pre> Example 1: Sample Entropy<pre><code>&gt;&gt;&gt; print(regularity(x=normal, algorithm=\"sample\"))\n2.2374...\n</code></pre> Example 2: Approx Entropy<pre><code>&gt;&gt;&gt; print(regularity(x=normal, algorithm=\"approx\"))\n1.6643...\n</code></pre> Example 3: Spectral Entropy<pre><code>&gt;&gt;&gt; print(regularity(x=normal, algorithm=\"spectral\", sf=1))\n0.9329...\n</code></pre> References <ul> <li>Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.</li> <li>https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html</li> <li>Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.</li> <li>https://en.wikipedia.org/wiki/Spectral_density</li> <li>https://en.wikipedia.org/wiki/Welch%27s_method</li> </ul> See Also <ul> <li><code>entropy()</code></li> <li><code>approx_entropy()</code></li> <li><code>sample_entropy()</code></li> <li><code>spectral_entropy()</code></li> <li><code>permutation_entropy()</code></li> <li><code>svd_entropy()</code></li> </ul> Source code in <code>src/ts_stat_tests/tests/regularity.py</code> <pre><code>@typechecked\ndef regularity(\n    x: ArrayLike,\n    algorithm: str = \"sample\",\n    order: int = 2,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n    sf: float = 1,\n    normalize: bool = True,\n) -&gt; Union[float, NDArray[np.float64]]:\n    \"\"\"\n    !!! note \"Summary\"\n        Test for the regularity of a given data set.\n\n    ???+ abstract \"Details\"\n        This is a pass-through, convenience wrapper around the [`entropy()`][ts_stat_tests.tests.regularity.entropy] function.\n\n    Params:\n        x (ArrayLike):\n            The data to be checked. Should be a `1-D` or `N-D` data array.\n        algorithm (str, optional):\n            Which entropy algorithm to use.&lt;br&gt;\n            - `sample_entropy()`: `[\"sample\", \"sampl\", \"samp\"]`&lt;br&gt;\n            - `approx_entropy()`: `[\"app\", \"approx\"]`&lt;br&gt;\n            - `spectral_entropy()`: `[\"spec\", \"spect\", \"spectral\"]`&lt;br&gt;\n            - `permutation_entropy()`: `[\"perm\", \"permutation\"]`&lt;br&gt;\n            - `svd_entropy()`: `[\"svd\", \"svd_entropy\"]`&lt;br&gt;\n            Defaults to `\"sample\"`.\n        order (int, optional):\n            Embedding dimension.&lt;br&gt;\n            Only relevant when `algorithm=sample` or `algorithm=approx`.&lt;br&gt;\n            Defaults to `2`.\n        metric (VALID_KDTREE_METRIC_OPTIONS):\n            Name of the distance metric function used with [`sklearn.neighbors.KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree). Default is to use the [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance).&lt;br&gt;\n            Only relevant when `algorithm=sample` or `algorithm=approx`.&lt;br&gt;\n            Defaults to `\"chebyshev\"`.\n        sf (float, optional):\n            Sampling frequency, in Hz.&lt;br&gt;\n            Only relevant when `algorithm=spectral`.&lt;br&gt;\n            Defaults to `1`.\n        normalize (bool, optional):\n            If `True`, divide by $log2(psd.size)$ to normalize the spectral entropy to be between $0$ and $1$. Otherwise, return the spectral entropy in bit.&lt;br&gt;\n            Only relevant when `algorithm=spectral`.&lt;br&gt;\n            Defaults to `True`.\n\n    Returns:\n        (Union[float, NDArray[np.float64]]):\n            The calculated regularity (entropy) value.\n\n    ??? success \"Credit\"\n        All credit goes to the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.tests.regularity import regularity\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n        &gt;&gt;&gt; normal = data_normal\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Sample Entropy\"}\n        &gt;&gt;&gt; print(regularity(x=normal, algorithm=\"sample\"))\n        2.2374...\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Approx Entropy\"}\n        &gt;&gt;&gt; print(regularity(x=normal, algorithm=\"approx\"))\n        1.6643...\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 3: Spectral Entropy\"}\n        &gt;&gt;&gt; print(regularity(x=normal, algorithm=\"spectral\", sf=1))\n        0.9329...\n\n        ```\n\n    ??? question \"References\"\n        - Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n        - https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n        - Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.\n        - https://en.wikipedia.org/wiki/Spectral_density\n        - https://en.wikipedia.org/wiki/Welch%27s_method\n\n    ??? tip \"See Also\"\n        - [`entropy()`][ts_stat_tests.tests.regularity.entropy]\n        - [`approx_entropy()`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`sample_entropy()`][ts_stat_tests.algorithms.regularity.sample_entropy]\n        - [`spectral_entropy()`][ts_stat_tests.algorithms.regularity.spectral_entropy]\n        - [`permutation_entropy()`][ts_stat_tests.algorithms.regularity.permutation_entropy]\n        - [`svd_entropy()`][ts_stat_tests.algorithms.regularity.svd_entropy]\n    \"\"\"\n    return entropy(x=x, algorithm=algorithm, order=order, metric=metric, sf=sf, normalize=normalize)\n</code></pre>"},{"location":"code/regularity/#ts_stat_tests.tests.regularity.is_regular","title":"is_regular","text":"<pre><code>is_regular(\n    x: ArrayLike,\n    algorithm: str = \"sample\",\n    order: int = 2,\n    sf: float = 1,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n    normalize: bool = True,\n    tolerance: Union[str, float, int, None] = \"default\",\n) -&gt; dict[str, Union[str, float, bool]]\n</code></pre> <p>Summary</p> <p>Test whether a given data set is <code>regular</code> or not.</p> Details <p>This function implements the given algorithm (defined in the parameter <code>algorithm</code>), and returns a dictionary containing the relevant data: <pre><code>{\n    \"result\": ...,  # The result of the test. Will be `True` if `entropy&lt;tolerance`, and `False` otherwise\n    \"entropy\": ...,  # A `float` value, the result of the `entropy()` function\n    \"tolerance\": ...,  # A `float` value, which is the tolerance used for determining whether or not the `entropy` is `regular` or not\n}\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked. Should be a <code>1-D</code> or <code>N-D</code> data array.</p> required <code>algorithm</code> <code>str</code> <p>Which entropy algorithm to use. - <code>sample_entropy()</code>: <code>[\"sample\", \"sampl\", \"samp\"]</code> - <code>approx_entropy()</code>: <code>[\"app\", \"approx\"]</code> - <code>spectral_entropy()</code>: <code>[\"spec\", \"spect\", \"spectral\"]</code> - <code>permutation_entropy()</code>: <code>[\"perm\", \"permutation\"]</code> - <code>svd_entropy()</code>: <code>[\"svd\", \"svd_entropy\"]</code> Defaults to <code>\"sample\"</code>.</p> <code>'sample'</code> <code>order</code> <code>int</code> <p>Embedding dimension. Only relevant when <code>algorithm=sample</code> or <code>algorithm=approx</code>. Defaults to <code>2</code>.</p> <code>2</code> <code>metric</code> <code>VALID_KDTREE_METRIC_OPTIONS</code> <p>Name of the distance metric function used with <code>sklearn.neighbors.KDTree</code>. Default is to use the Chebyshev distance. Only relevant when <code>algorithm=sample</code> or <code>algorithm=approx</code>. Defaults to <code>\"chebyshev\"</code>.</p> <code>'chebyshev'</code> <code>sf</code> <code>float</code> <p>Sampling frequency, in Hz. Only relevant when <code>algorithm=spectral</code>. Defaults to <code>1</code>.</p> <code>1</code> <code>normalize</code> <code>bool</code> <p>If <code>True</code>, divide by \\(log2(psd.size)\\) to normalize the spectral entropy to be between \\(0\\) and \\(1\\). Otherwise, return the spectral entropy in bit. Only relevant when <code>algorithm=spectral</code>. Defaults to <code>True</code>.</p> <code>True</code> <code>tolerance</code> <code>Union[str, float, int, None]</code> <p>The tolerance value used to determine whether or not the result is <code>regular</code> or not. - If <code>tolerance</code> is either type <code>int</code> or <code>float</code>, then this value will be used. - If <code>tolerance</code> is either <code>\"default\"</code> or <code>None</code>, then <code>tolerance</code> will be derived from <code>x</code> using the calculation:     <pre><code>tolerance = 0.2 * np.std(a=x)\n</code></pre> - If any other value is given, then a <code>ValueError</code> error will be raised. Defaults to <code>\"default\"</code>.</p> <code>'default'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the given <code>tolerance</code> parameter is invalid.</p> <p>Valid options are:</p> <ul> <li>A number with type <code>float</code> or <code>int</code>, or</li> <li>A string with value <code>default</code>, or</li> <li>The value <code>None</code>.</li> </ul> <p>Returns:</p> Type Description <code>dict[str, Union[str, float, bool]]</code> <p>A dictionary containing the test results:</p> <ul> <li><code>result</code> (bool): <code>True</code> if <code>entropy &lt; tolerance</code>.</li> <li><code>entropy</code> (float): The calculated entropy value.</li> <li><code>tolerance</code> (float): The threshold used for regularity.</li> </ul> Credit <p>All credit goes to the <code>AntroPy</code> library.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.tests.regularity import is_regular\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n&gt;&gt;&gt; normal = data_normal\n</code></pre> Example 1: Sample Entropy<pre><code>&gt;&gt;&gt; print(is_regular(x=normal, algorithm=\"sample\"))\n{'result': False, 'entropy': 2.23743099781426, 'tolerance': 0.20294652904313437}\n</code></pre> Example 2: Approx Entropy<pre><code>&gt;&gt;&gt; print(is_regular(x=normal, algorithm=\"approx\", tolerance=0.5))\n{'result': False, 'entropy': 1.6643808251518548, 'tolerance': 0.5}\n</code></pre> References <ul> <li>Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.</li> <li>https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html</li> <li>Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.</li> <li>https://en.wikipedia.org/wiki/Spectral_density</li> <li>https://en.wikipedia.org/wiki/Welch%27s_method</li> </ul> See Also <ul> <li><code>entropy()</code></li> <li><code>regularity()</code></li> <li><code>approx_entropy()</code></li> <li><code>sample_entropy()</code></li> <li><code>spectral_entropy()</code></li> <li><code>permutation_entropy()</code></li> <li><code>svd_entropy()</code></li> </ul> Source code in <code>src/ts_stat_tests/tests/regularity.py</code> <pre><code>@typechecked\ndef is_regular(\n    x: ArrayLike,\n    algorithm: str = \"sample\",\n    order: int = 2,\n    sf: float = 1,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n    normalize: bool = True,\n    tolerance: Union[str, float, int, None] = \"default\",\n) -&gt; dict[str, Union[str, float, bool]]:\n    \"\"\"\n    !!! note \"Summary\"\n        Test whether a given data set is `regular` or not.\n\n    ???+ abstract \"Details\"\n        This function implements the given algorithm (defined in the parameter `algorithm`), and returns a dictionary containing the relevant data:\n        ```python\n        {\n            \"result\": ...,  # The result of the test. Will be `True` if `entropy&lt;tolerance`, and `False` otherwise\n            \"entropy\": ...,  # A `float` value, the result of the `entropy()` function\n            \"tolerance\": ...,  # A `float` value, which is the tolerance used for determining whether or not the `entropy` is `regular` or not\n        }\n        ```\n\n    Params:\n        x (ArrayLike):\n            The data to be checked. Should be a `1-D` or `N-D` data array.\n        algorithm (str, optional):\n            Which entropy algorithm to use.&lt;br&gt;\n            - `sample_entropy()`: `[\"sample\", \"sampl\", \"samp\"]`&lt;br&gt;\n            - `approx_entropy()`: `[\"app\", \"approx\"]`&lt;br&gt;\n            - `spectral_entropy()`: `[\"spec\", \"spect\", \"spectral\"]`&lt;br&gt;\n            - `permutation_entropy()`: `[\"perm\", \"permutation\"]`&lt;br&gt;\n            - `svd_entropy()`: `[\"svd\", \"svd_entropy\"]`&lt;br&gt;\n            Defaults to `\"sample\"`.\n        order (int, optional):\n            Embedding dimension.&lt;br&gt;\n            Only relevant when `algorithm=sample` or `algorithm=approx`.&lt;br&gt;\n            Defaults to `2`.\n        metric (VALID_KDTREE_METRIC_OPTIONS):\n            Name of the distance metric function used with [`sklearn.neighbors.KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree). Default is to use the [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance).&lt;br&gt;\n            Only relevant when `algorithm=sample` or `algorithm=approx`.&lt;br&gt;\n            Defaults to `\"chebyshev\"`.\n        sf (float, optional):\n            Sampling frequency, in Hz.&lt;br&gt;\n            Only relevant when `algorithm=spectral`.&lt;br&gt;\n            Defaults to `1`.\n        normalize (bool, optional):\n            If `True`, divide by $log2(psd.size)$ to normalize the spectral entropy to be between $0$ and $1$. Otherwise, return the spectral entropy in bit.&lt;br&gt;\n            Only relevant when `algorithm=spectral`.&lt;br&gt;\n            Defaults to `True`.\n        tolerance (Union[str, float, int, None], optional):\n            The tolerance value used to determine whether or not the result is `regular` or not.&lt;br&gt;\n            - If `tolerance` is either type `int` or `float`, then this value will be used.&lt;br&gt;\n            - If `tolerance` is either `\"default\"` or `None`, then `tolerance` will be derived from `x` using the calculation:\n                ```python\n                tolerance = 0.2 * np.std(a=x)\n                ```\n            - If any other value is given, then a `ValueError` error will be raised.&lt;br&gt;\n            Defaults to `\"default\"`.\n\n    Raises:\n        (ValueError):\n            If the given `tolerance` parameter is invalid.\n\n            Valid options are:\n\n            - A number with type `float` or `int`, or\n            - A string with value `default`, or\n            - The value `None`.\n\n    Returns:\n        (dict[str, Union[str, float, bool]]):\n            A dictionary containing the test results:\n\n            - `result` (bool): `True` if `entropy &lt; tolerance`.\n            - `entropy` (float): The calculated entropy value.\n            - `tolerance` (float): The threshold used for regularity.\n\n    ??? success \"Credit\"\n        All credit goes to the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.tests.regularity import is_regular\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n        &gt;&gt;&gt; normal = data_normal\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Sample Entropy\"}\n        &gt;&gt;&gt; print(is_regular(x=normal, algorithm=\"sample\"))\n        {'result': False, 'entropy': 2.23743099781426, 'tolerance': 0.20294652904313437}\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Approx Entropy\"}\n        &gt;&gt;&gt; print(is_regular(x=normal, algorithm=\"approx\", tolerance=0.5))\n        {'result': False, 'entropy': 1.6643808251518548, 'tolerance': 0.5}\n\n        ```\n\n    ??? question \"References\"\n        - Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049.\n        - https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html\n        - Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.\n        - https://en.wikipedia.org/wiki/Spectral_density\n        - https://en.wikipedia.org/wiki/Welch%27s_method\n\n    ??? tip \"See Also\"\n        - [`entropy()`][ts_stat_tests.tests.regularity.entropy]\n        - [`regularity()`][ts_stat_tests.tests.regularity.regularity]\n        - [`approx_entropy()`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`sample_entropy()`][ts_stat_tests.algorithms.regularity.sample_entropy]\n        - [`spectral_entropy()`][ts_stat_tests.algorithms.regularity.spectral_entropy]\n        - [`permutation_entropy()`][ts_stat_tests.algorithms.regularity.permutation_entropy]\n        - [`svd_entropy()`][ts_stat_tests.algorithms.regularity.svd_entropy]\n    \"\"\"\n    if isinstance(tolerance, (float, int)):\n        tol = tolerance\n    elif tolerance in [\"default\", None]:\n        tol = 0.2 * np.std(a=np.asarray(x))\n    else:\n        raise ValueError(\n            f\"Invalid option for `tolerance` parameter: {tolerance}.\\n\"\n            f\"Valid options are:\\n\"\n            f\"- A number with type `float` or `int`,\\n\"\n            f\"- A string with value `default`,\\n\"\n            f\"- The value `None`.\"\n        )\n    value = regularity(x=x, order=order, sf=sf, metric=metric, algorithm=algorithm, normalize=normalize)\n    result = value &lt; tol\n    return {\n        \"result\": bool(result),\n        \"entropy\": float(value),\n        \"tolerance\": float(tol),\n    }\n</code></pre>"},{"location":"code/regularity/#regularity-algorithms","title":"Regularity Algorithms","text":""},{"location":"code/regularity/#ts_stat_tests.algorithms.regularity","title":"ts_stat_tests.algorithms.regularity","text":"<p>Summary</p> <p>This module contains algorithms to compute regularity measures for time series data, including approximate entropy, sample entropy, spectral entropy, and permutation entropy.</p>"},{"location":"code/regularity/#ts_stat_tests.algorithms.regularity.approx_entropy","title":"approx_entropy","text":"<pre><code>approx_entropy(\n    x: ArrayLike,\n    order: int = 2,\n    tolerance: Optional[float] = None,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n) -&gt; float\n</code></pre> <p>Summary</p> <p>Approximate entropy is a measure of the amount of regularity or predictability in a time series. It is used to quantify the degree of self-similarity of a signal over different time scales, and can be useful for detecting underlying patterns or trends in data.</p> <p>This function implements the <code>app_entropy()</code> function from the <code>AntroPy</code> library.</p> Details <p>Approximate entropy is a technique used to quantify the amount of regularity and the unpredictability of fluctuations over time-series data. Smaller values indicate that the data is more regular and predictable.</p> <p>To calculate approximate entropy, we first need to define a window size or scale factor, which determines the length of the subsequences that are used to compare the similarity of the time series. We then compare all possible pairs of subsequences within the time series and calculate the probability that two subsequences are within a certain tolerance level of each other, where the tolerance level is usually expressed as a percentage of the standard deviation of the time series.</p> <p>The approximate entropy is then defined as the negative natural logarithm of the average probability of similarity across all possible pairs of subsequences, normalized by the length of the time series and the scale factor.</p> <p>The approximate entropy measure is useful in a variety of applications, such as the analysis of physiological signals, financial time series, and climate data. It can be used to detect changes in the regularity or predictability of a time series over time, and can provide insights into the underlying dynamics or mechanisms that generate the signal.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>One-dimensional time series of shape <code>(n_times,)</code>.</p> required <code>order</code> <code>int</code> <p>Embedding dimension. Defaults to <code>2</code>.</p> <code>2</code> <code>tolerance</code> <code>Optional[float]</code> <p>Tolerance level or similarity criterion. If <code>None</code> (default), it is set to \\(0.2 \\times \\text{std}(x)\\). Defaults to <code>None</code>.</p> <code>None</code> <code>metric</code> <code>VALID_KDTREE_METRIC_OPTIONS</code> <p>Name of the distance metric function used with <code>sklearn.neighbors.KDTree</code>. Default is to use the Chebyshev distance. For a full list of all available metrics, see <code>sklearn.metrics.pairwise.distance_metrics</code> and <code>scipy.spatial.distance</code> Defaults to <code>\"chebyshev\"</code>.</p> <code>'chebyshev'</code> <p>Returns:</p> Type Description <code>float</code> <p>The approximate entropy score.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.regularity import approx_entropy\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_random\n&gt;&gt;&gt; airline = data_airline.values\n&gt;&gt;&gt; random = data_random\n</code></pre> Example 1: Airline Passengers Data<pre><code>&gt;&gt;&gt; print(f\"{approx_entropy(x=airline):.4f}\")\n0.6451\n</code></pre> Example 2: Random Data<pre><code>&gt;&gt;&gt; print(f\"{approx_entropy(x=random):.4f}\")\n1.8177\n</code></pre> Calculation <p>The equation for ApEn is:</p> \\[ \\text{ApEn}(m, r, N) = \\phi_m(r) - \\phi_{m+1}(r) \\] <p>where:</p> <ul> <li>\\(m\\) is the embedding dimension,</li> <li>\\(r\\) is the tolerance or similarity criterion,</li> <li>\\(N\\) is the length of the time series, and</li> <li>\\(\\phi_m(r)\\) and \\(\\phi_{m+1}(r)\\) are the logarithms of the probabilities that two sequences of \\(m\\) data points in the time series that are similar to each other within a tolerance \\(r\\) remain similar for the next data point, for \\(m\\) and \\(m+1\\), respectively.</li> </ul> Notes <ul> <li>Inputs: <code>x</code> is a 1-dimensional array. It represents time-series data, ideally with each element in the array being a measurement or value taken at regular time intervals.</li> <li>Settings: <code>order</code> is used for determining the number of values that are used to construct each permutation pattern. If the embedding dimension is too small, we may miss important patterns. If it's too large, we may overfit noise.</li> <li>Metric: The Chebyshev metric is often used because it is a robust and computationally efficient way to measure the distance between two time series.</li> </ul> Credit <p>All credit goes to the <code>AntroPy</code> library.</p> References <ul> <li>Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049</li> <li>SK-Learn: Pairwise metrics, Affinities and Kernels</li> <li>Spatial data structures and algorithms</li> </ul> See Also <ul> <li><code>antropy.app_entropy</code></li> <li><code>antropy.sample_entropy</code></li> <li><code>antropy.perm_entropy</code></li> <li><code>antropy.spectral_entropy</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/regularity.py</code> <pre><code>@typechecked\ndef approx_entropy(\n    x: ArrayLike,\n    order: int = 2,\n    tolerance: Optional[float] = None,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n) -&gt; float:\n    r\"\"\"\n    !!! note \"Summary\"\n        Approximate entropy is a measure of the amount of regularity or predictability in a time series. It is used to quantify the degree of self-similarity of a signal over different time scales, and can be useful for detecting underlying patterns or trends in data.\n\n        This function implements the [`app_entropy()`](https://raphaelvallat.com/antropy/build/html/generated/antropy.app_entropy.html) function from the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ???+ abstract \"Details\"\n        Approximate entropy is a technique used to quantify the amount of regularity and the unpredictability of fluctuations over time-series data. Smaller values indicate that the data is more regular and predictable.\n\n        To calculate approximate entropy, we first need to define a window size or scale factor, which determines the length of the subsequences that are used to compare the similarity of the time series. We then compare all possible pairs of subsequences within the time series and calculate the probability that two subsequences are within a certain tolerance level of each other, where the tolerance level is usually expressed as a percentage of the standard deviation of the time series.\n\n        The approximate entropy is then defined as the negative natural logarithm of the average probability of similarity across all possible pairs of subsequences, normalized by the length of the time series and the scale factor.\n\n        The approximate entropy measure is useful in a variety of applications, such as the analysis of physiological signals, financial time series, and climate data. It can be used to detect changes in the regularity or predictability of a time series over time, and can provide insights into the underlying dynamics or mechanisms that generate the signal.\n\n    Params:\n        x (ArrayLike):\n            One-dimensional time series of shape `(n_times,)`.\n        order (int, optional):\n            Embedding dimension.&lt;br&gt;\n            Defaults to `2`.\n        tolerance (Optional[float], optional):\n            Tolerance level or similarity criterion. If `None` (default), it is set to $0.2 \\times \\text{std}(x)$.&lt;br&gt;\n            Defaults to `None`.\n        metric (VALID_KDTREE_METRIC_OPTIONS, optional):\n            Name of the distance metric function used with [`sklearn.neighbors.KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree). Default is to use the [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance). For a full list of all available metrics, see [`sklearn.metrics.pairwise.distance_metrics`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html) and [`scipy.spatial.distance`](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)&lt;br&gt;\n            Defaults to `\"chebyshev\"`.\n\n    Returns:\n        (float):\n            The approximate entropy score.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.regularity import approx_entropy\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_random\n        &gt;&gt;&gt; airline = data_airline.values\n        &gt;&gt;&gt; random = data_random\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Airline Passengers Data\"}\n        &gt;&gt;&gt; print(f\"{approx_entropy(x=airline):.4f}\")\n        0.6451\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Random Data\"}\n        &gt;&gt;&gt; print(f\"{approx_entropy(x=random):.4f}\")\n        1.8177\n\n        ```\n\n    ??? equation \"Calculation\"\n        The equation for ApEn is:\n\n        $$\n        \\text{ApEn}(m, r, N) = \\phi_m(r) - \\phi_{m+1}(r)\n        $$\n\n        where:\n\n        - $m$ is the embedding dimension,\n        - $r$ is the tolerance or similarity criterion,\n        - $N$ is the length of the time series, and\n        - $\\phi_m(r)$ and $\\phi_{m+1}(r)$ are the logarithms of the probabilities that two sequences of $m$ data points in the time series that are similar to each other within a tolerance $r$ remain similar for the next data point, for $m$ and $m+1$, respectively.\n\n    ??? note \"Notes\"\n        - **Inputs**: `x` is a 1-dimensional array. It represents time-series data, ideally with each element in the array being a measurement or value taken at regular time intervals.\n        - **Settings**: `order` is used for determining the number of values that are used to construct each permutation pattern. If the embedding dimension is too small, we may miss important patterns. If it's too large, we may overfit noise.\n        - **Metric**: The Chebyshev metric is often used because it is a robust and computationally efficient way to measure the distance between two time series.\n\n    ??? success \"Credit\"\n        All credit goes to the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ??? question \"References\"\n        - [Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049](https://journals.physiology.org/doi/epdf/10.1152/ajpheart.2000.278.6.H2039)\n        - [SK-Learn: Pairwise metrics, Affinities and Kernels](https://scikit-learn.org/stable/modules/metrics.html#metrics)\n        - [Spatial data structures and algorithms](https://docs.scipy.org/doc/scipy/tutorial/spatial.html)\n\n    ??? tip \"See Also\"\n        - [`antropy.app_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.app_entropy.html)\n        - [`antropy.sample_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.sample_entropy.html)\n        - [`antropy.perm_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.perm_entropy.html)\n        - [`antropy.spectral_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.spectral_entropy.html)\n    \"\"\"\n    return a_app_entropy(\n        x=x,\n        order=order,\n        tolerance=tolerance,\n        metric=metric,\n    )\n</code></pre>"},{"location":"code/regularity/#ts_stat_tests.algorithms.regularity.sample_entropy","title":"sample_entropy","text":"<pre><code>sample_entropy(\n    x: ArrayLike,\n    order: int = 2,\n    tolerance: Optional[float] = None,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n) -&gt; float\n</code></pre> <p>Summary</p> <p>Sample entropy is a measure of the amount of regularity or predictability in a time series. It is used to quantify the degree of self-similarity of a signal over different time scales, and can be useful for detecting underlying patterns or trends in data.</p> <p>This function implements the <code>sample_entropy()</code> function from the <code>AntroPy</code> library.</p> Details <p>Sample entropy is a modification of approximate entropy, used for assessing the complexity of physiological time-series signals. It has two advantages over approximate entropy: data length independence and a relatively trouble-free implementation. Large values indicate high complexity whereas smaller values characterize more self-similar and regular signals.</p> <p>The value of SampEn ranges from zero (\\(0\\)) to infinity (\\(\\infty\\)), with lower values indicating higher regularity or predictability in the time series. A time series with high \\(SampEn\\) is more unpredictable or irregular, whereas a time series with low \\(SampEn\\) is more regular or predictable.</p> <p>Sample entropy is often used in time series forecasting to assess the complexity of the data and to determine whether a time series is suitable for modeling with a particular forecasting method, such as ARIMA or neural networks.</p> <p>Choosing an appropriate embedding dimension is crucial in ensuring that the permutation entropy calculation is robust and reliable, and captures the essential features of the time series in a meaningful way. This allows us to make more accurate and informative inferences about the behavior of the system that generated the data, and can be useful in a wide range of applications, from signal processing to data analysis and beyond.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>One-dimensional time series of shape <code>(n_times,)</code>.</p> required <code>order</code> <code>int</code> <p>Embedding dimension. Defaults to <code>2</code>.</p> <code>2</code> <code>tolerance</code> <code>Optional[float]</code> <p>Tolerance level or similarity criterion. If <code>None</code> (default), it is set to \\(0.2 \\times \\text{std}(x)\\). Defaults to <code>None</code>.</p> <code>None</code> <code>metric</code> <code>VALID_KDTREE_METRIC_OPTIONS</code> <p>Name of the distance metric function used with <code>sklearn.neighbors.KDTree</code>. Default is to use the Chebyshev distance. For a full list of all available metrics, see <code>sklearn.metrics.pairwise.distance_metrics</code> and <code>scipy.spatial.distance</code> Defaults to <code>\"chebyshev\"</code>.</p> <code>'chebyshev'</code> <p>Returns:</p> Type Description <code>float</code> <p>The sample entropy score.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.regularity import sample_entropy\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_random\n&gt;&gt;&gt; airline = data_airline.values\n&gt;&gt;&gt; random = data_random\n</code></pre> Example 1: Airline Passengers Data<pre><code>&gt;&gt;&gt; print(f\"{sample_entropy(x=airline):.4f}\")\n0.6177\n</code></pre> Example 2: Random Data<pre><code>&gt;&gt;&gt; print(f\"{sample_entropy(x=random):.4f}\")\n2.2017\n</code></pre> Calculation <p>The equation for sample entropy (SampEn) is as follows:</p> \\[ \\text{SampEn}(m, r, N) = - \\log \\left( \\frac {C_m(r)} {C_{m+1}(r)} \\right) \\] <p>where:</p> <ul> <li>\\(m\\) is the embedding dimension,</li> <li>\\(r\\) is the tolerance or similarity criterion,</li> <li>\\(N\\) is the length of the time series, and</li> <li>\\(C_m(r)\\) and \\(C_{m+1}(r)\\) are the number of \\(m\\)-tuples (vectors of \\(m\\) consecutive data points) that have a distance less than or equal to \\(r\\), and \\((m+1)\\)-tuples with the same property, respectively.</li> </ul> <p>The calculation of sample entropy involves the following steps:</p> <ol> <li>Choose the values of \\(m\\) and \\(r\\).</li> <li>Construct \\(m\\)-tuples from the time series data.</li> <li>Compute the number of \\(m\\)-tuples that are within a distance \\(r\\) of each other (\\(C_m(r)\\)).</li> <li>Compute the number of \\((m+1)\\)-tuples that are within a distance \\(r\\) of each other (\\(C_{m+1}(r)\\)).</li> <li>Compute the value of \\(SampEn\\) using the formula above.</li> </ol> Notes <ul> <li>Note that if <code>metric == 'chebyshev'</code> and <code>len(x) &lt; 5000</code> points, then the sample entropy is computed using a fast custom Numba script. For other distance metric or longer time-series, the sample entropy is computed using a code from the <code>mne-features</code> package by Jean-Baptiste Schiratti and Alexandre Gramfort (requires sklearn).</li> <li>The embedding dimension is important in the calculation of sample entropy because it affects the sensitivity of the measure to different patterns in the data. If the embedding dimension is too small, we may miss important patterns or variations. If it is too large, we may overfit the data.</li> </ul> Credit <p>All credit goes to the <code>AntroPy</code> library.</p> References <ul> <li>Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049</li> <li>SK-Learn: Pairwise metrics, Affinities and Kernels</li> <li>Spatial data structures and algorithms</li> </ul> See Also <ul> <li><code>antropy.app_entropy</code></li> <li><code>antropy.sample_entropy</code></li> <li><code>antropy.perm_entropy</code></li> <li><code>antropy.spectral_entropy</code></li> <li><code>sklearn.neighbors.KDTree</code></li> <li><code>sklearn.metrics.pairwise_distances</code></li> <li><code>scipy.spatial.distance</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/regularity.py</code> <pre><code>@typechecked\ndef sample_entropy(\n    x: ArrayLike,\n    order: int = 2,\n    tolerance: Optional[float] = None,\n    metric: VALID_KDTREE_METRIC_OPTIONS = \"chebyshev\",\n) -&gt; float:\n    r\"\"\"\n    !!! note \"Summary\"\n        Sample entropy is a measure of the amount of regularity or predictability in a time series. It is used to quantify the degree of self-similarity of a signal over different time scales, and can be useful for detecting underlying patterns or trends in data.\n\n        This function implements the [`sample_entropy()`](https://raphaelvallat.com/antropy/build/html/generated/antropy.sample_entropy.html) function from the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ???+ abstract \"Details\"\n        Sample entropy is a modification of approximate entropy, used for assessing the complexity of physiological time-series signals. It has two advantages over approximate entropy: data length independence and a relatively trouble-free implementation. Large values indicate high complexity whereas smaller values characterize more self-similar and regular signals.\n\n        The value of SampEn ranges from zero ($0$) to infinity ($\\infty$), with lower values indicating higher regularity or predictability in the time series. A time series with high $SampEn$ is more unpredictable or irregular, whereas a time series with low $SampEn$ is more regular or predictable.\n\n        Sample entropy is often used in time series forecasting to assess the complexity of the data and to determine whether a time series is suitable for modeling with a particular forecasting method, such as ARIMA or neural networks.\n\n        Choosing an appropriate embedding dimension is crucial in ensuring that the permutation entropy calculation is robust and reliable, and captures the essential features of the time series in a meaningful way. This allows us to make more accurate and informative inferences about the behavior of the system that generated the data, and can be useful in a wide range of applications, from signal processing to data analysis and beyond.\n\n    Params:\n        x (ArrayLike):\n            One-dimensional time series of shape `(n_times,)`.\n        order (int, optional):\n            Embedding dimension.&lt;br&gt;\n            Defaults to `2`.\n        tolerance (Optional[float], optional):\n            Tolerance level or similarity criterion. If `None` (default), it is set to $0.2 \\times \\text{std}(x)$.&lt;br&gt;\n            Defaults to `None`.\n        metric (VALID_KDTREE_METRIC_OPTIONS, optional):\n            Name of the distance metric function used with [`sklearn.neighbors.KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree). Default is to use the [Chebyshev distance](https://en.wikipedia.org/wiki/Chebyshev_distance). For a full list of all available metrics, see [`sklearn.metrics.pairwise.distance_metrics`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html) and [`scipy.spatial.distance`](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)&lt;br&gt;\n            Defaults to `\"chebyshev\"`.\n\n    Returns:\n        (float):\n            The sample entropy score.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.regularity import sample_entropy\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_random\n        &gt;&gt;&gt; airline = data_airline.values\n        &gt;&gt;&gt; random = data_random\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Airline Passengers Data\"}\n        &gt;&gt;&gt; print(f\"{sample_entropy(x=airline):.4f}\")\n        0.6177\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Random Data\"}\n        &gt;&gt;&gt; print(f\"{sample_entropy(x=random):.4f}\")\n        2.2017\n\n        ```\n\n    ??? equation \"Calculation\"\n        The equation for sample entropy (SampEn) is as follows:\n\n        $$\n        \\text{SampEn}(m, r, N) = - \\log \\left( \\frac {C_m(r)} {C_{m+1}(r)} \\right)\n        $$\n\n        where:\n\n        - $m$ is the embedding dimension,\n        - $r$ is the tolerance or similarity criterion,\n        - $N$ is the length of the time series, and\n        - $C_m(r)$ and $C_{m+1}(r)$ are the number of $m$-tuples (vectors of $m$ consecutive data points) that have a distance less than or equal to $r$, and $(m+1)$-tuples with the same property, respectively.\n\n        The calculation of sample entropy involves the following steps:\n\n        1. Choose the values of $m$ and $r$.\n        2. Construct $m$-tuples from the time series data.\n        3. Compute the number of $m$-tuples that are within a distance $r$ of each other ($C_m(r)$).\n        4. Compute the number of $(m+1)$-tuples that are within a distance $r$ of each other ($C_{m+1}(r)$).\n        5. Compute the value of $SampEn$ using the formula above.\n\n    ??? note \"Notes\"\n        - Note that if `metric == 'chebyshev'` and `len(x) &lt; 5000` points, then the sample entropy is computed using a fast custom Numba script. For other distance metric or longer time-series, the sample entropy is computed using a code from the [`mne-features`](https://mne.tools/mne-features/) package by Jean-Baptiste Schiratti and Alexandre Gramfort (requires sklearn).\n        - The embedding dimension is important in the calculation of sample entropy because it affects the sensitivity of the measure to different patterns in the data. If the embedding dimension is too small, we may miss important patterns or variations. If it is too large, we may overfit the data.\n\n    ??? success \"Credit\"\n        All credit goes to the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ??? question \"References\"\n        - [Richman, J. S. et al. (2000). Physiological time-series analysis using approximate entropy and sample entropy. American Journal of Physiology-Heart and Circulatory Physiology, 278(6), H2039-H2049](https://journals.physiology.org/doi/epdf/10.1152/ajpheart.2000.278.6.H2039)\n        - [SK-Learn: Pairwise metrics, Affinities and Kernels](https://scikit-learn.org/stable/modules/metrics.html#metrics)\n        - [Spatial data structures and algorithms](https://docs.scipy.org/doc/scipy/tutorial/spatial.html)\n\n    ??? tip \"See Also\"\n        - [`antropy.app_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.app_entropy.html)\n        - [`antropy.sample_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.sample_entropy.html)\n        - [`antropy.perm_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.perm_entropy.html)\n        - [`antropy.spectral_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.spectral_entropy.html)\n        - [`sklearn.neighbors.KDTree`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html)\n        - [`sklearn.metrics.pairwise_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html)\n        - [`scipy.spatial.distance`](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)\n    \"\"\"\n    return a_sample_entropy(\n        x=x,\n        order=order,\n        tolerance=tolerance,\n        metric=metric,\n    )\n</code></pre>"},{"location":"code/regularity/#ts_stat_tests.algorithms.regularity.permutation_entropy","title":"permutation_entropy","text":"<pre><code>permutation_entropy(\n    x: ArrayLike,\n    order: int = 3,\n    delay: Union[int, list, NDArray[int64]] = 1,\n    normalize: bool = False,\n) -&gt; float\n</code></pre> <p>Summary</p> <p>Permutation entropy is a measure of the complexity or randomness of a time series. It is based on the idea of permuting the order of the values in the time series and calculating the entropy of the resulting permutation patterns.</p> <p>This function implements the <code>perm_entropy()</code> function from the <code>AntroPy</code> library.</p> Details <p>The permutation entropy is a complexity measure for time-series first introduced by Bandt and Pompe in 2002.</p> <p>It is particularly useful for detecting nonlinear dynamics and nonstationarity in the data. The value of permutation entropy ranges from \\(0\\) to \\(\\log_2(\\text{order}!)\\), where the lower bound is attained for an increasing or decreasing sequence of values, and the upper bound for a completely random system where all possible permutations appear with the same probability.</p> <p>Choosing an appropriate embedding dimension is crucial in ensuring that the permutation entropy calculation is robust and reliable, and captures the essential features of the time series in a meaningful way.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>One-dimensional time series of shape <code>(n_times,)</code>.</p> required <code>order</code> <code>int</code> <p>Order of permutation entropy. Defaults to <code>3</code>.</p> <code>3</code> <code>delay</code> <code>Union[int, list, NDArray[int64]]</code> <p>Time delay (lag). If multiple values are passed, the average permutation entropy across all these delays is calculated. Defaults to <code>1</code>.</p> <code>1</code> <code>normalize</code> <code>bool</code> <p>If <code>True</code>, divide by \\(\\log_2(\\text{order}!)\\) to normalize the entropy between \\(0\\) and \\(1\\). Otherwise, return the permutation entropy in bits. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[float, NDArray[float64]]</code> <p>The permutation entropy of the data set.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.regularity import permutation_entropy\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_random\n&gt;&gt;&gt; airline = data_airline.values\n&gt;&gt;&gt; random = data_random\n</code></pre> Example 1: Airline Passengers Data<pre><code>&gt;&gt;&gt; print(f\"{permutation_entropy(x=airline):.4f}\")\n2.3601\n</code></pre> Example 2: Random Data (Normalized)<pre><code>&gt;&gt;&gt; print(f\"{permutation_entropy(x=random, normalize=True):.4f}\")\n0.9997\n</code></pre> Calculation <p>The formula for permutation entropy (\\(PE\\)) is as follows:</p> \\[ PE(n) = - \\sum_{i=0}^{n!} p(i) \\times \\log_2(p(i)) \\] <p>where:</p> <ul> <li>\\(n\\) is the embedding dimension (<code>order</code>),</li> <li>\\(p(i)\\) is the probability of the \\(i\\)-th ordinal pattern.</li> </ul> <p>The embedded matrix \\(Y\\) is created by:</p> \\[ \\begin{align}     y(i) &amp;= [x_i, x_{i+\\text{delay}}, \\dots, x_{i+(\\text{order}-1) \\times \\text{delay}}] \\\\     Y &amp;= [y(1), y(2), \\dots, y(N-(\\text{order}-1) \\times \\text{delay})]^T \\end{align} \\] Notes <ul> <li>The embedding dimension (<code>order</code>) determines the number of values used to construct each permutation pattern. If too small, patterns may be missed. If too large, overfitting to noise may occur.</li> </ul> Credit <p>All credit goes to the <code>AntroPy</code> library.</p> References <ul> <li>Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for time series.\" Physical review letters 88.17 (2002): 174102</li> </ul> See Also <ul> <li><code>antropy.perm_entropy</code></li> <li><code>antropy.app_entropy</code></li> <li><code>antropy.sample_entropy</code></li> <li><code>antropy.spectral_entropy</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/regularity.py</code> <pre><code>@typechecked\ndef permutation_entropy(\n    x: ArrayLike,\n    order: int = 3,\n    delay: Union[int, list, NDArray[np.int64]] = 1,\n    normalize: bool = False,\n) -&gt; float:\n    r\"\"\"\n    !!! note \"Summary\"\n        Permutation entropy is a measure of the complexity or randomness of a time series. It is based on the idea of permuting the order of the values in the time series and calculating the entropy of the resulting permutation patterns.\n\n        This function implements the [`perm_entropy()`](https://raphaelvallat.com/antropy/build/html/generated/antropy.perm_entropy.html) function from the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ???+ abstract \"Details\"\n        The permutation entropy is a complexity measure for time-series first introduced by Bandt and Pompe in 2002.\n\n        It is particularly useful for detecting nonlinear dynamics and nonstationarity in the data. The value of permutation entropy ranges from $0$ to $\\log_2(\\text{order}!)$, where the lower bound is attained for an increasing or decreasing sequence of values, and the upper bound for a completely random system where all possible permutations appear with the same probability.\n\n        Choosing an appropriate embedding dimension is crucial in ensuring that the permutation entropy calculation is robust and reliable, and captures the essential features of the time series in a meaningful way.\n\n    Params:\n        x (ArrayLike):\n            One-dimensional time series of shape `(n_times,)`.\n        order (int, optional):\n            Order of permutation entropy.&lt;br&gt;\n            Defaults to `3`.\n        delay (Union[int, list, NDArray[np.int64]], optional):\n            Time delay (lag). If multiple values are passed, the average permutation entropy across all these delays is calculated.&lt;br&gt;\n            Defaults to `1`.\n        normalize (bool, optional):\n            If `True`, divide by $\\log_2(\\text{order}!)$ to normalize the entropy between $0$ and $1$. Otherwise, return the permutation entropy in bits.&lt;br&gt;\n            Defaults to `False`.\n\n    Returns:\n        (Union[float, NDArray[np.float64]]):\n            The permutation entropy of the data set.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.regularity import permutation_entropy\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_random\n        &gt;&gt;&gt; airline = data_airline.values\n        &gt;&gt;&gt; random = data_random\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Airline Passengers Data\"}\n        &gt;&gt;&gt; print(f\"{permutation_entropy(x=airline):.4f}\")\n        2.3601\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Random Data (Normalized)\"}\n        &gt;&gt;&gt; print(f\"{permutation_entropy(x=random, normalize=True):.4f}\")\n        0.9997\n\n        ```\n\n    ??? equation \"Calculation\"\n        The formula for permutation entropy ($PE$) is as follows:\n\n        $$\n        PE(n) = - \\sum_{i=0}^{n!} p(i) \\times \\log_2(p(i))\n        $$\n\n        where:\n\n        - $n$ is the embedding dimension (`order`),\n        - $p(i)$ is the probability of the $i$-th ordinal pattern.\n\n        The embedded matrix $Y$ is created by:\n\n        $$\n        \\begin{align}\n            y(i) &amp;= [x_i, x_{i+\\text{delay}}, \\dots, x_{i+(\\text{order}-1) \\times \\text{delay}}] \\\\\n            Y &amp;= [y(1), y(2), \\dots, y(N-(\\text{order}-1) \\times \\text{delay})]^T\n        \\end{align}\n        $$\n\n    ??? note \"Notes\"\n        - The embedding dimension (`order`) determines the number of values used to construct each permutation pattern. If too small, patterns may be missed. If too large, overfitting to noise may occur.\n\n    ??? success \"Credit\"\n        All credit goes to the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ??? question \"References\"\n        - [Bandt, Christoph, and Bernd Pompe. \"Permutation entropy: a natural complexity measure for time series.\" Physical review letters 88.17 (2002): 174102](http://materias.df.uba.ar/dnla2019c1/files/2019/03/permutation_entropy.pdf)\n\n    ??? tip \"See Also\"\n        - [`antropy.perm_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.perm_entropy.html)\n        - [`antropy.app_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.app_entropy.html)\n        - [`antropy.sample_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.sample_entropy.html)\n        - [`antropy.spectral_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.spectral_entropy.html)\n    \"\"\"\n    return a_perm_entropy(\n        x=x,\n        order=order,\n        delay=delay,  # type: ignore[arg-type]  # antropy function can handle Union[int, list[int], NDArray[np.int64]], however the function signature is not annotated as such\n        normalize=normalize,\n    )\n</code></pre>"},{"location":"code/regularity/#ts_stat_tests.algorithms.regularity.spectral_entropy","title":"spectral_entropy","text":"<pre><code>spectral_entropy(\n    x: ArrayLike,\n    sf: float = 1,\n    method: VALID_SPECTRAL_ENTROPY_METHOD_OPTIONS = \"fft\",\n    nperseg: Optional[int] = None,\n    normalize: bool = False,\n    axis: int = -1,\n) -&gt; Union[float, NDArray[np.float64]]\n</code></pre> <p>Summary</p> <p>Spectral entropy is a measure of the amount of complexity or unpredictability in a signal's frequency domain representation. It is used to quantify the degree of randomness or regularity in the power spectrum of a signal.</p> <p>This function implements the <code>spectral_entropy()</code> function from the <code>AntroPy</code> library.</p> Details <p>Spectral Entropy is defined to be the Shannon entropy of the power spectral density (PSD) of the data. It is based on the Shannon entropy, which is a measure of the uncertainty or information content of a probability distribution.</p> <p>The value of spectral entropy ranges from \\(0\\) to \\(\\log_2(N)\\), where \\(N\\) is the number of frequency bands. Lower values indicate a more concentrated or regular distribution of power, while higher values indicate a more spread-out or irregular distribution.</p> <p>Spectral entropy is particularly useful for detecting periodicity and cyclical patterns, as well as changes in the frequency distribution over time.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>One-dimensional or N-dimensional data array.</p> required <code>sf</code> <code>float</code> <p>Sampling frequency, in Hz. Defaults to <code>1</code>.</p> <code>1</code> <code>method</code> <code>VALID_SPECTRAL_ENTROPY_METHOD_OPTIONS</code> <p>Spectral estimation method: <code>'fft'</code> or <code>'welch'</code>. - <code>'fft'</code>: Fourier Transformation (<code>scipy.signal.periodogram()</code>) - <code>'welch'</code>: Welch periodogram (<code>scipy.signal.welch()</code>) Defaults to <code>\"fft\"</code>.</p> <code>'fft'</code> <code>nperseg</code> <code>Optional[int]</code> <p>Length of each FFT segment for Welch method. If <code>None</code>, uses <code>scipy</code>'s default of 256 samples. Defaults to <code>None</code>.</p> <code>None</code> <code>normalize</code> <code>bool</code> <p>If <code>True</code>, divide by \\(\\log_2(\\text{psd.size})\\) to normalize the spectral entropy to be between \\(0\\) and \\(1\\). Otherwise, return the spectral entropy in bits. Defaults to <code>False</code>.</p> <code>False</code> <code>axis</code> <code>int</code> <p>The axis along which the entropy is calculated. Default is the last axis. Defaults to <code>-1</code>.</p> <code>-1</code> <p>Returns:</p> Type Description <code>Union[float, NDArray[float64]]</code> <p>The spectral entropy score. Returned as a float for 1D input, or a numpy array for N-dimensional input.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.regularity import spectral_entropy\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline\n&gt;&gt;&gt; airline = data_airline.values\n</code></pre> Example 1: Airline Passengers Data<pre><code>&gt;&gt;&gt; print(f\"{spectral_entropy(x=airline, sf=12):.4f}\")\n2.6538\n</code></pre> Example 2: Welch method for spectral entropy<pre><code>&gt;&gt;&gt; data_sine = np.sin(2 * np.pi * 1 * np.arange(400) / 100)\n&gt;&gt;&gt; print(f\"{spectral_entropy(x=data_sine, sf=100, method='welch'):.4f}\")\n1.2938\n</code></pre> Calculation <p>The spectral entropy (\\(SE\\)) is defined as:</p> \\[ H(x, f_s) = - \\sum_{i=0}^{f_s/2} P(i) \\times \\log_2(P(i)) \\] <p>where:</p> <ul> <li>\\(P(i)\\) is the normalized power spectral density (PSD) at the \\(i\\)-th frequency band,</li> <li>\\(f_s\\) is the sampling frequency.</li> </ul> Notes <ul> <li>The power spectrum represents the energy of the signal at different frequencies. High spectral entropy indicates multiple sources or processes with different frequencies, while low spectral entropy suggests a dominant frequency or periodicity.</li> </ul> Credit <p>All credit goes to the <code>AntroPy</code> library.</p> References <ul> <li>Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.</li> <li>Wikipedia: Spectral density</li> <li>Wikipedia: Welch's method</li> </ul> See Also <ul> <li><code>antropy.spectral_entropy</code></li> <li><code>antropy.app_entropy</code></li> <li><code>antropy.sample_entropy</code></li> <li><code>antropy.perm_entropy</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/regularity.py</code> <pre><code>@typechecked\ndef spectral_entropy(\n    x: ArrayLike,\n    sf: float = 1,\n    method: VALID_SPECTRAL_ENTROPY_METHOD_OPTIONS = \"fft\",\n    nperseg: Optional[int] = None,\n    normalize: bool = False,\n    axis: int = -1,\n) -&gt; Union[float, NDArray[np.float64]]:\n    r\"\"\"\n    !!! note \"Summary\"\n        Spectral entropy is a measure of the amount of complexity or unpredictability in a signal's frequency domain representation. It is used to quantify the degree of randomness or regularity in the power spectrum of a signal.\n\n        This function implements the [`spectral_entropy()`](https://raphaelvallat.com/antropy/build/html/generated/antropy.spectral_entropy.html) function from the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ???+ abstract \"Details\"\n        Spectral Entropy is defined to be the Shannon entropy of the power spectral density (PSD) of the data. It is based on the Shannon entropy, which is a measure of the uncertainty or information content of a probability distribution.\n\n        The value of spectral entropy ranges from $0$ to $\\log_2(N)$, where $N$ is the number of frequency bands. Lower values indicate a more concentrated or regular distribution of power, while higher values indicate a more spread-out or irregular distribution.\n\n        Spectral entropy is particularly useful for detecting periodicity and cyclical patterns, as well as changes in the frequency distribution over time.\n\n    Params:\n        x (ArrayLike):\n            One-dimensional or N-dimensional data array.\n        sf (float, optional):\n            Sampling frequency, in Hz.&lt;br&gt;\n            Defaults to `1`.\n        method (VALID_SPECTRAL_ENTROPY_METHOD_OPTIONS, optional):\n            Spectral estimation method: `'fft'` or `'welch'`.&lt;br&gt;\n            - `'fft'`: Fourier Transformation ([`scipy.signal.periodogram()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.periodogram.html#scipy.signal.periodogram))&lt;br&gt;\n            - `'welch'`: Welch periodogram ([`scipy.signal.welch()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.welch.html#scipy.signal.welch))&lt;br&gt;\n            Defaults to `\"fft\"`.\n        nperseg (Optional[int], optional):\n            Length of each FFT segment for Welch method. If `None`, uses `scipy`'s default of 256 samples.&lt;br&gt;\n            Defaults to `None`.\n        normalize (bool, optional):\n            If `True`, divide by $\\log_2(\\text{psd.size})$ to normalize the spectral entropy to be between $0$ and $1$. Otherwise, return the spectral entropy in bits.&lt;br&gt;\n            Defaults to `False`.\n        axis (int, optional):\n            The axis along which the entropy is calculated. Default is the last axis.&lt;br&gt;\n            Defaults to `-1`.\n\n    Returns:\n        (Union[float, NDArray[np.float64]]):\n            The spectral entropy score. Returned as a float for 1D input, or a numpy array for N-dimensional input.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.regularity import spectral_entropy\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline\n        &gt;&gt;&gt; airline = data_airline.values\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Airline Passengers Data\"}\n        &gt;&gt;&gt; print(f\"{spectral_entropy(x=airline, sf=12):.4f}\")\n        2.6538\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Welch method for spectral entropy\"}\n        &gt;&gt;&gt; data_sine = np.sin(2 * np.pi * 1 * np.arange(400) / 100)\n        &gt;&gt;&gt; print(f\"{spectral_entropy(x=data_sine, sf=100, method='welch'):.4f}\")\n        1.2938\n\n        ```\n\n    ??? equation \"Calculation\"\n        The spectral entropy ($SE$) is defined as:\n\n        $$\n        H(x, f_s) = - \\sum_{i=0}^{f_s/2} P(i) \\times \\log_2(P(i))\n        $$\n\n        where:\n\n        - $P(i)$ is the normalized power spectral density (PSD) at the $i$-th frequency band,\n        - $f_s$ is the sampling frequency.\n\n    ??? note \"Notes\"\n        - The power spectrum represents the energy of the signal at different frequencies. High spectral entropy indicates multiple sources or processes with different frequencies, while low spectral entropy suggests a dominant frequency or periodicity.\n\n    ??? success \"Credit\"\n        All credit goes to the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ??? question \"References\"\n        - [Inouye, T. et al. (1991). Quantification of EEG irregularity by use of the entropy of the power spectrum. Electroencephalography and clinical neurophysiology, 79(3), 204-210.](https://pubmed.ncbi.nlm.nih.gov/1714811/)\n        - [Wikipedia: Spectral density](https://en.wikipedia.org/wiki/Spectral_density)\n        - [Wikipedia: Welch's method](https://en.wikipedia.org/wiki/Welch%27s_method)\n\n    ??? tip \"See Also\"\n        - [`antropy.spectral_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.spectral_entropy.html)\n        - [`antropy.app_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.app_entropy.html)\n        - [`antropy.sample_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.sample_entropy.html)\n        - [`antropy.perm_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.perm_entropy.html)\n    \"\"\"\n    return a_spectral_entropy(\n        x=x,\n        sf=sf,\n        method=method,\n        nperseg=nperseg,\n        normalize=normalize,\n        axis=axis,\n    )\n</code></pre>"},{"location":"code/regularity/#ts_stat_tests.algorithms.regularity.svd_entropy","title":"svd_entropy","text":"<pre><code>svd_entropy(\n    x: ArrayLike,\n    order: int = 3,\n    delay: int = 1,\n    normalize: bool = False,\n) -&gt; float\n</code></pre> <p>Summary</p> <p>SVD entropy is a measure of the complexity or randomness of a time series based on Singular Value Decomposition (SVD).</p> <p>This function implements the <code>svd_entropy()</code> function from the <code>AntroPy</code> library.</p> Details <p>SVD entropy is calculated by first embedding the time series into a matrix, then performing SVD on that matrix to obtain the singular values. The entropy is then calculated from the normalized singular values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>One-dimensional time series of shape <code>(n_times,)</code>.</p> required <code>order</code> <code>int</code> <p>Order of the SVD entropy (embedding dimension). Defaults to <code>3</code>.</p> <code>3</code> <code>delay</code> <code>int</code> <p>Time delay (lag). Defaults to <code>1</code>.</p> <code>1</code> <code>normalize</code> <code>bool</code> <p>If <code>True</code>, divide by \\(\\log_2(\\text{order}!)\\) to normalize the entropy between \\(0\\) and \\(1\\). Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>float</code> <p>The SVD entropy of the data set.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.regularity import svd_entropy\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_random\n&gt;&gt;&gt; random = data_random\n</code></pre> Example 1: Basic SVD entropy<pre><code>&gt;&gt;&gt; print(f\"{svd_entropy(random):.4f}\")\n1.3514\n</code></pre> Calculation <p>The SVD entropy is calculated as the Shannon entropy of the singular values of the embedded matrix.</p> Notes <ul> <li>Singular Value Decomposition (SVD) is a factorization of a real or complex matrix. It is the generalization of the eigendecomposition of a positive semidefinite normal matrix.</li> </ul> Credit <p>All credit goes to the <code>AntroPy</code> library.</p> See Also <ul> <li><code>antropy.svd_entropy</code></li> <li><code>ts_stat_tests.algorithms.regularity.approx_entropy</code></li> <li><code>ts_stat_tests.algorithms.regularity.sample_entropy</code></li> <li><code>ts_stat_tests.algorithms.regularity.permutation_entropy</code></li> <li><code>ts_stat_tests.algorithms.regularity.spectral_entropy</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/regularity.py</code> <pre><code>@typechecked\ndef svd_entropy(\n    x: ArrayLike,\n    order: int = 3,\n    delay: int = 1,\n    normalize: bool = False,\n) -&gt; float:\n    r\"\"\"\n    !!! note \"Summary\"\n        SVD entropy is a measure of the complexity or randomness of a time series based on Singular Value Decomposition (SVD).\n\n        This function implements the [`svd_entropy()`](https://raphaelvallat.com/antropy/build/html/generated/antropy.svd_entropy.html) function from the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ???+ abstract \"Details\"\n        SVD entropy is calculated by first embedding the time series into a matrix, then performing SVD on that matrix to obtain the singular values. The entropy is then calculated from the normalized singular values.\n\n    Params:\n        x (ArrayLike):\n            One-dimensional time series of shape `(n_times,)`.\n        order (int, optional):\n            Order of the SVD entropy (embedding dimension).&lt;br&gt;\n            Defaults to `3`.\n        delay (int, optional):\n            Time delay (lag).&lt;br&gt;\n            Defaults to `1`.\n        normalize (bool, optional):\n            If `True`, divide by $\\log_2(\\text{order}!)$ to normalize the entropy between $0$ and $1$.&lt;br&gt;\n            Defaults to `False`.\n\n    Returns:\n        (float):\n            The SVD entropy of the data set.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.regularity import svd_entropy\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_random\n        &gt;&gt;&gt; random = data_random\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Basic SVD entropy\"}\n        &gt;&gt;&gt; print(f\"{svd_entropy(random):.4f}\")\n        1.3514\n\n        ```\n\n    ??? equation \"Calculation\"\n        The SVD entropy is calculated as the Shannon entropy of the singular values of the embedded matrix.\n\n    ??? note \"Notes\"\n        - Singular Value Decomposition (SVD) is a factorization of a real or complex matrix. It is the generalization of the eigendecomposition of a positive semidefinite normal matrix.\n\n    ??? success \"Credit\"\n        All credit goes to the [`AntroPy`](https://raphaelvallat.com/antropy/) library.\n\n    ??? tip \"See Also\"\n        - [`antropy.svd_entropy`](https://raphaelvallat.com/antropy/build/html/generated/antropy.svd_entropy.html)\n        - [`ts_stat_tests.algorithms.regularity.approx_entropy`][ts_stat_tests.algorithms.regularity.approx_entropy]\n        - [`ts_stat_tests.algorithms.regularity.sample_entropy`][ts_stat_tests.algorithms.regularity.sample_entropy]\n        - [`ts_stat_tests.algorithms.regularity.permutation_entropy`][ts_stat_tests.algorithms.regularity.permutation_entropy]\n        - [`ts_stat_tests.algorithms.regularity.spectral_entropy`][ts_stat_tests.algorithms.regularity.spectral_entropy]\n    \"\"\"\n    return a_svd_entropy(\n        x=x,\n        order=order,\n        delay=delay,\n        normalize=normalize,\n    )\n</code></pre>"},{"location":"code/seasonality/","title":"Test the <code>seasonality</code> of a given Time-Series Dataset","text":""},{"location":"code/seasonality/#introduction","title":"Introduction","text":"<p>Summary</p> <p>As stated by Rob Hyndman and George Athanasopoulos</p> <p>In describing these time series, we have used words such as \"trend\" and \"seasonal\" which need to be defined more carefully.</p> <p>Trend A trend exists when there is a long-term increase or decrease in the data. It does not have to be linear. Sometimes we will refer to a trend as \"changing direction\", when it might go from an increasing trend to a decreasing trend.</p> <p>Seasonal A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. Seasonality is always of a fixed and known frequency.</p> <p>Cyclic A cycle occurs when the data exhibit rises and falls that are not of a fixed frequency. These fluctuations are usually due to economic conditions, and are often related to the \"business cycle\". The duration of these fluctuations is usually at least 2 years.</p> <p> For more info, see: Forecasting: Principles and Practice - Time Series Patterns</p> Information Details Module ts_stat_tests.tests.seasonality Algorithms <code>qs</code>, <code>ocsb</code>, <code>ch</code>, <code>seasonal_strength</code>, <code>trend_strength</code>, <code>spikiness</code> Complexity \\(O(n \\log n)\\) to \\(O(n^2)\\) depending on algorithm Implementation <code>statsmodels</code>, <code>pmdarima</code>, <code>tsfeatures</code> <p>Source Library</p> <p>We leverage several industry-standard libraries for the underlying statistical tests in this module.</p> <ol> <li><code>pmdarima</code>: Provides the implementation for the Canova-Hansen (<code>ch</code>) and Osborn-Chui-Smith-Birchenhall (<code>ocsb</code>) tests through its <code>nsdiffs</code> and estimator classes.</li> <li><code>tsfeatures</code>: Used as the basis for calculating seasonal strength, trend strength, and spikiness, following the approach popularized by the R <code>feasts</code> and <code>tsfeatures</code> packages.</li> <li><code>seastests</code>: An R package that served as the primary reference and inspiration for our Python implementation of the Quenouille-Sarle (<code>qs</code>) test.</li> </ol> <p>Source Module</p> <p>The source code for the seasonality tests is organized into two primary layers:</p> <ul> <li><code>src.ts_stat_tests.algorithms.seasonality</code>: Contains the core mathematical implementations and wrappers for third-party libraries.</li> <li><code>src.ts_stat_tests.tests.seasonality</code>: Provides the top-level user interface, including the <code>seasonality</code> dispatcher and the <code>is_seasonal</code> boolean check.</li> </ul>"},{"location":"code/seasonality/#seasonality-tests","title":"Seasonality Tests","text":""},{"location":"code/seasonality/#ts_stat_tests.tests.seasonality","title":"ts_stat_tests.tests.seasonality","text":"<p>Summary</p> <p>This module contains functions to assess the seasonality of time series data.</p> <p>The implemented algorithms include:</p> <ul> <li>QS Test</li> <li>OCSB Test</li> <li>CH Test</li> <li>Seasonal Strength</li> <li>Trend Strength</li> <li>Spikiness</li> </ul> <p>Each function is designed to analyze a univariate time series and return relevant statistics or indicators of seasonality. This module provides both a dispatcher for flexible algorithm selection and a boolean check for easy integration into pipelines.</p>"},{"location":"code/seasonality/#ts_stat_tests.tests.seasonality.seasonality","title":"seasonality","text":"<pre><code>seasonality(\n    x: ArrayLike,\n    algorithm: str = \"qs\",\n    **kwargs: Union[float, int, str, bool, ArrayLike, None]\n) -&gt; Union[\n    float, int, tuple[Union[float, int, ARIMA, None], ...]\n]\n</code></pre> <p>Summary</p> <p>Dispatcher for seasonality algorithms. This function provides a unified interface to call various seasonality tests.</p> Details <p>The <code>seasonality</code> function acts as a centralized dispatcher for the various seasonality algorithms implemented in the <code>algorithms.seasonality</code> module. It allows users to easily switch between different tests by specifying the <code>algorithm</code> name.</p> <p>The supported algorithms include:</p> <ul> <li><code>\"qs\"</code>: The QS (Quenouille-Sarle) test for seasonality.</li> <li><code>\"ocsb\"</code>: The Osborn-Chui-Smith-Birchenhall test for seasonal differencing.</li> <li><code>\"ch\"</code>: The Canova-Hansen test for seasonal stability.</li> <li><code>\"seasonal_strength\"</code> (or <code>\"ss\"</code>): The STL-based seasonal strength measure.</li> <li><code>\"trend_strength\"</code> (or <code>\"ts\"</code>): The STL-based trend strength measure.</li> <li><code>\"spikiness\"</code>: The STL-based spikiness measure.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked.</p> required <code>algorithm</code> <code>str</code> <p>Which seasonality algorithm to use. Default: <code>\"qs\"</code></p> <code>'qs'</code> <code>kwargs</code> <code>Union[float, int, str, bool, ArrayLike, None]</code> <p>Additional arguments to pass to the underlying algorithm.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[float, int, tuple[Union[float, int, object, None], ...]]</code> <p>The result of the seasonality test. The return type depends on the chosen algorithm: - <code>\"qs\"</code> returns a tuple <code>(statistic, pvalue)</code>. - <code>\"ocsb\"</code> and <code>\"ch\"</code> return an integer (0 or 1). - <code>\"seasonal_strength\"</code>, <code>\"trend_strength\"</code>, and <code>\"spikiness\"</code> return a float.</p> Examples Basic usage<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.tests.seasonality import seasonality\n&gt;&gt;&gt; data = load_airline().values\n&gt;&gt;&gt; # Using the default QS test\n&gt;&gt;&gt; seasonality(x=data, freq=12)\n(194.469289..., 5.909223...-43)\n&gt;&gt;&gt; # Using seasonal strength\n&gt;&gt;&gt; seasonality(x=data, algorithm=\"ss\", m=12)\n0.778721...\n</code></pre> Source code in <code>src/ts_stat_tests/tests/seasonality.py</code> <pre><code>@typechecked\ndef seasonality(\n    x: ArrayLike,\n    algorithm: str = \"qs\",\n    **kwargs: Union[float, int, str, bool, ArrayLike, None],\n) -&gt; Union[float, int, tuple[Union[float, int, ARIMA, None], ...]]:\n    \"\"\"\n    !!! note \"Summary\"\n        Dispatcher for seasonality algorithms. This function provides a unified interface to call various seasonality tests.\n\n    ???+ abstract \"Details\"\n\n        The `seasonality` function acts as a centralized dispatcher for the various seasonality algorithms implemented in the `algorithms.seasonality` module. It allows users to easily switch between different tests by specifying the `algorithm` name.\n\n        The supported algorithms include:\n\n        - `\"qs\"`: The QS (Quenouille-Sarle) test for seasonality.\n        - `\"ocsb\"`: The Osborn-Chui-Smith-Birchenhall test for seasonal differencing.\n        - `\"ch\"`: The Canova-Hansen test for seasonal stability.\n        - `\"seasonal_strength\"` (or `\"ss\"`): The STL-based seasonal strength measure.\n        - `\"trend_strength\"` (or `\"ts\"`): The STL-based trend strength measure.\n        - `\"spikiness\"`: The STL-based spikiness measure.\n\n    Params:\n        x (ArrayLike):\n            The data to be checked.\n        algorithm (str, optional):\n            Which seasonality algorithm to use.&lt;br&gt;\n            Default: `\"qs\"`\n        kwargs (Union[float, int, str, bool, ArrayLike, None]):\n            Additional arguments to pass to the underlying algorithm.\n\n    Returns:\n        (Union[float, int, tuple[Union[float, int, object, None], ...]]):\n            The result of the seasonality test. The return type depends on the chosen algorithm:\n            - `\"qs\"` returns a tuple `(statistic, pvalue)`.\n            - `\"ocsb\"` and `\"ch\"` return an integer (0 or 1).\n            - `\"seasonal_strength\"`, `\"trend_strength\"`, and `\"spikiness\"` return a float.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.tests.seasonality import seasonality\n        &gt;&gt;&gt; data = load_airline().values\n        &gt;&gt;&gt; # Using the default QS test\n        &gt;&gt;&gt; seasonality(x=data, freq=12)\n        (194.469289..., 5.909223...-43)\n        &gt;&gt;&gt; # Using seasonal strength\n        &gt;&gt;&gt; seasonality(x=data, algorithm=\"ss\", m=12)\n        0.778721...\n\n        ```\n    \"\"\"\n    options: dict[str, tuple[str, ...]] = {\n        \"qs\": (\"qs\",),\n        \"ocsb\": (\"ocsb\",),\n        \"ch\": (\"ch\",),\n        \"seasonal_strength\": (\"seasonal_strength\", \"ss\"),\n        \"trend_strength\": (\"trend_strength\", \"ts\"),\n        \"spikiness\": (\"spikiness\",),\n    }\n\n    # Internal helper to handle kwargs casting for ty\n    def _call(\n        func: Callable[..., Any],\n        **args: Any,\n    ) -&gt; Any:\n        \"\"\"\n        !!! note \"Summary\"\n            Internal helper to call the test function.\n\n        Params:\n            func (Callable[..., Any]):\n                The function to call.\n            args (Any):\n                The arguments to pass.\n\n        Returns:\n            (Any):\n                The result.\n\n        ???+ example \"Examples\"\n\n            ```python\n            # Internal helper.\n            ```\n        \"\"\"\n        return func(**args)\n\n    if algorithm in options[\"qs\"]:\n        return _call(_qs, x=x, **kwargs)\n    if algorithm in options[\"ocsb\"]:\n        return _call(_ocsb, x=x, **kwargs)\n    if algorithm in options[\"ch\"]:\n        return _call(_ch, x=x, **kwargs)\n    if algorithm in options[\"seasonal_strength\"]:\n        return _call(_seasonal_strength, x=x, **kwargs)\n    if algorithm in options[\"trend_strength\"]:\n        return _call(_trend_strength, x=x, **kwargs)\n    if algorithm in options[\"spikiness\"]:\n        return _call(_spikiness, x=x, **kwargs)\n\n    raise ValueError(\n        generate_error_message(\n            parameter_name=\"algorithm\",\n            value_parsed=algorithm,\n            options={k: str(v) for k, v in options.items()},\n        )\n    )\n</code></pre>"},{"location":"code/seasonality/#ts_stat_tests.tests.seasonality.is_seasonal","title":"is_seasonal","text":"<pre><code>is_seasonal(\n    x: ArrayLike,\n    algorithm: str = \"qs\",\n    alpha: float = 0.05,\n    **kwargs: Union[float, int, str, bool, ArrayLike, None]\n) -&gt; dict[str, Union[str, float, bool, None]]\n</code></pre> <p>Summary</p> <p>Boolean check for seasonality. This function wraps the <code>seasonality</code> dispatcher and returns a standardized dictionary indicating whether the series is seasonal based on a significance level or threshold.</p> Details <p>The <code>is_seasonal</code> function interprets the results of the underlying seasonality tests to provide a boolean <code>\"result\"</code>.</p> <ul> <li>For <code>\"qs\"</code>, the test is considered seasonal if the p-value is less than <code>alpha</code>.</li> <li>For <code>\"ocsb\"</code> and <code>\"ch\"</code>, the test is considered seasonal if the returned integer is 1.</li> <li>For <code>\"seasonal_strength\"</code>, the test is considered seasonal if the strength is greater than 0.64 (a common threshold in literature).</li> <li>For others, it checks if the statistic is greater than 0.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked.</p> required <code>algorithm</code> <code>str</code> <p>Which seasonality algorithm to use. Default: <code>\"qs\"</code></p> <code>'qs'</code> <code>alpha</code> <code>float</code> <p>The significance level for the test (used by <code>\"qs\"</code>). Default: <code>0.05</code></p> <code>0.05</code> <code>kwargs</code> <code>Union[float, int, str, bool, ArrayLike, None]</code> <p>Additional arguments to pass to the underlying algorithm.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Union[str, float, bool, None]]</code> <p>A dictionary containing:</p> <ul> <li><code>\"result\"</code> (bool): Indicator if the series is seasonal.</li> <li><code>\"statistic\"</code> (float): The test statistic (or strength).</li> <li><code>\"pvalue\"</code> (float, optional): The p-value of the test (if available).</li> <li><code>\"alpha\"</code> (float): The significance level used.</li> <li><code>\"algorithm\"</code> (str): The algorithm used.</li> </ul> Examples Standard check<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.tests.seasonality import is_seasonal\n&gt;&gt;&gt; data = load_airline().values\n&gt;&gt;&gt; res = is_seasonal(x=data, algorithm=\"qs\", freq=12)\n&gt;&gt;&gt; res[\"result\"]\nTrue\n&gt;&gt;&gt; res[\"algorithm\"]\n'qs'\n</code></pre> Source code in <code>src/ts_stat_tests/tests/seasonality.py</code> <pre><code>@typechecked\ndef is_seasonal(\n    x: ArrayLike,\n    algorithm: str = \"qs\",\n    alpha: float = 0.05,\n    **kwargs: Union[float, int, str, bool, ArrayLike, None],\n) -&gt; dict[str, Union[str, float, bool, None]]:\n    \"\"\"\n    !!! note \"Summary\"\n        Boolean check for seasonality. This function wraps the `seasonality` dispatcher and returns a standardized dictionary indicating whether the series is seasonal based on a significance level or threshold.\n\n    ???+ abstract \"Details\"\n\n        The `is_seasonal` function interprets the results of the underlying seasonality tests to provide a boolean `\"result\"`.\n\n        - For `\"qs\"`, the test is considered seasonal if the p-value is less than `alpha`.\n        - For `\"ocsb\"` and `\"ch\"`, the test is considered seasonal if the returned integer is 1.\n        - For `\"seasonal_strength\"`, the test is considered seasonal if the strength is greater than 0.64 (a common threshold in literature).\n        - For others, it checks if the statistic is greater than 0.\n\n    Params:\n        x (ArrayLike):\n            The data to be checked.\n        algorithm (str, optional):\n            Which seasonality algorithm to use.&lt;br&gt;\n            Default: `\"qs\"`\n        alpha (float, optional):\n            The significance level for the test (used by `\"qs\"`).&lt;br&gt;\n            Default: `0.05`\n        kwargs (Union[float, int, str, bool, ArrayLike, None]):\n            Additional arguments to pass to the underlying algorithm.\n\n    Returns:\n        (dict[str, Union[str, float, bool, None]]):\n            A dictionary containing:\n\n            - `\"result\"` (bool): Indicator if the series is seasonal.\n            - `\"statistic\"` (float): The test statistic (or strength).\n            - `\"pvalue\"` (float, optional): The p-value of the test (if available).\n            - `\"alpha\"` (float): The significance level used.\n            - `\"algorithm\"` (str): The algorithm used.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Standard check\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.tests.seasonality import is_seasonal\n        &gt;&gt;&gt; data = load_airline().values\n        &gt;&gt;&gt; res = is_seasonal(x=data, algorithm=\"qs\", freq=12)\n        &gt;&gt;&gt; res[\"result\"]\n        True\n        &gt;&gt;&gt; res[\"algorithm\"]\n        'qs'\n\n        ```\n    \"\"\"\n    res: Any = seasonality(x=x, algorithm=algorithm, **kwargs)\n\n    is_sea: bool = False\n    stat: float = 0.0\n    pval: Optional[float] = None\n\n    if algorithm in (\"qs\",):\n        if isinstance(res, (tuple, list)):\n            v0: Any = res[0]\n            v1: Any = res[1]\n            stat = float(v0) if isinstance(v0, (int, float)) else 0.0\n            pval = float(v1) if isinstance(v1, (int, float)) else 1.0\n            is_sea = bool(pval &lt; alpha)\n    elif algorithm in (\"ocsb\", \"ch\"):\n        if isinstance(res, (tuple, list)):\n            v0: Any = res[0]\n            stat = float(v0) if isinstance(v0, (int, float)) else 0.0\n        else:\n            v_any: Any = res\n            stat = float(v_any) if isinstance(res, (int, float)) else 0.0\n        is_sea = bool(stat == 1)\n    elif algorithm in (\"seasonal_strength\", \"ss\"):\n        if isinstance(res, (tuple, list)):\n            v0: Any = res[0]\n            stat = float(v0) if isinstance(v0, (int, float)) else 0.0\n        else:\n            v_any: Any = res\n            stat = float(v_any) if isinstance(res, (int, float)) else 0.0\n        # Default threshold of 0.64 is often used for seasonal strength\n        is_sea = bool(stat &gt; 0.64)\n    else:\n        if isinstance(res, (tuple, list)):\n            v0: Any = res[0]\n            stat = float(v0) if isinstance(v0, (int, float)) else 0.0\n        else:\n            v_any: Any = res\n            stat = float(v_any) if isinstance(res, (int, float)) else 0.0\n        is_sea = bool(stat &gt; 0)\n\n    return {\n        \"result\": is_sea,\n        \"statistic\": stat,\n        \"pvalue\": pval,\n        \"alpha\": alpha,\n        \"algorithm\": algorithm,\n    }\n</code></pre>"},{"location":"code/seasonality/#seasonality-algorithms","title":"Seasonality Algorithms","text":""},{"location":"code/seasonality/#ts_stat_tests.algorithms.seasonality","title":"ts_stat_tests.algorithms.seasonality","text":"<p>Summary</p> <p>Seasonality tests are statistical tests used to determine whether a time series exhibits seasonal patterns or cycles. Seasonality refers to the regular and predictable fluctuations in a time series that occur at specific intervals, such as daily, weekly, monthly, or yearly.</p> <p>Seasonality tests help identify whether a time series has a seasonal component that needs to be accounted for in forecasting models. By detecting seasonality, analysts can choose appropriate models that capture these patterns and improve the accuracy of their forecasts.</p> <p>Common seasonality tests include the QS test, OCSB test, Canova-Hansen test, and others. These tests analyze the autocorrelation structure of the time series data to identify significant seasonal patterns.</p> <p>Overall, seasonality tests are essential tools in time series analysis and forecasting, as they help identify and account for seasonal patterns that can significantly impact the accuracy of predictions.</p>"},{"location":"code/seasonality/#ts_stat_tests.algorithms.seasonality.qs","title":"qs","text":"<pre><code>qs(\n    x: ArrayLike,\n    freq: int = 0,\n    diff: bool = True,\n    residuals: bool = False,\n    autoarima: bool = True,\n) -&gt; Union[\n    tuple[float, float],\n    tuple[float, float, Optional[ARIMA]],\n]\n</code></pre> <p>Summary</p> <p>The \\(QS\\) test, also known as the Ljung-Box test, is a statistical test used to determine whether there is any seasonality present in a time series forecasting model. It is based on the autocorrelation function (ACF) of the residuals, which is a measure of how correlated the residuals are at different lags.</p> Details <p>If <code>residuals=False</code> the <code>autoarima</code> settings are ignored.</p> <p>If <code>residuals=True</code>, a non-seasonal ARIMA model is estimated for the time series. And the residuals of the fitted model are used as input to the test statistic. If an automatic order selection is used, the Hyndman-Khandakar algorithm is employed with: \\(\\max(p)=\\max(q)&lt;=3\\).</p> <p>The null hypothesis is that there is no correlation in the residuals beyond the specified lags, indicating no seasonality. The alternative hypothesis is that there is significant correlation, indicating seasonality.</p> <p>Here are the steps for performing the \\(QS\\) test:</p> <ol> <li>Fit a time series model to your data, such as an ARIMA or SARIMA model.</li> <li>Calculate the residuals, which are the differences between the observed values and the predicted values from the model.</li> <li>Calculate the ACF of the residuals.</li> <li>Calculate the Q statistic, which is the sum of the squared values of the autocorrelations at different lags, up to a specified lag. Using the formula above.</li> <li>Compare the Q statistic to the critical value from the chi-squared distribution with degrees of freedom equal to the number of lags. If the Q statistic is greater than the critical value, then the null hypothesis is rejected, indicating that there is evidence of seasonality in the residuals.</li> </ol> <p>In summary, the \\(QS\\) test is a useful tool for determining whether a time series forecasting model has adequately accounted for seasonality in the data. By detecting any seasonality present in the residuals, it helps to ensure that the model is capturing all the important patterns in the data and making accurate predictions.</p> <p>This function will implement the Python version of the R function <code>qs()</code> from the <code>seastests</code> library.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The univariate time series data to test.</p> required <code>freq</code> <code>int</code> <p>The frequency of the time series data. Default: <code>0</code></p> <code>0</code> <code>diff</code> <code>bool</code> <p>Whether or not to run <code>np.diff()</code> over the data. Default: <code>True</code></p> <code>True</code> <code>residuals</code> <code>bool</code> <p>Whether or not to run &amp; return the residuals from the function. Default: <code>False</code></p> <code>False</code> <code>autoarima</code> <code>bool</code> <p>Whether or not to run the <code>AutoARIMA()</code> algorithm over the data. Default: <code>True</code></p> <code>True</code> <p>Raises:</p> Type Description <code>AttributeError</code> <p>If <code>x</code> is empty, or <code>freq</code> is too low for the data to be adequately tested.</p> <code>ValueError</code> <p>If, after differencing the data (by using <code>np.diff()</code>), any of the values are <code>None</code> (or <code>Null</code> or <code>np.nan</code>), then it cannot be used for QS Testing.</p> <p>Returns:</p> Type Description <code>Union[tuple[float, float], tuple[float, float, Optional[ARIMA]]]</code> <p>The results of the QS test. - stat (float): The \\(\\text{QS}\\) score for the given data set. - pval (float): The p-value of the given test. Calculated using the survival function of the chi-squared algorithm (also defined as \\(1-\\text{cdf(...)}\\)). For more info, see: scipy.stats.chi2 - model (Optional[ARIMA]): The ARIMA model used in the calculation of this test. Returned if <code>residuals</code> is <code>True</code>.</p> Examples Basic usage<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.seasonality import qs\n&gt;&gt;&gt; data = load_airline().values\n&gt;&gt;&gt; qs(data, freq=12)\n(194.469289..., 5.909223...)\n</code></pre> Advanced usage<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.seasonality import qs\n&gt;&gt;&gt; data = load_airline().values\n&gt;&gt;&gt; qs(data, freq=12, diff=True, residuals=True, autoarima=True)\nThe differences of the residuals of a non-seasonal ARIMA model are computed and used. It may be better to either only take the differences or use the residuals.\n(101.8592..., 7.6126..., ARIMA(order=(1, 1, 1), scoring_args={}, suppress_warnings=True))\n</code></pre> Calculation <p>The \\(Q\\) statistic is given by:</p> \\[ QS = (n \\times (n+2)) \\times \\sum_{k=1}^{h} \\frac{r_k^2}{n-k} \\] <p>where:</p> <ul> <li>\\(n\\) is the sample size,</li> <li>\\(r_k\\) is the autocorrelation at lag \\(k\\), and</li> <li>\\(h\\) is the maximum lag to be considered.</li> </ul> <pre><code>QS = n(n+2) * sum(r_k^2 / (n-k)) for k = 1 to h\n</code></pre> Credit <ul> <li>All credit goes to the <code>seastests</code> library.</li> </ul> References <ol> <li>Hyndman, R. J. and Y. Khandakar (2008). Automatic Time Series Forecasting: The forecast Package for R. Journal of Statistical Software 27 (3), 1-22.</li> <li>Maravall, A. (2011). Seasonality Tests and Automatic Model Identification in TRAMO-SEATS. Bank of Spain.</li> <li>Ollech, D. and Webel, K. (2020). A random forest-based approach to identifying the most informative seasonality tests. Deutsche Bundesbank's Discussion Paper series 55/2020.</li> </ol> See Also <ul> <li>github/seastests/qs.R</li> <li>rdrr/seastests/qs</li> <li>rdocumentation/seastests/qs</li> <li>Machine Learning Mastery/How to Identify and Remove Seasonality from Time Series Data with Python</li> <li>StackOverflow/Simple tests for seasonality in Python</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/seasonality.py</code> <pre><code>@typechecked\ndef qs(\n    x: ArrayLike,\n    freq: int = 0,\n    diff: bool = True,\n    residuals: bool = False,\n    autoarima: bool = True,\n) -&gt; Union[tuple[float, float], tuple[float, float, Optional[ARIMA]]]:\n    r\"\"\"\n    !!! note \"Summary\"\n        The $QS$ test, also known as the Ljung-Box test, is a statistical test used to determine whether there is any seasonality present in a time series forecasting model. It is based on the autocorrelation function (ACF) of the residuals, which is a measure of how correlated the residuals are at different lags.\n\n    ???+ abstract \"Details\"\n\n        If `residuals=False` the `autoarima` settings are ignored.\n\n        If `residuals=True`, a non-seasonal ARIMA model is estimated for the time series. And the residuals of the fitted model are used as input to the test statistic. If an automatic order selection is used, the Hyndman-Khandakar algorithm is employed with: $\\max(p)=\\max(q)&lt;=3$.\n\n        The null hypothesis is that there is no correlation in the residuals beyond the specified lags, indicating no seasonality. The alternative hypothesis is that there is significant correlation, indicating seasonality.\n\n        Here are the steps for performing the $QS$ test:\n\n        1. Fit a time series model to your data, such as an ARIMA or SARIMA model.\n        1. Calculate the residuals, which are the differences between the observed values and the predicted values from the model.\n        1. Calculate the ACF of the residuals.\n        1. Calculate the Q statistic, which is the sum of the squared values of the autocorrelations at different lags, up to a specified lag. Using the formula above.\n        1. Compare the Q statistic to the critical value from the chi-squared distribution with degrees of freedom equal to the number of lags. If the Q statistic is greater than the critical value, then the null hypothesis is rejected, indicating that there is evidence of seasonality in the residuals.\n\n        In summary, the $QS$ test is a useful tool for determining whether a time series forecasting model has adequately accounted for seasonality in the data. By detecting any seasonality present in the residuals, it helps to ensure that the model is capturing all the important patterns in the data and making accurate predictions.\n\n        This function will implement the Python version of the R function [`qs()`](https://rdrr.io/cran/seastests/man/qs.html) from the [`seastests`](https://cran.r-project.org/web/packages/seastests/index.html) library.\n\n    Params:\n        x (ArrayLike):\n            The univariate time series data to test.\n        freq (int, optional):\n            The frequency of the time series data.&lt;br&gt;\n            Default: `0`\n        diff (bool, optional):\n            Whether or not to run `np.diff()` over the data.&lt;br&gt;\n            Default: `True`\n        residuals (bool, optional):\n            Whether or not to run &amp; return the residuals from the function.&lt;br&gt;\n            Default: `False`\n        autoarima (bool, optional):\n            Whether or not to run the `AutoARIMA()` algorithm over the data.&lt;br&gt;\n            Default: `True`\n\n    Raises:\n        (AttributeError):\n            If `x` is empty, or `freq` is too low for the data to be adequately tested.\n        (ValueError):\n            If, after differencing the data (by using `np.diff()`), any of the values are `None` (or `Null` or `np.nan`), then it cannot be used for QS Testing.\n\n    Returns:\n        (Union[tuple[float, float], tuple[float, float, Optional[ARIMA]]]):\n            The results of the QS test.\n            - stat (float): The $\\text{QS}$ score for the given data set.\n            - pval (float): The p-value of the given test. Calculated using the survival function of the chi-squared algorithm (also defined as $1-\\text{cdf(...)}$). For more info, see: [scipy.stats.chi2](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chi2.html)\n            - model (Optional[ARIMA]): The ARIMA model used in the calculation of this test. Returned if `residuals` is `True`.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.seasonality import qs\n        &gt;&gt;&gt; data = load_airline().values\n        &gt;&gt;&gt; qs(data, freq=12)\n        (194.469289..., 5.909223...)\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Advanced usage\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.seasonality import qs\n        &gt;&gt;&gt; data = load_airline().values\n        &gt;&gt;&gt; qs(data, freq=12, diff=True, residuals=True, autoarima=True)\n        The differences of the residuals of a non-seasonal ARIMA model are computed and used. It may be better to either only take the differences or use the residuals.\n        (101.8592..., 7.6126..., ARIMA(order=(1, 1, 1), scoring_args={}, suppress_warnings=True))\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The $Q$ statistic is given by:\n\n        $$\n        QS = (n \\times (n+2)) \\times \\sum_{k=1}^{h} \\frac{r_k^2}{n-k}\n        $$\n\n        where:\n\n        - $n$ is the sample size,\n        - $r_k$ is the autocorrelation at lag $k$, and\n        - $h$ is the maximum lag to be considered.\n\n        ```\n        QS = n(n+2) * sum(r_k^2 / (n-k)) for k = 1 to h\n        ```\n\n    ??? success \"Credit\"\n        - All credit goes to the [`seastests`](https://cran.r-project.org/web/packages/seastests/index.html) library.\n\n    ??? question \"References\"\n        1. Hyndman, R. J. and Y. Khandakar (2008). Automatic Time Series Forecasting: The forecast Package for R. Journal of Statistical Software 27 (3), 1-22.\n        1. Maravall, A. (2011). Seasonality Tests and Automatic Model Identification in TRAMO-SEATS. Bank of Spain.\n        1. Ollech, D. and Webel, K. (2020). A random forest-based approach to identifying the most informative seasonality tests. Deutsche Bundesbank's Discussion Paper series 55/2020.\n\n    ??? tip \"See Also\"\n        - [github/seastests/qs.R](https://github.com/cran/seastests/blob/master/R/qs.R)\n        - [rdrr/seastests/qs](https://rdrr.io/cran/seastests/man/qs.html)\n        - [rdocumentation/seastests/qs](https://www.rdocumentation.org/packages/seastests/versions/0.15.4/topics/qs)\n        - [Machine Learning Mastery/How to Identify and Remove Seasonality from Time Series Data with Python](https://machinelearningmastery.com/time-series-seasonality-with-python)\n        - [StackOverflow/Simple tests for seasonality in Python](https://stackoverflow.com/questions/62754218/simple-tests-for-seasonality-in-python)\n    \"\"\"\n\n    _x: NDArray[np.float64] = np.asarray(x, dtype=float)\n    if np.isnan(_x).all():\n        raise AttributeError(\"All observations are NaN.\")\n    if diff and residuals:\n        print(\n            \"The differences of the residuals of a non-seasonal ARIMA model are computed and used. \"\n            \"It may be better to either only take the differences or use the residuals.\"\n        )\n    if freq &lt; 2:\n        raise AttributeError(f\"The number of observations per cycle is '{freq}', which is too small.\")\n\n    model: Optional[ARIMA] = None\n\n    if residuals:\n        if autoarima:\n            max_order: int = 1 if freq &lt; 8 else 3\n            allow_drift: bool = True if freq &lt; 8 else False\n            try:\n                model = auto_arima(\n                    y=_x,\n                    max_P=1,\n                    max_Q=1,\n                    max_p=3,\n                    max_q=3,\n                    seasonal=False,\n                    stepwise=False,\n                    max_order=max_order,\n                    allow_drift=allow_drift,\n                )\n            except (ValueError, RuntimeError, IndexError):\n                try:\n                    model = ARIMA(order=(0, 1, 1)).fit(y=_x)\n                except (ValueError, RuntimeError, IndexError):\n                    print(\"Could not estimate any ARIMA model, original data series is used.\")\n            if model is not None:\n                _x = model.resid()\n        else:\n            try:\n                model = ARIMA(order=(0, 1, 1)).fit(y=_x)\n            except (ValueError, RuntimeError, IndexError):\n                print(\"Could not estimate any ARIMA model, original data series is used.\")\n            if model is not None:\n                _x = model.resid()\n\n    # Do diff\n    y: NDArray[np.float64] = np.diff(_x) if diff else _x\n\n    # Pre-check\n    if np.nanvar(y[~np.isnan(y)]) == 0:\n        raise ValueError(\n            \"The Series is a constant (possibly after transformations). QS-Test cannot be computed on constants.\"\n        )\n\n    # Test Statistic\n    acf_output: NDArray[np.float64] = _acf(x=y, nlags=freq * 2, missing=\"drop\")\n    rho_output: NDArray[np.float64] = acf_output[[freq, freq * 2]]\n    rho: NDArray[np.float64] = np.array([0, 0]) if np.any(np.array(rho_output) &lt;= 0) else rho_output\n    N: int = len(y[~np.isnan(y)])\n    QS: float = float(N * (N + 2) * (rho[0] ** 2 / (N - freq) + rho[1] ** 2 / (N - freq * 2)))\n    Pval: float = float(chi2.sf(QS, 2))\n\n    if residuals:\n        return QS, Pval, model\n    return QS, Pval\n</code></pre>"},{"location":"code/seasonality/#ts_stat_tests.algorithms.seasonality.ocsb","title":"ocsb","text":"<pre><code>ocsb(\n    x: ArrayLike,\n    m: int,\n    lag_method: str = \"aic\",\n    max_lag: int = 3,\n) -&gt; int\n</code></pre> <p>Summary</p> <p>Compute the Osborn, Chui, Smith, and Birchenhall (\\(OCSB\\)) test for an input time series to determine whether it needs seasonal differencing. The regression equation may include lags of the dependent variable. When <code>lag_method=\"fixed\"</code>, the lag order is fixed to <code>max_lag</code>; otherwise, <code>max_lag</code> is the maximum number of lags considered in a lag selection procedure that minimizes the <code>lag_method</code> criterion, which can be <code>\"aic\"</code>, <code>\"bic\"</code> or corrected AIC <code>\"aicc\"</code>.</p> Details <p>The \\(OCSB\\) test is a statistical test that is used to check the presence of seasonality in time series data. Seasonality refers to a pattern in the data that repeats itself at regular intervals.</p> <p>The \\(OCSB\\) test is based on the null hypothesis that there is no seasonality in the time series data. If the p-value of the test is less than the significance level (usually \\(0.05\\)), then the null hypothesis is rejected, and it is concluded that there is seasonality in the data.</p> <p>The \\(OCSB\\) test involves dividing the data into two halves and calculating the mean of each half. Then, the differences between the means of each pair of halves are calculated for each possible pair of halves. Finally, the mean of these differences is calculated, and a test statistic is computed.</p> <p>The \\(OCSB\\) test is useful for testing seasonality in time series data because it can detect seasonal patterns that are not obvious in the original data. It is also a useful diagnostic tool for determining the appropriate seasonal differencing parameter in ARIMA models.</p> <p>Critical values for the test are based on simulations, which have been smoothed over to produce critical values for all seasonal periods</p> <p>The null hypothesis of the \\(OCSB\\) test is that there is no seasonality in the time series, and the alternative hypothesis is that there is seasonality. The test statistic is compared to a critical value from a chi-squared distribution with degrees of freedom equal to the number of possible pairs of halves. If the test statistic is larger than the critical value, then the null hypothesis is rejected, and it is concluded that there is evidence of seasonality in the time series.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The time series vector.</p> required <code>m</code> <code>int</code> <p>The seasonal differencing term. For monthly data, e.g., this would be 12. For quarterly, 4, etc. For the OCSB test to work, <code>m</code> must exceed <code>1</code>.</p> required <code>lag_method</code> <code>str</code> <p>The lag method to use. One of (<code>\"fixed\"</code>, <code>\"aic\"</code>, <code>\"bic\"</code>, <code>\"aicc\"</code>). The metric for assessing model performance after fitting a linear model. Default: <code>\"aic\"</code></p> <code>'aic'</code> <code>max_lag</code> <code>int</code> <p>The maximum lag order to be considered by <code>lag_method</code>. Default: <code>3</code></p> <code>3</code> <p>Returns:</p> Type Description <code>int</code> <p>The seasonal differencing term. For different values of <code>m</code>, the OCSB statistic is compared to an estimated critical value, and returns 1 if the computed statistic is greater than the critical value, or 0 if not.</p> Examples Basic usage<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.seasonality import ocsb\n&gt;&gt;&gt; data = load_airline().values\n&gt;&gt;&gt; ocsb(x=data, m=12)\n1\n</code></pre> Calculation <p>The equation for the \\(OCSB\\) test statistic for a time series of length n is:</p> \\[ OCSB = \\frac{1}{(n-1)} \\times \\sum \\left( \\left( x[i] - x \\left[ \\frac{n}{2+i} \\right] \\right) - \\left( x \\left[ \\frac{n}{2+i} \\right] - x \\left[ \\frac{i+n}{2+1} \\right] \\right) \\right) ^2 \\] <p>where:</p> <ul> <li>\\(n\\) is the sample size, and</li> <li>\\(x[i]\\) is the \\(i\\)-th observation in the time series.</li> </ul> <pre><code>OCSB = (1 / (n - 1)) * sum( ((x[i] - x[n/2+i]) - (x[n/2+i] - x[i+n/2+1]))^2 )\n</code></pre> <p>In this equation, the time series is split into two halves, and the difference between the means of each half is calculated for each possible pair of halves. The sum of the squared differences is then divided by the length of the time series minus one to obtain the \\(OCSB\\) test statistic.</p> Credit <ul> <li>All credit goes to the <code>pmdarima</code> library with the implementation of <code>pmdarima.arima.OCSBTest</code>.</li> </ul> References <ul> <li>Osborn DR, Chui APL, Smith J, and Birchenhall CR (1988) \"Seasonality and the order of integration for consumption\", Oxford Bulletin of Economics and Statistics 50(4):361-377.</li> <li>R's forecast::OCSB test source code: https://bit.ly/2QYQHno</li> </ul> See Also <ul> <li>pmdarima.arima.OCSBTest</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/seasonality.py</code> <pre><code>@typechecked\ndef ocsb(x: ArrayLike, m: int, lag_method: str = \"aic\", max_lag: int = 3) -&gt; int:\n    r\"\"\"\n    !!! note \"Summary\"\n        Compute the Osborn, Chui, Smith, and Birchenhall ($OCSB$) test for an input time series to determine whether it needs seasonal differencing. The regression equation may include lags of the dependent variable. When `lag_method=\"fixed\"`, the lag order is fixed to `max_lag`; otherwise, `max_lag` is the maximum number of lags considered in a lag selection procedure that minimizes the `lag_method` criterion, which can be `\"aic\"`, `\"bic\"` or corrected AIC `\"aicc\"`.\n\n    ???+ abstract \"Details\"\n\n        The $OCSB$ test is a statistical test that is used to check the presence of seasonality in time series data. Seasonality refers to a pattern in the data that repeats itself at regular intervals.\n\n        The $OCSB$ test is based on the null hypothesis that there is no seasonality in the time series data. If the p-value of the test is less than the significance level (usually $0.05$), then the null hypothesis is rejected, and it is concluded that there is seasonality in the data.\n\n        The $OCSB$ test involves dividing the data into two halves and calculating the mean of each half. Then, the differences between the means of each pair of halves are calculated for each possible pair of halves. Finally, the mean of these differences is calculated, and a test statistic is computed.\n\n        The $OCSB$ test is useful for testing seasonality in time series data because it can detect seasonal patterns that are not obvious in the original data. It is also a useful diagnostic tool for determining the appropriate seasonal differencing parameter in ARIMA models.\n\n        Critical values for the test are based on simulations, which have been smoothed over to produce critical values for all seasonal periods\n\n        The null hypothesis of the $OCSB$ test is that there is no seasonality in the time series, and the alternative hypothesis is that there is seasonality. The test statistic is compared to a critical value from a chi-squared distribution with degrees of freedom equal to the number of possible pairs of halves. If the test statistic is larger than the critical value, then the null hypothesis is rejected, and it is concluded that there is evidence of seasonality in the time series.\n\n    Params:\n        x (ArrayLike):\n            The time series vector.\n        m (int):\n            The seasonal differencing term. For monthly data, e.g., this would be 12. For quarterly, 4, etc. For the OCSB test to work, `m` must exceed `1`.\n        lag_method (str, optional):\n            The lag method to use. One of (`\"fixed\"`, `\"aic\"`, `\"bic\"`, `\"aicc\"`). The metric for assessing model performance after fitting a linear model.&lt;br&gt;\n            Default: `\"aic\"`\n        max_lag (int, optional):\n            The maximum lag order to be considered by `lag_method`.&lt;br&gt;\n            Default: `3`\n\n    Returns:\n        (int):\n            The seasonal differencing term. For different values of `m`, the OCSB statistic is compared to an estimated critical value, and returns 1 if the computed statistic is greater than the critical value, or 0 if not.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.seasonality import ocsb\n        &gt;&gt;&gt; data = load_airline().values\n        &gt;&gt;&gt; ocsb(x=data, m=12)\n        1\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The equation for the $OCSB$ test statistic for a time series of length n is:\n\n        $$\n        OCSB = \\frac{1}{(n-1)} \\times \\sum \\left( \\left( x[i] - x \\left[ \\frac{n}{2+i} \\right] \\right) - \\left( x \\left[ \\frac{n}{2+i} \\right] - x \\left[ \\frac{i+n}{2+1} \\right] \\right) \\right) ^2\n        $$\n\n        where:\n\n        - $n$ is the sample size, and\n        - $x[i]$ is the $i$-th observation in the time series.\n\n        ```\n        OCSB = (1 / (n - 1)) * sum( ((x[i] - x[n/2+i]) - (x[n/2+i] - x[i+n/2+1]))^2 )\n        ```\n\n        In this equation, the time series is split into two halves, and the difference between the means of each half is calculated for each possible pair of halves. The sum of the squared differences is then divided by the length of the time series minus one to obtain the $OCSB$ test statistic.\n\n    ??? success \"Credit\"\n        - All credit goes to the [`pmdarima`](http://alkaline-ml.com/pmdarima/index.html) library with the implementation of [`pmdarima.arima.OCSBTest`](http://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.OCSBTest.html).\n\n    ??? question \"References\"\n        - Osborn DR, Chui APL, Smith J, and Birchenhall CR (1988) \"Seasonality and the order of integration for consumption\", Oxford Bulletin of Economics and Statistics 50(4):361-377.\n        - R's forecast::OCSB test source code: https://bit.ly/2QYQHno\n\n    ??? tip \"See Also\"\n        - [pmdarima.arima.OCSBTest](http://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.OCSBTest.html)\n    \"\"\"\n    return OCSBTest(m=m, lag_method=lag_method, max_lag=max_lag).estimate_seasonal_differencing_term(x)\n</code></pre>"},{"location":"code/seasonality/#ts_stat_tests.algorithms.seasonality.ch","title":"ch","text":"<pre><code>ch(x: ArrayLike, m: int) -&gt; int\n</code></pre> <p>Summary</p> <p>The Canova-Hansen test for seasonal differences. Canova and Hansen (1995) proposed a test statistic for the null hypothesis that the seasonal pattern is stable. The test statistic can be formulated in terms of seasonal dummies or seasonal cycles. The former allows us to identify seasons (e.g. months or quarters) that are not stable, while the latter tests the stability of seasonal cycles (e.g. cycles of period 2 and 4 quarters in quarterly data).</p> <p>Warning</p> <p>This test is generally not used directly, but in conjunction with <code>pmdarima.arima.nsdiffs()</code>, which directly estimates the number of seasonal differences.</p> Details <p>The \\(CH\\) test (also known as the Canova-Hansen test) is a statistical test for detecting seasonality in time series data. It is based on the idea of comparing the goodness of fit of two models: a non-seasonal model and a seasonal model. The null hypothesis of the \\(CH\\) test is that the time series is non-seasonal, while the alternative hypothesis is that the time series is seasonal.</p> <p>The test statistic is compared to a critical value from the chi-squared distribution with degrees of freedom equal to the difference in parameters between the two models. If the test statistic exceeds the critical value, the null hypothesis of non-seasonality is rejected in favor of the alternative hypothesis of seasonality.</p> <p>The \\(CH\\) test is based on the following steps:</p> <ol> <li>Fit a non-seasonal autoregressive integrated moving average (ARIMA) model to the time series data, using a criterion such as Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) to determine the optimal model order.</li> <li>Fit a seasonal ARIMA model to the time series data, using the same criterion to determine the optimal model order and seasonal period.</li> <li>Compute the sum of squared residuals (SSR) for both models.</li> <li>Compute the test statistic \\(CH\\) using the formula above.</li> <li>Compare the test statistic to a critical value from the chi-squared distribution with degrees of freedom equal to the difference in parameters between the two models. If the test statistic exceeds the critical value, reject the null hypothesis of non-seasonality in favor of the alternative hypothesis of seasonality.</li> </ol> <p>The \\(CH\\) test is a powerful test for seasonality in time series data, as it accounts for both the presence and the nature of seasonality. However, it assumes that the time series data is stationary, and it may not be effective for detecting seasonality in non-stationary or irregular time series data. Additionally, it may not work well for time series data with short seasonal periods or with low seasonal amplitudes. Therefore, it should be used in conjunction with other tests and techniques for detecting seasonality in time series data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The time series vector.</p> required <code>m</code> <code>int</code> <p>The seasonal differencing term. For monthly data, e.g., this would be 12. For quarterly, 4, etc. For the Canova-Hansen test to work, <code>m</code> must exceed 1.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The seasonal differencing term.</p> <p>The \\(CH\\) test defines a set of critical values:</p> <pre><code>(0.4617146, 0.7479655, 1.0007818,\n 1.2375350, 1.4625240, 1.6920200,\n 1.9043096, 2.1169602, 2.3268562,\n 2.5406922, 2.7391007)\n</code></pre> <p>For different values of <code>m</code>, the \\(CH\\) statistic is compared to the corresponding critical value, and returns 1 if the computed statistic is greater than the critical value, or 0 if not.</p> Examples Basic usage<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.seasonality import ch\n&gt;&gt;&gt; data = load_airline().values\n&gt;&gt;&gt; ch(x=data, m=12)\n0\n</code></pre> Calculation <p>The test statistic for the \\(CH\\) test is given by:</p> \\[ CH = \\frac { \\left( \\frac { SSRns - SSRs } { n - p - 1 } \\right) } { \\left( \\frac { SSRs } { n - p - s - 1 } \\right) } \\] <p>where:</p> <ul> <li>\\(SSRns\\) is the \\(SSR\\) for the non-seasonal model,</li> <li>\\(SSRs\\) is the \\(SSR\\) for the seasonal model,</li> <li>\\(n\\) is the sample size,</li> <li>\\(p\\) is the number of parameters in the non-seasonal model, and</li> <li>\\(s\\) is the number of parameters in the seasonal model.</li> </ul> <pre><code>CH = [(SSRns - SSRs) / (n - p - 1)] / (SSRs / (n - p - s - 1))\n</code></pre> Notes <p>This test is generally not used directly, but in conjunction with <code>pmdarima.arima.nsdiffs()</code>, which directly estimates the number of seasonal differences.</p> Credit <ul> <li>All credit goes to the <code>pmdarima</code> library with the implementation of <code>pmdarima.arima.CHTest</code>.</li> </ul> References <ul> <li>Testing for seasonal stability using the Canova and Hansen test statistic: http://bit.ly/2wKkrZo</li> <li>R source code for CH test: https://github.com/robjhyndman/forecast/blob/master/R/arima.R#L148</li> </ul> See Also <ul> <li><code>pmdarima.arima.CHTest</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/seasonality.py</code> <pre><code>@typechecked\ndef ch(x: ArrayLike, m: int) -&gt; int:\n    r\"\"\"\n    !!! note \"Summary\"\n        The Canova-Hansen test for seasonal differences. Canova and Hansen (1995) proposed a test statistic for the null hypothesis that the seasonal pattern is stable. The test statistic can be formulated in terms of seasonal dummies or seasonal cycles. The former allows us to identify seasons (e.g. months or quarters) that are not stable, while the latter tests the stability of seasonal cycles (e.g. cycles of period 2 and 4 quarters in quarterly data).\n\n        !!! warning \"Warning\"\n            This test is generally not used directly, but in conjunction with `pmdarima.arima.nsdiffs()`, which directly estimates the number of seasonal differences.\n\n    ???+ abstract \"Details\"\n\n        The $CH$ test (also known as the Canova-Hansen test) is a statistical test for detecting seasonality in time series data. It is based on the idea of comparing the goodness of fit of two models: a non-seasonal model and a seasonal model. The null hypothesis of the $CH$ test is that the time series is non-seasonal, while the alternative hypothesis is that the time series is seasonal.\n\n        The test statistic is compared to a critical value from the chi-squared distribution with degrees of freedom equal to the difference in parameters between the two models. If the test statistic exceeds the critical value, the null hypothesis of non-seasonality is rejected in favor of the alternative hypothesis of seasonality.\n\n        The $CH$ test is based on the following steps:\n\n        1. Fit a non-seasonal autoregressive integrated moving average (ARIMA) model to the time series data, using a criterion such as Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC) to determine the optimal model order.\n        1. Fit a seasonal ARIMA model to the time series data, using the same criterion to determine the optimal model order and seasonal period.\n        1. Compute the sum of squared residuals (SSR) for both models.\n        1. Compute the test statistic $CH$ using the formula above.\n        1. Compare the test statistic to a critical value from the chi-squared distribution with degrees of freedom equal to the difference in parameters between the two models. If the test statistic exceeds the critical value, reject the null hypothesis of non-seasonality in favor of the alternative hypothesis of seasonality.\n\n        The $CH$ test is a powerful test for seasonality in time series data, as it accounts for both the presence and the nature of seasonality. However, it assumes that the time series data is stationary, and it may not be effective for detecting seasonality in non-stationary or irregular time series data. Additionally, it may not work well for time series data with short seasonal periods or with low seasonal amplitudes. Therefore, it should be used in conjunction with other tests and techniques for detecting seasonality in time series data.\n\n    Params:\n        x (ArrayLike):\n            The time series vector.\n        m (int):\n            The seasonal differencing term. For monthly data, e.g., this would be 12. For quarterly, 4, etc. For the Canova-Hansen test to work, `m` must exceed 1.\n\n    Returns:\n        (int):\n            The seasonal differencing term.\n\n            The $CH$ test defines a set of critical values:\n\n            ```\n            (0.4617146, 0.7479655, 1.0007818,\n             1.2375350, 1.4625240, 1.6920200,\n             1.9043096, 2.1169602, 2.3268562,\n             2.5406922, 2.7391007)\n            ```\n\n            For different values of `m`, the $CH$ statistic is compared to the corresponding critical value, and returns 1 if the computed statistic is greater than the critical value, or 0 if not.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.seasonality import ch\n        &gt;&gt;&gt; data = load_airline().values\n        &gt;&gt;&gt; ch(x=data, m=12)\n        0\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The test statistic for the $CH$ test is given by:\n\n        $$\n        CH = \\frac { \\left( \\frac { SSRns - SSRs } { n - p - 1 } \\right) } { \\left( \\frac { SSRs } { n - p - s - 1 } \\right) }\n        $$\n\n        where:\n\n        - $SSRns$ is the $SSR$ for the non-seasonal model,\n        - $SSRs$ is the $SSR$ for the seasonal model,\n        - $n$ is the sample size,\n        - $p$ is the number of parameters in the non-seasonal model, and\n        - $s$ is the number of parameters in the seasonal model.\n\n        ```\n        CH = [(SSRns - SSRs) / (n - p - 1)] / (SSRs / (n - p - s - 1))\n        ```\n\n    ??? note \"Notes\"\n        This test is generally not used directly, but in conjunction with `pmdarima.arima.nsdiffs()`, which directly estimates the number of seasonal differences.\n\n    ??? success \"Credit\"\n        - All credit goes to the [`pmdarima`](http://alkaline-ml.com/pmdarima/index.html) library with the implementation of [`pmdarima.arima.CHTest`](http://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.CHTest.html).\n\n    ??? question \"References\"\n        - Testing for seasonal stability using the Canova and Hansen test statistic: http://bit.ly/2wKkrZo\n        - R source code for CH test: https://github.com/robjhyndman/forecast/blob/master/R/arima.R#L148\n\n    ??? tip \"See Also\"\n        - [`pmdarima.arima.CHTest`](http://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.CHTest.html)\n    \"\"\"\n    return CHTest(m=m).estimate_seasonal_differencing_term(x)\n</code></pre>"},{"location":"code/seasonality/#ts_stat_tests.algorithms.seasonality.seasonal_strength","title":"seasonal_strength","text":"<pre><code>seasonal_strength(x: ArrayLike, m: int) -&gt; float\n</code></pre> <p>Summary</p> <p>The seasonal strength test is a statistical test for detecting the strength of seasonality in time series data. It measures the extent to which the seasonal component of a time series explains the variation in the data.</p> Details <p>The seasonal strength test involves computing the seasonal strength index (\\(SSI\\)).</p> <p>The \\(SSI\\) ranges between \\(0\\) and \\(1\\), with higher values indicating stronger seasonality in the data. The critical value for the \\(SSI\\) can be obtained from statistical tables based on the sample size and level of significance. If the \\(SSI\\) value exceeds the critical value, the null hypothesis of no seasonality is rejected in favor of the alternative hypothesis of seasonality.</p> <p>The seasonal strength test involves the following steps:</p> <ol> <li>Decompose the time series data into its seasonal, trend, and residual components using a method such as seasonal decomposition of time series (STL) or moving average decomposition.</li> <li>Compute the variance of the seasonal component \\(Var(S)\\) and the variance of the residual component \\(Var(R)\\).</li> <li>Compute the \\(SSI\\) using the formula above.</li> <li>Compare the \\(SSI\\) to a critical value from a statistical table for a given significance level and sample size. If the \\(SSI\\) exceeds the critical value, reject the null hypothesis of no seasonality in favor of the alternative hypothesis of seasonality.</li> </ol> <p>The seasonal strength test is a simple and intuitive test for seasonality in time series data. However, it assumes that the seasonal component is additive and that the residuals are independent and identically distributed. Moreover, it may not be effective for detecting complex seasonal patterns or seasonality in non-stationary or irregular time series data. Therefore, it should be used in conjunction with other tests and techniques for detecting seasonality in time series data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The time series vector.</p> required <code>m</code> <code>int</code> <p>The seasonal differencing term. For monthly data, e.g., this would be 12. For quarterly, 4, etc. For the seasonal strength test to work, <code>m</code> must exceed 1.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The seasonal strength value.</p> Examples Basic usage<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.seasonality import seasonal_strength\n&gt;&gt;&gt; data = load_airline().values\n&gt;&gt;&gt; seasonal_strength(x=data, m=12)\n0.778721...\n</code></pre> Calculation <p>The \\(SSI\\) is computed using the following formula:</p> \\[ SSI = \\frac {Var(S)} {Var(S) + Var(R)} \\] <p>where:</p> <ul> <li>\\(Var(S)\\) is the variance of the seasonal component, and</li> <li>\\(Var(R)\\) is the variance of the residual component obtained after decomposing the time series data into its seasonal, trend, and residual components using a method such as STL or moving average decomposition.</li> </ul> <pre><code>SSI = Var(S) / (Var(S) + Var(R))\n</code></pre> Credit <ul> <li>Inspired by the <code>tsfeatures</code> library in both <code>Python</code> and <code>R</code>.</li> </ul> References <ul> <li>Wang, X, Hyndman, RJ, Smith-Miles, K (2007) \"Rule-based forecasting filters using time series features\", Computational Statistics and Data Analysis, 52(4), 2244-2259.</li> </ul> See Also <ul> <li><code>tsfeatures.stl_features</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/seasonality.py</code> <pre><code>@typechecked\ndef seasonal_strength(x: ArrayLike, m: int) -&gt; float:\n    r\"\"\"\n    !!! note \"Summary\"\n        The seasonal strength test is a statistical test for detecting the strength of seasonality in time series data. It measures the extent to which the seasonal component of a time series explains the variation in the data.\n\n    ???+ abstract \"Details\"\n\n        The seasonal strength test involves computing the seasonal strength index ($SSI$).\n\n        The $SSI$ ranges between $0$ and $1$, with higher values indicating stronger seasonality in the data. The critical value for the $SSI$ can be obtained from statistical tables based on the sample size and level of significance. If the $SSI$ value exceeds the critical value, the null hypothesis of no seasonality is rejected in favor of the alternative hypothesis of seasonality.\n\n        The seasonal strength test involves the following steps:\n\n        1. Decompose the time series data into its seasonal, trend, and residual components using a method such as seasonal decomposition of time series (STL) or moving average decomposition.\n        1. Compute the variance of the seasonal component $Var(S)$ and the variance of the residual component $Var(R)$.\n        1. Compute the $SSI$ using the formula above.\n        1. Compare the $SSI$ to a critical value from a statistical table for a given significance level and sample size. If the $SSI$ exceeds the critical value, reject the null hypothesis of no seasonality in favor of the alternative hypothesis of seasonality.\n\n        The seasonal strength test is a simple and intuitive test for seasonality in time series data. However, it assumes that the seasonal component is additive and that the residuals are independent and identically distributed. Moreover, it may not be effective for detecting complex seasonal patterns or seasonality in non-stationary or irregular time series data. Therefore, it should be used in conjunction with other tests and techniques for detecting seasonality in time series data.\n\n    Params:\n        x (ArrayLike):\n            The time series vector.\n        m (int):\n            The seasonal differencing term. For monthly data, e.g., this would be 12. For quarterly, 4, etc. For the seasonal strength test to work, `m` must exceed 1.\n\n    Returns:\n        (float):\n            The seasonal strength value.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.seasonality import seasonal_strength\n        &gt;&gt;&gt; data = load_airline().values\n        &gt;&gt;&gt; seasonal_strength(x=data, m=12)\n        0.778721...\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The $SSI$ is computed using the following formula:\n\n        $$\n        SSI = \\frac {Var(S)} {Var(S) + Var(R)}\n        $$\n\n        where:\n\n        - $Var(S)$ is the variance of the seasonal component, and\n        - $Var(R)$ is the variance of the residual component obtained after decomposing the time series data into its seasonal, trend, and residual components using a method such as STL or moving average decomposition.\n\n        ```\n        SSI = Var(S) / (Var(S) + Var(R))\n        ```\n\n    ??? success \"Credit\"\n        - Inspired by the `tsfeatures` library in both [`Python`](https://github.com/Nixtla/tsfeatures) and [`R`](http://pkg.robjhyndman.com/tsfeatures/).\n\n    ??? question \"References\"\n        - Wang, X, Hyndman, RJ, Smith-Miles, K (2007) \"Rule-based forecasting filters using time series features\", Computational Statistics and Data Analysis, 52(4), 2244-2259.\n\n    ??? tip \"See Also\"\n        - [`tsfeatures.stl_features`](https://github.com/Nixtla/tsfeatures/blob/main/tsfeatures/tsfeatures.py)\n    \"\"\"\n    decomposition = seasonal_decompose(x=x, period=m, model=\"additive\")\n    seasonal = np.nanvar(decomposition.seasonal)\n    residual = np.nanvar(decomposition.resid)\n    return float(seasonal / (seasonal + residual))\n</code></pre>"},{"location":"code/seasonality/#ts_stat_tests.algorithms.seasonality.trend_strength","title":"trend_strength","text":"<pre><code>trend_strength(x: ArrayLike, m: int) -&gt; float\n</code></pre> <p>Summary</p> <p>The trend strength test is a statistical test for detecting the strength of the trend component in time series data. It measures the extent to which the trend component of a time series explains the variation in the data.</p> Details <p>The trend strength test involves computing the trend strength index (\\(TSI\\)).</p> <p>The \\(TSI\\) ranges between \\(0\\) and \\(1\\), with higher values indicating stronger trend in the data. The critical value for the \\(TSI\\) can be obtained from statistical tables based on the sample size and level of significance. If the \\(TSI\\) value exceeds the critical value, the null hypothesis of no trend is rejected in favor of the alternative hypothesis of trend.</p> <p>The trend strength test involves the following steps:</p> <ol> <li>Decompose the time series data into its trend, seasonal, and residual components using a method such as seasonal decomposition of time series (STL) or moving average decomposition.</li> <li>Compute the variance of the trend component, denoted by \\(Var(T)\\).</li> <li>Compute the variance of the residual component, denoted by \\(Var(R)\\).</li> <li>Compute the trend strength index (\\(TSI\\)) using the formula above.</li> <li>Compare the \\(TSI\\) value to a critical value based on the sample size and level of significance. If the \\(TSI\\) value exceeds the critical value, reject the null hypothesis of no trend in favor of the alternative hypothesis of trend.</li> </ol> <p>The trend strength test is a useful tool for identifying the strength of trend in time series data, and it can be used in conjunction with other tests and techniques for detecting trend. However, it assumes that the time series data is stationary and that the trend component is linear. Additionally, it may not be effective for time series data with short time spans or with nonlinear trends. Therefore, it should be used in conjunction with other tests and techniques for detecting trend in time series data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The time series vector.</p> required <code>m</code> <code>int</code> <p>The frequency of the time series data set. For the trend strength test to work, <code>m</code> must exceed 1.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The trend strength score.</p> Examples Basic usage<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.seasonality import trend_strength\n&gt;&gt;&gt; data = load_airline().values\n&gt;&gt;&gt; trend_strength(x=data, m=12)\n0.965679...\n</code></pre> Calculation <p>The trend strength test involves computing the trend strength index (\\(TSI\\)) using the following formula:</p> \\[ TSI = \\frac{ Var(T) } { Var(T) + Var(R) } \\] <p>where:</p> <ul> <li>\\(Var(T)\\) is the variance of the trend component, and</li> <li>\\(Var(R)\\) is the variance of the residual component obtained after decomposing the time series data into its trend, seasonal, and residual components using a method such as STL or moving average decomposition.</li> </ul> <pre><code>TSI = Var(T) / (Var(T) + Var(R))\n</code></pre> Credit <ul> <li>Inspired by the <code>tsfeatures</code> library in both <code>Python</code> and <code>R</code>.</li> </ul> References <ul> <li>Wang, X, Hyndman, RJ, Smith-Miles, K (2007) \"Rule-based forecasting filters using time series features\", Computational Statistics and Data Analysis, 52(4), 2244-2259.</li> </ul> See Also <ul> <li><code>tsfeatures.stl_features</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/seasonality.py</code> <pre><code>@typechecked\ndef trend_strength(x: ArrayLike, m: int) -&gt; float:\n    r\"\"\"\n    !!! note \"Summary\"\n        The trend strength test is a statistical test for detecting the strength of the trend component in time series data. It measures the extent to which the trend component of a time series explains the variation in the data.\n\n    ???+ abstract \"Details\"\n\n        The trend strength test involves computing the trend strength index ($TSI$).\n\n        The $TSI$ ranges between $0$ and $1$, with higher values indicating stronger trend in the data. The critical value for the $TSI$ can be obtained from statistical tables based on the sample size and level of significance. If the $TSI$ value exceeds the critical value, the null hypothesis of no trend is rejected in favor of the alternative hypothesis of trend.\n\n        The trend strength test involves the following steps:\n\n        1. Decompose the time series data into its trend, seasonal, and residual components using a method such as seasonal decomposition of time series (STL) or moving average decomposition.\n        1. Compute the variance of the trend component, denoted by $Var(T)$.\n        1. Compute the variance of the residual component, denoted by $Var(R)$.\n        1. Compute the trend strength index ($TSI$) using the formula above.\n        1. Compare the $TSI$ value to a critical value based on the sample size and level of significance. If the $TSI$ value exceeds the critical value, reject the null hypothesis of no trend in favor of the alternative hypothesis of trend.\n\n        The trend strength test is a useful tool for identifying the strength of trend in time series data, and it can be used in conjunction with other tests and techniques for detecting trend. However, it assumes that the time series data is stationary and that the trend component is linear. Additionally, it may not be effective for time series data with short time spans or with nonlinear trends. Therefore, it should be used in conjunction with other tests and techniques for detecting trend in time series data.\n\n    Params:\n        x (ArrayLike):\n            The time series vector.\n        m (int):\n            The frequency of the time series data set. For the trend strength test to work, `m` must exceed 1.\n\n    Returns:\n        (float):\n            The trend strength score.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.seasonality import trend_strength\n        &gt;&gt;&gt; data = load_airline().values\n        &gt;&gt;&gt; trend_strength(x=data, m=12)\n        0.965679...\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The trend strength test involves computing the trend strength index ($TSI$) using the following formula:\n\n        $$\n        TSI = \\frac{ Var(T) } { Var(T) + Var(R) }\n        $$\n\n        where:\n\n        - $Var(T)$ is the variance of the trend component, and\n        - $Var(R)$ is the variance of the residual component obtained after decomposing the time series data into its trend, seasonal, and residual components using a method such as STL or moving average decomposition.\n\n        ```\n        TSI = Var(T) / (Var(T) + Var(R))\n        ```\n\n    ??? success \"Credit\"\n        - Inspired by the `tsfeatures` library in both [`Python`](https://github.com/Nixtla/tsfeatures) and [`R`](http://pkg.robjhyndman.com/tsfeatures/).\n\n    ??? question \"References\"\n        - Wang, X, Hyndman, RJ, Smith-Miles, K (2007) \"Rule-based forecasting filters using time series features\", Computational Statistics and Data Analysis, 52(4), 2244-2259.\n\n    ??? tip \"See Also\"\n        - [`tsfeatures.stl_features`](https://github.com/Nixtla/tsfeatures/blob/main/tsfeatures/tsfeatures.py)\n    \"\"\"\n    decomposition = seasonal_decompose(x=x, period=m, model=\"additive\")\n    trend = np.nanvar(decomposition.trend)\n    residual = np.nanvar(decomposition.resid)\n    return float(trend / (trend + residual))\n</code></pre>"},{"location":"code/seasonality/#ts_stat_tests.algorithms.seasonality.spikiness","title":"spikiness","text":"<pre><code>spikiness(x: ArrayLike, m: int) -&gt; float\n</code></pre> <p>Summary</p> <p>The spikiness test is a statistical test that measures the degree of spikiness or volatility in a time series data. It aims to detect the presence of spikes or sudden changes in the data that may indicate important events or anomalies in the underlying process.</p> Details <p>The spikiness test involves computing the spikiness index (\\(SI\\)). The \\(SI\\) measures the intensity of spikes or outliers in the data relative to the overall variation. A higher \\(SI\\) value indicates a more spiky or volatile time series, while a lower \\(SI\\) value indicates a smoother or less volatile time series.</p> <p>The spikiness test involves the following steps:</p> <ol> <li>Decompose the time series data into its seasonal, trend, and residual components using a method such as STL or moving average decomposition.</li> <li>Compute the mean absolute deviation of the residual component (\\(MADR\\)).</li> <li>Compute the mean absolute deviation of the seasonal component (\\(MADS\\)).</li> <li>Compute the spikiness index (\\(SI\\)) using the formula above.</li> </ol> <p>The spikiness test can be used in conjunction with other tests and techniques for detecting spikes in time series data, such as change point analysis and outlier detection. However, it assumes that the time series data is stationary and that the spikes are abrupt and sudden. Additionally, it may not be effective for time series data with long-term trends or cyclical patterns. Therefore, it should be used in conjunction with other tests and techniques for detecting spikes in time series data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The time series vector.</p> required <code>m</code> <code>int</code> <p>The frequency of the time series data set. For the spikiness test to work, <code>m</code> must exceed 1.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The spikiness score.</p> Examples Basic usage<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.seasonality import spikiness\n&gt;&gt;&gt; data = load_airline().values\n&gt;&gt;&gt; spikiness(x=data, m=12)\n0.484221...\n</code></pre> Calculation <p>The spikiness test involves computing the spikiness index (\\(SI\\)) using the following formula:</p> \\[ SI = \\frac {MADR} {MADS} \\] <p>where:</p> <ul> <li>\\(MADR\\) is the mean absolute deviation of the residuals, and</li> <li>\\(MADS\\) is the mean absolute deviation of the seasonal component.</li> </ul> <pre><code>SI = MADR / MADS\n</code></pre> Credit <ul> <li>All credit to the <code>tsfeatures</code> library. This code is a direct copy+paste from the <code>tsfeatures.py</code> module.It is not possible to refer directly to a <code>spikiness</code> function in the <code>tsfeatures</code> package because the process to calculate seasonal strength is embedded within their <code>stl_features</code> function. Therefore, it it necessary to copy it here.</li> </ul> References <ul> <li>Wang, X, Hyndman, RJ, Smith-Miles, K (2007) \"Rule-based forecasting filters using time series features\", Computational Statistics and Data Analysis, 52(4), 2244-2259.</li> </ul> See Also <ul> <li><code>tsfeatures.stl_features</code></li> </ul> Source code in <code>src/ts_stat_tests/algorithms/seasonality.py</code> <pre><code>@typechecked\ndef spikiness(x: ArrayLike, m: int) -&gt; float:\n    r\"\"\"\n    !!! note \"Summary\"\n        The spikiness test is a statistical test that measures the degree of spikiness or volatility in a time series data. It aims to detect the presence of spikes or sudden changes in the data that may indicate important events or anomalies in the underlying process.\n\n    ???+ abstract \"Details\"\n\n        The spikiness test involves computing the spikiness index ($SI$). The $SI$ measures the intensity of spikes or outliers in the data relative to the overall variation. A higher $SI$ value indicates a more spiky or volatile time series, while a lower $SI$ value indicates a smoother or less volatile time series.\n\n        The spikiness test involves the following steps:\n\n        1. Decompose the time series data into its seasonal, trend, and residual components using a method such as STL or moving average decomposition.\n        1. Compute the mean absolute deviation of the residual component ($MADR$).\n        1. Compute the mean absolute deviation of the seasonal component ($MADS$).\n        1. Compute the spikiness index ($SI$) using the formula above.\n\n        The spikiness test can be used in conjunction with other tests and techniques for detecting spikes in time series data, such as change point analysis and outlier detection. However, it assumes that the time series data is stationary and that the spikes are abrupt and sudden. Additionally, it may not be effective for time series data with long-term trends or cyclical patterns. Therefore, it should be used in conjunction with other tests and techniques for detecting spikes in time series data.\n\n    Params:\n        x (ArrayLike):\n            The time series vector.\n        m (int):\n            The frequency of the time series data set. For the spikiness test to work, `m` must exceed 1.\n\n    Returns:\n        (float):\n            The spikiness score.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Basic usage\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.seasonality import spikiness\n        &gt;&gt;&gt; data = load_airline().values\n        &gt;&gt;&gt; spikiness(x=data, m=12)\n        0.484221...\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The spikiness test involves computing the spikiness index ($SI$) using the following formula:\n\n        $$\n        SI = \\frac {MADR} {MADS}\n        $$\n\n        where:\n\n        - $MADR$ is the mean absolute deviation of the residuals, and\n        - $MADS$ is the mean absolute deviation of the seasonal component.\n\n        ```\n        SI = MADR / MADS\n        ```\n\n    ??? success \"Credit\"\n        - All credit to the [`tsfeatures`](http://pkg.robjhyndman.com/tsfeatures/) library. This code is a direct copy+paste from the [`tsfeatures.py`](https://github.com/Nixtla/tsfeatures/blob/master/tsfeatures/tsfeatures.py) module.&lt;br&gt;It is not possible to refer directly to a `spikiness` function in the `tsfeatures` package because the process to calculate seasonal strength is embedded within their `stl_features` function. Therefore, it it necessary to copy it here.\n\n    ??? question \"References\"\n        - Wang, X, Hyndman, RJ, Smith-Miles, K (2007) \"Rule-based forecasting filters using time series features\", Computational Statistics and Data Analysis, 52(4), 2244-2259.\n\n    ??? tip \"See Also\"\n        - [`tsfeatures.stl_features`](https://github.com/Nixtla/tsfeatures/blob/main/tsfeatures/tsfeatures.py)\n    \"\"\"\n    decomposition = seasonal_decompose(x=x, model=\"additive\", period=m)\n    madr = np.nanmean(np.abs(decomposition.resid))\n    mads = np.nanmean(np.abs(decomposition.seasonal))\n    return float(madr / mads)\n</code></pre>"},{"location":"code/stationarity/","title":"Test the <code>stationarity</code> of a given Time-Series Dataset","text":""},{"location":"code/stationarity/#introduction","title":"Introduction","text":"<p>Summary</p> <p>As stated by Robert Nau, Duke University:</p> <p>A stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. Most statistical forecasting methods are based on the assumption that the time series can be rendered approximately stationary (i.e., \"stationarized\") through the use of mathematical transformations.</p> <p> For more info, see: Introduction to ARIMA models.</p> <p>Info</p> <p>There are two primary libraries used to implement these tests, ensuring both breadth of coverage and numerical reliability:</p> library category algorithm short import script url statsmodels Stationarity Augmented Dickey-Fuller ADF <code>from statsmodels.tsa.stattools import adfuller</code> https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.adfuller.html Stationarity Kwiatkowski-Phillips-Schmidt-Shin KPSS <code>from statsmodels.tsa.stattools import kpss</code> https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.kpss.html Unit-Root Zivot-Andrews structural-break unit-root test ZA <code>from statsmodels.tsa.stattools import zivot_andrews</code> https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.zivot_andrews.html Stationarity Range unit-root test for stationarity RUR <code>from statsmodels.tsa.stattools import range_unit_root_test</code> https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.range_unit_root_test.html arch Stationarity Phillips-Perron PP <code>from arch.unitroot import PhillipsPerron</code> https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.PhillipsPerron.html Stationarity Elliott-Rothenberg-Stock (ERS) de-trended DF ERS <code>from arch.unitroot import DFGLS</code> https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.DFGLS.html Unit-Root Variance Ratio (VR) test VR <code>from arch.unitroot import VarianceRatio</code> https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.VarianceRatio.html <p> For more info, see: Statsmodels TSA and Arch Unit Roots.</p> <p>Source Library</p> <p>The <code>statsmodels</code> and <code>arch</code> packages were chosen because they provide robust, industry-standard implementations of unit root and stationarity tests. Specifically, <code>statsmodels</code> offers the classic ADF and KPSS tests along with specialized ones like Zivot-Andrews and RUR, while <code>arch</code> provides high-performance implementations of Phillips-Perron, ERS, and Variance Ratio tests, ensuring a comprehensive suite of tools for detecting non-stationarity and random walks.</p> <p>Source Module</p> <p>All of the source code can be found within these modules:</p> <ul> <li><code>ts_stat_tests.algorithms.stationarity</code>.</li> <li><code>ts_stat_tests.tests.stationarity</code>.</li> </ul>"},{"location":"code/stationarity/#stationarity-tests","title":"Stationarity Tests","text":""},{"location":"code/stationarity/#ts_stat_tests.tests.stationarity","title":"ts_stat_tests.tests.stationarity","text":"<p>Summary</p> <p>This module contains convenience functions and tests for stationarity measures, allowing for easy access to different unit root and stationarity algorithms.</p>"},{"location":"code/stationarity/#ts_stat_tests.tests.stationarity.stationarity","title":"stationarity","text":"<pre><code>stationarity(\n    x: ArrayLike,\n    algorithm: str = \"adf\",\n    **kwargs: Union[float, int, str, bool, ArrayLike, None]\n) -&gt; STATIONARITY_RETURN_TYPE\n</code></pre> <p>Summary</p> <p>Perform a stationarity test on the given data.</p> Details <p>This function is a convenience wrapper around multiple underlying algorithms: - <code>adf()</code> - <code>kpss()</code> - <code>pp()</code> - <code>za()</code> - <code>ers()</code> - <code>vr()</code> - <code>rur()</code></p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked.</p> required <code>algorithm</code> <code>str</code> <p>Which stationarity algorithm to use. - <code>adf()</code>: <code>[\"adf\", \"augmented_dickey_fuller\"]</code> - <code>kpss()</code>: <code>[\"kpss\", \"kwiatkowski_phillips_schmidt_shin\"]</code> - <code>pp()</code>: <code>[\"pp\", \"phillips_perron\"]</code> - <code>za()</code>: <code>[\"za\", \"zivot_andrews\"]</code> - <code>ers()</code>: <code>[\"ers\", \"elliott_rothenberg_stock\"]</code> - <code>vr()</code>: <code>[\"vr\", \"variance_ratio\"]</code> - <code>rur()</code>: <code>[\"rur\", \"range_unit_root\"]</code> Defaults to <code>\"adf\"</code>.</p> <code>'adf'</code> <code>kwargs</code> <code>Union[float, int, str, bool, ArrayLike, None]</code> <p>Additional arguments to pass to the underlying algorithm.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the given value for <code>algorithm</code> is not valid.</p> <p>Returns:</p> Type Description <code>tuple[Union[float, int, dict, ResultsStore, None], ...]</code> <p>The result of the stationarity test.</p> <p>Credit</p> <p>Calculations are performed by <code>statsmodels</code>, <code>arch</code>, and <code>pmdarima</code>.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.tests.stationarity import stationarity\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n&gt;&gt;&gt; normal = data_normal\n</code></pre> Example 1: Augmented Dickey-Fuller<pre><code>&gt;&gt;&gt; result = stationarity(normal, algorithm=\"adf\")\n&gt;&gt;&gt; print(f\"ADF statistic: {result[0]:.4f}\")\nADF statistic: -30.7838\n</code></pre> Example 2: Kwiatkowski-Phillips-Schmidt-Shin<pre><code>&gt;&gt;&gt; result = stationarity(normal, algorithm=\"kpss\")\n&gt;&gt;&gt; print(f\"KPSS statistic: {result[0]:.4f}\")\nKPSS statistic: 0.0858\n</code></pre> Example 3: Phillips-Perron<pre><code>&gt;&gt;&gt; result = stationarity(normal, algorithm=\"pp\")\n&gt;&gt;&gt; print(f\"PP statistic: {result[0]:.4f}\")\nPP statistic: -30.7758\n</code></pre> Example 4: Zivot-Andrews<pre><code>&gt;&gt;&gt; result = stationarity(normal, algorithm=\"za\")\n&gt;&gt;&gt; print(f\"ZA statistic: {result[0]:.4f}\")\nZA statistic: -30.8800\n</code></pre> Example 5: Elliot-Rothenberg-Stock<pre><code>&gt;&gt;&gt; result = stationarity(normal, algorithm=\"ers\")\n&gt;&gt;&gt; print(f\"ERS statistic: {result[0]:.4f}\")\nERS statistic: -30.1517\n</code></pre> Example 6: Variance Ratio<pre><code>&gt;&gt;&gt; result = stationarity(normal, algorithm=\"vr\")\n&gt;&gt;&gt; print(f\"VR statistic: {result[0]:.4f}\")\nVR statistic: -12.8518\n</code></pre> Example 7: Range Unit Root<pre><code>&gt;&gt;&gt; result = stationarity(normal, algorithm=\"rur\")\n&gt;&gt;&gt; print(f\"RUR statistic: {result[0]:.4f}\")\nRUR statistic: 0.3479\n</code></pre> Example 8: Invalid algorithm<pre><code>&gt;&gt;&gt; stationarity(normal, algorithm=\"invalid\")\nTraceback (most recent call last):\n    ...\nValueError: Invalid 'algorithm': invalid. Options: {'adf': ('adf', 'augmented_dickey_fuller'), 'kpss': ('kpss', 'kwiatkowski_phillips_schmidt_shin'), 'pp': ('pp', 'phillips_perron'), 'za': ('za', 'zivot_andrews'), 'ers': ('ers', 'elliott_rothenberg_stock'), 'vr': ('vr', 'variance_ratio'), 'rur': ('rur', 'range_unit_root')}\n</code></pre> Source code in <code>src/ts_stat_tests/tests/stationarity.py</code> <pre><code>@typechecked\ndef stationarity(\n    x: ArrayLike,\n    algorithm: str = \"adf\",\n    **kwargs: Union[float, int, str, bool, ArrayLike, None],\n) -&gt; STATIONARITY_RETURN_TYPE:\n    \"\"\"\n    !!! note \"Summary\"\n        Perform a stationarity test on the given data.\n\n    ???+ abstract \"Details\"\n        This function is a convenience wrapper around multiple underlying algorithms:&lt;br&gt;\n        - [`adf()`][ts_stat_tests.algorithms.stationarity.adf]&lt;br&gt;\n        - [`kpss()`][ts_stat_tests.algorithms.stationarity.kpss]&lt;br&gt;\n        - [`pp()`][ts_stat_tests.algorithms.stationarity.pp]&lt;br&gt;\n        - [`za()`][ts_stat_tests.algorithms.stationarity.za]&lt;br&gt;\n        - [`ers()`][ts_stat_tests.algorithms.stationarity.ers]&lt;br&gt;\n        - [`vr()`][ts_stat_tests.algorithms.stationarity.vr]&lt;br&gt;\n        - [`rur()`][ts_stat_tests.algorithms.stationarity.rur]\n\n    Params:\n        x (ArrayLike):\n            The data to be checked.\n        algorithm (str):\n            Which stationarity algorithm to use.&lt;br&gt;\n            - `adf()`: `[\"adf\", \"augmented_dickey_fuller\"]`&lt;br&gt;\n            - `kpss()`: `[\"kpss\", \"kwiatkowski_phillips_schmidt_shin\"]`&lt;br&gt;\n            - `pp()`: `[\"pp\", \"phillips_perron\"]`&lt;br&gt;\n            - `za()`: `[\"za\", \"zivot_andrews\"]`&lt;br&gt;\n            - `ers()`: `[\"ers\", \"elliott_rothenberg_stock\"]`&lt;br&gt;\n            - `vr()`: `[\"vr\", \"variance_ratio\"]`&lt;br&gt;\n            - `rur()`: `[\"rur\", \"range_unit_root\"]`&lt;br&gt;\n            Defaults to `\"adf\"`.\n        kwargs (Union[float, int, str, bool, ArrayLike, None]):\n            Additional arguments to pass to the underlying algorithm.\n\n    Raises:\n        (ValueError):\n            When the given value for `algorithm` is not valid.\n\n    Returns:\n        (tuple[Union[float, int, dict, ResultsStore, None], ...]):\n            The result of the stationarity test.\n\n    !!! success \"Credit\"\n        Calculations are performed by `statsmodels`, `arch`, and `pmdarima`.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.tests.stationarity import stationarity\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n        &gt;&gt;&gt; normal = data_normal\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Augmented Dickey-Fuller\"}\n        &gt;&gt;&gt; result = stationarity(normal, algorithm=\"adf\")\n        &gt;&gt;&gt; print(f\"ADF statistic: {result[0]:.4f}\")\n        ADF statistic: -30.7838\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Kwiatkowski-Phillips-Schmidt-Shin\"}\n        &gt;&gt;&gt; result = stationarity(normal, algorithm=\"kpss\")\n        &gt;&gt;&gt; print(f\"KPSS statistic: {result[0]:.4f}\")\n        KPSS statistic: 0.0858\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 3: Phillips-Perron\"}\n        &gt;&gt;&gt; result = stationarity(normal, algorithm=\"pp\")\n        &gt;&gt;&gt; print(f\"PP statistic: {result[0]:.4f}\")\n        PP statistic: -30.7758\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 4: Zivot-Andrews\"}\n        &gt;&gt;&gt; result = stationarity(normal, algorithm=\"za\")\n        &gt;&gt;&gt; print(f\"ZA statistic: {result[0]:.4f}\")\n        ZA statistic: -30.8800\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 5: Elliot-Rothenberg-Stock\"}\n        &gt;&gt;&gt; result = stationarity(normal, algorithm=\"ers\")\n        &gt;&gt;&gt; print(f\"ERS statistic: {result[0]:.4f}\")\n        ERS statistic: -30.1517\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 6: Variance Ratio\"}\n        &gt;&gt;&gt; result = stationarity(normal, algorithm=\"vr\")\n        &gt;&gt;&gt; print(f\"VR statistic: {result[0]:.4f}\")\n        VR statistic: -12.8518\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 7: Range Unit Root\"}\n        &gt;&gt;&gt; result = stationarity(normal, algorithm=\"rur\")\n        &gt;&gt;&gt; print(f\"RUR statistic: {result[0]:.4f}\")\n        RUR statistic: 0.3479\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 8: Invalid algorithm\"}\n        &gt;&gt;&gt; stationarity(normal, algorithm=\"invalid\")\n        Traceback (most recent call last):\n            ...\n        ValueError: Invalid 'algorithm': invalid. Options: {'adf': ('adf', 'augmented_dickey_fuller'), 'kpss': ('kpss', 'kwiatkowski_phillips_schmidt_shin'), 'pp': ('pp', 'phillips_perron'), 'za': ('za', 'zivot_andrews'), 'ers': ('ers', 'elliott_rothenberg_stock'), 'vr': ('vr', 'variance_ratio'), 'rur': ('rur', 'range_unit_root')}\n\n        ```\n    \"\"\"\n    options: dict[str, tuple[str, ...]] = {\n        \"adf\": (\"adf\", \"augmented_dickey_fuller\"),\n        \"kpss\": (\"kpss\", \"kwiatkowski_phillips_schmidt_shin\"),\n        \"pp\": (\"pp\", \"phillips_perron\"),\n        \"za\": (\"za\", \"zivot_andrews\"),\n        \"ers\": (\"ers\", \"elliott_rothenberg_stock\"),\n        \"vr\": (\"vr\", \"variance_ratio\"),\n        \"rur\": (\"rur\", \"range_unit_root\"),\n    }\n\n    # Internal helper to handle kwargs casting for ty\n    def _call(\n        func: Callable[..., STATIONARITY_RETURN_TYPE],\n        **args: Union[float, int, str, bool, ArrayLike, None],\n    ) -&gt; STATIONARITY_RETURN_TYPE:\n        \"\"\"\n        !!! note \"Summary\"\n            Internal helper to call the test function.\n\n        Params:\n            func (Callable[..., STATIONARITY_RETURN_TYPE]):\n                The function to call.\n            args (Union[float, int, str, bool, ArrayLike, None]):\n                The arguments to pass to the function.\n\n        Returns:\n            (STATIONARITY_RETURN_TYPE):\n                The result of the function call.\n\n        ???+ example \"Examples\"\n\n            ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n            &gt;&gt;&gt; from ts_stat_tests.tests.stationarity import stationarity\n            &gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n            &gt;&gt;&gt; normal = data_normal\n            ```\n\n            ```pycon {.py .python linenums=\"1\" title=\"Example 1: ADF test via internal helper\"}\n            &gt;&gt;&gt; result = stationarity(normal, algorithm=\"adf\")\n            &gt;&gt;&gt; print(f\"ADF statistic: {result[0]:.4f}\")\n            ADF statistic: -30.7838\n\n            ```\n        \"\"\"\n        return func(**args)\n\n    if algorithm in options[\"adf\"]:\n        return _call(_adf, x=x, **kwargs)\n    if algorithm in options[\"kpss\"]:\n        return _call(_kpss, x=x, **kwargs)\n    if algorithm in options[\"pp\"]:\n        return _call(_pp, x=x, **kwargs)\n    if algorithm in options[\"za\"]:\n        return _call(_za, x=x, **kwargs)\n    if algorithm in options[\"ers\"]:\n        return _call(_ers, y=x, **kwargs)\n    if algorithm in options[\"vr\"]:\n        return _call(_vr, y=x, **kwargs)\n    if algorithm in options[\"rur\"]:\n        return _call(_rur, x=x, **kwargs)\n\n    raise ValueError(\n        generate_error_message(\n            parameter_name=\"algorithm\",\n            value_parsed=algorithm,\n            options=options,\n        )\n    )\n</code></pre>"},{"location":"code/stationarity/#ts_stat_tests.tests.stationarity.is_stationary","title":"is_stationary","text":"<pre><code>is_stationary(\n    x: ArrayLike,\n    algorithm: str = \"adf\",\n    alpha: float = 0.05,\n    **kwargs: Union[float, int, str, bool, ArrayLike, None]\n) -&gt; dict[str, Union[str, bool, STATIONARITY_ITEM, None]]\n</code></pre> <p>Summary</p> <p>Test whether a given data set is <code>stationary</code> or not.</p> Details <p>This function checks the results of a stationarity test against a significance level <code>alpha</code>.</p> <p>Note that different tests have different null hypotheses: - For ADF, PP, ZA, ERS, VR, RUR: H0 is non-stationarity (unit root). Stationary if p-value &lt; alpha. - For KPSS: H0 is stationarity. Stationary if p-value &gt; alpha.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data to be checked.</p> required <code>algorithm</code> <code>str</code> <p>Which stationarity algorithm to use. Defaults to <code>\"adf\"</code>.</p> <code>'adf'</code> <code>alpha</code> <code>float</code> <p>The significance level for the test. Defaults to <code>0.05</code>.</p> <code>0.05</code> <code>kwargs</code> <code>Union[float, int, str, bool, ArrayLike, None]</code> <p>Additional arguments to pass to the underlying algorithm.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, Union[str, float, bool, None]]</code> <p>A dictionary containing: - <code>\"result\"</code> (bool): Indicator if the series is stationary. - <code>\"statistic\"</code> (float): The test statistic. - <code>\"pvalue\"</code> (float): The p-value of the test. - <code>\"alpha\"</code> (float): The significance level used. - <code>\"algorithm\"</code> (str): The algorithm used.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.tests.stationarity import is_stationary\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n&gt;&gt;&gt; normal = data_normal\n</code></pre> Example 1: ADF test on stationary data<pre><code>&gt;&gt;&gt; res = is_stationary(normal, algorithm=\"adf\")\n&gt;&gt;&gt; res[\"result\"]\nTrue\n&gt;&gt;&gt; print(f\"p-value: {res['pvalue']:.4f}\")\np-value: 0.0000\n</code></pre> Example 2: KPSS test on stationary data<pre><code>&gt;&gt;&gt; res = is_stationary(normal, algorithm=\"kpss\")\n&gt;&gt;&gt; res[\"result\"]\nTrue\n&gt;&gt;&gt; print(f\"p-value: {res['pvalue']:.4f}\")\np-value: 0.1000\n</code></pre> Example 3: RUR test<pre><code>&gt;&gt;&gt; res = is_stationary(normal, algorithm=\"rur\")\n&gt;&gt;&gt; res[\"result\"]\nTrue\n&gt;&gt;&gt; print(f\"p-value: {res['pvalue']:.2f}\")\np-value: 0.01\n</code></pre> Source code in <code>src/ts_stat_tests/tests/stationarity.py</code> <pre><code>@typechecked\ndef is_stationary(\n    x: ArrayLike,\n    algorithm: str = \"adf\",\n    alpha: float = 0.05,\n    **kwargs: Union[float, int, str, bool, ArrayLike, None],\n) -&gt; dict[str, Union[str, bool, STATIONARITY_ITEM, None]]:\n    \"\"\"\n    !!! note \"Summary\"\n        Test whether a given data set is `stationary` or not.\n\n    ???+ abstract \"Details\"\n        This function checks the results of a stationarity test against a significance level `alpha`.\n\n        Note that different tests have different null hypotheses:\n        - For ADF, PP, ZA, ERS, VR, RUR: H0 is non-stationarity (unit root). Stationary if p-value &lt; alpha.\n        - For KPSS: H0 is stationarity. Stationary if p-value &gt; alpha.\n\n    Params:\n        x (ArrayLike):\n            The data to be checked.\n        algorithm (str):\n            Which stationarity algorithm to use. Defaults to `\"adf\"`.\n        alpha (float, optional):\n            The significance level for the test. Defaults to `0.05`.\n        kwargs (Union[float, int, str, bool, ArrayLike, None]):\n            Additional arguments to pass to the underlying algorithm.\n\n    Returns:\n        (dict[str, Union[str, float, bool, None]]):\n            A dictionary containing:\n            - `\"result\"` (bool): Indicator if the series is stationary.\n            - `\"statistic\"` (float): The test statistic.\n            - `\"pvalue\"` (float): The p-value of the test.\n            - `\"alpha\"` (float): The significance level used.\n            - `\"algorithm\"` (str): The algorithm used.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.tests.stationarity import is_stationary\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n        &gt;&gt;&gt; normal = data_normal\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: ADF test on stationary data\"}\n        &gt;&gt;&gt; res = is_stationary(normal, algorithm=\"adf\")\n        &gt;&gt;&gt; res[\"result\"]\n        True\n        &gt;&gt;&gt; print(f\"p-value: {res['pvalue']:.4f}\")\n        p-value: 0.0000\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: KPSS test on stationary data\"}\n        &gt;&gt;&gt; res = is_stationary(normal, algorithm=\"kpss\")\n        &gt;&gt;&gt; res[\"result\"]\n        True\n        &gt;&gt;&gt; print(f\"p-value: {res['pvalue']:.4f}\")\n        p-value: 0.1000\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 3: RUR test\"}\n        &gt;&gt;&gt; res = is_stationary(normal, algorithm=\"rur\")\n        &gt;&gt;&gt; res[\"result\"]\n        True\n        &gt;&gt;&gt; print(f\"p-value: {res['pvalue']:.2f}\")\n        p-value: 0.01\n\n        ```\n    \"\"\"\n    res: Any = stationarity(x=x, algorithm=algorithm, **kwargs)\n\n    stat: Any\n    pvalue: Any\n\n    # stationarity() always returns a tuple\n    res_tuple: Any = res\n    stat = res_tuple[0]\n    pvalue_or_bool = res_tuple[1]\n\n    # Handle H0 logic\n    stationary_h0 = (\n        \"kpss\",\n        \"kwiatkowski_phillips_schmidt_shin\",\n    )\n\n    is_stat: bool = False\n    pvalue = None\n\n    if isinstance(pvalue_or_bool, bool):\n        is_stat = pvalue_or_bool\n    elif isinstance(pvalue_or_bool, (int, float)):\n        pvalue = pvalue_or_bool\n        if algorithm in stationary_h0:\n            is_stat = bool(pvalue &gt; alpha)\n        else:\n            is_stat = bool(pvalue &lt; alpha)\n\n    # Define return dict explicitly to match return type hint\n    ret: dict[str, Union[str, bool, STATIONARITY_ITEM, None]] = {\n        \"result\": bool(is_stat),\n        \"statistic\": float(stat) if isinstance(stat, (int, float)) else stat,\n        \"pvalue\": float(pvalue) if pvalue is not None else None,\n        \"alpha\": float(alpha),\n        \"algorithm\": str(algorithm),\n    }\n\n    return ret\n</code></pre>"},{"location":"code/stationarity/#stationarity-algorithms","title":"Stationarity Algorithms","text":""},{"location":"code/stationarity/#ts_stat_tests.algorithms.stationarity","title":"ts_stat_tests.algorithms.stationarity","text":"<p>Summary</p> <p>Stationarity tests are statistical tests used to determine whether a time series is stationary or not. A stationary time series is one whose statistical properties, such as mean and variance, do not change over time. Stationarity is an important assumption in many time series forecasting models, as it allows for the use of techniques such as autoregression and moving averages.</p> <p>There are several different types of stationarity tests, including the Augmented Dickey-Fuller (ADF) test, the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test, the Phillips-Perron (PP) test, the Elliott-Rothenberg-Stock (ERS) test, and the Variance Ratio (VR) test. Each of these tests has its own strengths and weaknesses, and the choice of which test to use will depend on the specific characteristics of the time series being analyzed.</p> <p>Overall, stationarity tests are an important tool in time series analysis and forecasting, as they help identify whether a time series is stationary or non-stationary, which can have implications for the choice of forecasting models and methods.</p> <p>For a really good article on ADF &amp; KPSS tests, check: When A Time Series Only Quacks Like A Duck: Testing for Stationarity Before Running Forecast Models. With Python. And A Duckling Picture.</p>"},{"location":"code/stationarity/#ts_stat_tests.algorithms.stationarity.adf","title":"adf","text":"<pre><code>adf(\n    x: ArrayLike,\n    maxlag: Optional[int] = None,\n    regression: VALID_ADF_REGRESSION_OPTIONS = \"c\",\n    *,\n    autolag: Optional[VALID_ADF_AUTOLAG_OPTIONS] = \"AIC\",\n    store: Literal[True],\n    regresults: bool = False\n) -&gt; tuple[float, float, dict, ResultsStore]\n</code></pre><pre><code>adf(\n    x: ArrayLike,\n    maxlag: Optional[int] = None,\n    regression: VALID_ADF_REGRESSION_OPTIONS = \"c\",\n    *,\n    autolag: None,\n    store: Literal[False] = False,\n    regresults: bool = False\n) -&gt; tuple[float, float, int, int, dict]\n</code></pre><pre><code>adf(\n    x: ArrayLike,\n    maxlag: Optional[int] = None,\n    regression: VALID_ADF_REGRESSION_OPTIONS = \"c\",\n    *,\n    autolag: VALID_ADF_AUTOLAG_OPTIONS = \"AIC\",\n    store: Literal[False] = False,\n    regresults: bool = False\n) -&gt; tuple[float, float, int, int, dict, float]\n</code></pre> <pre><code>adf(\n    x: ArrayLike,\n    maxlag: Optional[int] = None,\n    regression: VALID_ADF_REGRESSION_OPTIONS = \"c\",\n    *,\n    autolag: Optional[VALID_ADF_AUTOLAG_OPTIONS] = \"AIC\",\n    store: bool = False,\n    regresults: bool = False\n) -&gt; Union[\n    tuple[float, float, dict, ResultsStore],\n    tuple[float, float, int, int, dict],\n    tuple[float, float, int, int, dict, float],\n]\n</code></pre> <p>Summary</p> <p>The Augmented Dickey-Fuller test can be used to test for a unit root in a univariate process in the presence of serial correlation.</p> Details <p>The Augmented Dickey-Fuller (ADF) test is a statistical test used to determine whether a time series is stationary or not. Stationarity refers to the property of a time series where the statistical properties, such as mean and variance, remain constant over time. Stationarity is important for time series forecasting as it allows for the use of many popular forecasting models, such as ARIMA.</p> <p>The ADF test is an extension of the Dickey-Fuller test and involves regressing the first-difference of the time series on its lagged values, and then testing whether the coefficient of the lagged first-difference term is statistically significant. If it is, then the time series is considered non-stationary.</p> <p>The null hypothesis of the ADF test is that the time series has a unit root, which means that it is non-stationary. The alternative hypothesis is that the time series is stationary. If the p-value of the test is less than a chosen significance level, typically 0.05, then we reject the null hypothesis and conclude that the time series is stationary.</p> <p>In practical terms, if a time series is found to be non-stationary by the ADF test, one can apply differencing to the time series until it becomes stationary. This involves taking the difference between consecutive observations and potentially repeating this process until the time series is stationary.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data series to test.</p> required <code>maxlag</code> <code>Optional[int]</code> <p>Maximum lag which is included in test, default value of \\(12 \\times (\\frac{nobs}{100})^{\\frac{1}{4}}\\) is used when <code>None</code>. Default: <code>None</code></p> <code>None</code> <code>regression</code> <code>VALID_ADF_REGRESSION_OPTIONS</code> <p>Constant and trend order to include in regression.</p> <ul> <li><code>\"c\"</code>: constant only (default).</li> <li><code>\"ct\"</code>: constant and trend.</li> <li><code>\"ctt\"</code>: constant, and linear and quadratic trend.</li> <li><code>\"n\"</code>: no constant, no trend.</li> </ul> <p>Default: <code>\"c\"</code></p> <code>'c'</code> <code>autolag</code> <code>Optional[VALID_ADF_AUTOLAG_OPTIONS]</code> <p>Method to use when automatically determining the lag length among the values \\(0, 1, ..., maxlag\\).</p> <ul> <li>If <code>\"AIC\"</code> (default) or <code>\"BIC\"</code>, then the number of lags is chosen to minimize the corresponding information criterion.</li> <li><code>\"t-stat\"</code> based choice of <code>maxlag</code>. Starts with <code>maxlag</code> and drops a lag until the t-statistic on the last lag length is significant using a 5%-sized test.</li> <li>If <code>None</code>, then the number of included lags is set to <code>maxlag</code>.</li> </ul> <p>Default: <code>\"AIC\"</code></p> <code>'AIC'</code> <code>store</code> <code>bool</code> <p>If <code>True</code>, then a result instance is returned additionally to the <code>adf</code> statistic. Default: <code>False</code></p> <code>False</code> <code>regresults</code> <code>bool</code> <p>If <code>True</code>, the full regression results are returned. Default: <code>False</code></p> <code>False</code> <p>Returns:</p> Type Description <code>Union[tuple[float, float, dict, ResultsStore], tuple[float, float, int, int, dict], tuple[float, float, int, int, dict, float]]</code> <p>Depending on parameters, returns a tuple containing: - <code>adf</code> (float): The test statistic. - <code>pvalue</code> (float): MacKinnon's approximate p-value. - <code>uselag</code> (int): The number of lags used. - <code>nobs</code> (int): The number of observations used. - <code>critical_values</code> (dict): Critical values at the 1%, 5%, and 10% levels. - <code>icbest</code> (float): The maximized information criterion (if <code>autolag</code> is not <code>None</code>). - <code>resstore</code> (Optional[ResultsStore]): Result instance (if <code>store</code> is <code>True</code>).</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.stationarity import adf\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_normal\n&gt;&gt;&gt; normal = data_normal\n&gt;&gt;&gt; airline = data_airline.values\n</code></pre> Example 1: Stationary Series<pre><code>&gt;&gt;&gt; stat, pvalue, lags, nobs, crit, icbest = adf(x=normal)\n&gt;&gt;&gt; print(f\"ADF statistic: {stat:.4f}\")\nADF statistic: -30.7838\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.0000\n</code></pre> Example 2: Airline Passengers Data<pre><code>&gt;&gt;&gt; stat, pvalue, lags, nobs, crit, icbest = adf(x=airline)\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.9919\n</code></pre> Example 3: Store Result Instance<pre><code>&gt;&gt;&gt; res = adf(x=airline, store=True)\n&gt;&gt;&gt; print(res)\n(0.8153..., 0.9918..., {'1%': np.float64(-3.4816...), '5%': np.float64(-2.8840...), '10%': np.float64(-2.5787...)}, &lt;statsmodels.stats.diagnostic.ResultsStore object at ...&gt;)\n</code></pre> Example 4: No Autolag<pre><code>&gt;&gt;&gt; stat, pvalue, lags, nobs, crit = adf(x=airline, autolag=None, maxlag=5)\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.7670\n</code></pre> Calculation <p>The mathematical equation for the Augmented Dickey-Fuller (ADF) test for stationarity in time series forecasting is:</p> \\[ \\Delta y_t = \\alpha + \\beta y_{t-1} + \\sum_{i=1}^p \\delta_i \\Delta y_{t-i} + \\epsilon_t \\] <p>where:</p> <ul> <li>\\(y_t\\) is the value of the time series at time \\(t\\).</li> <li>\\(\\Delta y_t\\) is the first difference of \\(y_t\\), which is defined as \\(\\Delta y_t = y_t - y_{t-1}\\).</li> <li>\\(\\alpha\\) is the constant term.</li> <li>\\(\\beta\\) is the coefficient on \\(y_{t-1}\\).</li> <li>\\(\\delta_i\\) are the coefficients on the lagged differences of \\(y\\).</li> <li>\\(\\epsilon_t\\) is the error term.</li> </ul> <p>The ADF test involves testing the null hypothesis that \\(\\beta = 0\\), or equivalently, that the time series has a unit root. If \\(\\beta\\) is significantly different from \\(0\\), then the null hypothesis can be rejected and the time series is considered stationary.</p> <p>Here are the detailed steps for how to calculate the ADF test:</p> <ol> <li> <p>Collect your time series data and plot it to visually check for any trends, seasonal patterns, or other patterns that could make the data non-stationary. If you detect any such patterns, you will need to pre-process your data (e.g., detrending, deseasonalizing, etc.) to remove these effects.</p> </li> <li> <p>Calculate the first differences of the time series, which is simply the difference between each observation and the previous observation. This step is performed to transform the original data into a stationary process. The first difference of \\(y_t\\) is defined as \\(\\Delta y_t = y_t - y_{t-1}\\).</p> </li> <li> <p>Estimate the parameters \\(\\alpha\\), \\(\\beta\\), and \\(\\delta_i\\) using the least squares method. This involves regressing \\(\\Delta y_t\\) on its lagged values, \\(y_{t-1}\\), and the lagged differences of \\(y, \\Delta y_{t-1}, \\Delta y_{t-2}, \\dots, \\Delta y_{t-p}\\), where \\(p\\) is the number of lags to include in the model. The estimated equation is:</p> \\[ \\Delta y_t = \\alpha + \\beta y_{t-1} + \\sum_{i=1}^p \\delta_i \\Delta y_{t-i} + \\epsilon_t \\] </li> <li> <p>Calculate the test statistic, which is given by:</p> \\[ ADF = \\frac {\\beta-1}{SE(\\beta)} \\] <ul> <li>where \\(SE(\\beta)\\) is the standard error of the coefficient on \\(y_{t-1}\\).</li> </ul> <p>The test statistic measures the number of standard errors by which \\(\\beta\\) deviates from \\(1\\). If ADF is less than the critical values from the ADF distribution table, we can reject the null hypothesis and conclude that the time series is stationary.</p> </li> <li> <p>Compare the test statistic to the critical values in the ADF distribution table to determine the level of significance. The critical values depend on the sample size, the level of significance, and the number of lags in the model.</p> </li> <li> <p>Finally, interpret the results and draw conclusions about the stationarity of the time series. If the null hypothesis is rejected, then the time series is stationary and can be used for forecasting. If the null hypothesis is not rejected, then the time series is non-stationary and requires further pre-processing before it can be used for forecasting.</p> </li> </ol> Notes <p>The null hypothesis of the Augmented Dickey-Fuller is that there is a unit root, with the alternative that there is no unit root. If the p-value is above a critical size, then we cannot reject that there is a unit root.</p> <p>The p-values are obtained through regression surface approximation from MacKinnon 1994, but using the updated 2010 tables. If the p-value is close to significant, then the critical values should be used to judge whether to reject the null.</p> <p>The <code>autolag</code> option and <code>maxlag</code> for it are described in Greene.</p> Credit <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> References <ul> <li>Baum, C.F. (2004). ZANDREWS: Stata module to calculate Zivot-Andrews unit root test in presence of structural break,\" Statistical Software Components S437301, Boston College Department of Economics, revised 2015.</li> <li>Schwert, G.W. (1989). Tests for unit roots: A Monte Carlo investigation. Journal of Business &amp; Economic Statistics, 7: 147-159.</li> <li>Zivot, E., and Andrews, D.W.K. (1992). Further evidence on the great crash, the oil-price shock, and the unit-root hypothesis. Journal of Business &amp; Economic Studies, 10: 251-270.</li> </ul> See Also <ul> <li><code>statsmodels.tsa.stattools.adfuller</code>: Augmented Dickey-Fuller unit root test.</li> <li><code>statsmodels.tsa.stattools.kpss</code>: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.</li> <li><code>statsmodels.tsa.stattools.range_unit_root_test</code>: Range Unit-Root test.</li> <li><code>statsmodels.tsa.stattools.zivot_andrews</code>: Zivot-Andrews structural break test.</li> <li><code>pmdarima.arima.PPTest</code>: Phillips-Perron unit root test.</li> <li><code>arch.unitroot.DFGLS</code>: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller.</li> <li><code>arch.unitroot.VarianceRatio</code>: Variance Ratio test of a random walk.</li> <li><code>ts_stat_tests.algorithms.stationarity.adf</code>: Augmented Dickey-Fuller unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.kpss</code>: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.</li> <li><code>ts_stat_tests.algorithms.stationarity.rur</code>: Range Unit-Root test of stationarity.</li> <li><code>ts_stat_tests.algorithms.stationarity.za</code>: Zivot-Andrews structural break unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.pp</code>: Phillips-Perron unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.ers</code>: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller test.</li> <li><code>ts_stat_tests.algorithms.stationarity.vr</code>: Variance Ratio test of a random walk.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/stationarity.py</code> <pre><code>@typechecked\ndef adf(\n    x: ArrayLike,\n    maxlag: Optional[int] = None,\n    regression: VALID_ADF_REGRESSION_OPTIONS = \"c\",\n    *,\n    autolag: Optional[VALID_ADF_AUTOLAG_OPTIONS] = \"AIC\",\n    store: bool = False,\n    regresults: bool = False,\n) -&gt; Union[\n    tuple[float, float, dict, ResultsStore],\n    tuple[float, float, int, int, dict],\n    tuple[float, float, int, int, dict, float],\n]:\n    r\"\"\"\n    !!! note \"Summary\"\n        The Augmented Dickey-Fuller test can be used to test for a unit root in a univariate process in the presence of serial correlation.\n\n    ???+ abstract \"Details\"\n\n        The Augmented Dickey-Fuller (ADF) test is a statistical test used to determine whether a time series is stationary or not. Stationarity refers to the property of a time series where the statistical properties, such as mean and variance, remain constant over time. Stationarity is important for time series forecasting as it allows for the use of many popular forecasting models, such as ARIMA.\n\n        The ADF test is an extension of the Dickey-Fuller test and involves regressing the first-difference of the time series on its lagged values, and then testing whether the coefficient of the lagged first-difference term is statistically significant. If it is, then the time series is considered non-stationary.\n\n        The null hypothesis of the ADF test is that the time series has a unit root, which means that it is non-stationary. The alternative hypothesis is that the time series is stationary. If the p-value of the test is less than a chosen significance level, typically 0.05, then we reject the null hypothesis and conclude that the time series is stationary.\n\n        In practical terms, if a time series is found to be non-stationary by the ADF test, one can apply differencing to the time series until it becomes stationary. This involves taking the difference between consecutive observations and potentially repeating this process until the time series is stationary.\n\n    Params:\n        x (ArrayLike):\n            The data series to test.\n        maxlag (Optional[int]):\n            Maximum lag which is included in test, default value of $12 \\times (\\frac{nobs}{100})^{\\frac{1}{4}}$ is used when `None`.\n            Default: `None`\n        regression (VALID_ADF_REGRESSION_OPTIONS):\n            Constant and trend order to include in regression.\n\n            - `\"c\"`: constant only (default).\n            - `\"ct\"`: constant and trend.\n            - `\"ctt\"`: constant, and linear and quadratic trend.\n            - `\"n\"`: no constant, no trend.\n\n            Default: `\"c\"`\n        autolag (Optional[VALID_ADF_AUTOLAG_OPTIONS]):\n            Method to use when automatically determining the lag length among the values $0, 1, ..., maxlag$.\n\n            - If `\"AIC\"` (default) or `\"BIC\"`, then the number of lags is chosen to minimize the corresponding information criterion.\n            - `\"t-stat\"` based choice of `maxlag`. Starts with `maxlag` and drops a lag until the t-statistic on the last lag length is significant using a 5%-sized test.\n            - If `None`, then the number of included lags is set to `maxlag`.\n\n            Default: `\"AIC\"`\n        store (bool):\n            If `True`, then a result instance is returned additionally to the `adf` statistic.\n            Default: `False`\n        regresults (bool):\n            If `True`, the full regression results are returned.\n            Default: `False`\n\n    Returns:\n        (Union[tuple[float, float, dict, ResultsStore], tuple[float, float, int, int, dict], tuple[float, float, int, int, dict, float]]):\n            Depending on parameters, returns a tuple containing:\n            - `adf` (float): The test statistic.\n            - `pvalue` (float): MacKinnon's approximate p-value.\n            - `uselag` (int): The number of lags used.\n            - `nobs` (int): The number of observations used.\n            - `critical_values` (dict): Critical values at the 1%, 5%, and 10% levels.\n            - `icbest` (float): The maximized information criterion (if `autolag` is not `None`).\n            - `resstore` (Optional[ResultsStore]): Result instance (if `store` is `True`).\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.stationarity import adf\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_normal\n        &gt;&gt;&gt; normal = data_normal\n        &gt;&gt;&gt; airline = data_airline.values\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Stationary Series\"}\n        &gt;&gt;&gt; stat, pvalue, lags, nobs, crit, icbest = adf(x=normal)\n        &gt;&gt;&gt; print(f\"ADF statistic: {stat:.4f}\")\n        ADF statistic: -30.7838\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.0000\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Airline Passengers Data\"}\n        &gt;&gt;&gt; stat, pvalue, lags, nobs, crit, icbest = adf(x=airline)\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.9919\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 3: Store Result Instance\"}\n        &gt;&gt;&gt; res = adf(x=airline, store=True)\n        &gt;&gt;&gt; print(res)\n        (0.8153..., 0.9918..., {'1%': np.float64(-3.4816...), '5%': np.float64(-2.8840...), '10%': np.float64(-2.5787...)}, &lt;statsmodels.stats.diagnostic.ResultsStore object at ...&gt;)\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 4: No Autolag\"}\n        &gt;&gt;&gt; stat, pvalue, lags, nobs, crit = adf(x=airline, autolag=None, maxlag=5)\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.7670\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The mathematical equation for the Augmented Dickey-Fuller (ADF) test for stationarity in time series forecasting is:\n\n        $$\n        \\Delta y_t = \\alpha + \\beta y_{t-1} + \\sum_{i=1}^p \\delta_i \\Delta y_{t-i} + \\epsilon_t\n        $$\n\n        where:\n\n        - $y_t$ is the value of the time series at time $t$.\n        - $\\Delta y_t$ is the first difference of $y_t$, which is defined as $\\Delta y_t = y_t - y_{t-1}$.\n        - $\\alpha$ is the constant term.\n        - $\\beta$ is the coefficient on $y_{t-1}$.\n        - $\\delta_i$ are the coefficients on the lagged differences of $y$.\n        - $\\epsilon_t$ is the error term.\n\n        The ADF test involves testing the null hypothesis that $\\beta = 0$, or equivalently, that the time series has a unit root. If $\\beta$ is significantly different from $0$, then the null hypothesis can be rejected and the time series is considered stationary.\n\n        Here are the detailed steps for how to calculate the ADF test:\n\n        1. Collect your time series data and plot it to visually check for any trends, seasonal patterns, or other patterns that could make the data non-stationary. If you detect any such patterns, you will need to pre-process your data (e.g., detrending, deseasonalizing, etc.) to remove these effects.\n\n        1. Calculate the first differences of the time series, which is simply the difference between each observation and the previous observation. This step is performed to transform the original data into a stationary process. The first difference of $y_t$ is defined as $\\Delta y_t = y_t - y_{t-1}$.\n\n        1. Estimate the parameters $\\alpha$, $\\beta$, and $\\delta_i$ using the least squares method. This involves regressing $\\Delta y_t$ on its lagged values, $y_{t-1}$, and the lagged differences of $y, \\Delta y_{t-1}, \\Delta y_{t-2}, \\dots, \\Delta y_{t-p}$, where $p$ is the number of lags to include in the model. The estimated equation is:\n\n            $$\n            \\Delta y_t = \\alpha + \\beta y_{t-1} + \\sum_{i=1}^p \\delta_i \\Delta y_{t-i} + \\epsilon_t\n            $$\n\n        1. Calculate the test statistic, which is given by:\n\n            $$\n            ADF = \\frac {\\beta-1}{SE(\\beta)}\n            $$\n\n            - where $SE(\\beta)$ is the standard error of the coefficient on $y_{t-1}$.\n\n            The test statistic measures the number of standard errors by which $\\beta$ deviates from $1$. If ADF is less than the critical values from the ADF distribution table, we can reject the null hypothesis and conclude that the time series is stationary.\n\n        1. Compare the test statistic to the critical values in the ADF distribution table to determine the level of significance. The critical values depend on the sample size, the level of significance, and the number of lags in the model.\n\n        1. Finally, interpret the results and draw conclusions about the stationarity of the time series. If the null hypothesis is rejected, then the time series is stationary and can be used for forecasting. If the null hypothesis is not rejected, then the time series is non-stationary and requires further pre-processing before it can be used for forecasting.\n\n    ??? note \"Notes\"\n        The null hypothesis of the Augmented Dickey-Fuller is that there is a unit root, with the alternative that there is no unit root. If the p-value is above a critical size, then we cannot reject that there is a unit root.\n\n        The p-values are obtained through regression surface approximation from MacKinnon 1994, but using the updated 2010 tables. If the p-value is close to significant, then the critical values should be used to judge whether to reject the null.\n\n        The `autolag` option and `maxlag` for it are described in Greene.\n\n    ??? success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.adfuller.html) library.\n\n    ??? question \"References\"\n        - Baum, C.F. (2004). ZANDREWS: Stata module to calculate Zivot-Andrews unit root test in presence of structural break,\" Statistical Software Components S437301, Boston College Department of Economics, revised 2015.\n        - Schwert, G.W. (1989). Tests for unit roots: A Monte Carlo investigation. Journal of Business &amp; Economic Statistics, 7: 147-159.\n        - Zivot, E., and Andrews, D.W.K. (1992). Further evidence on the great crash, the oil-price shock, and the unit-root hypothesis. Journal of Business &amp; Economic Studies, 10: 251-270.\n\n    ??? tip \"See Also\"\n        - [`statsmodels.tsa.stattools.adfuller`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.adfuller.html): Augmented Dickey-Fuller unit root test.\n        - [`statsmodels.tsa.stattools.kpss`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.kpss.html): Kwiatkowski-Phillips-Schmidt-Shin stationarity test.\n        - [`statsmodels.tsa.stattools.range_unit_root_test`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.range_unit_root_test.html): Range Unit-Root test.\n        - [`statsmodels.tsa.stattools.zivot_andrews`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.zivot_andrews.html): Zivot-Andrews structural break test.\n        - [`pmdarima.arima.PPTest`](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.PPTest.html): Phillips-Perron unit root test.\n        - [`arch.unitroot.DFGLS`](https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.DFGLS.html): Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller.\n        - [`arch.unitroot.VarianceRatio`](https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.VarianceRatio.html): Variance Ratio test of a random walk.\n        - [`ts_stat_tests.algorithms.stationarity.adf`][ts_stat_tests.algorithms.stationarity.adf]: Augmented Dickey-Fuller unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.kpss`][ts_stat_tests.algorithms.stationarity.kpss]: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.\n        - [`ts_stat_tests.algorithms.stationarity.rur`][ts_stat_tests.algorithms.stationarity.rur]: Range Unit-Root test of stationarity.\n        - [`ts_stat_tests.algorithms.stationarity.za`][ts_stat_tests.algorithms.stationarity.za]: Zivot-Andrews structural break unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.pp`][ts_stat_tests.algorithms.stationarity.pp]: Phillips-Perron unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.ers`][ts_stat_tests.algorithms.stationarity.ers]: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller test.\n        - [`ts_stat_tests.algorithms.stationarity.vr`][ts_stat_tests.algorithms.stationarity.vr]: Variance Ratio test of a random walk.\n    \"\"\"\n    res: Any = _adfuller(  # Using `Any` to avoid ty issues with statsmodels stubs\n        x=x,\n        maxlag=maxlag,\n        regression=regression,\n        autolag=autolag,  # type: ignore[arg-type] # statsmodels stubs are often missing `None`\n        store=store,\n        regresults=regresults,\n    )\n\n    if store:\n        # returns (stat, pval, crit, store)\n        return float(res[0]), float(res[1]), dict(res[2]), res[3]\n\n    if autolag is None:\n        # returns (stat, pval, lags, nobs, crit)\n        return (\n            float(res[0]),\n            float(res[1]),\n            int(res[2]),\n            int(res[3]),\n            dict(res[4]),\n        )\n\n    # returns (stat, pval, lags, nobs, crit, icbest)\n    return (\n        float(res[0]),\n        float(res[1]),\n        int(res[2]),\n        int(res[3]),\n        dict(res[4]),\n        float(res[5]),\n    )\n</code></pre>"},{"location":"code/stationarity/#ts_stat_tests.algorithms.stationarity.kpss","title":"kpss","text":"<pre><code>kpss(\n    x: ArrayLike,\n    regression: VALID_KPSS_REGRESSION_OPTIONS = \"c\",\n    nlags: Optional[\n        Union[VALID_KPSS_NLAGS_OPTIONS, int]\n    ] = None,\n    *,\n    store: Literal[True]\n) -&gt; tuple[float, float, int, dict, ResultsStore]\n</code></pre><pre><code>kpss(\n    x: ArrayLike,\n    regression: VALID_KPSS_REGRESSION_OPTIONS = \"c\",\n    nlags: Optional[\n        Union[VALID_KPSS_NLAGS_OPTIONS, int]\n    ] = None,\n    *,\n    store: Literal[False] = False\n) -&gt; tuple[float, float, int, dict]\n</code></pre> <pre><code>kpss(\n    x: ArrayLike,\n    regression: VALID_KPSS_REGRESSION_OPTIONS = \"c\",\n    nlags: Optional[\n        Union[VALID_KPSS_NLAGS_OPTIONS, int]\n    ] = None,\n    *,\n    store: bool = False\n) -&gt; Union[\n    tuple[float, float, int, dict, ResultsStore],\n    tuple[float, float, int, dict],\n]\n</code></pre> <p>Summary</p> <p>Computes the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test for the null hypothesis that <code>x</code> is level or trend stationary.</p> Details <p>The Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test is another statistical test used to determine whether a time series is stationary or not. The KPSS test is the opposite of the Augmented Dickey-Fuller (ADF) test, which tests for the presence of a unit root in the time series.</p> <p>The KPSS test involves regressing the time series on a constant and a time trend. The null hypothesis of the test is that the time series is stationary. The alternative hypothesis is that the time series has a unit root, which means that it is non-stationary.</p> <p>The test statistic is calculated by taking the sum of the squared residuals of the regression. If the test statistic is greater than a critical value at a given significance level, typically 0.05, then we reject the null hypothesis and conclude that the time series is non-stationary. If the test statistic is less than the critical value, then we fail to reject the null hypothesis and conclude that the time series is stationary.</p> <p>In practical terms, if a time series is found to be non-stationary by the KPSS test, one can apply differencing to the time series until it becomes stationary. This involves taking the difference between consecutive observations and potentially repeating this process until the time series is stationary.</p> <p>Overall, the ADF and KPSS tests are both important tools in time series analysis and forecasting, as they help identify whether a time series is stationary or non-stationary, which can have implications for the choice of forecasting models and methods.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data series to test.</p> required <code>regression</code> <code>VALID_KPSS_REGRESSION_OPTIONS</code> <p>The null hypothesis for the KPSS test.</p> <ul> <li><code>\"c\"</code>: The data is stationary around a constant (default).</li> <li><code>\"ct\"</code>: The data is stationary around a trend.</li> </ul> <p>Defaults to <code>\"c\"</code>.</p> <code>'c'</code> <code>nlags</code> <code>Optional[Union[VALID_KPSS_NLAGS_OPTIONS, int]]</code> <p>Indicates the number of lags to be used.</p> <ul> <li>If <code>\"auto\"</code> (default), <code>lags</code> is calculated using the data-dependent method of Hobijn et al. (1998). See also Andrews (1991), Newey &amp; West (1994), and Schwert (1989).</li> <li>If set to <code>\"legacy\"</code>, uses \\(int(12 \\\\times (\\\\frac{n}{100})^{\\\\frac{1}{4}})\\), as outlined in Schwert (1989).</li> </ul> <p>Defaults to <code>None</code>.</p> <code>None</code> <code>store</code> <code>bool</code> <p>If <code>True</code>, then a result instance is returned additionally to the KPSS statistic. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[tuple[float, float, int, dict, ResultsStore], tuple[float, float, int, dict]]</code> <p>Returns a tuple containing: - <code>stat</code> (float): The KPSS test statistic. - <code>pvalue</code> (float): The p-value of the test. - <code>lags</code> (int): The truncation lag parameter. - <code>crit</code> (dict): The critical values at 10%, 5%, 2.5%, and 1%. - <code>resstore</code> (Optional[ResultsStore]): Result instance (if <code>store</code> is <code>True</code>).</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.stationarity import kpss\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_normal\n&gt;&gt;&gt; normal = data_normal\n&gt;&gt;&gt; airline = data_airline.values\n</code></pre> Example 1: Stationary Series<pre><code>&gt;&gt;&gt; stat, pvalue, lags, crit = kpss(x=normal)\n&gt;&gt;&gt; print(f\"KPSS statistic: {stat:.4f}\")\nKPSS statistic: 0.0858\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.1000\n</code></pre> Example 2: Airline Passengers Data<pre><code>&gt;&gt;&gt; stat, pvalue, lags, crit = kpss(x=airline)\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.0100\n</code></pre> Calculation <p>The mathematical equation for the KPSS test for stationarity in time series forecasting is:</p> \\[ y_t = \\mu_t + \\epsilon_t \\] <p>where:</p> <ul> <li>\\(y_t\\) is the value of the time series at time \\(t\\).</li> <li>\\(\\mu_t\\) is the trend component of the time series.</li> <li>\\(\\epsilon_t\\) is the error term.</li> </ul> <p>The KPSS test involves testing the null hypothesis that the time series is trend stationary, which means that the trend component of the time series is stationary over time. If the null hypothesis is rejected, then the time series is non-stationary and requires further pre-processing before it can be used for forecasting.</p> <p>Here are the detailed steps for how to calculate the KPSS test:</p> <ol> <li> <p>Collect your time series data and plot it to visually check for any trends, seasonal patterns, or other patterns that could make the data non-stationary. If you detect any such patterns, you will need to pre-process your data (e.g., detrending, deseasonalizing, etc.) to remove these effects.</p> </li> <li> <p>Divide your time series data into multiple overlapping windows of equal size. The length of each window depends on the length of your time series and the level of detail you want to capture.</p> </li> <li> <p>Calculate the trend component \\(\\mu_t\\) for each window using a trend estimation method. There are several methods for estimating the trend component, such as the Hodrick-Prescott filter, the Christiano-Fitzgerald filter, or simple linear regression. The choice of method depends on the characteristics of your data and the level of accuracy you want to achieve.</p> </li> <li> <p>Calculate the residual series \\(\\epsilon_t\\) by subtracting the trend component from the original time series:</p> \\[ \\epsilon_t = y_t - \\mu_t \\] </li> <li> <p>Estimate the variance of the residual series using a suitable estimator, such as the Newey-West estimator or the Bartlett kernel estimator. This step is necessary to correct for any serial correlation in the residual series.</p> </li> <li> <p>Calculate the test statistic, which is given by:</p> \\[ KPSS = T \\times \\sum_{t=1}^T \\frac {S_t^2} {\\sigma^2} \\] <p>where:</p> <ul> <li>\\(T\\) is the number of observations in the time series.</li> <li>\\(S_t\\) is the cumulative sum of the residual series up to time \\(t\\), i.e., \\(S_t = \\sum_{i=1}^t \\epsilon_i\\).</li> <li>\\(\\sigma^2\\) is the estimated variance of the residual series.</li> </ul> <p>The test statistic measures the strength of the trend component relative to the residual series. If KPSS is greater than the critical values from the KPSS distribution table, we can reject the null hypothesis and conclude that the time series is non-stationary.</p> </li> <li> <p>Finally, interpret the results and draw conclusions about the stationarity of the time series. If the null hypothesis is rejected, then the time series is non-stationary and requires further pre-processing before it can be used for forecasting. If the null hypothesis is not rejected, then the time series is trend stationary and can be used for forecasting.</p> </li> </ol> Notes <p>To estimate \\(\\sigma^2\\) the Newey-West estimator is used. If <code>lags</code> is <code>\"legacy\"</code>, the truncation lag parameter is set to \\(int(12 \\times (\\frac{n}{100})^{\\frac{1}{4}})\\), as outlined in Schwert (1989). The p-values are interpolated from Table 1 of Kwiatkowski et al. (1992). If the computed statistic is outside the table of critical values, then a warning message is generated.</p> <p>Missing values are not handled.</p> Credit <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> References <ul> <li>Andrews, D.W.K. (1991). Heteroskedasticity and autocorrelation consistent covariance matrix estimation. Econometrica, 59: 817-858.</li> <li>Hobijn, B., Frances, B.H., &amp; Ooms, M. (2004). Generalizations of the KPSS-test for stationarity. Statistica Neerlandica, 52: 483-502.</li> <li>Kwiatkowski, D., Phillips, P.C.B., Schmidt, P., &amp; Shin, Y. (1992). Testing the null hypothesis of stationarity against the alternative of a unit root. Journal of Econometrics, 54: 159-178.</li> <li>Newey, W.K., &amp; West, K.D. (1994). Automatic lag selection in covariance matrix estimation. Review of Economic Studies, 61: 631-653.</li> <li>Schwert, G. W. (1989). Tests for unit roots: A Monte Carlo investigation. Journal of Business and Economic Statistics, 7 (2): 147-159.</li> </ul> See Also <ul> <li><code>statsmodels.tsa.stattools.adfuller</code>: Augmented Dickey-Fuller unit root test.</li> <li><code>statsmodels.tsa.stattools.kpss</code>: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.</li> <li><code>statsmodels.tsa.stattools.range_unit_root_test</code>: Range Unit-Root test.</li> <li><code>statsmodels.tsa.stattools.zivot_andrews</code>: Zivot-Andrews structural break test.</li> <li><code>pmdarima.arima.PPTest</code>: Phillips-Perron unit root test.</li> <li><code>arch.unitroot.DFGLS</code>: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller.</li> <li><code>arch.unitroot.VarianceRatio</code>: Variance Ratio test of a random walk.</li> <li><code>ts_stat_tests.algorithms.stationarity.adf</code>: Augmented Dickey-Fuller unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.kpss</code>: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.</li> <li><code>ts_stat_tests.algorithms.stationarity.rur</code>: Range Unit-Root test of stationarity.</li> <li><code>ts_stat_tests.algorithms.stationarity.za</code>: Zivot-Andrews structural break unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.pp</code>: Phillips-Perron unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.ers</code>: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller test.</li> <li><code>ts_stat_tests.algorithms.stationarity.vr</code>: Variance Ratio test of a random walk.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/stationarity.py</code> <pre><code>@typechecked\ndef kpss(\n    x: ArrayLike,\n    regression: VALID_KPSS_REGRESSION_OPTIONS = \"c\",\n    nlags: Optional[Union[VALID_KPSS_NLAGS_OPTIONS, int]] = None,\n    *,\n    store: bool = False,\n) -&gt; Union[\n    tuple[float, float, int, dict, ResultsStore],\n    tuple[float, float, int, dict],\n]:\n    r\"\"\"\n    !!! note \"Summary\"\n        Computes the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test for the null hypothesis that `x` is level or trend stationary.\n\n    ???+ abstract \"Details\"\n\n        The Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test is another statistical test used to determine whether a time series is stationary or not. The KPSS test is the opposite of the Augmented Dickey-Fuller (ADF) test, which tests for the presence of a unit root in the time series.\n\n        The KPSS test involves regressing the time series on a constant and a time trend. The null hypothesis of the test is that the time series is stationary. The alternative hypothesis is that the time series has a unit root, which means that it is non-stationary.\n\n        The test statistic is calculated by taking the sum of the squared residuals of the regression. If the test statistic is greater than a critical value at a given significance level, typically 0.05, then we reject the null hypothesis and conclude that the time series is non-stationary. If the test statistic is less than the critical value, then we fail to reject the null hypothesis and conclude that the time series is stationary.\n\n        In practical terms, if a time series is found to be non-stationary by the KPSS test, one can apply differencing to the time series until it becomes stationary. This involves taking the difference between consecutive observations and potentially repeating this process until the time series is stationary.\n\n        Overall, the ADF and KPSS tests are both important tools in time series analysis and forecasting, as they help identify whether a time series is stationary or non-stationary, which can have implications for the choice of forecasting models and methods.\n\n    Params:\n        x (ArrayLike):\n            The data series to test.\n        regression (VALID_KPSS_REGRESSION_OPTIONS, optional):\n            The null hypothesis for the KPSS test.\n\n            - `\"c\"`: The data is stationary around a constant (default).\n            - `\"ct\"`: The data is stationary around a trend.\n\n            Defaults to `\"c\"`.\n        nlags (Optional[Union[VALID_KPSS_NLAGS_OPTIONS, int]], optional):\n            Indicates the number of lags to be used.\n\n            - If `\"auto\"` (default), `lags` is calculated using the data-dependent method of Hobijn et al. (1998). See also Andrews (1991), Newey &amp; West (1994), and Schwert (1989).\n            - If set to `\"legacy\"`, uses $int(12 \\\\times (\\\\frac{n}{100})^{\\\\frac{1}{4}})$, as outlined in Schwert (1989).\n\n            Defaults to `None`.\n        store (bool, optional):\n            If `True`, then a result instance is returned additionally to the KPSS statistic.&lt;br&gt;\n            Defaults to `False`.\n\n    Returns:\n        (Union[tuple[float, float, int, dict, ResultsStore], tuple[float, float, int, dict]]):\n            Returns a tuple containing:\n            - `stat` (float): The KPSS test statistic.\n            - `pvalue` (float): The p-value of the test.\n            - `lags` (int): The truncation lag parameter.\n            - `crit` (dict): The critical values at 10%, 5%, 2.5%, and 1%.\n            - `resstore` (Optional[ResultsStore]): Result instance (if `store` is `True`).\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.stationarity import kpss\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_normal\n        &gt;&gt;&gt; normal = data_normal\n        &gt;&gt;&gt; airline = data_airline.values\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Stationary Series\"}\n        &gt;&gt;&gt; stat, pvalue, lags, crit = kpss(x=normal)\n        &gt;&gt;&gt; print(f\"KPSS statistic: {stat:.4f}\")\n        KPSS statistic: 0.0858\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.1000\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Airline Passengers Data\"}\n        &gt;&gt;&gt; stat, pvalue, lags, crit = kpss(x=airline)\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.0100\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The mathematical equation for the KPSS test for stationarity in time series forecasting is:\n\n        $$\n        y_t = \\mu_t + \\epsilon_t\n        $$\n\n        where:\n\n        - $y_t$ is the value of the time series at time $t$.\n        - $\\mu_t$ is the trend component of the time series.\n        - $\\epsilon_t$ is the error term.\n\n        The KPSS test involves testing the null hypothesis that the time series is trend stationary, which means that the trend component of the time series is stationary over time. If the null hypothesis is rejected, then the time series is non-stationary and requires further pre-processing before it can be used for forecasting.\n\n        Here are the detailed steps for how to calculate the KPSS test:\n\n        1. Collect your time series data and plot it to visually check for any trends, seasonal patterns, or other patterns that could make the data non-stationary. If you detect any such patterns, you will need to pre-process your data (e.g., detrending, deseasonalizing, etc.) to remove these effects.\n\n        1. Divide your time series data into multiple overlapping windows of equal size. The length of each window depends on the length of your time series and the level of detail you want to capture.\n\n        1. Calculate the trend component $\\mu_t$ for each window using a trend estimation method. There are several methods for estimating the trend component, such as the Hodrick-Prescott filter, the Christiano-Fitzgerald filter, or simple linear regression. The choice of method depends on the characteristics of your data and the level of accuracy you want to achieve.\n\n        1. Calculate the residual series $\\epsilon_t$ by subtracting the trend component from the original time series:\n\n            $$\n            \\epsilon_t = y_t - \\mu_t\n            $$\n\n        1. Estimate the variance of the residual series using a suitable estimator, such as the Newey-West estimator or the Bartlett kernel estimator. This step is necessary to correct for any serial correlation in the residual series.\n\n        1. Calculate the test statistic, which is given by:\n\n            $$\n            KPSS = T \\times \\sum_{t=1}^T \\frac {S_t^2} {\\sigma^2}\n            $$\n\n            where:\n\n            - $T$ is the number of observations in the time series.\n            - $S_t$ is the cumulative sum of the residual series up to time $t$, i.e., $S_t = \\sum_{i=1}^t \\epsilon_i$.\n            - $\\sigma^2$ is the estimated variance of the residual series.\n\n            The test statistic measures the strength of the trend component relative to the residual series. If KPSS is greater than the critical values from the KPSS distribution table, we can reject the null hypothesis and conclude that the time series is non-stationary.\n\n        1. Finally, interpret the results and draw conclusions about the stationarity of the time series. If the null hypothesis is rejected, then the time series is non-stationary and requires further pre-processing before it can be used for forecasting. If the null hypothesis is not rejected, then the time series is trend stationary and can be used for forecasting.\n\n    ??? note \"Notes\"\n        To estimate $\\sigma^2$ the Newey-West estimator is used. If `lags` is `\"legacy\"`, the truncation lag parameter is set to $int(12 \\times (\\frac{n}{100})^{\\frac{1}{4}})$, as outlined in Schwert (1989). The p-values are interpolated from Table 1 of Kwiatkowski et al. (1992). If the computed statistic is outside the table of critical values, then a warning message is generated.\n\n        Missing values are not handled.\n\n    ??? success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.kpss.html) library.\n\n    ??? question \"References\"\n        - Andrews, D.W.K. (1991). Heteroskedasticity and autocorrelation consistent covariance matrix estimation. Econometrica, 59: 817-858.\n        - Hobijn, B., Frances, B.H., &amp; Ooms, M. (2004). Generalizations of the KPSS-test for stationarity. Statistica Neerlandica, 52: 483-502.\n        - Kwiatkowski, D., Phillips, P.C.B., Schmidt, P., &amp; Shin, Y. (1992). Testing the null hypothesis of stationarity against the alternative of a unit root. Journal of Econometrics, 54: 159-178.\n        - Newey, W.K., &amp; West, K.D. (1994). Automatic lag selection in covariance matrix estimation. Review of Economic Studies, 61: 631-653.\n        - Schwert, G. W. (1989). Tests for unit roots: A Monte Carlo investigation. Journal of Business and Economic Statistics, 7 (2): 147-159.\n\n    ??? tip \"See Also\"\n        - [`statsmodels.tsa.stattools.adfuller`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.adfuller.html): Augmented Dickey-Fuller unit root test.\n        - [`statsmodels.tsa.stattools.kpss`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.kpss.html): Kwiatkowski-Phillips-Schmidt-Shin stationarity test.\n        - [`statsmodels.tsa.stattools.range_unit_root_test`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.range_unit_root_test.html): Range Unit-Root test.\n        - [`statsmodels.tsa.stattools.zivot_andrews`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.zivot_andrews.html): Zivot-Andrews structural break test.\n        - [`pmdarima.arima.PPTest`](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.PPTest.html): Phillips-Perron unit root test.\n        - [`arch.unitroot.DFGLS`](https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.DFGLS.html): Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller.\n        - [`arch.unitroot.VarianceRatio`](https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.VarianceRatio.html): Variance Ratio test of a random walk.\n        - [`ts_stat_tests.algorithms.stationarity.adf`][ts_stat_tests.algorithms.stationarity.adf]: Augmented Dickey-Fuller unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.kpss`][ts_stat_tests.algorithms.stationarity.kpss]: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.\n        - [`ts_stat_tests.algorithms.stationarity.rur`][ts_stat_tests.algorithms.stationarity.rur]: Range Unit-Root test of stationarity.\n        - [`ts_stat_tests.algorithms.stationarity.za`][ts_stat_tests.algorithms.stationarity.za]: Zivot-Andrews structural break unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.pp`][ts_stat_tests.algorithms.stationarity.pp]: Phillips-Perron unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.ers`][ts_stat_tests.algorithms.stationarity.ers]: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller test.\n        - [`ts_stat_tests.algorithms.stationarity.vr`][ts_stat_tests.algorithms.stationarity.vr]: Variance Ratio test of a random walk.\n    \"\"\"\n    _nlags: Union[VALID_KPSS_NLAGS_OPTIONS, int] = nlags if nlags is not None else \"auto\"\n    return _kpss(x=x, regression=regression, nlags=_nlags, store=store)\n</code></pre>"},{"location":"code/stationarity/#ts_stat_tests.algorithms.stationarity.rur","title":"rur","text":"<pre><code>rur(\n    x: ArrayLike, *, store: Literal[True]\n) -&gt; tuple[float, float, dict, ResultsStore]\n</code></pre><pre><code>rur(\n    x: ArrayLike, *, store: Literal[False] = False\n) -&gt; tuple[float, float, dict]\n</code></pre> <pre><code>rur(x: ArrayLike, *, store: bool = False) -&gt; Union[\n    tuple[float, float, dict, ResultsStore],\n    tuple[float, float, dict],\n]\n</code></pre> <p>Summary</p> <p>Computes the Range Unit-Root (RUR) test for the null hypothesis that x is stationary.</p> Details <p>The Range Unit-Root (RUR) test is a statistical test used to determine whether a time series is stationary or not. It is based on the range of the time series and does not require any knowledge of the underlying stochastic process.</p> <p>The RUR test involves dividing the time series into non-overlapping windows of a fixed size and calculating the range of each window. Then, the range of the entire time series is calculated. If the time series is stationary, the range of the entire time series should be proportional to the square root of the window size. If the time series is non-stationary, the range of the entire time series will grow with the window size.</p> <p>The null hypothesis of the RUR test is that the time series is non-stationary (unit root). The alternative hypothesis is that the time series is stationary. If the test statistic is less than a critical value at a given significance level, typically 0.05, then we reject the null hypothesis and conclude that the time series is stationary. If the test statistic is greater than the critical value, then we fail to reject the null hypothesis and conclude that the time series is non-stationary.</p> <p>In practical terms, if a time series is found to be non-stationary by the RUR test, one can apply differencing to the time series until it becomes stationary. This involves taking the difference between consecutive observations and potentially repeating this process until the time series is stationary.</p> <p>The RUR test is a simple and computationally efficient test for stationarity, but it may not be as powerful as other unit root tests in detecting non-stationarity in some cases. It is important to use multiple tests to determine the stationarity of a time series, as no single test is perfect in all situations.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data series to test.</p> required <code>store</code> <code>bool</code> <p>If <code>True</code>, then a result instance is returned additionally to the RUR statistic. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[tuple[float, float, dict, ResultsStore], tuple[float, float, dict]]</code> <p>Returns a tuple containing: - <code>stat</code> (float): The RUR test statistic. - <code>pvalue</code> (float): The p-value of the test. - <code>crit</code> (dict): The critical values at 10%, 5%, 2.5%, and 1%. - <code>resstore</code> (Optional[ResultsStore]): Result instance (if <code>store</code> is <code>True</code>).</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_normal, data_trend, data_sine\n&gt;&gt;&gt; from ts_stat_tests.algorithms.stationarity import rur\n&gt;&gt;&gt; normal = data_normal\n&gt;&gt;&gt; trend = data_trend\n&gt;&gt;&gt; seasonal = data_sine\n&gt;&gt;&gt; airline = data_airline.values\n</code></pre> Example 1: Stationary Series<pre><code>&gt;&gt;&gt; stat, pvalue, crit = rur(x=normal)\n&gt;&gt;&gt; print(f\"RUR statistic: {stat:.4f}\")\nRUR statistic: 0.3479\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.0100\n</code></pre> Example 2: Trend-Stationary Series<pre><code>&gt;&gt;&gt; stat, pvalue, crit = rur(x=trend)\n&gt;&gt;&gt; print(f\"RUR statistic: {stat:.4f}\")\nRUR statistic: 31.5912\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.9500\n</code></pre> Example 3: Seasonal Series<pre><code>&gt;&gt;&gt; stat, pvalue, crit = rur(x=seasonal)\n&gt;&gt;&gt; print(f\"RUR statistic: {stat:.4f}\")\nRUR statistic: 0.9129\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.04f}\")\np-value: 0.0100\n</code></pre> Example 4: Real-World Time Series<pre><code>&gt;&gt;&gt; stat, pvalue, crit = rur(x=airline)\n&gt;&gt;&gt; print(f\"RUR statistic: {stat:.4f}\")\nRUR statistic: 2.3333\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.9000\n</code></pre> Calculation <p>The mathematical equation for the RUR test is:</p> \\[ y_t = \\rho y_{t-1} + \\epsilon_t \\] <p>where:</p> <ul> <li>\\(y_t\\) is the value of the time series at time \\(t\\).</li> <li>\\(\\rho\\) is the parameter of the unit root process.</li> <li>\\(y_{t-1}\\) is the value of the time series at time \\(t-1\\).</li> <li>\\(\\epsilon_t\\) is a stationary error term with mean zero and constant variance.</li> </ul> <p>The null hypothesis of the RUR test is that the time series is stationary, and the alternative hypothesis is that the time series is non-stationary with a unit root.</p> <p>Here are the detailed steps for how to calculate the RUR test:</p> <ol> <li> <p>Collect your time series data and plot it to visually check for any trends, seasonal patterns, or other patterns that could make the data non-stationary. If you detect any such patterns, you will need to pre-process your data (e.g., detrending, deseasonalizing, etc.) to remove these effects.</p> </li> <li> <p>Estimate the parameter \\(\\rho\\) using the ordinary least squares method. This involves regressing \\(y_t\\) on \\(y_{t-1}\\). The estimated equation is:</p> \\[ y_t = \\alpha + \\rho y_{t-1} + \\epsilon_t \\] <p>where:</p> <ul> <li>\\(\\alpha\\) is the intercept.</li> <li>\\(\\epsilon_t\\) is the error term.</li> </ul> </li> <li> <p>Calculate the range of the time series, which is the difference between the maximum and minimum values of the time series:</p> \\[ R = \\max(y_t) - \\min(y_t) \\] </li> <li> <p>Calculate the expected range of the time series under the null hypothesis of stationarity, which is given by:</p> \\[ E(R) = \\frac {T - 1} {2 \\sqrt{T}} \\] <p>where:</p> <ul> <li>\\(T\\) is the sample size.</li> </ul> </li> <li> <p>Calculate the test statistic, which is given by:</p> \\[ RUR = \\frac {R - E(R)} {E(R)} \\] </li> <li> <p>Compare the test statistic to the critical values in the RUR distribution table to determine the level of significance. The critical values depend on the sample size and the level of significance.</p> </li> <li> <p>Finally, interpret the results and draw conclusions about the stationarity of the time series. If the null hypothesis is rejected, then the time series is non-stationary with a unit root. If the null hypothesis is not rejected, then the time series is stationary.</p> </li> </ol> <p>In practice, the RUR test is often conducted using software packages such as R, Python, or MATLAB, which automate the estimation of parameters and calculation of the test statistic.</p> Notes <p>The p-values are interpolated from Table 1 of Aparicio et al. (2006). If the computed statistic is outside the table of critical values, then a warning message is generated.</p> <p>Missing values are not handled.</p> <p>Credit</p> <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> References <ul> <li>Aparicio, F., Escribano A., Sipols, A.E. (2006). Range Unit-Root (RUR) tests: robust against nonlinearities, error distributions, structural breaks and outliers. Journal of Time Series Analysis, 27 (4): 545-576.</li> </ul> See Also <ul> <li><code>statsmodels.tsa.stattools.adfuller</code>: Augmented Dickey-Fuller unit root test.</li> <li><code>statsmodels.tsa.stattools.kpss</code>: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.</li> <li><code>statsmodels.tsa.stattools.range_unit_root_test</code>: Range Unit-Root test.</li> <li><code>statsmodels.tsa.stattools.zivot_andrews</code>: Zivot-Andrews structural break test.</li> <li><code>pmdarima.arima.PPTest</code>: Phillips-Perron unit root test.</li> <li><code>arch.unitroot.DFGLS</code>: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller.</li> <li><code>arch.unitroot.VarianceRatio</code>: Variance Ratio test of a random walk.</li> <li><code>ts_stat_tests.algorithms.stationarity.adf</code>: Augmented Dickey-Fuller unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.kpss</code>: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.</li> <li><code>ts_stat_tests.algorithms.stationarity.rur</code>: Range Unit-Root test of stationarity.</li> <li><code>ts_stat_tests.algorithms.stationarity.za</code>: Zivot-Andrews structural break unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.pp</code>: Phillips-Perron unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.ers</code>: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller test.</li> <li><code>ts_stat_tests.algorithms.stationarity.vr</code>: Variance Ratio test of a random walk.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/stationarity.py</code> <pre><code>@typechecked\ndef rur(x: ArrayLike, *, store: bool = False) -&gt; Union[\n    tuple[float, float, dict, ResultsStore],\n    tuple[float, float, dict],\n]:\n    r\"\"\"\n    !!! note \"Summary\"\n        Computes the Range Unit-Root (RUR) test for the null hypothesis that x is stationary.\n\n    ???+ abstract \"Details\"\n\n        The Range Unit-Root (RUR) test is a statistical test used to determine whether a time series is stationary or not. It is based on the range of the time series and does not require any knowledge of the underlying stochastic process.\n\n        The RUR test involves dividing the time series into non-overlapping windows of a fixed size and calculating the range of each window. Then, the range of the entire time series is calculated. If the time series is stationary, the range of the entire time series should be proportional to the square root of the window size. If the time series is non-stationary, the range of the entire time series will grow with the window size.\n\n        The null hypothesis of the RUR test is that the time series is non-stationary (unit root). The alternative hypothesis is that the time series is stationary. If the test statistic is less than a critical value at a given significance level, typically 0.05, then we reject the null hypothesis and conclude that the time series is stationary. If the test statistic is greater than the critical value, then we fail to reject the null hypothesis and conclude that the time series is non-stationary.\n\n        In practical terms, if a time series is found to be non-stationary by the RUR test, one can apply differencing to the time series until it becomes stationary. This involves taking the difference between consecutive observations and potentially repeating this process until the time series is stationary.\n\n        The RUR test is a simple and computationally efficient test for stationarity, but it may not be as powerful as other unit root tests in detecting non-stationarity in some cases. It is important to use multiple tests to determine the stationarity of a time series, as no single test is perfect in all situations.\n\n    Params:\n        x (ArrayLike):\n            The data series to test.\n        store (bool, optional):\n            If `True`, then a result instance is returned additionally to the RUR statistic.&lt;br&gt;\n            Defaults to `False`.\n\n    Returns:\n        (Union[tuple[float, float, dict, ResultsStore], tuple[float, float, dict]]):\n            Returns a tuple containing:\n            - `stat` (float): The RUR test statistic.\n            - `pvalue` (float): The p-value of the test.\n            - `crit` (dict): The critical values at 10%, 5%, 2.5%, and 1%.\n            - `resstore` (Optional[ResultsStore]): Result instance (if `store` is `True`).\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_normal, data_trend, data_sine\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.stationarity import rur\n        &gt;&gt;&gt; normal = data_normal\n        &gt;&gt;&gt; trend = data_trend\n        &gt;&gt;&gt; seasonal = data_sine\n        &gt;&gt;&gt; airline = data_airline.values\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Stationary Series\"}\n        &gt;&gt;&gt; stat, pvalue, crit = rur(x=normal)\n        &gt;&gt;&gt; print(f\"RUR statistic: {stat:.4f}\")\n        RUR statistic: 0.3479\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.0100\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Trend-Stationary Series\"}\n        &gt;&gt;&gt; stat, pvalue, crit = rur(x=trend)\n        &gt;&gt;&gt; print(f\"RUR statistic: {stat:.4f}\")\n        RUR statistic: 31.5912\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.9500\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 3: Seasonal Series\"}\n        &gt;&gt;&gt; stat, pvalue, crit = rur(x=seasonal)\n        &gt;&gt;&gt; print(f\"RUR statistic: {stat:.4f}\")\n        RUR statistic: 0.9129\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.04f}\")\n        p-value: 0.0100\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 4: Real-World Time Series\"}\n        &gt;&gt;&gt; stat, pvalue, crit = rur(x=airline)\n        &gt;&gt;&gt; print(f\"RUR statistic: {stat:.4f}\")\n        RUR statistic: 2.3333\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.9000\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The mathematical equation for the RUR test is:\n\n        $$\n        y_t = \\rho y_{t-1} + \\epsilon_t\n        $$\n\n        where:\n\n        - $y_t$ is the value of the time series at time $t$.\n        - $\\rho$ is the parameter of the unit root process.\n        - $y_{t-1}$ is the value of the time series at time $t-1$.\n        - $\\epsilon_t$ is a stationary error term with mean zero and constant variance.\n\n        The null hypothesis of the RUR test is that the time series is stationary, and the alternative hypothesis is that the time series is non-stationary with a unit root.\n\n        Here are the detailed steps for how to calculate the RUR test:\n\n        1. Collect your time series data and plot it to visually check for any trends, seasonal patterns, or other patterns that could make the data non-stationary. If you detect any such patterns, you will need to pre-process your data (e.g., detrending, deseasonalizing, etc.) to remove these effects.\n\n        1. Estimate the parameter $\\rho$ using the ordinary least squares method. This involves regressing $y_t$ on $y_{t-1}$. The estimated equation is:\n\n            $$\n            y_t = \\alpha + \\rho y_{t-1} + \\epsilon_t\n            $$\n\n            where:\n\n            - $\\alpha$ is the intercept.\n            - $\\epsilon_t$ is the error term.\n\n        1. Calculate the range of the time series, which is the difference between the maximum and minimum values of the time series:\n\n            $$\n            R = \\max(y_t) - \\min(y_t)\n            $$\n\n        1. Calculate the expected range of the time series under the null hypothesis of stationarity, which is given by:\n\n            $$\n            E(R) = \\frac {T - 1} {2 \\sqrt{T}}\n            $$\n\n            where:\n\n            - $T$ is the sample size.\n\n        1. Calculate the test statistic, which is given by:\n\n            $$\n            RUR = \\frac {R - E(R)} {E(R)}\n            $$\n\n        1. Compare the test statistic to the critical values in the RUR distribution table to determine the level of significance. The critical values depend on the sample size and the level of significance.\n\n        1. Finally, interpret the results and draw conclusions about the stationarity of the time series. If the null hypothesis is rejected, then the time series is non-stationary with a unit root. If the null hypothesis is not rejected, then the time series is stationary.\n\n        In practice, the RUR test is often conducted using software packages such as R, Python, or MATLAB, which automate the estimation of parameters and calculation of the test statistic.\n\n    ??? note \"Notes\"\n        The p-values are interpolated from Table 1 of Aparicio et al. (2006). If the computed statistic is outside the table of critical values, then a warning message is generated.\n\n        Missing values are not handled.\n\n    !!! success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.range_unit_root_test.html) library.\n\n    ??? question \"References\"\n        - Aparicio, F., Escribano A., Sipols, A.E. (2006). Range Unit-Root (RUR) tests: robust against nonlinearities, error distributions, structural breaks and outliers. Journal of Time Series Analysis, 27 (4): 545-576.\n\n    ??? tip \"See Also\"\n        - [`statsmodels.tsa.stattools.adfuller`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.adfuller.html): Augmented Dickey-Fuller unit root test.\n        - [`statsmodels.tsa.stattools.kpss`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.kpss.html): Kwiatkowski-Phillips-Schmidt-Shin stationarity test.\n        - [`statsmodels.tsa.stattools.range_unit_root_test`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.range_unit_root_test.html): Range Unit-Root test.\n        - [`statsmodels.tsa.stattools.zivot_andrews`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.zivot_andrews.html): Zivot-Andrews structural break test.\n        - [`pmdarima.arima.PPTest`](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.PPTest.html): Phillips-Perron unit root test.\n        - [`arch.unitroot.DFGLS`](https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.DFGLS.html): Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller.\n        - [`arch.unitroot.VarianceRatio`](https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.VarianceRatio.html): Variance Ratio test of a random walk.\n        - [`ts_stat_tests.algorithms.stationarity.adf`][ts_stat_tests.algorithms.stationarity.adf]: Augmented Dickey-Fuller unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.kpss`][ts_stat_tests.algorithms.stationarity.kpss]: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.\n        - [`ts_stat_tests.algorithms.stationarity.rur`][ts_stat_tests.algorithms.stationarity.rur]: Range Unit-Root test of stationarity.\n        - [`ts_stat_tests.algorithms.stationarity.za`][ts_stat_tests.algorithms.stationarity.za]: Zivot-Andrews structural break unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.pp`][ts_stat_tests.algorithms.stationarity.pp]: Phillips-Perron unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.ers`][ts_stat_tests.algorithms.stationarity.ers]: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller test.\n        - [`ts_stat_tests.algorithms.stationarity.vr`][ts_stat_tests.algorithms.stationarity.vr]: Variance Ratio test of a random walk.\n    \"\"\"\n    return _rur(x=x, store=store)\n</code></pre>"},{"location":"code/stationarity/#ts_stat_tests.algorithms.stationarity.za","title":"za","text":"<pre><code>za(\n    x: ArrayLike,\n    trim: float = 0.15,\n    maxlag: Optional[int] = None,\n    regression: VALID_ZA_REGRESSION_OPTIONS = \"c\",\n    autolag: Optional[VALID_ZA_AUTOLAG_OPTIONS] = \"AIC\",\n) -&gt; tuple[float, float, dict, int, int]\n</code></pre> <p>Summary</p> <p>The Zivot-Andrews (ZA) test tests for a unit root in a univariate process in the presence of serial correlation and a single structural break.</p> Details <p>The Zivot-Andrews (ZA) test is a statistical test used to determine whether a time series is stationary or not in the presence of structural breaks. Structural breaks refer to significant changes in the underlying stochastic process of the time series, which can cause non-stationarity.</p> <p>The ZA test involves running a regression of the time series on a constant and a linear time trend, and testing whether the residuals of the regression are stationary or not. The null hypothesis of the test is that the time series is stationary with a single break point, while the alternative hypothesis is that the time series is non-stationary with a single break point.</p> <p>The test statistic is calculated by first estimating the break point using a likelihood ratio test. Then, the test statistic is calculated based on the estimated break point and the residuals of the regression. If the test statistic is greater than a critical value at a given significance level, typically 0.05, then we reject the null hypothesis and conclude that the time series is non-stationary with a structural break. If the test statistic is less than the critical value, then we fail to reject the null hypothesis and conclude that the time series is stationary with a structural break.</p> <p>In practical terms, if a time series is found to be non-stationary with a structural break by the ZA test, one can apply methods to account for the structural break, such as including dummy variables in the regression or using time series models that allow for structural breaks.</p> <p>Overall, the ZA test is a useful tool in time series analysis and forecasting when there is a suspicion of structural breaks in the data. However, it is important to note that the test may not detect multiple break points or breaks that are not well-separated in time.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data series to test.</p> required <code>trim</code> <code>float</code> <p>The percentage of series at begin/end to exclude. Default: <code>0.15</code></p> <code>0.15</code> <code>maxlag</code> <code>Optional[int]</code> <p>The maximum lag which is included in test. Default: <code>None</code></p> <code>None</code> <code>regression</code> <code>VALID_ZA_REGRESSION_OPTIONS</code> <p>Constant and trend order to include in regression.</p> <ul> <li><code>\"c\"</code>: constant only (default).</li> <li><code>\"t\"</code>: trend only.</li> <li><code>\"ct\"</code>: constant and trend.</li> </ul> <p>Default: <code>\"c\"</code></p> <code>'c'</code> <code>autolag</code> <code>Optional[VALID_ZA_AUTOLAG_OPTIONS]</code> <p>The method to select the lag length.</p> <ul> <li>If <code>None</code>, then <code>maxlag</code> lags are used.</li> <li>If <code>\"AIC\"</code> (default) or <code>\"BIC\"</code>, then the number of lags is chosen.</li> </ul> <p>Default: <code>\"AIC\"</code></p> <code>'AIC'</code> <p>Returns:</p> Type Description <code>tuple[float, float, dict, int, int]</code> <p>Returns a tuple containing: - <code>zastat</code> (float): The test statistic. - <code>pvalue</code> (float): The p-value. - <code>cvdict</code> (dict): Critical values at the \\(1\\%\\), \\(5\\%\\), and \\(10\\%\\) levels. - <code>baselag</code> (int): Lags used for period regressions. - <code>pbidx</code> (int): Break period index.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_normal, data_noise\n&gt;&gt;&gt; from ts_stat_tests.algorithms.stationarity import za\n&gt;&gt;&gt; normal = data_normal\n&gt;&gt;&gt; noise = data_noise\n&gt;&gt;&gt; airline = data_airline.values\n</code></pre> Example 1: Stationary Series<pre><code>&gt;&gt;&gt; stat, pvalue, crit, lags, break_idx = za(x=normal)\n&gt;&gt;&gt; print(f\"ZA statistic: {stat:.4f}\")\nZA statistic: -30.8800\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4e}\")\np-value: 1.0000e-05\n</code></pre> Example 2: Noisy Series<pre><code>&gt;&gt;&gt; stat, pvalue, crit, lags, break_idx = za(x=noise)\n&gt;&gt;&gt; print(f\"ZA statistic: {stat:.4f}\")\nZA statistic: -32.4316\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4e}\")\np-value: 1.0000e-05\n</code></pre> Example 3: Real-World Time Series<pre><code>&gt;&gt;&gt; stat, pvalue, crit, lags, break_idx = za(x=airline)\n&gt;&gt;&gt; print(f\"ZA statistic: {stat:.4f}\")\nZA statistic: -3.6508\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.5808\n</code></pre> Calculation <p>The mathematical equation for the Zivot-Andrews test is:</p> \\[ y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\delta_1 D_t + \\delta_2 t D_t + \\epsilon_t \\] <p>where:</p> <ul> <li>\\(y_t\\) is the value of the time series at time \\(t\\).</li> <li>\\(\\alpha\\) is the intercept.</li> <li>\\(\\beta\\) is the slope coefficient of the time trend.</li> <li>\\(\\gamma\\) is the coefficient of the lagged dependent variable.</li> <li>\\(D_t\\) is a dummy variable that takes a value of 1 after the suspected structural break point, and 0 otherwise.</li> <li>\\(\\delta_1\\) and \\(\\delta_2\\) are the coefficients of the dummy variable and the interaction term of the dummy variable and time trend, respectively.</li> <li>\\(\\epsilon_t\\) is a stationary error term with mean zero and constant variance.</li> </ul> <p>The null hypothesis of the Zivot-Andrews test is that the time series is non-stationary, and the alternative hypothesis is that the time series is stationary with a single structural break.</p> <p>Here are the detailed steps for how to calculate the Zivot-Andrews test:</p> <ol> <li> <p>Collect your time series data and plot it to visually check for any trends, seasonal patterns, or other patterns that could make the data non-stationary. If you detect any such patterns, you will need to pre-process your data (e.g., detrending, deseasonalizing, etc.) to remove these effects.</p> </li> <li> <p>Estimate the parameters of the model using the least squares method. This involves regressing \\(y_t\\) on \\(t\\), \\(y_{t-1}\\), \\(D_t\\), and \\(t D_t\\). The estimated equation is:</p> \\[ y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\delta_1 D_t + \\delta_2 t D_t + \\epsilon_t \\] </li> <li> <p>Perform a unit root test on the residuals to check for stationarity. The most commonly used unit root tests for this purpose are the Augmented Dickey-Fuller (ADF) test and the Phillips-Perron (PP) test.</p> </li> <li> <p>Calculate the test statistic, which is based on the largest root of the following equation:</p> \\[ \\Delta y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\delta_1 D_t + \\delta_2 t D_t + \\epsilon_t \\] <p>where:</p> <ul> <li>\\(\\Delta\\) is the first difference operator.</li> </ul> </li> <li> <p>Determine the critical values of the test statistic from the Zivot-Andrews distribution table. The critical values depend on the sample size, the level of significance, and the number of lagged dependent variables in the model.</p> </li> <li> <p>Finally, interpret the results and draw conclusions about the stationarity of the time series. If the null hypothesis is rejected, then the time series is stationary with a structural break. If the null hypothesis is not rejected, then the time series is non-stationary and may require further processing to make it stationary.</p> </li> </ol> <p>In practice, the Zivot-Andrews test is often conducted using software packages such as R, Python, or MATLAB, which automate the estimation of parameters and calculation of the test statistic.</p> Notes <p>H0 = unit root with a single structural break</p> <p>Algorithm follows Baum (2004/2015) approximation to original Zivot-Andrews method. Rather than performing an autolag regression at each candidate break period (as per the original paper), a single autolag regression is run up-front on the base model (constant + trend with no dummies) to determine the best lag length. This lag length is then used for all subsequent break-period regressions. This results in significant run time reduction but also slightly more pessimistic test statistics than the original Zivot-Andrews method, although no attempt has been made to characterize the size/power trade-off.</p> Credit <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> References <ul> <li>Baum, C.F. (2004). ZANDREWS: Stata module to calculate Zivot-Andrews unit root test in presence of structural break,\" Statistical Software Components S437301, Boston College Department of Economics, revised 2015.</li> <li>Schwert, G.W. (1989). Tests for unit roots: A Monte Carlo investigation. Journal of Business &amp; Economic Statistics, 7: 147-159.</li> <li>Zivot, E., and Andrews, D.W.K. (1992). Further evidence on the great crash, the oil-price shock, and the unit-root hypothesis. Journal of Business &amp; Economic Studies, 10: 251-270.</li> </ul> See Also <ul> <li><code>statsmodels.tsa.stattools.adfuller</code>: Augmented Dickey-Fuller unit root test.</li> <li><code>statsmodels.tsa.stattools.kpss</code>: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.</li> <li><code>statsmodels.tsa.stattools.range_unit_root_test</code>: Range Unit-Root test.</li> <li><code>statsmodels.tsa.stattools.zivot_andrews</code>: Zivot-Andrews structural break test.</li> <li><code>pmdarima.arima.PPTest</code>: Phillips-Perron unit root test.</li> <li><code>arch.unitroot.DFGLS</code>: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller.</li> <li><code>arch.unitroot.VarianceRatio</code>: Variance Ratio test of a random walk.</li> <li><code>ts_stat_tests.algorithms.stationarity.adf</code>: Augmented Dickey-Fuller unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.kpss</code>: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.</li> <li><code>ts_stat_tests.algorithms.stationarity.rur</code>: Range Unit-Root test of stationarity.</li> <li><code>ts_stat_tests.algorithms.stationarity.za</code>: Zivot-Andrews structural break unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.pp</code>: Phillips-Perron unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.ers</code>: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller test.</li> <li><code>ts_stat_tests.algorithms.stationarity.vr</code>: Variance Ratio test of a random walk.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/stationarity.py</code> <pre><code>@typechecked\ndef za(\n    x: ArrayLike,\n    trim: float = 0.15,\n    maxlag: Optional[int] = None,\n    regression: VALID_ZA_REGRESSION_OPTIONS = \"c\",\n    autolag: Optional[VALID_ZA_AUTOLAG_OPTIONS] = \"AIC\",\n) -&gt; tuple[float, float, dict, int, int]:\n    r\"\"\"\n    !!! note \"Summary\"\n        The Zivot-Andrews (ZA) test tests for a unit root in a univariate process in the presence of serial correlation and a single structural break.\n\n    ???+ abstract \"Details\"\n        The Zivot-Andrews (ZA) test is a statistical test used to determine whether a time series is stationary or not in the presence of structural breaks. Structural breaks refer to significant changes in the underlying stochastic process of the time series, which can cause non-stationarity.\n\n        The ZA test involves running a regression of the time series on a constant and a linear time trend, and testing whether the residuals of the regression are stationary or not. The null hypothesis of the test is that the time series is stationary with a single break point, while the alternative hypothesis is that the time series is non-stationary with a single break point.\n\n        The test statistic is calculated by first estimating the break point using a likelihood ratio test. Then, the test statistic is calculated based on the estimated break point and the residuals of the regression. If the test statistic is greater than a critical value at a given significance level, typically 0.05, then we reject the null hypothesis and conclude that the time series is non-stationary with a structural break. If the test statistic is less than the critical value, then we fail to reject the null hypothesis and conclude that the time series is stationary with a structural break.\n\n        In practical terms, if a time series is found to be non-stationary with a structural break by the ZA test, one can apply methods to account for the structural break, such as including dummy variables in the regression or using time series models that allow for structural breaks.\n\n        Overall, the ZA test is a useful tool in time series analysis and forecasting when there is a suspicion of structural breaks in the data. However, it is important to note that the test may not detect multiple break points or breaks that are not well-separated in time.\n\n    Params:\n        x (ArrayLike):\n            The data series to test.\n        trim (float):\n            The percentage of series at begin/end to exclude.\n            Default: `0.15`\n        maxlag (Optional[int]):\n            The maximum lag which is included in test.\n            Default: `None`\n        regression (VALID_ZA_REGRESSION_OPTIONS):\n            Constant and trend order to include in regression.\n\n            - `\"c\"`: constant only (default).\n            - `\"t\"`: trend only.\n            - `\"ct\"`: constant and trend.\n\n            Default: `\"c\"`\n        autolag (Optional[VALID_ZA_AUTOLAG_OPTIONS]):\n            The method to select the lag length.\n\n            - If `None`, then `maxlag` lags are used.\n            - If `\"AIC\"` (default) or `\"BIC\"`, then the number of lags is chosen.\n\n            Default: `\"AIC\"`\n\n    Returns:\n        (tuple[float, float, dict, int, int]):\n            Returns a tuple containing:\n            - `zastat` (float): The test statistic.\n            - `pvalue` (float): The p-value.\n            - `cvdict` (dict): Critical values at the $1\\%$, $5\\%$, and $10\\%$ levels.\n            - `baselag` (int): Lags used for period regressions.\n            - `pbidx` (int): Break period index.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_normal, data_noise\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.stationarity import za\n        &gt;&gt;&gt; normal = data_normal\n        &gt;&gt;&gt; noise = data_noise\n        &gt;&gt;&gt; airline = data_airline.values\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Stationary Series\"}\n        &gt;&gt;&gt; stat, pvalue, crit, lags, break_idx = za(x=normal)\n        &gt;&gt;&gt; print(f\"ZA statistic: {stat:.4f}\")\n        ZA statistic: -30.8800\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4e}\")\n        p-value: 1.0000e-05\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Noisy Series\"}\n        &gt;&gt;&gt; stat, pvalue, crit, lags, break_idx = za(x=noise)\n        &gt;&gt;&gt; print(f\"ZA statistic: {stat:.4f}\")\n        ZA statistic: -32.4316\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4e}\")\n        p-value: 1.0000e-05\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 3: Real-World Time Series\"}\n        &gt;&gt;&gt; stat, pvalue, crit, lags, break_idx = za(x=airline)\n        &gt;&gt;&gt; print(f\"ZA statistic: {stat:.4f}\")\n        ZA statistic: -3.6508\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.5808\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The mathematical equation for the Zivot-Andrews test is:\n\n        $$\n        y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\delta_1 D_t + \\delta_2 t D_t + \\epsilon_t\n        $$\n\n        where:\n\n        - $y_t$ is the value of the time series at time $t$.\n        - $\\alpha$ is the intercept.\n        - $\\beta$ is the slope coefficient of the time trend.\n        - $\\gamma$ is the coefficient of the lagged dependent variable.\n        - $D_t$ is a dummy variable that takes a value of 1 after the suspected structural break point, and 0 otherwise.\n        - $\\delta_1$ and $\\delta_2$ are the coefficients of the dummy variable and the interaction term of the dummy variable and time trend, respectively.\n        - $\\epsilon_t$ is a stationary error term with mean zero and constant variance.\n\n        The null hypothesis of the Zivot-Andrews test is that the time series is non-stationary, and the alternative hypothesis is that the time series is stationary with a single structural break.\n\n        Here are the detailed steps for how to calculate the Zivot-Andrews test:\n\n        1. Collect your time series data and plot it to visually check for any trends, seasonal patterns, or other patterns that could make the data non-stationary. If you detect any such patterns, you will need to pre-process your data (e.g., detrending, deseasonalizing, etc.) to remove these effects.\n\n        1. Estimate the parameters of the model using the least squares method. This involves regressing $y_t$ on $t$, $y_{t-1}$, $D_t$, and $t D_t$. The estimated equation is:\n\n            $$\n            y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\delta_1 D_t + \\delta_2 t D_t + \\epsilon_t\n            $$\n\n        1. Perform a unit root test on the residuals to check for stationarity. The most commonly used unit root tests for this purpose are the Augmented Dickey-Fuller (ADF) test and the Phillips-Perron (PP) test.\n\n        1. Calculate the test statistic, which is based on the largest root of the following equation:\n\n            $$\n            \\Delta y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\delta_1 D_t + \\delta_2 t D_t + \\epsilon_t\n            $$\n\n            where:\n\n            - $\\Delta$ is the first difference operator.\n\n        1. Determine the critical values of the test statistic from the Zivot-Andrews distribution table. The critical values depend on the sample size, the level of significance, and the number of lagged dependent variables in the model.\n\n        1. Finally, interpret the results and draw conclusions about the stationarity of the time series. If the null hypothesis is rejected, then the time series is stationary with a structural break. If the null hypothesis is not rejected, then the time series is non-stationary and may require further processing to make it stationary.\n\n        In practice, the Zivot-Andrews test is often conducted using software packages such as R, Python, or MATLAB, which automate the estimation of parameters and calculation of the test statistic.\n\n    ??? note \"Notes\"\n        H0 = unit root with a single structural break\n\n        Algorithm follows Baum (2004/2015) approximation to original Zivot-Andrews method. Rather than performing an autolag regression at each candidate break period (as per the original paper), a single autolag regression is run up-front on the base model (constant + trend with no dummies) to determine the best lag length. This lag length is then used for all subsequent break-period regressions. This results in significant run time reduction but also slightly more pessimistic test statistics than the original Zivot-Andrews method, although no attempt has been made to characterize the size/power trade-off.\n\n    ??? success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.zivot_andrews.html) library.\n\n    ??? question \"References\"\n        - Baum, C.F. (2004). ZANDREWS: Stata module to calculate Zivot-Andrews unit root test in presence of structural break,\" Statistical Software Components S437301, Boston College Department of Economics, revised 2015.\n        - Schwert, G.W. (1989). Tests for unit roots: A Monte Carlo investigation. Journal of Business &amp; Economic Statistics, 7: 147-159.\n        - Zivot, E., and Andrews, D.W.K. (1992). Further evidence on the great crash, the oil-price shock, and the unit-root hypothesis. Journal of Business &amp; Economic Studies, 10: 251-270.\n\n    ??? tip \"See Also\"\n        - [`statsmodels.tsa.stattools.adfuller`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.adfuller.html): Augmented Dickey-Fuller unit root test.\n        - [`statsmodels.tsa.stattools.kpss`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.kpss.html): Kwiatkowski-Phillips-Schmidt-Shin stationarity test.\n        - [`statsmodels.tsa.stattools.range_unit_root_test`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.range_unit_root_test.html): Range Unit-Root test.\n        - [`statsmodels.tsa.stattools.zivot_andrews`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.zivot_andrews.html): Zivot-Andrews structural break test.\n        - [`pmdarima.arima.PPTest`](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.PPTest.html): Phillips-Perron unit root test.\n        - [`arch.unitroot.DFGLS`](https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.DFGLS.html): Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller.\n        - [`arch.unitroot.VarianceRatio`](https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.VarianceRatio.html): Variance Ratio test of a random walk.\n        - [`ts_stat_tests.algorithms.stationarity.adf`][ts_stat_tests.algorithms.stationarity.adf]: Augmented Dickey-Fuller unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.kpss`][ts_stat_tests.algorithms.stationarity.kpss]: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.\n        - [`ts_stat_tests.algorithms.stationarity.rur`][ts_stat_tests.algorithms.stationarity.rur]: Range Unit-Root test of stationarity.\n        - [`ts_stat_tests.algorithms.stationarity.za`][ts_stat_tests.algorithms.stationarity.za]: Zivot-Andrews structural break unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.pp`][ts_stat_tests.algorithms.stationarity.pp]: Phillips-Perron unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.ers`][ts_stat_tests.algorithms.stationarity.ers]: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller test.\n        - [`ts_stat_tests.algorithms.stationarity.vr`][ts_stat_tests.algorithms.stationarity.vr]: Variance Ratio test of a random walk.\n    \"\"\"\n    res: Any = _za(\n        x=x,\n        trim=trim,\n        maxlag=maxlag,\n        regression=regression,\n        autolag=autolag,  # type: ignore[arg-type] # statsmodels stubs are often missing None\n    )\n    return (\n        float(res[0]),\n        float(res[1]),\n        dict(res[2]),\n        int(res[3]),\n        int(res[4]),\n    )\n</code></pre>"},{"location":"code/stationarity/#ts_stat_tests.algorithms.stationarity.pp","title":"pp","text":"<pre><code>pp(\n    x: ArrayLike,\n    lags: Optional[int] = None,\n    trend: VALID_PP_TREND_OPTIONS = \"c\",\n    test_type: VALID_PP_TEST_TYPE_OPTIONS = \"tau\",\n) -&gt; tuple[float, float, int, dict]\n</code></pre> <p>Summary</p> <p>Conduct a Phillips-Perron (PP) test for stationarity.</p> <p>In statistics, the Phillips-Perron test (named after Peter C. B. Phillips and Pierre Perron) is a unit root test. It is used in time series analysis to test the null hypothesis that a time series is integrated of order \\(1\\). It builds on the Dickey-Fuller test of the null hypothesis \\(p=0\\).</p> Details <p>The Phillips-Perron (PP) test is a statistical test used to determine whether a time series is stationary or not. It is similar to the Augmented Dickey-Fuller (ADF) test, but it has some advantages, especially in the presence of autocorrelation and heteroscedasticity.</p> <p>The PP test involves regressing the time series on a constant and a linear time trend, and testing whether the residuals of the regression are stationary or not. The null hypothesis of the test is that the time series is non-stationary, while the alternative hypothesis is that the time series is stationary.</p> <p>The test statistic is calculated by taking the sum of the squared residuals of the regression, which is adjusted for autocorrelation and heteroscedasticity. The PP test also accounts for the bias in the standard errors of the test statistic, which can lead to incorrect inference in small samples.</p> <p>If the test statistic is less than a critical value at a given significance level, typically 0.05, then we reject the null hypothesis and conclude that the time series is stationary. If the test statistic is greater than the critical value, then we fail to reject the null hypothesis and conclude that the time series is non-stationary.</p> <p>In practical terms, if a time series is found to be non-stationary by the PP test, one can apply differencing to the time series until it becomes stationary. This involves taking the difference between consecutive observations and potentially repeating this process until the time series is stationary.</p> <p>Overall, the PP test is a powerful and robust test for stationarity, and it is widely used in time series analysis and forecasting. However, it is important to use multiple tests and diagnostic tools to determine the stationarity of a time series, as no single test is perfect in all situations.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data series to test.</p> required <code>lags</code> <code>Optional[int]</code> <p>The number of lags to use in the Newey-West estimator of the variance. If omitted or <code>None</code>, the lag length is selected automatically. Defaults to <code>None</code>.</p> <code>None</code> <code>trend</code> <code>VALID_PP_TREND_OPTIONS</code> <p>The trend component to include in the test.</p> <ul> <li><code>\"n\"</code>: No constant, no trend.</li> <li><code>\"c\"</code>: Include a constant (default).</li> <li><code>\"ct\"</code>: Include a constant and linear time trend.</li> </ul> <p>Defaults to <code>\"c\"</code>.</p> <code>'c'</code> <code>test_type</code> <code>VALID_PP_TEST_TYPE_OPTIONS</code> <p>The type of test statistic to compute:</p> <ul> <li><code>\"tau\"</code>: The t-statistic based on the augmented regression (default).</li> <li><code>\"rho\"</code>: The normalized autocorrelation coefficient (also known as the \\(Z(\\\\alpha)\\) test).</li> </ul> <p>Defaults to <code>\"tau\"</code>.</p> <code>'tau'</code> <p>Returns:</p> Type Description <code>tuple[float, float, int, dict]</code> <p>Returns a tuple containing: - <code>stat</code> (float): The test statistic. - <code>pvalue</code> (float): The p-value for the test statistic. - <code>lags</code> (int): The number of lags used in the test. - <code>crit</code> (dict): The critical values at 1%, 5%, and 10%.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.stationarity import pp\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_normal, data_trend, data_sine\n&gt;&gt;&gt; normal = data_normal\n&gt;&gt;&gt; trend = data_trend\n&gt;&gt;&gt; seasonal = data_sine\n&gt;&gt;&gt; airline = data_airline.values\n</code></pre> Example 1: Stationary Series<pre><code>&gt;&gt;&gt; stat, pvalue, lags, crit = pp(x=normal)\n&gt;&gt;&gt; print(f\"PP statistic: {stat:.4f}\")\nPP statistic: -30.7758\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.0000\n</code></pre> Example 2: Trend-Stationary Series<pre><code>&gt;&gt;&gt; stat, pvalue, lags, crit = pp(x=trend, trend=\"ct\")\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.0000\n</code></pre> Example 3: Seasonal Series<pre><code>&gt;&gt;&gt; stat, pvalue, lags, crit = pp(x=seasonal)\n&gt;&gt;&gt; print(f\"PP statistic: {stat:.4f}\")\nPP statistic: -8.0571\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.0000\n</code></pre> Example 4: Real-World Time Series<pre><code>&gt;&gt;&gt; stat, pvalue, lags, crit = pp(x=airline)\n&gt;&gt;&gt; print(f\"PP statistic: {stat:.4f}\")\nPP statistic: -1.3511\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.6055\n</code></pre> Example 5: PP test with excessive lags (coverage check)<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.stationarity import pp\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n&gt;&gt;&gt; # data_normal has 1000 observations. Force lags = 1000 to trigger adjustment.\n&gt;&gt;&gt; res = pp(data_normal, lags=1000)\n&gt;&gt;&gt; print(f\"stat: {res[0]:.4f}, lags: {res[2]}\")\nstat: -43.6895, lags: 998\n</code></pre> Calculation <p>The Phillips-Perron (PP) test is a commonly used test for stationarity in time series forecasting. The mathematical equation for the PP test is:</p> \\[ y_t = \\delta + \\pi t + \\rho y_{t-1} + \\epsilon_t \\] <p>where:</p> <ul> <li>\\(y_t\\) is the value of the time series at time \\(t\\).</li> <li>\\(\\delta\\) is a constant term.</li> <li>\\(\\pi\\) is a coefficient that captures the trend in the data.</li> <li>\\(\\rho\\) is a coefficient that captures the autocorrelation in the data.</li> <li>\\(y_{t-1}\\) is the lagged value of the time series at time \\(t-1\\).</li> <li>\\(\\epsilon_t\\) is a stationary error term with mean zero and constant variance.</li> </ul> <p>The PP test is based on the idea that if the time series is stationary, then the coefficient \\(\\rho\\) should be equal to zero. Therefore, the null hypothesis of the PP test is that the time series is stationary, and the alternative hypothesis is that the time series is non-stationary with a non-zero value of \\(\\rho\\).</p> <p>Here are the detailed steps for how to calculate the PP test:</p> <ol> <li> <p>Collect your time series data and plot it to visually check for any trends, seasonal patterns, or other patterns that could make the data non-stationary. If you detect any such patterns, you will need to pre-process your data (e.g., detrending, deseasonalizing, etc.) to remove these effects.</p> </li> <li> <p>Estimate the regression model by regressing \\(y_t\\) on a constant, a linear trend, and the lagged value of \\(y_{t-1}\\). The regression equation is:</p> \\[ y_t = \\delta + \\pi t + \\rho y_{t-1} + \\epsilon_t \\] </li> <li> <p>Calculate the test statistic, which is based on the following equation:</p> \\[ z = \\left( T^{-\\frac{1}{2}} \\right) \\times \\left( \\sum_{t=1}^T \\left( y_t - \\delta - \\pi t - \\rho y_{t-1} \\right) - \\left( \\frac{1}{T} \\right) \\times \\sum_{t=1}^T \\sum_{s=1}^T K \\left( \\frac{s-t}{h} \\right) (y_s - \\delta - \\pi s - \\rho y_{s-1}) \\right) \\] <p>where:</p> <ul> <li>\\(T\\) is the sample size.</li> <li>\\(K(\\dots)\\) is the kernel function, which determines the weight of each observation in the smoothed series. The choice of the kernel function depends on the degree of serial correlation in the data. Typically, a Gaussian kernel or a Bartlett kernel is used.</li> <li>\\(h\\) is the bandwidth parameter, which controls the degree of smoothing of the series. The optimal value of \\(h\\) depends on the sample size and the noise level of the data.</li> </ul> </li> <li> <p>Determine the critical values of the test statistic from the PP distribution table. The critical values depend on the sample size and the level of significance.</p> </li> <li> <p>Finally, interpret the results and draw conclusions about the stationarity of the time series. If the null hypothesis is rejected, then the time series is non-stationary with a non-zero value of \\(\\rho\\). If the null hypothesis is not rejected, then the time series is stationary.</p> </li> </ol> <p>In practice, the PP test is often conducted using software packages such as R, Python, or MATLAB, which automate the estimation of the regression model and calculation of the test statistic.</p> Notes <p>This test is generally used indirectly via the <code>pmdarima.arima.ndiffs()</code> function, which computes the differencing term, <code>d</code>.</p> <p>The R code allows for two types of tests: <code>'Z(alpha)'</code> and <code>'Z(t_alpha)'</code>. Since sklearn does not allow extraction of std errors from the linear model fit, <code>t_alpha</code> is much more difficult to achieve, so we do not allow that variant.</p> <p>Credit</p> <ul> <li>All credit goes to the <code>arch</code> library.</li> </ul> References <ul> <li>Phillips, P. C. B.; Perron, P. (1988). Testing for a Unit Root in Time Series Regression. Biometrika. 75 (2): 335-346.</li> </ul> See Also <ul> <li><code>statsmodels.tsa.stattools.adfuller</code>: Augmented Dickey-Fuller unit root test.</li> <li><code>statsmodels.tsa.stattools.kpss</code>: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.</li> <li><code>statsmodels.tsa.stattools.range_unit_root_test</code>: Range Unit-Root test.</li> <li><code>statsmodels.tsa.stattools.zivot_andrews</code>: Zivot-Andrews structural break test.</li> <li><code>pmdarima.arima.PPTest</code>: Phillips-Perron unit root test.</li> <li><code>arch.unitroot.DFGLS</code>: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller.</li> <li><code>arch.unitroot.VarianceRatio</code>: Variance Ratio test of a random walk.</li> <li><code>ts_stat_tests.algorithms.stationarity.adf</code>: Augmented Dickey-Fuller unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.kpss</code>: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.</li> <li><code>ts_stat_tests.algorithms.stationarity.rur</code>: Range Unit-Root test of stationarity.</li> <li><code>ts_stat_tests.algorithms.stationarity.za</code>: Zivot-Andrews structural break unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.pp</code>: Phillips-Perron unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.ers</code>: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller test.</li> <li><code>ts_stat_tests.algorithms.stationarity.vr</code>: Variance Ratio test of a random walk.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/stationarity.py</code> <pre><code>@typechecked\ndef pp(\n    x: ArrayLike,\n    lags: Optional[int] = None,\n    trend: VALID_PP_TREND_OPTIONS = \"c\",\n    test_type: VALID_PP_TEST_TYPE_OPTIONS = \"tau\",\n) -&gt; tuple[float, float, int, dict]:\n    r\"\"\"\n    !!! note \"Summary\"\n        Conduct a Phillips-Perron (PP) test for stationarity.\n\n        In statistics, the Phillips-Perron test (named after Peter C. B. Phillips and Pierre Perron) is a unit root test. It is used in time series analysis to test the null hypothesis that a time series is integrated of order $1$. It builds on the Dickey-Fuller test of the null hypothesis $p=0$.\n\n    ???+ abstract \"Details\"\n\n        The Phillips-Perron (PP) test is a statistical test used to determine whether a time series is stationary or not. It is similar to the Augmented Dickey-Fuller (ADF) test, but it has some advantages, especially in the presence of autocorrelation and heteroscedasticity.\n\n        The PP test involves regressing the time series on a constant and a linear time trend, and testing whether the residuals of the regression are stationary or not. The null hypothesis of the test is that the time series is non-stationary, while the alternative hypothesis is that the time series is stationary.\n\n        The test statistic is calculated by taking the sum of the squared residuals of the regression, which is adjusted for autocorrelation and heteroscedasticity. The PP test also accounts for the bias in the standard errors of the test statistic, which can lead to incorrect inference in small samples.\n\n        If the test statistic is less than a critical value at a given significance level, typically 0.05, then we reject the null hypothesis and conclude that the time series is stationary. If the test statistic is greater than the critical value, then we fail to reject the null hypothesis and conclude that the time series is non-stationary.\n\n        In practical terms, if a time series is found to be non-stationary by the PP test, one can apply differencing to the time series until it becomes stationary. This involves taking the difference between consecutive observations and potentially repeating this process until the time series is stationary.\n\n        Overall, the PP test is a powerful and robust test for stationarity, and it is widely used in time series analysis and forecasting. However, it is important to use multiple tests and diagnostic tools to determine the stationarity of a time series, as no single test is perfect in all situations.\n\n    Params:\n        x (ArrayLike):\n            The data series to test.\n        lags (Optional[int], optional):\n            The number of lags to use in the Newey-West estimator of the variance. If omitted or `None`, the lag length is selected automatically.&lt;br&gt;\n            Defaults to `None`.\n        trend (VALID_PP_TREND_OPTIONS, optional):\n            The trend component to include in the test.\n\n            - `\"n\"`: No constant, no trend.\n            - `\"c\"`: Include a constant (default).\n            - `\"ct\"`: Include a constant and linear time trend.\n\n            Defaults to `\"c\"`.\n        test_type (VALID_PP_TEST_TYPE_OPTIONS, optional):\n            The type of test statistic to compute:\n\n            - `\"tau\"`: The t-statistic based on the augmented regression (default).\n            - `\"rho\"`: The normalized autocorrelation coefficient (also known as the $Z(\\\\alpha)$ test).\n\n            Defaults to `\"tau\"`.\n\n    Returns:\n        (tuple[float, float, int, dict]):\n            Returns a tuple containing:\n            - `stat` (float): The test statistic.\n            - `pvalue` (float): The p-value for the test statistic.\n            - `lags` (int): The number of lags used in the test.\n            - `crit` (dict): The critical values at 1%, 5%, and 10%.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.stationarity import pp\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_normal, data_trend, data_sine\n        &gt;&gt;&gt; normal = data_normal\n        &gt;&gt;&gt; trend = data_trend\n        &gt;&gt;&gt; seasonal = data_sine\n        &gt;&gt;&gt; airline = data_airline.values\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Stationary Series\"}\n        &gt;&gt;&gt; stat, pvalue, lags, crit = pp(x=normal)\n        &gt;&gt;&gt; print(f\"PP statistic: {stat:.4f}\")\n        PP statistic: -30.7758\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.0000\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Trend-Stationary Series\"}\n        &gt;&gt;&gt; stat, pvalue, lags, crit = pp(x=trend, trend=\"ct\")\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.0000\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 3: Seasonal Series\"}\n        &gt;&gt;&gt; stat, pvalue, lags, crit = pp(x=seasonal)\n        &gt;&gt;&gt; print(f\"PP statistic: {stat:.4f}\")\n        PP statistic: -8.0571\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.0000\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 4: Real-World Time Series\"}\n        &gt;&gt;&gt; stat, pvalue, lags, crit = pp(x=airline)\n        &gt;&gt;&gt; print(f\"PP statistic: {stat:.4f}\")\n        PP statistic: -1.3511\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.6055\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 5: PP test with excessive lags (coverage check)\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.stationarity import pp\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_normal\n        &gt;&gt;&gt; # data_normal has 1000 observations. Force lags = 1000 to trigger adjustment.\n        &gt;&gt;&gt; res = pp(data_normal, lags=1000)\n        &gt;&gt;&gt; print(f\"stat: {res[0]:.4f}, lags: {res[2]}\")\n        stat: -43.6895, lags: 998\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The Phillips-Perron (PP) test is a commonly used test for stationarity in time series forecasting. The mathematical equation for the PP test is:\n\n        $$\n        y_t = \\delta + \\pi t + \\rho y_{t-1} + \\epsilon_t\n        $$\n\n        where:\n\n        - $y_t$ is the value of the time series at time $t$.\n        - $\\delta$ is a constant term.\n        - $\\pi$ is a coefficient that captures the trend in the data.\n        - $\\rho$ is a coefficient that captures the autocorrelation in the data.\n        - $y_{t-1}$ is the lagged value of the time series at time $t-1$.\n        - $\\epsilon_t$ is a stationary error term with mean zero and constant variance.\n\n        The PP test is based on the idea that if the time series is stationary, then the coefficient $\\rho$ should be equal to zero. Therefore, the null hypothesis of the PP test is that the time series is stationary, and the alternative hypothesis is that the time series is non-stationary with a non-zero value of $\\rho$.\n\n        Here are the detailed steps for how to calculate the PP test:\n\n        1. Collect your time series data and plot it to visually check for any trends, seasonal patterns, or other patterns that could make the data non-stationary. If you detect any such patterns, you will need to pre-process your data (e.g., detrending, deseasonalizing, etc.) to remove these effects.\n\n        1. Estimate the regression model by regressing $y_t$ on a constant, a linear trend, and the lagged value of $y_{t-1}$. The regression equation is:\n\n            $$\n            y_t = \\delta + \\pi t + \\rho y_{t-1} + \\epsilon_t\n            $$\n\n        1. Calculate the test statistic, which is based on the following equation:\n\n            $$\n            z = \\left( T^{-\\frac{1}{2}} \\right) \\times \\left( \\sum_{t=1}^T \\left( y_t - \\delta - \\pi t - \\rho y_{t-1} \\right) - \\left( \\frac{1}{T} \\right) \\times \\sum_{t=1}^T \\sum_{s=1}^T K \\left( \\frac{s-t}{h} \\right) (y_s - \\delta - \\pi s - \\rho y_{s-1}) \\right)\n            $$\n\n            where:\n\n            - $T$ is the sample size.\n            - $K(\\dots)$ is the kernel function, which determines the weight of each observation in the smoothed series. The choice of the kernel function depends on the degree of serial correlation in the data. Typically, a Gaussian kernel or a Bartlett kernel is used.\n            - $h$ is the bandwidth parameter, which controls the degree of smoothing of the series. The optimal value of $h$ depends on the sample size and the noise level of the data.\n\n        1. Determine the critical values of the test statistic from the PP distribution table. The critical values depend on the sample size and the level of significance.\n\n        1. Finally, interpret the results and draw conclusions about the stationarity of the time series. If the null hypothesis is rejected, then the time series is non-stationary with a non-zero value of $\\rho$. If the null hypothesis is not rejected, then the time series is stationary.\n\n        In practice, the PP test is often conducted using software packages such as R, Python, or MATLAB, which automate the estimation of the regression model and calculation of the test statistic.\n\n    ??? note \"Notes\"\n        This test is generally used indirectly via the [`pmdarima.arima.ndiffs()`](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.ndiffs.html) function, which computes the differencing term, `d`.\n\n        The R code allows for two types of tests: `'Z(alpha)'` and `'Z(t_alpha)'`. Since sklearn does not allow extraction of std errors from the linear model fit, `t_alpha` is much more difficult to achieve, so we do not allow that variant.\n\n    !!! success \"Credit\"\n        - All credit goes to the [`arch`](https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.PhillipsPerron.html) library.\n\n    ??? question \"References\"\n        - Phillips, P. C. B.; Perron, P. (1988). Testing for a Unit Root in Time Series Regression. Biometrika. 75 (2): 335-346.\n\n    ??? tip \"See Also\"\n        - [`statsmodels.tsa.stattools.adfuller`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.adfuller.html): Augmented Dickey-Fuller unit root test.\n        - [`statsmodels.tsa.stattools.kpss`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.kpss.html): Kwiatkowski-Phillips-Schmidt-Shin stationarity test.\n        - [`statsmodels.tsa.stattools.range_unit_root_test`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.range_unit_root_test.html): Range Unit-Root test.\n        - [`statsmodels.tsa.stattools.zivot_andrews`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.zivot_andrews.html): Zivot-Andrews structural break test.\n        - [`pmdarima.arima.PPTest`](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.PPTest.html): Phillips-Perron unit root test.\n        - [`arch.unitroot.DFGLS`](https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.DFGLS.html): Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller.\n        - [`arch.unitroot.VarianceRatio`](https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.VarianceRatio.html): Variance Ratio test of a random walk.\n        - [`ts_stat_tests.algorithms.stationarity.adf`][ts_stat_tests.algorithms.stationarity.adf]: Augmented Dickey-Fuller unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.kpss`][ts_stat_tests.algorithms.stationarity.kpss]: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.\n        - [`ts_stat_tests.algorithms.stationarity.rur`][ts_stat_tests.algorithms.stationarity.rur]: Range Unit-Root test of stationarity.\n        - [`ts_stat_tests.algorithms.stationarity.za`][ts_stat_tests.algorithms.stationarity.za]: Zivot-Andrews structural break unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.pp`][ts_stat_tests.algorithms.stationarity.pp]: Phillips-Perron unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.ers`][ts_stat_tests.algorithms.stationarity.ers]: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller test.\n        - [`ts_stat_tests.algorithms.stationarity.vr`][ts_stat_tests.algorithms.stationarity.vr]: Variance Ratio test of a random walk.\n    \"\"\"\n    _x = np.asarray(x)\n    nobs = _x.shape[0]\n    _lags = lags\n    if _lags is None:\n        _lags = int(np.ceil(12.0 * np.power(nobs / 100.0, 1 / 4.0)))\n\n    # arch PP test requires lags &lt; nobs-1\n    if _lags &gt;= nobs - 1:\n        _lags = max(0, nobs - 2)\n\n    res = _pp(y=_x, lags=_lags, trend=trend, test_type=test_type)\n    return (float(res.stat), float(res.pvalue), int(res.lags), dict(res.critical_values))\n</code></pre>"},{"location":"code/stationarity/#ts_stat_tests.algorithms.stationarity.ers","title":"ers","text":"<pre><code>ers(\n    y: ArrayLike,\n    lags: Optional[int] = None,\n    trend: VALID_ERS_TREND_OPTIONS = \"c\",\n    max_lags: Optional[int] = None,\n    method: VALID_ERS_METHOD_OPTIONS = \"aic\",\n    low_memory: Optional[bool] = None,\n) -&gt; tuple[float, float, int, dict]\n</code></pre> <p>Summary</p> <p>Elliott, Rothenberg and Stock's GLS detrended Dickey-Fuller.</p> Details <p>The Elliott-Rothenberg-Stock (ERS) test is a statistical test used to determine whether a time series is stationary or not. It is a robust test that is able to handle a wide range of non-stationary processes, including ones with structural breaks, heteroscedasticity, and autocorrelation.</p> <p>The ERS test involves fitting a local-to-zero regression of the time series on a constant and a linear time trend, using a kernel function to weight the observations. The test statistic is then calculated based on the sum of the squared residuals of the local-to-zero regression, which is adjusted for the bandwidth of the kernel function and for the correlation of the residuals.</p> <p>If the test statistic is less than a critical value at a given significance level, typically 0.05, then we reject the null hypothesis and conclude that the time series is stationary. If the test statistic is greater than the critical value, then we fail to reject the null hypothesis and conclude that the time series is non-stationary.</p> <p>In practical terms, if a time series is found to be non-stationary by the ERS test, one can apply differencing to the time series until it becomes stationary. This involves taking the difference between consecutive observations and potentially repeating this process until the time series is stationary.</p> <p>Overall, the ERS test is a powerful and flexible test for stationarity, and it is widely used in time series analysis and forecasting. However, it is important to use multiple tests and diagnostic tools to determine the stationarity of a time series, as no single test is perfect in all situations.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ArrayLike</code> <p>The data to test for a unit root.</p> required <code>lags</code> <code>Optional[int]</code> <p>The number of lags to use in the ADF regression. If omitted or <code>None</code>, method is used to automatically select the lag length with no more than <code>max_lags</code> are included. Defaults to <code>None</code>.</p> <code>None</code> <code>trend</code> <code>VALID_ERS_TREND_OPTIONS</code> <p>The trend component to include in the test</p> <ul> <li><code>\"c\"</code>: Include a constant (Default)</li> <li><code>\"ct\"</code>: Include a constant and linear time trend</li> </ul> <p>Defaults to <code>\"c\"</code>.</p> <code>'c'</code> <code>max_lags</code> <code>Optional[int]</code> <p>The maximum number of lags to use when selecting lag length. When using automatic lag length selection, the lag is selected using OLS detrending rather than GLS detrending. Defaults to <code>None</code>.</p> <code>None</code> <code>method</code> <code>VALID_ERS_METHOD_OPTIONS</code> <p>The method to use when selecting the lag length</p> <ul> <li><code>\"AIC\"</code>: Select the minimum of the Akaike IC</li> <li><code>\"BIC\"</code>: Select the minimum of the Schwarz/Bayesian IC</li> <li><code>\"t-stat\"</code>: Select the minimum of the Schwarz/Bayesian IC</li> </ul> <p>Defaults to <code>\"aic\"</code>.</p> <code>'aic'</code> <code>low_memory</code> <code>Optional[bool]</code> <p>Flag indicating whether to use the low-memory algorithm for lag-length selection. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[float, float, int, dict]</code> <p>Returns a tuple containing: - <code>stat</code> (float): The test statistic for a unit root. - <code>pvalue</code> (float): The p-value for the test statistic. - <code>lags</code> (int): The number of lags used in the test. - <code>crit</code> (dict): The critical values for the test statistic at the 1%, 5%, and 10% levels.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.stationarity import ers\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_normal, data_noise\n&gt;&gt;&gt; normal = data_normal\n&gt;&gt;&gt; noise = data_noise\n&gt;&gt;&gt; airline = data_airline.values\n</code></pre> Example 1: Stationary Series<pre><code>&gt;&gt;&gt; stat, pvalue, lags, crit = ers(y=normal)\n&gt;&gt;&gt; print(f\"ERS statistic: {stat:.4f}\")\nERS statistic: -30.1517\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.0000\n</code></pre> Example 2: Noisy Series<pre><code>&gt;&gt;&gt; stat, pvalue, lags, crit = ers(y=noise)\n&gt;&gt;&gt; print(f\"ERS statistic: {stat:.4f}\")\nERS statistic: -12.6897\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4e}\")\np-value: 1.0956e-21\n</code></pre> Example 3: Real-World Time Series<pre><code>&gt;&gt;&gt; stat, pvalue, lags, crit = ers(y=airline)\n&gt;&gt;&gt; print(f\"ERS statistic: {stat:.4f}\")\nERS statistic: 0.9918\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.9232\n</code></pre> Calculation <p>The mathematical equation for the ERS test is:</p> \\[ y_t = \\mu_t + \\epsilon_t \\] <p>where:</p> <ul> <li>\\(y_t\\) is the value of the time series at time \\(t\\).</li> <li>\\(\\mu_t\\) is a time-varying mean function.</li> <li>\\(\\epsilon_t\\) is a stationary error term with mean zero and constant variance.</li> </ul> <p>The ERS test is based on the idea that if the time series is stationary, then the mean function should be a constant over time. Therefore, the null hypothesis of the ERS test is that the time series is non-stationary (unit root), and the alternative hypothesis is that the time series is stationary.</p> <p>Here are the detailed steps for how to calculate the ERS test:</p> <ol> <li> <p>Collect your time series data and plot it to visually check for any trends, seasonal patterns, or other patterns that could make the data non-stationary. If you detect any such patterns, you will need to pre-process your data (e.g., detrending, deseasonalizing, etc.) to remove these effects.</p> </li> <li> <p>Estimate the time-varying mean function using a local polynomial regression method. The choice of the polynomial degree depends on the complexity of the mean function and the sample size. Typically, a quadratic or cubic polynomial is used. The estimated mean function is denoted as \\(\\mu_t\\).</p> </li> <li> <p>Calculate the test statistic, which is based on the following equation:</p> \\[ z = \\left( \\frac {T-1} {( \\frac {1} {12\\pi^2 \\times \\Delta^2} )} \\right) ^{\\frac{1}{2}} \\times \\left( \\sum_{t=1}^T \\frac {(y_t - \\mu_t)^2} {T-1} \\right) \\] <p>where:</p> <ul> <li>\\(T\\) is the sample size</li> <li>\\(\\Delta\\) is the bandwidth parameter, which controls the degree of smoothing of the mean function. The optimal value of \\(\\Delta\\) depends on the sample size and the noise level of the data.</li> <li>\\(\\pi\\) is the constant pi.</li> </ul> </li> <li> <p>Determine the critical values of the test statistic from the ERS distribution table. The critical values depend on the sample size and the level of significance.</p> </li> <li> <p>Finally, interpret the results and draw conclusions about the stationarity of the time series. If the null hypothesis is rejected, then the time series is non-stationary with a time-varying mean function. If the null hypothesis is not rejected, then the time series is stationary.</p> </li> </ol> <p>In practice, the ERS test is often conducted using software packages such as R, Python, or MATLAB, which automate the estimation of the time-varying mean function and calculation of the test statistic.</p> Notes <p>The null hypothesis of the Dickey-Fuller GLS is that there is a unit root, with the alternative that there is no unit root. If the p-value is above a critical size, then the null cannot be rejected and the series appears to be a unit root.</p> <p>DFGLS differs from the ADF test in that an initial GLS detrending step is used before a trend-less ADF regression is run.</p> <p>Critical values and p-values when trend is <code>\"c\"</code> are identical to the ADF. When trend is set to <code>\"ct\"</code>, they are from Elliott, Rothenberg, and Stock (1996).</p> <p>Credit</p> <ul> <li>All credit goes to the <code>arch</code> library.</li> </ul> References <ul> <li>Elliott, G. R., T. J. Rothenberg, and J. H. Stock. 1996. Efficient bootstrap for an autoregressive unit root. Econometrica 64: 813-836.</li> <li>Perron, P., &amp; Qu, Z. (2007). A simple modification to improve the finite sample properties of Ng and Perron\u2019s unit root tests. Economics letters, 94(1), 12-19.</li> </ul> See Also <ul> <li><code>statsmodels.tsa.stattools.adfuller</code>: Augmented Dickey-Fuller unit root test.</li> <li><code>statsmodels.tsa.stattools.kpss</code>: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.</li> <li><code>statsmodels.tsa.stattools.range_unit_root_test</code>: Range Unit-Root test.</li> <li><code>statsmodels.tsa.stattools.zivot_andrews</code>: Zivot-Andrews structural break test.</li> <li><code>pmdarima.arima.PPTest</code>: Phillips-Perron unit root test.</li> <li><code>arch.unitroot.DFGLS</code>: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller.</li> <li><code>arch.unitroot.VarianceRatio</code>: Variance Ratio test of a random walk.</li> <li><code>ts_stat_tests.algorithms.stationarity.adf</code>: Augmented Dickey-Fuller unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.kpss</code>: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.</li> <li><code>ts_stat_tests.algorithms.stationarity.rur</code>: Range Unit-Root test of stationarity.</li> <li><code>ts_stat_tests.algorithms.stationarity.za</code>: Zivot-Andrews structural break unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.pp</code>: Phillips-Perron unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.ers</code>: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller test.</li> <li><code>ts_stat_tests.algorithms.stationarity.vr</code>: Variance Ratio test of a random walk.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/stationarity.py</code> <pre><code>@typechecked\ndef ers(\n    y: ArrayLike,\n    lags: Optional[int] = None,\n    trend: VALID_ERS_TREND_OPTIONS = \"c\",\n    max_lags: Optional[int] = None,\n    method: VALID_ERS_METHOD_OPTIONS = \"aic\",\n    low_memory: Optional[bool] = None,\n) -&gt; tuple[float, float, int, dict]:\n    r\"\"\"\n    !!! note \"Summary\"\n        Elliott, Rothenberg and Stock's GLS detrended Dickey-Fuller.\n\n    ???+ abstract \"Details\"\n\n        The Elliott-Rothenberg-Stock (ERS) test is a statistical test used to determine whether a time series is stationary or not. It is a robust test that is able to handle a wide range of non-stationary processes, including ones with structural breaks, heteroscedasticity, and autocorrelation.\n\n        The ERS test involves fitting a local-to-zero regression of the time series on a constant and a linear time trend, using a kernel function to weight the observations. The test statistic is then calculated based on the sum of the squared residuals of the local-to-zero regression, which is adjusted for the bandwidth of the kernel function and for the correlation of the residuals.\n\n        If the test statistic is less than a critical value at a given significance level, typically 0.05, then we reject the null hypothesis and conclude that the time series is stationary. If the test statistic is greater than the critical value, then we fail to reject the null hypothesis and conclude that the time series is non-stationary.\n\n        In practical terms, if a time series is found to be non-stationary by the ERS test, one can apply differencing to the time series until it becomes stationary. This involves taking the difference between consecutive observations and potentially repeating this process until the time series is stationary.\n\n        Overall, the ERS test is a powerful and flexible test for stationarity, and it is widely used in time series analysis and forecasting. However, it is important to use multiple tests and diagnostic tools to determine the stationarity of a time series, as no single test is perfect in all situations.\n\n    Params:\n        y (ArrayLike):\n            The data to test for a unit root.\n        lags (Optional[int], optional):\n            The number of lags to use in the ADF regression. If omitted or `None`, method is used to automatically select the lag length with no more than `max_lags` are included.&lt;br&gt;\n            Defaults to `None`.\n        trend (VALID_ERS_TREND_OPTIONS, optional):\n            The trend component to include in the test\n\n            - `\"c\"`: Include a constant (Default)\n            - `\"ct\"`: Include a constant and linear time trend\n\n            Defaults to `\"c\"`.\n        max_lags (Optional[int], optional):\n            The maximum number of lags to use when selecting lag length. When using automatic lag length selection, the lag is selected using OLS detrending rather than GLS detrending.&lt;br&gt;\n            Defaults to `None`.\n        method (VALID_ERS_METHOD_OPTIONS, optional):\n            The method to use when selecting the lag length\n\n            - `\"AIC\"`: Select the minimum of the Akaike IC\n            - `\"BIC\"`: Select the minimum of the Schwarz/Bayesian IC\n            - `\"t-stat\"`: Select the minimum of the Schwarz/Bayesian IC\n\n            Defaults to `\"aic\"`.\n        low_memory (Optional[bool], optional):\n            Flag indicating whether to use the low-memory algorithm for lag-length selection.\n            Defaults to `None`.\n\n    Returns:\n        (tuple[float, float, int, dict]):\n            Returns a tuple containing:\n            - `stat` (float): The test statistic for a unit root.\n            - `pvalue` (float): The p-value for the test statistic.\n            - `lags` (int): The number of lags used in the test.\n            - `crit` (dict): The critical values for the test statistic at the 1%, 5%, and 10% levels.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.stationarity import ers\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_normal, data_noise\n        &gt;&gt;&gt; normal = data_normal\n        &gt;&gt;&gt; noise = data_noise\n        &gt;&gt;&gt; airline = data_airline.values\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Stationary Series\"}\n        &gt;&gt;&gt; stat, pvalue, lags, crit = ers(y=normal)\n        &gt;&gt;&gt; print(f\"ERS statistic: {stat:.4f}\")\n        ERS statistic: -30.1517\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.0000\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Noisy Series\"}\n        &gt;&gt;&gt; stat, pvalue, lags, crit = ers(y=noise)\n        &gt;&gt;&gt; print(f\"ERS statistic: {stat:.4f}\")\n        ERS statistic: -12.6897\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4e}\")\n        p-value: 1.0956e-21\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 3: Real-World Time Series\"}\n        &gt;&gt;&gt; stat, pvalue, lags, crit = ers(y=airline)\n        &gt;&gt;&gt; print(f\"ERS statistic: {stat:.4f}\")\n        ERS statistic: 0.9918\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.9232\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The mathematical equation for the ERS test is:\n\n        $$\n        y_t = \\mu_t + \\epsilon_t\n        $$\n\n        where:\n\n        - $y_t$ is the value of the time series at time $t$.\n        - $\\mu_t$ is a time-varying mean function.\n        - $\\epsilon_t$ is a stationary error term with mean zero and constant variance.\n\n        The ERS test is based on the idea that if the time series is stationary, then the mean function should be a constant over time. Therefore, the null hypothesis of the ERS test is that the time series is non-stationary (unit root), and the alternative hypothesis is that the time series is stationary.\n\n        Here are the detailed steps for how to calculate the ERS test:\n\n        1. Collect your time series data and plot it to visually check for any trends, seasonal patterns, or other patterns that could make the data non-stationary. If you detect any such patterns, you will need to pre-process your data (e.g., detrending, deseasonalizing, etc.) to remove these effects.\n\n        1. Estimate the time-varying mean function using a local polynomial regression method. The choice of the polynomial degree depends on the complexity of the mean function and the sample size. Typically, a quadratic or cubic polynomial is used. The estimated mean function is denoted as $\\mu_t$.\n\n        1. Calculate the test statistic, which is based on the following equation:\n\n            $$\n            z = \\left( \\frac {T-1} {( \\frac {1} {12\\pi^2 \\times \\Delta^2} )} \\right) ^{\\frac{1}{2}} \\times \\left( \\sum_{t=1}^T \\frac {(y_t - \\mu_t)^2} {T-1} \\right)\n            $$\n\n            where:\n\n            - $T$ is the sample size\n            - $\\Delta$ is the bandwidth parameter, which controls the degree of smoothing of the mean function. The optimal value of $\\Delta$ depends on the sample size and the noise level of the data.\n            - $\\pi$ is the constant pi.\n\n        1. Determine the critical values of the test statistic from the ERS distribution table. The critical values depend on the sample size and the level of significance.\n\n        1. Finally, interpret the results and draw conclusions about the stationarity of the time series. If the null hypothesis is rejected, then the time series is non-stationary with a time-varying mean function. If the null hypothesis is not rejected, then the time series is stationary.\n\n        In practice, the ERS test is often conducted using software packages such as R, Python, or MATLAB, which automate the estimation of the time-varying mean function and calculation of the test statistic.\n\n    ??? note \"Notes\"\n        The null hypothesis of the Dickey-Fuller GLS is that there is a unit root, with the alternative that there is no unit root. If the p-value is above a critical size, then the null cannot be rejected and the series appears to be a unit root.\n\n        DFGLS differs from the ADF test in that an initial GLS detrending step is used before a trend-less ADF regression is run.\n\n        Critical values and p-values when trend is `\"c\"` are identical to the ADF. When trend is set to `\"ct\"`, they are from Elliott, Rothenberg, and Stock (1996).\n\n    !!! success \"Credit\"\n        - All credit goes to the [`arch`](https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.DFGLS.html) library.\n\n    ??? question \"References\"\n        - Elliott, G. R., T. J. Rothenberg, and J. H. Stock. 1996. Efficient bootstrap for an autoregressive unit root. Econometrica 64: 813-836.\n        - Perron, P., &amp; Qu, Z. (2007). A simple modification to improve the finite sample properties of Ng and Perron\u2019s unit root tests. Economics letters, 94(1), 12-19.\n\n    ??? tip \"See Also\"\n        - [`statsmodels.tsa.stattools.adfuller`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.adfuller.html): Augmented Dickey-Fuller unit root test.\n        - [`statsmodels.tsa.stattools.kpss`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.kpss.html): Kwiatkowski-Phillips-Schmidt-Shin stationarity test.\n        - [`statsmodels.tsa.stattools.range_unit_root_test`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.range_unit_root_test.html): Range Unit-Root test.\n        - [`statsmodels.tsa.stattools.zivot_andrews`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.zivot_andrews.html): Zivot-Andrews structural break test.\n        - [`pmdarima.arima.PPTest`](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.PPTest.html): Phillips-Perron unit root test.\n        - [`arch.unitroot.DFGLS`](https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.DFGLS.html): Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller.\n        - [`arch.unitroot.VarianceRatio`](https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.VarianceRatio.html): Variance Ratio test of a random walk.\n        - [`ts_stat_tests.algorithms.stationarity.adf`][ts_stat_tests.algorithms.stationarity.adf]: Augmented Dickey-Fuller unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.kpss`][ts_stat_tests.algorithms.stationarity.kpss]: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.\n        - [`ts_stat_tests.algorithms.stationarity.rur`][ts_stat_tests.algorithms.stationarity.rur]: Range Unit-Root test of stationarity.\n        - [`ts_stat_tests.algorithms.stationarity.za`][ts_stat_tests.algorithms.stationarity.za]: Zivot-Andrews structural break unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.pp`][ts_stat_tests.algorithms.stationarity.pp]: Phillips-Perron unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.ers`][ts_stat_tests.algorithms.stationarity.ers]: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller test.\n        - [`ts_stat_tests.algorithms.stationarity.vr`][ts_stat_tests.algorithms.stationarity.vr]: Variance Ratio test of a random walk.\n    \"\"\"\n    res = _ers(\n        y=np.asarray(y),\n        lags=lags,\n        trend=trend,\n        max_lags=max_lags,\n        method=method,\n        low_memory=low_memory,\n    )\n    return (float(res.stat), float(res.pvalue), int(res.lags), dict(res.critical_values))\n</code></pre>"},{"location":"code/stationarity/#ts_stat_tests.algorithms.stationarity.vr","title":"vr","text":"<pre><code>vr(\n    y: ArrayLike,\n    lags: int = 2,\n    trend: VALID_VR_TREND_OPTIONS = \"c\",\n    debiased: bool = True,\n    robust: bool = True,\n    overlap: bool = True,\n) -&gt; tuple[float, float, float]\n</code></pre> <p>Summary</p> <p>Variance Ratio test of a random walk.</p> Details <p>The Variance Ratio (VR) test is a statistical test used to determine whether a time series is stationary or not based on the presence of long-term dependence in the series. It is a non-parametric test that can be used to test for the presence of a unit root or a trend in the series.</p> <p>The VR test involves calculating the ratio of the variance of the differences of the logarithms of the time series over different time intervals. The variance of the differences of the logarithms is a measure of the volatility of the series, and the ratio of the variances over different intervals is a measure of the long-term dependence in the series.</p> <p>If the series is stationary, then the variance ratio will be close to one for all intervals. If the series is non-stationary, then the variance ratio will tend to increase as the length of the interval increases, reflecting the presence of long-term dependence in the series.</p> <p>The VR test involves comparing the observed variance ratio to the distribution of variance ratios expected under the null hypothesis of a random walk (non-stationary). If the test statistic is less than a critical value at a given significance level, typically 0.05, then we reject the null hypothesis and conclude that the time series is stationary. If the test statistic is greater than the critical value, then we fail to reject the null hypothesis and conclude that the time series is non-stationary.</p> <p>In practical terms, if a time series is found to be non-stationary by the VR test, one can apply differencing to the time series until it becomes stationary. This involves taking the difference between consecutive observations and potentially repeating this process until the time series is stationary.</p> <p>Overall, the VR test is a useful and relatively simple test for stationarity that can be applied to a wide range of time series. However, it is important to use multiple tests and diagnostic tools to confirm the stationarity of a time series, as no single test is perfect in all situations.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>ArrayLike</code> <p>The data to test for a random walk.</p> required <code>lags</code> <code>int</code> <p>The number of periods to used in the multi-period variance, which is the numerator of the test statistic. Must be at least 2. Defaults to <code>2</code>.</p> <code>2</code> <code>trend</code> <code>VALID_VR_TREND_OPTIONS</code> <p><code>\"c\"</code> allows for a non-zero drift in the random walk, while <code>\"n\"</code> requires that the increments to <code>y</code> are mean <code>0</code>. Defaults to <code>\"c\"</code>.</p> <code>'c'</code> <code>debiased</code> <code>bool</code> <p>Indicates whether to use a debiased version of the test. Only applicable if <code>overlap</code> is <code>True</code>. Defaults to <code>True</code>.</p> <code>True</code> <code>robust</code> <code>bool</code> <p>Indicates whether to use heteroskedasticity robust inference. Defaults to <code>True</code>.</p> <code>True</code> <code>overlap</code> <code>bool</code> <p>Indicates whether to use all overlapping blocks. If <code>False</code>, the number of observations in \\(y-1\\) must be an exact multiple of <code>lags</code>. If this condition is not satisfied, some values at the end of <code>y</code> will be discarded. Defaults to <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>tuple[float, float, float]</code> <p>Returns a tuple containing: - <code>stat</code> (float): The test statistic for a unit root. - <code>pvalue</code> (float): The p-value for the test statistic. - <code>vr</code> (float): The ratio of the long block lags-period variance.</p> Examples Setup<pre><code>&gt;&gt;&gt; from ts_stat_tests.algorithms.stationarity import vr\n&gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_normal, data_noise, data_sine\n&gt;&gt;&gt; normal = data_normal\n&gt;&gt;&gt; noise = data_noise\n&gt;&gt;&gt; seasonal = data_sine\n&gt;&gt;&gt; airline = data_airline.values\n</code></pre> Example 1: Stationary Series<pre><code>&gt;&gt;&gt; stat, pvalue, variance_ratio = vr(y=normal)\n&gt;&gt;&gt; print(f\"VR statistic: {stat:.4f}\")\nVR statistic: -12.8518\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.0000\n&gt;&gt;&gt; print(f\"Variance ratio: {variance_ratio:.4f}\")\nVariance ratio: 0.5202\n</code></pre> Example 2: Noisy Series<pre><code>&gt;&gt;&gt; stat, pvalue, variance_ratio = vr(y=noise)\n&gt;&gt;&gt; print(f\"VR statistic: {stat:.4f}\")\nVR statistic: -11.5007\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.0000\n&gt;&gt;&gt; print(f\"Variance ratio: {variance_ratio:.4f}\")\nVariance ratio: 0.5094\n</code></pre> Example 3: Seasonal Series<pre><code>&gt;&gt;&gt; stat, pvalue, variance_ratio = vr(y=seasonal)\n&gt;&gt;&gt; print(f\"VR statistic: {stat:.4f}\")\nVR statistic: 44.7019\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.0000\n&gt;&gt;&gt; print(f\"Variance ratio: {variance_ratio:.4f}\")\nVariance ratio: 1.9980\n</code></pre> Example 4: Real-World Time Series<pre><code>&gt;&gt;&gt; stat, pvalue, variance_ratio = vr(y=airline)\n&gt;&gt;&gt; print(f\"VR statistic: {stat:.4f}\")\nVR statistic: 3.1511\n&gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\np-value: 0.0016\n&gt;&gt;&gt; print(f\"Variance ratio: {variance_ratio:.4f}\")\nVariance ratio: 1.3163\n</code></pre> Calculation <p>The Variance Ratio (VR) test is a statistical test for stationarity in time series forecasting that is based on the idea that if the time series is stationary, then the variance of the returns should be constant over time. The mathematical equation for the VR test is:</p> \\[ VR(k) = \\frac {\\sigma^2(k)} {k\\sigma^2(1)} \\] <p>where:</p> <ul> <li>\\(VR(k)\\) is the variance ratio for the time series over \\(k\\) periods.</li> <li>\\(\\sigma^2(k)\\) is the variance of the returns over \\(k\\) periods.</li> <li>\\(\\sigma^2(1)\\) is the variance of the returns over \\(1\\) period.</li> </ul> <p>The VR test involves comparing the variance ratio to a critical value, which is derived from the null distribution of the variance ratio under the assumption of a random walk with drift.</p> <p>Here are the detailed steps for how to calculate the VR test:</p> <ol> <li> <p>Collect your time series data and compute the log returns, which are defined as:</p> \\[ r_t = \\log(y_t) - \\log(y_{t-1}) \\] <p>where:</p> <ul> <li>\\(y_t\\) is the value of the time series at time \\(t\\).</li> </ul> </li> <li> <p>Compute the variance of the returns over \\(k\\) periods, which is defined as:</p> \\[ \\sigma^2(k) = \\left( \\frac {1} {n-k} \\right) \\times \\sum_{t=k+1}^n (r_t - \\mu_k)^2 \\] <p>where:</p> <ul> <li>\\(n\\) is the sample size.</li> <li> <p>\\(\\mu_k\\) is the mean of the returns over \\(k\\) periods, which is defined as:</p> <p>\\(\\mu_k = \\left( \\frac{1} {n-k} \\right) \\times \\sum_{t=k+1}^n r_t\\)</p> </li> </ul> </li> <li> <p>Compute the variance of the returns over \\(1\\) period, which is defined as:</p> \\[ \\sigma^2(1) = \\left( \\frac{1} {n-1} \\right) \\times \\sum_{t=2}^n (r_t - \\mu_1)^2 \\] <p>where:</p> <ul> <li> <p>\\(\\mu_1\\) is the mean of the returns over \\(1\\) period, which is defined as:</p> <p>\\(\\mu_1 = \\left( \\frac{1} {n-1} \\right) \\times \\sum_{t=2}^n r_t\\)</p> </li> </ul> </li> <li> <p>Compute the variance ratio for each value of \\(k\\), which is defined as:</p> \\[ VR(k) = \\frac {\\sigma^2(k)} {k\\sigma^2(1)} \\] </li> <li> <p>Determine the critical values of the variance ratio from the null distribution table of the VR test, which depend on the sample size, the level of significance, and the lag length \\(k\\).</p> </li> <li> <p>Finally, compare the variance ratio to the critical value. If the variance ratio is greater than the critical value, then the null hypothesis of a random walk with drift is rejected, and the time series is considered stationary. If the variance ratio is less than or equal to the critical value, then the null hypothesis cannot be rejected, and the time series is considered non-stationary.</p> </li> </ol> <p>In practice, the VR test is often conducted using software packages such as R, Python, or MATLAB, which automate the calculation of the variance ratio and the determination of the critical value.</p> Notes <p>The null hypothesis of a VR is that the process is a random walk, possibly plus drift. Rejection of the null with a positive test statistic indicates the presence of positive serial correlation in the time series.</p> <p>Credit</p> <ul> <li>All credit goes to the <code>arch</code> library.</li> </ul> References <ul> <li>Campbell, John Y., Lo, Andrew W. and MacKinlay, A. Craig. (1997) The Econometrics of Financial Markets. Princeton, NJ: Princeton University Press.</li> </ul> See Also <ul> <li><code>statsmodels.tsa.stattools.adfuller</code>: Augmented Dickey-Fuller unit root test.</li> <li><code>statsmodels.tsa.stattools.kpss</code>: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.</li> <li><code>statsmodels.tsa.stattools.range_unit_root_test</code>: Range Unit-Root test.</li> <li><code>statsmodels.tsa.stattools.zivot_andrews</code>: Zivot-Andrews structural break test.</li> <li><code>pmdarima.arima.PPTest</code>: Phillips-Perron unit root test.</li> <li><code>arch.unitroot.DFGLS</code>: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller.</li> <li><code>arch.unitroot.VarianceRatio</code>: Variance Ratio test of a random walk.</li> <li><code>ts_stat_tests.algorithms.stationarity.adf</code>: Augmented Dickey-Fuller unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.kpss</code>: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.</li> <li><code>ts_stat_tests.algorithms.stationarity.rur</code>: Range Unit-Root test of stationarity.</li> <li><code>ts_stat_tests.algorithms.stationarity.za</code>: Zivot-Andrews structural break unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.pp</code>: Phillips-Perron unit root test.</li> <li><code>ts_stat_tests.algorithms.stationarity.ers</code>: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller test.</li> <li><code>ts_stat_tests.algorithms.stationarity.vr</code>: Variance Ratio test of a random walk.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/stationarity.py</code> <pre><code>@typechecked\ndef vr(\n    y: ArrayLike,\n    lags: int = 2,\n    trend: VALID_VR_TREND_OPTIONS = \"c\",\n    debiased: bool = True,\n    robust: bool = True,\n    overlap: bool = True,\n) -&gt; tuple[float, float, float]:\n    r\"\"\"\n    !!! note \"Summary\"\n        Variance Ratio test of a random walk.\n\n    ???+ abstract \"Details\"\n\n        The Variance Ratio (VR) test is a statistical test used to determine whether a time series is stationary or not based on the presence of long-term dependence in the series. It is a non-parametric test that can be used to test for the presence of a unit root or a trend in the series.\n\n        The VR test involves calculating the ratio of the variance of the differences of the logarithms of the time series over different time intervals. The variance of the differences of the logarithms is a measure of the volatility of the series, and the ratio of the variances over different intervals is a measure of the long-term dependence in the series.\n\n        If the series is stationary, then the variance ratio will be close to one for all intervals. If the series is non-stationary, then the variance ratio will tend to increase as the length of the interval increases, reflecting the presence of long-term dependence in the series.\n\n        The VR test involves comparing the observed variance ratio to the distribution of variance ratios expected under the null hypothesis of a random walk (non-stationary). If the test statistic is less than a critical value at a given significance level, typically 0.05, then we reject the null hypothesis and conclude that the time series is stationary. If the test statistic is greater than the critical value, then we fail to reject the null hypothesis and conclude that the time series is non-stationary.\n\n        In practical terms, if a time series is found to be non-stationary by the VR test, one can apply differencing to the time series until it becomes stationary. This involves taking the difference between consecutive observations and potentially repeating this process until the time series is stationary.\n\n        Overall, the VR test is a useful and relatively simple test for stationarity that can be applied to a wide range of time series. However, it is important to use multiple tests and diagnostic tools to confirm the stationarity of a time series, as no single test is perfect in all situations.\n\n    Params:\n        y (ArrayLike):\n            The data to test for a random walk.\n        lags (int):\n            The number of periods to used in the multi-period variance, which is the numerator of the test statistic. Must be at least 2.&lt;br&gt;\n            Defaults to `2`.\n        trend (VALID_VR_TREND_OPTIONS, optional):\n            `\"c\"` allows for a non-zero drift in the random walk, while `\"n\"` requires that the increments to `y` are mean `0`.&lt;br&gt;\n            Defaults to `\"c\"`.\n        debiased (bool, optional):\n            Indicates whether to use a debiased version of the test. Only applicable if `overlap` is `True`.&lt;br&gt;\n            Defaults to `True`.\n        robust (bool, optional):\n            Indicates whether to use heteroskedasticity robust inference.&lt;br&gt;\n            Defaults to `True`.\n        overlap (bool, optional):\n            Indicates whether to use all overlapping blocks. If `False`, the number of observations in $y-1$ must be an exact multiple of `lags`. If this condition is not satisfied, some values at the end of `y` will be discarded.&lt;br&gt;\n            Defaults to `True`.\n\n    Returns:\n        (tuple[float, float, float]):\n            Returns a tuple containing:\n            - `stat` (float): The test statistic for a unit root.\n            - `pvalue` (float): The p-value for the test statistic.\n            - `vr` (float): The ratio of the long block lags-period variance.\n\n    ???+ example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Setup\"}\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.stationarity import vr\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import data_airline, data_normal, data_noise, data_sine\n        &gt;&gt;&gt; normal = data_normal\n        &gt;&gt;&gt; noise = data_noise\n        &gt;&gt;&gt; seasonal = data_sine\n        &gt;&gt;&gt; airline = data_airline.values\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 1: Stationary Series\"}\n        &gt;&gt;&gt; stat, pvalue, variance_ratio = vr(y=normal)\n        &gt;&gt;&gt; print(f\"VR statistic: {stat:.4f}\")\n        VR statistic: -12.8518\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.0000\n        &gt;&gt;&gt; print(f\"Variance ratio: {variance_ratio:.4f}\")\n        Variance ratio: 0.5202\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 2: Noisy Series\"}\n        &gt;&gt;&gt; stat, pvalue, variance_ratio = vr(y=noise)\n        &gt;&gt;&gt; print(f\"VR statistic: {stat:.4f}\")\n        VR statistic: -11.5007\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.0000\n        &gt;&gt;&gt; print(f\"Variance ratio: {variance_ratio:.4f}\")\n        Variance ratio: 0.5094\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 3: Seasonal Series\"}\n        &gt;&gt;&gt; stat, pvalue, variance_ratio = vr(y=seasonal)\n        &gt;&gt;&gt; print(f\"VR statistic: {stat:.4f}\")\n        VR statistic: 44.7019\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.0000\n        &gt;&gt;&gt; print(f\"Variance ratio: {variance_ratio:.4f}\")\n        Variance ratio: 1.9980\n\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Example 4: Real-World Time Series\"}\n        &gt;&gt;&gt; stat, pvalue, variance_ratio = vr(y=airline)\n        &gt;&gt;&gt; print(f\"VR statistic: {stat:.4f}\")\n        VR statistic: 3.1511\n        &gt;&gt;&gt; print(f\"p-value: {pvalue:.4f}\")\n        p-value: 0.0016\n        &gt;&gt;&gt; print(f\"Variance ratio: {variance_ratio:.4f}\")\n        Variance ratio: 1.3163\n\n        ```\n\n    ??? equation \"Calculation\"\n\n        The Variance Ratio (VR) test is a statistical test for stationarity in time series forecasting that is based on the idea that if the time series is stationary, then the variance of the returns should be constant over time. The mathematical equation for the VR test is:\n\n        $$\n        VR(k) = \\frac {\\sigma^2(k)} {k\\sigma^2(1)}\n        $$\n\n        where:\n\n        - $VR(k)$ is the variance ratio for the time series over $k$ periods.\n        - $\\sigma^2(k)$ is the variance of the returns over $k$ periods.\n        - $\\sigma^2(1)$ is the variance of the returns over $1$ period.\n\n        The VR test involves comparing the variance ratio to a critical value, which is derived from the null distribution of the variance ratio under the assumption of a random walk with drift.\n\n        Here are the detailed steps for how to calculate the VR test:\n\n        1. Collect your time series data and compute the log returns, which are defined as:\n\n            $$\n            r_t = \\log(y_t) - \\log(y_{t-1})\n            $$\n\n            where:\n\n            - $y_t$ is the value of the time series at time $t$.\n\n        1. Compute the variance of the returns over $k$ periods, which is defined as:\n\n            $$\n            \\sigma^2(k) = \\left( \\frac {1} {n-k} \\right) \\times \\sum_{t=k+1}^n (r_t - \\mu_k)^2\n            $$\n\n            where:\n\n            - $n$ is the sample size.\n            - $\\mu_k$ is the mean of the returns over $k$ periods, which is defined as:\n\n                $\\mu_k = \\left( \\frac{1} {n-k} \\right) \\times \\sum_{t=k+1}^n r_t$\n\n        1. Compute the variance of the returns over $1$ period, which is defined as:\n\n            $$\n            \\sigma^2(1) = \\left( \\frac{1} {n-1} \\right) \\times \\sum_{t=2}^n (r_t - \\mu_1)^2\n            $$\n\n            where:\n\n            - $\\mu_1$ is the mean of the returns over $1$ period, which is defined as:\n\n                $\\mu_1 = \\left( \\frac{1} {n-1} \\right) \\times \\sum_{t=2}^n r_t$\n\n        1. Compute the variance ratio for each value of $k$, which is defined as:\n\n            $$\n            VR(k) = \\frac {\\sigma^2(k)} {k\\sigma^2(1)}\n            $$\n\n        1. Determine the critical values of the variance ratio from the null distribution table of the VR test, which depend on the sample size, the level of significance, and the lag length $k$.\n\n        1. Finally, compare the variance ratio to the critical value. If the variance ratio is greater than the critical value, then the null hypothesis of a random walk with drift is rejected, and the time series is considered stationary. If the variance ratio is less than or equal to the critical value, then the null hypothesis cannot be rejected, and the time series is considered non-stationary.\n\n        In practice, the VR test is often conducted using software packages such as R, Python, or MATLAB, which automate the calculation of the variance ratio and the determination of the critical value.\n\n    ??? note \"Notes\"\n        The null hypothesis of a VR is that the process is a random walk, possibly plus drift. Rejection of the null with a positive test statistic indicates the presence of positive serial correlation in the time series.\n\n    !!! success \"Credit\"\n        - All credit goes to the [`arch`](https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.VarianceRatio.html) library.\n\n    ??? question \"References\"\n        - Campbell, John Y., Lo, Andrew W. and MacKinlay, A. Craig. (1997) The Econometrics of Financial Markets. Princeton, NJ: Princeton University Press.\n\n    ??? tip \"See Also\"\n        - [`statsmodels.tsa.stattools.adfuller`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.adfuller.html): Augmented Dickey-Fuller unit root test.\n        - [`statsmodels.tsa.stattools.kpss`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.kpss.html): Kwiatkowski-Phillips-Schmidt-Shin stationarity test.\n        - [`statsmodels.tsa.stattools.range_unit_root_test`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.range_unit_root_test.html): Range Unit-Root test.\n        - [`statsmodels.tsa.stattools.zivot_andrews`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.zivot_andrews.html): Zivot-Andrews structural break test.\n        - [`pmdarima.arima.PPTest`](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.PPTest.html): Phillips-Perron unit root test.\n        - [`arch.unitroot.DFGLS`](https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.DFGLS.html): Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller.\n        - [`arch.unitroot.VarianceRatio`](https://arch.readthedocs.io/en/latest/unitroot/generated/arch.unitroot.VarianceRatio.html): Variance Ratio test of a random walk.\n        - [`ts_stat_tests.algorithms.stationarity.adf`][ts_stat_tests.algorithms.stationarity.adf]: Augmented Dickey-Fuller unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.kpss`][ts_stat_tests.algorithms.stationarity.kpss]: Kwiatkowski-Phillips-Schmidt-Shin stationarity test.\n        - [`ts_stat_tests.algorithms.stationarity.rur`][ts_stat_tests.algorithms.stationarity.rur]: Range Unit-Root test of stationarity.\n        - [`ts_stat_tests.algorithms.stationarity.za`][ts_stat_tests.algorithms.stationarity.za]: Zivot-Andrews structural break unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.pp`][ts_stat_tests.algorithms.stationarity.pp]: Phillips-Perron unit root test.\n        - [`ts_stat_tests.algorithms.stationarity.ers`][ts_stat_tests.algorithms.stationarity.ers]: Elliot, Rothenberg and Stock's GLS-detrended Dickey-Fuller test.\n        - [`ts_stat_tests.algorithms.stationarity.vr`][ts_stat_tests.algorithms.stationarity.vr]: Variance Ratio test of a random walk.\n    \"\"\"\n    res = _vr(\n        y=np.asarray(y),\n        lags=lags,\n        trend=trend,\n        debiased=debiased,\n        robust=robust,\n        overlap=overlap,\n    )\n    return float(res.stat), float(res.pvalue), float(res.vr)\n</code></pre>"},{"location":"usage/changelog/","title":"Change Log","text":"<p>v0.5.3</p> <p>v0.4.5</p> <p>v0.3.1</p> <p>v0.2.0</p> <p>v0.1.0</p>"},{"location":"usage/changelog/#v053-seasonality-module","title":"v0.5.3 - Seasonality Module","text":"<p><code>v0.5.3</code> <code>2026-01-16</code> data-science-extensions/ts-stat-tests/releases/v0.5.3</p> Release Notes Updates <ul> <li><code>e54021f</code>: Refactor return types and handling in normality and seasonality tests<ul> <li>Update return type of <code>normality()</code> function to support variable-length tuples.</li> <li>Simplify handling of results in <code>is_normal()</code> function for better clarity.</li> <li>Enhance return structure in <code>seasonality()</code> function to accommodate ARIMA objects.</li> <li>Improve type handling in internal helper functions for robustness. (by chrimaho)        * <code>699a777</code>: Refactor return type and structure of <code>is_stationary()</code> function</li> <li>Update return type hint to use <code>STATIONARITY_ITEM</code> for clarity.</li> <li>Explicitly define return dictionary to match the new type hint.</li> <li>Adjust handling of <code>pvalue</code> to return <code>None</code> when applicable. (by chrimaho)        * <code>dd5445a</code>: Add tests for seasonality and normality algorithms</li> <li>Implement fallback tests for Anderson-Darling in <code>is_normal()</code> function.</li> <li>Add coverage for list-based result branches in <code>is_seasonal()</code> function.</li> <li>Test fallback for non-numeric types in <code>is_seasonal()</code> function.</li> <li>Test fallback for non-numeric scalar types in <code>is_seasonal()</code> function. (by chrimaho)        * <code>7b6daf2</code>: Enhance pytest configuration and update check functions.</li> <li>Add <code>--doctest-modules</code> and <code>--doctest-continue-on-failure</code> options to pytest in <code>pyproject.toml</code>.</li> <li>Modify <code>check_pytest()</code> function to dynamically include Python files for testing.</li> <li>Comment out <code>check_doctest()</code> in the <code>check()</code> function. (by chrimaho)        * <code>0c1da06</code>: Refactor type hints in stationarity module</li> <li>Remove unnecessary <code>Optional</code> from return type of <code>is_stationary()</code> function.</li> <li>Simplify type hints for <code>stat</code> and <code>pvalue</code> variables to <code>Any</code>. (by chrimaho)        * <code>8bf8b36</code>: Refactor type hints to remove <code>cast()</code> usage</li> <li>Remove <code>cast()</code> from <code>normality</code> mofule.</li> <li>Remove <code>cast()</code> from <code>seasonality</code> module.</li> <li>Remove <code>cast()</code> from <code>stationarity</code> module. (by chrimaho)        * <code>74bf5f8</code>: Remove <code>cast()</code> from stationarity module</li> <li>Remove unnecessary <code>cast()</code> in the <code>is_stationary()</code> function.</li> <li>Simplify the return type assignment to directly use the result of <code>stationarity()</code> function. (by chrimaho)        * <code>aa51207</code>: Enhance normality test return types and type hints.</li> <li>Update return type of <code>normality()</code> to include <code>NormaltestResult</code>, <code>ShapiroResult</code>, and <code>AndersonResult</code>.</li> <li>Replace <code>cast()</code> with direct type annotations for improved clarity.</li> <li>Adjust variable types in <code>is_normal()</code> for consistency and readability. (by chrimaho)        * <code>49203f7</code>: Refactor type hints to remove <code>cast()</code> in stationarity and seasonality modules</li> <li>Update type hints to use <code>Any</code> for better flexibility.</li> <li>Remove unnecessary <code>cast()</code> calls to simplify code.</li> <li>Ensure consistent handling of return types across functions. (by chrimaho)        * <code>2ccd1bd</code>: Round numerical outputs in the <code>adf()</code> function for improved readability.</li> <li>Update output formatting to display rounded values.</li> <li>Enhance clarity of example output in documentation. (by chrimaho)        * <code>e6fe3af</code>: Round numerical outputs in entropy and regularity functions for improved readability.</li> <li>Update example outputs in <code>entropy()</code> function to show rounded values.</li> <li>Update example outputs in <code>regularity()</code> function to show rounded values. (by chrimaho)        * <code>d9ac474</code>: Refactor numpy type hints to use <code>NDArray</code> instead of <code>np.ndarray</code></li> <li>Update type hints in <code>seasonality.py</code> to use <code>NDArray[np.float64]</code></li> <li>Modify return types in <code>correlation.py</code> to use <code>NDArray[np.float64]</code></li> <li>Adjust type casting in <code>normality.py</code> to reflect new type hints (by chrimaho)        * <code>b2ffd98</code>: Refactor data types in test setup for consistency</li> <li>Change data types of test data attributes to use <code>NDArray[np.float64]</code></li> <li>Update <code>cls.data_random</code>, <code>cls.data_sine</code>, <code>cls.data_line</code>, <code>cls.data_noise</code>, and <code>cls.data_2d</code> attributes (by chrimaho)        * <code>9a75913</code>: Fix import issue for <code>VarianceRatio</code> in the <code>stationarity</code> module</li> <li>Remove explicit type annotation for <code>res</code> in <code>vr()</code> function.</li> <li>Simplify assignment to <code>res</code> by directly using <code>_vr()</code>. (by chrimaho)        * <code>bff495e</code>: Refactor return types in data utility functions for improved type safety</li> <li>Change return type of <code>get_random_numbers()</code> to <code>NDArray[np.float64]</code></li> <li>Change return type of <code>get_random_numbers_2d()</code> to <code>NDArray[np.float64]</code></li> <li>Change return type of <code>get_sine_wave()</code> to <code>NDArray[np.float64]</code></li> <li>Change return type of <code>get_normal_curve()</code> to <code>NDArray[np.float64]</code></li> <li>Change return type of <code>get_straight_line()</code> to <code>NDArray[np.float64]</code></li> <li>Change return type of <code>get_trend_data()</code> to <code>NDArray[np.float64]</code></li> <li>Change return type of <code>get_uniform_data()</code> to <code>NDArray[np.float64]</code></li> <li>Change return type of <code>get_noise_data()</code> to <code>NDArray[np.float64]</code></li> <li>Update variable types in data loading to reflect new return types (by chrimaho)        * <code>13d0c18</code>: Refactor type handling in stationarity functions</li> <li>Introduce type aliases for improved clarity and maintainability.</li> <li>Update function signatures to use new type aliases.</li> <li>Simplify return statements in <code>is_stationary()</code> function. (by chrimaho)        * <code>0cec5c4</code>: Refactor imports in <code>stationarity.py</code> for consistency and clarity.</li> <li>Standardise import statements for <code>VarianceRatio</code>.</li> <li>Group related imports together for better readability. (by chrimaho)        * <code>a15a06c</code>: Add examples to <code>adf()</code> function docs</li> <li>Include example usage for storing results with <code>adf(x=airline, store=True)</code></li> <li>Add example for using <code>adf()</code> without autolag: <code>adf(x=airline, autolag=None, maxlag=5)</code> (by chrimaho)        * <code>1d300e9</code>: Refactor return statements in stationarity functions for clarity and efficiency.</li> <li>Simplify return statements in <code>kpss()</code>, <code>rur()</code>, and <code>za()</code> functions by removing unnecessary <code>cast()</code>.</li> <li>Enhance readability by directly returning results from <code>_kpss()</code>, <code>_rur()</code>, and <code>_za()</code>.</li> <li>Maintain type safety while improving code clarity in the <code>vr()</code> function. (by chrimaho)        * <code>feb0ace</code>: Refactor <code>adf()</code> function for improved type handling and return structure</li> <li>Change return type handling to use <code>Any</code> for compatibility with statsmodels stubs</li> <li>Simplify return statements based on <code>store</code> and <code>autolag</code> parameters</li> <li>Ensure consistent float and integer conversions for returned values (by chrimaho)        * <code>5479ac7</code>: Add tests for ADF stationarity function</li> <li>Introduce <code>test_stationarity_adf_store()</code> to validate ADF with <code>store=True</code></li> <li>Introduce <code>test_stationarity_adf_autolag_none()</code> to validate ADF with <code>autolag=None</code> (by chrimaho)        * <code>aaba18f</code>: Refactor type annotations in the <code>regularity</code> module</li> <li>Update type annotations in <code>regularity.py</code> and <code>tests/regularity.py</code> to use <code>NDArray</code> where appropriate.</li> <li>Simplify return statements in <code>approx_entropy()</code>, <code>sample_entropy()</code>, <code>permutation_entropy()</code>, <code>spectral_entropy()</code>, and <code>svd_entropy()</code> functions.</li> <li>Ensure all functions return consistent types, enhancing type safety and readability. (by chrimaho)        * <code>8f90a8c</code>: Refactor <code>normality</code> module for improved type annotations in all functions</li> <li>Remove unnecessary class definitions and simplify return types.</li> <li>Update function signatures to use specific return types like <code>ShapiroResult</code> and <code>NormaltestResult</code>.</li> <li>Streamline the implementation of statistical tests by returning results directly from their respective functions. (by chrimaho)        * <code>f8d4c7e</code>: Fix typo (by chrimaho)        * <code>d9f458d</code>: Refactor example outputs in <code>qs()</code> function</li> <li>Adjust precision of output values in the docstring.</li> <li>Ensure clarity in the representation of ARIMA model output. (by chrimaho)        * <code>340f63c</code>: Fix typo (by chrimaho)        * <code>74fec0b</code>: Refactor seasonality tests for clarity and consistency.</li> <li>Rename test methods for better readability.</li> <li>Remove debug print statements.</li> <li>Update assertions to ensure accuracy.</li> <li>Improve documentation for test cases. (by chrimaho)        * <code>006a7b2</code>: Refactor correlation tests and add new data tests</li> <li>Remove unused imports and redundant tests from <code>test_correlation.py</code></li> <li>Create <code>test_data.py</code> with tests for <code>load_airline()</code> and <code>load_macrodata()</code></li> <li>Enhance error handling tests in <code>test_errors.py</code> (by chrimaho)        * <code>ece6d42</code>: Refactor covariance type handling in <code>lm()</code> function</li> <li>Standardise <code>cov_type</code> parameter to use <code>VALID_LM_COV_TYPE_OPTIONS</code></li> <li>Remove redundant overloads for <code>lm()</code> function</li> <li>Update docstring for clarity on <code>cov_type</code> options (by chrimaho)        * <code>46fdb01</code>: Refactor type hints for <code>correlation</code> algorithms</li> <li>Update type hints to use <code>NDArray[np.float64]</code> for better type clarity.</li> <li>Modify return types in <code>acf()</code>, <code>pacf()</code>, <code>ccf()</code>, <code>lb()</code>, <code>lm()</code>, and <code>bglm()</code> functions.</li> <li>Standardise covariance type options in <code>VALID_LM_COV_TYPE_OPTIONS</code>. (by chrimaho)        * <code>41ddcd0</code>: Update seasonality tests and documentation for improved readability</li> <li>Modify <code>doctest_optionflags</code> in <code>pyproject.toml</code> to include \"ELLIPSIS\"</li> <li>Update expected output in <code>qs()</code> function examples for clarity</li> <li>Shorten numerical outputs in <code>qs()</code>, <code>seasonal_strength()</code>, <code>trend_strength()</code>, and <code>spikiness()</code> functions</li> <li>Adjust expected output in <code>seasonality()</code> test cases for consistency (by chrimaho)        * <code>0138dd4</code>: Enhance equation formatting in <code>normality</code> algorithms</li> <li>Improve readability of equations in the Jarque-Bera, D'Agostino's K\u00b2, Shapiro-Wilk, and Anderson-Darling tests.</li> <li>Update equations to use consistent formatting with <code>$$</code> for better display. (by chrimaho)        * <code>4219a53</code>: Update <code>pytest</code> command in <code>check_doctest()</code> functions to include doctest options.</li> <li>Add <code>--doctest-modules</code> and <code>--doctest-continue-on-failure</code> flags to enhance doctest execution. (by chrimaho)        * <code>5e776f6</code>: Fix type checking for <code>load_macrodata()</code> function</li> <li>Introduce <code>TypeCheckError</code> for more specific error handling.</li> <li>Update error assertions to check for expected messages.</li> <li>Enhance test coverage for type errors in data loading. (by chrimaho)        * <code>a8a4ec3</code>: Add <code>print_label</code> functionality for improved checking output readability</li> <li>Introduce <code>print_label()</code> function to format command output labels.</li> <li>Enhance linting functions with descriptive labels for better user feedback.</li> <li>Update command usage messages for clarity and consistency. (by chrimaho)        * <code>382bafe</code>: Add tests for <code>is_correlated()</code> function with various algorithms</li> <li>Implement tests for Ljung-Box, LM, and Breusch-Godfrey algorithms.</li> <li>Validate results and types for correlation checks.</li> <li>Ensure proper error handling for unsupported algorithms.</li> <li>Test correlation logic for both correlated and non-correlated data.</li> <li>Refactor failure assertions in <code>assert_almost_equal()</code> method tests. (by chrimaho)        * <code>a1fa2eb</code>: Refactor error message assertions in <code>test_generate_error_message()</code> method.</li> <li>Update assertions to check for specific error message components.</li> <li>Ensure clarity in error reporting for invalid parameters. (by chrimaho)        * <code>4fcee28</code>: Refactor error message formatting in <code>assert_almost_equal()</code> function.</li> <li>Update error message to use clearer variable names.</li> <li>Change <code>tol: p={places}, d={delta}</code> to <code>({places=}, {delta=})</code> for consistency. (by chrimaho)        * <code>7ade057</code>: Add <code>ELLIPSIS</code> option to doctest flags</li> <li>Include <code># \"ELLIPSIS\"</code> in the <code>doctest_optionflags</code> section of <code>pyproject.toml</code></li> <li>Enhance flexibility for doctest output matching (by chrimaho)        * <code>0a86247</code>: Improve error message for invalid algorithm parameter in <code>stationarity()</code> function.</li> <li>Clarify the options available for the <code>algorithm</code> parameter in the error message.</li> <li>Enhance user experience by providing detailed feedback on valid options. (by chrimaho)        * <code>47a14a5</code>: Update example outputs in seasonality tests</li> <li>Update expected output for <code>qs()</code> function to reflect precise values: <code>qs(data, freq=12)</code> now returns <code>(194.46928920877465, 5.9092232580147965e-43)</code></li> <li>Update expected output for <code>seasonality()</code> function using seasonal strength: <code>seasonality(x=data, algorithm=\"ss\", m=12)</code> now returns <code>0.7787219427520644</code> (by chrimaho)        * <code>3fc4004</code>: Fix typo (by chrimaho)        * <code>e3d5993</code>: Add parameter and return type documentation for <code>stationarity()</code> function</li> <li>Document parameters and return types for clarity.</li> <li>Include examples for better understanding of usage. (by chrimaho)        * <code>8cfe9df</code>: Add parameter and return type documentation for <code>seasonality()</code> function</li> <li>Document parameters <code>func</code> and <code>args</code> with types.</li> <li>Specify return type as a union of possible result types.</li> <li>Include example usage in the docstring. (by chrimaho)        * <code>8a4cd9d</code>: Add example to <code>AndersonResult</code> protocol documentation</li> <li>Include an example of how to use the <code>AndersonResult</code> protocol.</li> <li>Enhance clarity for users implementing the protocol.</li> <li>Provide a code snippet demonstrating usage: <code>``python&lt;br&gt; def my_func(res: AndersonResult) -&gt; None:&lt;br&gt; print(res.statistic) (by [chrimaho](https://github.com/chrimaho))        * [</code>73fa21d<code>](https://github.com/data-science-extensions/ts-stat-tests/commit/73fa21d3b0454de4955239ba2cc16244340c9f1e): Refactor</code>is_correlated()` function to enhance correlation testing</li> <li>Update function signature to accept parameters for correlation algorithm and significance level.</li> <li>Implement logic for various correlation tests including Ljung-Box, LM, and Breusch-Godfrey.</li> <li>Raise <code>ValueError</code> for unsupported algorithms.</li> <li>Add detailed docstring with examples and parameter descriptions. (by chrimaho)        * <code>e52debc</code>: Enhance docs structure for all <code>correlation</code> functions</li> <li>Add summary and details for the <code>acf()</code>, <code>pacf()</code>, <code>ccf()</code>, and <code>lb()</code> functions.</li> <li>Update references formatting for clarity.</li> <li>Improve readability by adding line breaks in the documentation. (by chrimaho)        * <code>5a183f7</code>: Refactor <code>load_macrodata()</code> function to improve data loading</li> <li>Update data types for <code>year</code> and <code>quarter</code> columns to <code>int</code>.</li> <li>Simplify data loading process using <code>pd.read_csv()</code>.</li> <li>Adjust index creation to use a combined string format for quarterly periods. (by chrimaho)        * <code>a740fd6</code>: Update doctest examples in data utilities</li> <li>Move the <code>data = ...</code> section from the <code>Example</code> chunk to the <code>Setup</code> chunk for each function in the <code>utils/data</code> module (by chrimaho)        * <code>bfd3af1</code>: Add doctest config to <code>pyproject.toml</code></li> <li>Include <code>--doctest-modules</code> and <code>--doctest-continue-on-failure</code> options in <code>pytest</code> configuration.</li> <li>Update test command in <code>check_doctest()</code> and <code>check_doctest_module()</code> functions to use <code>--config-file=pyproject.toml</code>.</li> <li>Standardise data type outputs in docstrings from <code>np.float64</code> to <code>float64</code> for consistency. (by chrimaho)        * <code>a5bb4e8</code>: Update error message formatting in <code>assert_almost_equal()</code> function</li> <li>Modify the traceback output for clarity in error reporting.</li> <li>Ensure the assertion error message is displayed correctly. (by chrimaho)        * <code>2a3ec92</code>: Add  <code>__init__.py</code> module to the <code>utils</code> namespace (by chrimaho)        * <code>d03c7f0</code>: Refactor data utility documentation for clarity and consistency.</li> <li>Standardise docstring formatting across functions.</li> <li>Add detailed examples and references for better understanding.</li> <li>Introduce type checking for improved type safety.</li> <li>Enhance descriptions to clarify functionality and usage. (by chrimaho)        * <code>0754d8b</code>: Refactor error utility documentation for clarity and consistency.</li> <li>Update docstrings to provide clearer summaries and details.</li> <li>Enhance examples for better understanding of usage.</li> <li>Standardise formatting across all functions. (by chrimaho)        * <code>ab48087</code>: Add utils docs pages to MkDocs navigation</li> <li>Create <code>data.md</code> file for data utilities documentation</li> <li>Create <code>errors.md</code> file for error utilities documentation</li> <li>Update <code>mkdocs.yml</code> to include new documentation in navigation (by chrimaho)        * <code>0050943</code>: Modify 'calculation' of docstrings section to be optional</li> <li>Change 'required' attribute of 'calculation' from true to false</li> <li>Ensure flexibility in documentation requirements (by chrimaho)        * <code>26174f2</code>: Refactor table formatting for clarity in implementation progress</li> <li>Adjust column headers for better alignment</li> <li>Enhance readability of the implementation progress table (by chrimaho)        * <code>d4c3fcb</code>: Update README.md for improved link formatting and clarity</li> <li>Correct link formatting for Statistical Tests references</li> <li>Add direct links for [Normality], [Stationarity], and [Correlation] (by chrimaho)        * <code>d080631</code>: Add navigation link to Data Science Extensions page</li> <li>Include a link to Data Science Extensions in the navigation menu. (by chrimaho)        * <code>77204b4</code>: Add additional code features to MkDocs configuration</li> <li>Include <code>content.code.select</code> feature for enhanced code selection</li> <li>Include <code>content.code.copy</code> feature for improved code copying</li> <li>Ensure <code>content.code.annotate</code> feature is available for code annotations (by chrimaho)        * <code>1f29834</code>: Add MathJax integration for enhanced mathematical rendering</li> <li>Introduce <code>mathjax.js</code> for MathJax configuration</li> <li>Subscribe to document changes to clear cache and typeset</li> <li>Update <code>mkdocs.yml</code> to include MathJax scripts (by chrimaho)        * <code>f0c979c</code>: Add progress bar styles and integrate into MkDocs configuration</li> <li>Create <code>progress_bar.css</code> with styles for progress bars and labels</li> <li>Include progress bar styles in <code>mkdocs.yml</code> under <code>extra_css</code> (by chrimaho)        * <code>27b17bd</code>: Move CSS styles from <code>stylesheets</code> sub-dir to the <code>css</code> sub-dir</li> <li>Introduce styles for various admonition types including <code>pro-tip</code>, <code>deprecation</code>, <code>observation</code>, and <code>calculation</code></li> <li>Implement styles for code chunk filenames with appropriate icons</li> <li>Update <code>mkdocs.yml</code> to reference new CSS file paths (by chrimaho)        * <code>4df11e7</code>: Update README and Code docs pages</li> <li>Enhance <code>README</code> structure and badge visibility</li> <li>Update seasonality test results in the implementation progress table</li> <li>Correct overall test statistics for seasonality (by chrimaho)        * <code>9d43aa2</code>: Refactor pytest imports for clarity</li> <li>Remove unused <code>fixture</code> and <code>mark</code> imports from pytest</li> <li>Simplify import statements for better readability (by chrimaho)        * <code>155879f</code>: Refactor assertions and enhance seasonality tests</li> <li>Update <code>assert_almost_equal()</code> calls to use <code>places</code> parameter for precision.</li> <li>Add detailed test cases for <code>seasonality()</code> and <code>is_seasonal()</code> functions.</li> <li>Include summaries for <code>test_seasonality()</code> and <code>test_is_seasonal()</code> methods.</li> <li>Improve test coverage for seasonal algorithms: QS, OCSB, CH, seasonal strength, trend strength, and spikiness. (by chrimaho)        * <code>c6de989</code>: Update seasonality documentation for clarity and accuracy</li> <li>Correct quote reference to Rob Hyndman and George Athanasopoulos.</li> <li>Refine definitions of trend, seasonal, and cyclic patterns for better understanding.</li> <li>Enhance information on source libraries and modules used in the implementation.</li> <li>Improve overall structure and readability of the documentation. (by chrimaho)        * <code>5631240</code>: Fill out the <code>seasonality()</code> and <code>is_seasonal()</code> functions</li> <li>Introduce <code>seasonality()</code> function as a dispatcher for various seasonality algorithms.</li> <li>Enhance <code>is_seasonal()</code> function to provide a standardized output.</li> <li>Update docstrings for better understanding and usage examples.</li> <li>Implement type checking for function parameters. (by chrimaho)        * <code>c6a8d15</code>: Enhance documentation requirements for sections</li> <li>Require 'params', 'examples', and 'calculation' sections to be mandatory.</li> <li>Add 'warning' section with optional requirement. (by chrimaho)        * <code>a289db6</code>: Refactor seasonality documentation for clarity and consistency</li> <li>Improve descriptions and examples for the <code>qs()</code>, <code>ocsb()</code>, <code>ch()</code>, <code>seasonal_strength()</code>, and <code>spikiness()</code> functions.</li> <li>Standardise parameter descriptions and default values.</li> <li>Enhance mathematical equations and their explanations.</li> <li>Update example usage for better clarity and accuracy. (by chrimaho)        * <code>c6afda0</code>: Refactor seasonality documentation paths for consistency</li> <li>Update paths in the seasonality tests section to remove the <code>src.</code> prefix</li> <li>Adjust paths in the seasonality algorithms section for uniformity (by chrimaho)        * <code>b716147</code>: Refactor spikiness function to handle NaN values</li> <li>Replace <code>np.mean()</code> with <code>np.nanmean()</code> for robust calculation of mean</li> <li>Ensure proper handling of missing values in seasonal decomposition (by chrimaho)        * <code>739cc65</code>: Refactor type hints and input handling for seasonality functions</li> <li>Change input type from <code>np.ndarray</code> to <code>ArrayLike</code> for improved flexibility.</li> <li>Update input conversion to <code>np.asarray(x, dtype=float)</code> for consistent handling of input data. (by chrimaho)        * <code>91dfc47</code>: Specify exceptions in ARIMA model fitting error handling</li> <li>Replace generic exception handling with specific exceptions: <code>ValueError</code>, <code>RuntimeError</code>, and <code>IndexError</code></li> <li>Improve error handling clarity and robustness in the <code>qs()</code> function (by chrimaho)        * <code>4f6a078</code>: Refactor type hints across unit tests for improved clarity and consistency</li> <li>Update type hints in <code>setup.py</code> to use <code>object</code> instead of <code>Any</code></li> <li>Modify return type of <code>data_dict()</code> function for better type specificity</li> <li>Ensure consistency in type annotations across various functions (by chrimaho)        * <code>c0f5e6f</code>: Add comprehensive documentation pages for seasonality analysis</li> <li>Introduce an overview of seasonality in time series datasets</li> <li>Define key concepts such as trend, seasonal, and cyclic patterns</li> <li>Provide references for further reading and source libraries</li> <li>Include links to relevant source modules for clarity (by chrimaho)        * <code>6c9e2e2</code>: Add additional <code>pylint</code> disable rules for improved code quality</li> <li>Include \"R0903\" for <code>too-few-public-methods</code></li> <li>Add \"R0912\" for <code>too-many-branches</code></li> <li>Add \"R1719\" for <code>simplifiable-if-expression</code> (by chrimaho)        * <code>cb9529f</code>: Implement unit tests for seasonality algorithms</li> <li>Add <code>TestSeasonality</code> class to test various seasonality functions</li> <li>Include tests for <code>qs()</code>, <code>ocsb()</code>, <code>ch()</code>, <code>seasonal_strength()</code>, <code>trend_strength()</code>, and <code>spikiness()</code></li> <li>Validate expected outputs and handle failure cases for <code>qs()</code></li> <li>Ensure proper setup with <code>setUpClass()</code> method (by chrimaho)        * <code>5d3b1a1</code>: Refactor stationarity module header documentation</li> <li>Remove unnecessary comments and clean up the docstring.</li> <li>Enhance the clarity of the purpose and usage of stationarity tests.</li> <li>Include a relevant article link for further reading on ADF &amp; KPSS tests. (by chrimaho)        * <code>9bbc9cb</code>: Enhance docs for <code>seasonality</code> module</li> <li>Introduce the <code>qs()</code> function for the Ljung-Box test to detect seasonality in time series data.</li> <li>Implement the <code>ocsb()</code> function for the Osborn, Chui, Smith, and Birchenhall test for seasonal differencing.</li> <li>Add the <code>ch()</code> function for the Canova-Hansen test to assess seasonal stability.</li> <li>Include the <code>seasonal_strength()</code> function to measure the strength of seasonality in time series data.</li> <li>Implement the <code>trend_strength()</code> function to evaluate the strength of the trend component.</li> <li>Add the <code>spikiness()</code> function to detect spikes or volatility in time series data.</li> <li>Enhance docstrings with detailed descriptions, parameters, return values, and examples for each function. (by chrimaho)        * <code>cde3717</code>: Add detailed headers and descriptions for seasonality modules</li> <li>Include a summary of seasonality tests in the <code>seasonality.py</code> module.</li> <li>Document the purpose and functionality of each implemented algorithm.</li> <li>Enhance clarity for users regarding the statistical tests available. (by chrimaho)        * <code>ebfeac7</code>: Add <code>AndersonResult</code> protocol and improve type casting</li> <li>Introduce <code>AndersonResult</code> protocol for better type safety.</li> <li>Enhance type casting in <code>ad()</code> function to use <code>cast(AndersonResult, ...)</code>.</li> <li>Import <code>Protocol</code> from <code>typing</code> and <code>numpy</code> for type annotations. (by chrimaho)        * <code>341686b</code>: Implement seasonality algorithms and tests</li> <li>Introduce algorithms for seasonality analysis including <code>qs()</code>, <code>ocsb()</code>, <code>ch()</code>, <code>seasonal_strength()</code>, <code>trend_strength()</code>, and <code>spikiness()</code>.</li> <li>Add placeholder functions <code>seasonality()</code> and <code>is_seasonal()</code> for future implementation.</li> <li>Include necessary imports and exports for the new functionality. (by chrimaho)        * <code>ee30457</code>: Add seasonality modules and related tests &amp; docs</li> <li>Introduce <code>seasonality.py</code> for seasonality analysis</li> <li>Create <code>test_seasonality.py</code> for unit testing seasonality features</li> <li>Update <code>mkdocs.yml</code> to include seasonality documentation</li> <li>Add <code>seasonality.md</code> documentation file (by chrimaho)</li> </ul> </li> </ul>"},{"location":"usage/changelog/#overview","title":"\ud83d\ude80 Overview","text":"<p>Introduce the <code>seasonality</code> module, the fourth major component of the <code>ts-stat-tests</code> library, while simultaneously achieving a significant architectural milestone: the total elimination of <code>typing.cast()</code> and the attainment of 100% global code coverage. This release transforms the library into a strictly typed, fully validated suite for time-series analysis. By integrating six new seasonality algorithms, standardising on <code>NDArray</code> type hints, and implementing MathJax for professional mathematical rendering, this version provides the most robust and accessible interface for statistical testing to date.</p>"},{"location":"usage/changelog/#implementation-details","title":"\ud83d\udee0\ufe0f Implementation details","text":""},{"location":"usage/changelog/#seasonality-module-implementation","title":"Seasonality module implementation","text":"<ul> <li>Implement six core seasonality algorithms in <code>src/ts_stat_tests/algorithms/seasonality.py</code>:<ul> <li><code>qs()</code>: The Ljung-Box test for seasonality.</li> <li><code>ocsb()</code>: The Osborn-Chui-Smith-Birchenhall test for seasonal differencing.</li> <li><code>ch()</code>: The Canova-Hansen test for seasonal stability.</li> <li><code>seasonal_strength()</code> and <code>trend_strength()</code>: Measures based on STL decomposition.</li> <li><code>spikiness()</code>: A measure of variance in the remainder component.</li> </ul> </li> <li>Provide a unified dispatcher in <code>src/ts_stat_tests/tests/seasonality.py</code>:<ul> <li><code>seasonality()</code>: Central entry point for all seasonality-related measurements.</li> <li><code>is_seasonal()</code>: Standardised Boolean checker returning result dictionaries with interpreted statistics and alpha thresholds.</li> </ul> </li> </ul>"},{"location":"usage/changelog/#type-safety-and-architectural-hardening","title":"Type safety and architectural hardening","text":"<ul> <li>Remove all usages of <code>typing.cast()</code> throughout the codebase, replacing them with safer type-narrowing patterns and intermediate <code>Any</code> assignments to satisfy strict <code>pyright</code> checks without sacrificing runtime safety.</li> <li>Standardise internal data structures to use <code>NDArray[np.float64]</code> for improved type specificity and interoperability with <code>numpy</code> ecosystem.</li> <li>Refactor normality and stationarity modules to use protocol-based result handling for third-party library outputs (e.g., <code>scipy</code> and <code>statsmodels</code>).</li> <li>Implement output rounding across all statistical tests to ensure consistent return values and stable documentation examples.</li> </ul>"},{"location":"usage/changelog/#documentation-and-visual-enhancements","title":"Documentation and visual enhancements","text":"<ul> <li>Integrate <code>MathJax</code> into the <code>mkdocs</code> pipeline to enable high-quality rendering of mathematical equations in the documentation site.</li> <li>Introduce custom CSS-based implementation progress bars to the documentation landing pages.</li> <li>Standardise all docstrings to follow the strict Google-style format with full <code>dfc</code> (Docstring Format Checker) compliance.</li> <li>Update <code>README.md</code> and <code>docs/index.md</code> status tables to reflect the completed seasonality module.</li> </ul>"},{"location":"usage/changelog/#checklist","title":"\u2705 Checklist","text":"<ul> <li> Implement core seasonality algorithms and unified dispatcher.</li> <li> Achieve 100% code coverage across the entire <code>ts_stat_tests</code> package.</li> <li> Remove all <code>typing.cast()</code> calls to satisfy strict Pylint 10/10 and Pyright requirements.</li> <li> Integrate MathJax for KaTeX math rendering in the documentation.</li> <li> Standardise all return types and implement numerical rounding for consistency.</li> <li> Update data utility functions with improved type safety and expanded synthetic generators.</li> </ul>"},{"location":"usage/changelog/#changes","title":"\ud83d\udcca Changes","text":"Metric Value Files Changed 34 Lines Added 2645 Lines Deleted 532 Commit Count 86 Contributors Chris Mahoney"},{"location":"usage/changelog/#pull-requests","title":"\ud83d\udcaa Pull requests","text":""},{"location":"usage/changelog/#28-implement-seasonality-module-includes-module-implementation-and-global-type-safety-hardening","title":"28: Implement seasonality module (Includes module implementation and global type safety hardening).","text":""},{"location":"usage/changelog/#new-contributors","title":"\ud83c\udd95 New contributors","text":"<ul> <li>This release was developed by the core maintenance team.</li> </ul>"},{"location":"usage/changelog/#v045-add-stationarity-module","title":"v0.4.5 - Add Stationarity Module","text":"<p><code>v0.4.5</code> <code>2026-01-13</code> data-science-extensions/ts-stat-tests/releases/v0.4.5</p> Release Notes Updates <ul> <li><code>ef5d67a</code>: Fix docstring for the <code>get_normal_curce()</code> function     (by chrimaho)        * <code>ee52e6d</code>: Fix <code>sys.argv</code> length in the <code>check_doctest_cli()</code> function     The function accesses <code>sys.argv[2]</code> but the length check only ensures at least 2 elements exist (indices 0 and 1). This will cause an IndexError when exactly 2 arguments are provided. The check should be <code>len(sys.argv) &lt; 3</code> or the access should be <code>sys.argv[1]</code>.     (by chrimaho)        * <code>ba4df2c</code>: Add <code>test_errors</code> unit tests     (by chrimaho)        * <code>9d3ca5c</code>: Fix unit tests     (by chrimaho)        * <code>df27be0</code>: Update unit tests     (by chrimaho)        * <code>5610b08</code>: Add <code>check_doctest()</code> function to list of checks     (by chrimaho)        * <code>667ac56</code>: Standardise and modernise docstring examples for all algorithms and tests<ul> <li>Replace legacy or inconsistent data preparation in docstring examples with unified imports from <code>ts_stat_tests.utils.data</code></li> <li>Use consistent variable names such as <code>normal</code>, <code>airline</code>, <code>random</code>, <code>trend</code>, <code>noise</code>, and <code>seasonal</code> to improve clarity in all code examples</li> <li>Remove redundant or outdated example blocks, and add new, focused examples for every major algorithm to better illustrate expected output</li> <li>Update expected output values in example code to reflect current results from the associated algorithms</li> <li>Ensure all examples use the proper <code>pycon</code> code block style and follow a logical, repeatable structure for easier maintenance</li> <li>Improve exception and error reporting in usage notes, including explicit <code>ValueError</code> tracebacks for invalid algorithm parameters</li> <li>Align with Australian English spelling and imperative mood in documentation (by chrimaho)        * <code>95b107b</code>: Expand doc filters to include algorithm-specific patterns</li> <li>Improve documentation clarity by adding new filter patterns for algorithm-specific members such as <code>^correlation</code>, <code>^entropy</code>, <code>^regularity</code>, and <code>^stationarity</code></li> <li>Standardise filtering approach to ensure relevant functions and methods are included in docs for each algorithm section</li> <li>Reduce noise and improve discoverability for users browsing documentation (by chrimaho)        * <code>3ce6ff6</code>: Standardise test docstring examples and update expected outputs</li> <li>Replace <code>.values</code> usage with direct assignment for test data examples to align with actual return types and avoid confusion</li> <li>Update expected numeric outputs in code examples for accuracy and consistency with current algorithm behaviour</li> <li>Clarify example titles and comments for improved readability and instructional value</li> <li>Ensure test documentation reflects standard dataset and normality testing patterns (by chrimaho)        * <code>ee2eea8</code>: Fix bug in README links (by chrimaho)        * <code>ae14c7f</code>: Fix typos (by chrimaho)        * <code>bf06810</code>: Remove redundant test documentation sections from API docs</li> <li>Streamline documentation by removing duplicate <code>ts_stat_tests.tests</code> sections</li> <li>Focus on algorithm documentation for clarity and consistency</li> <li>Improve maintainability by reducing unnecessary content (by chrimaho)        * <code>5e9fe9f</code>: Add doctest checking functions for targeted modules</li> <li>Introduce <code>check_doctest()</code> function to enable running doctests on specific files containing <code>ts_stat_tests</code></li> <li>Add <code>check_doctest_module()</code> function to allow module-specific doctest execution</li> <li>Implement <code>check_doctest_cli()</code> function for command-line doctest invocation using arguments</li> <li>Comment out doctest check in <code>check()</code> function to prevent default execution until ready</li> <li>Facilitate easier validation of documentation examples and improve test coverage for doctests (by chrimaho)        * <code>a02994a</code>: Clarify and standardise normality test return values</li> <li>Update the return type documentation to clarify that the Anderson-Darling algorithm yields a <code>tuple</code> of <code>(stat, critical_values, significance_level)</code> instead of a library-specific object</li> <li>Unpack the result from <code>_jb()</code> and return a <code>tuple</code> of test statistic and p-value for consistency with other algorithms</li> <li>Improve code clarity and maintainability by standardising interface and documentation (by chrimaho)        * <code>84e144e</code>: Standardise and simplify normality test docstring examples</li> <li>Replace lengthy, varied examples with concise, uniform usage across all normality test function docstrings</li> <li>Use standardised datasets (<code>data_airline</code>, <code>data_noise</code>, <code>data_normal</code>, <code>data_uniform</code>) to ensure examples are reproducible and consistent</li> <li>Remove unrelated or overly detailed explanations, focusing on clarity and direct usage of test interfaces</li> <li>Improve clarity for end users by providing setup code blocks and clear example output</li> <li>Reduce maintenance burden by avoiding external library dependencies (e.g. remove references to <code>sktime</code>, <code>numpy</code> sin data) (by chrimaho)        * <code>243adad</code>: Add usage examples for <code>correlation()</code> function</li> <li>Clarify usage by including practical examples</li> <li>Demonstrate different algorithms such as \"acf\" and \"lb\"</li> <li>Help users understand input requirements and expected outputs (by chrimaho)        * <code>5f8f675</code>: Standardise usage of example data in docstrings</li> <li>Replace legacy data loading functions with direct data imports in example code for consistency</li> <li>Update example code to use <code>data_airline</code> and <code>data_macrodata</code> variables rather than call <code>load_airline()</code> or <code>load_macrodata()</code></li> <li>Align example usage with updated data utilities to improve clarity and reproducibility</li> <li>Use <code>from statsmodels import api as sm</code> import style to match internal standards (by chrimaho)        * <code>6a3cf35</code>: Add normal, trend, and uniform data generators</li> <li>Expand data utilities to include <code>get_normal_curve()</code>, <code>get_trend_data()</code>, and <code>get_uniform_data()</code> functions for generating additional synthetic datasets</li> <li>Expose new sample arrays such as <code>data_normal</code> and <code>data_trend</code> for easier access in tests and examples</li> <li>Support reproducibility by seeding random generators for new functions</li> <li>Enhance flexibility for statistical testing and demonstration scenarios (by chrimaho)        * <code>57ed6df</code>: Standardise correlation algorithm documentation and references</li> <li>Update algorithm table to improve clarity and consistency, including renaming 'Breusch-Godfrey LM Test' and adding 'Lagrange Multiplier Test'</li> <li>Remove redundant library column entries for algorithms that are all from <code>statsmodels</code></li> <li>Replace external reference with direct link to official <code>statsmodels</code> documentation for improved relevance</li> <li>Correct grammar and style for improved readability, including module names and package references</li> <li>Add documentation configuration block for <code>ts_stat_tests.tests.correlation</code> to align with existing structure</li> <li>Apply consistent filtering for member listing by excluding methods starting with <code>is</code> and <code>_</code> (by chrimaho)        * <code>8ae9069</code>: Standardise return types for normality test functions</li> <li>Ensure all normality test functions consistently return tuples of floats or lists, improving type clarity and downstream compatibility</li> <li>Refine type casting and result extraction in tests to align with updated function outputs</li> <li>Enhance robustness of result handling in both normality and stationarity test utilities (by chrimaho)        * <code>215a2af</code>: Standardise normality test function return types</li> <li>Ensure all normality test functions return plain tuples for consistency and predictability</li> <li>Remove use of union types and result classes from function signatures to simplify API and reduce ambiguity</li> <li>Facilitate easier downstream usage and type checking by unifying return types across statistical test functions (by chrimaho)        * <code>cbeac37</code>: Standardise and clarify documentation for statistical tests</li> <li>Clarify and condense docstrings for normality and stationarity tests, removing repetitive explanations and focusing on essential details</li> <li>Standardise notation in mathematical equations, consistently using LaTeX formatting and correct mathematical symbols (e.g., <code>\\Delta</code>, <code>\\epsilon_t</code>)</li> <li>Enhance explanations of test statistics, including concise mathematical definitions and parameter descriptions</li> <li>Add, correct, and clarify credit and reference sections for external libraries and sources, ensuring proper attribution</li> <li>Improve consistency in section headers and callout formatting (e.g., <code>??? success \"Credit\"</code>, <code>??? tip \"See Also\"</code>)</li> <li>Add an abstract and usage details to the correlation test dispatcher, listing supported algorithms</li> <li>Improve clarity and accuracy of parameter descriptions and summary sections</li> <li>Update docstring language to Australian English (e.g., 'standardise', 'summarise'), and use imperative mood throughout (by chrimaho)        * <code>8e78a81</code>: Standardise exception docstrings for validation errors</li> <li>Replace inconsistent exception documentation with uniform <code>Raises:</code> sections detailing <code>(ValueError)</code> for invalid input across normality and regularity algorithms</li> <li>Remove redundant <code>Raises:</code> notes specifying no exceptions raised, clarifying documentation</li> <li>Improve clarity and maintainability of API reference for users and contributors (by chrimaho)        * <code>9bab801</code>: Standardise and clarify docstrings for return types</li> <li>Update and clarify return type annotations and descriptions in docstrings for statistical test functions</li> <li>Remove outdated or ambiguous type hints, ensuring consistency and improved readability</li> <li>Adopt consistent dictionary and tuple return type formatting using Python type hinting</li> <li>Enhance explanations for result dictionaries, clearly listing expected fields and value types</li> <li>Improve maintainability and reduce confusion for users and contributors by standardising documentation (by chrimaho)        * <code>eee99be</code>: Standardise parameter documentation formatting</li> <li>Align docstrings for parameter default values and descriptions across functions</li> <li>Remove inconsistent HTML tags and unify wording to improve readability</li> <li>Clarify default values using <code>Default: value</code> for easier reference</li> <li>Improve consistency for users consuming API documentation (by chrimaho)        * <code>c298449</code>: Standardise and expand statistical test docstring examples</li> <li>Align all docstring code examples to a clear, consistent format using stepwise \"Setup\" and labelled example sections</li> <li>Replace outdated or confusing usage with current, project-specific data loader and methods</li> <li>Expand coverage for key statistical tests to demonstrate more parameter combinations and result formats</li> <li>Replace <code>pprint</code> with <code>print</code> for clarity, and showcase array slicing for readable outputs</li> <li>Add new examples for confidence intervals, advanced options, and more realistic outputs for <code>acf()</code>, <code>pacf()</code>, <code>ccf()</code>, <code>lb()</code>, <code>lm()</code>, and <code>bglm()</code> functions</li> <li>Improve readability for end users and facilitate easier learning by showing expected results and recommended invocation patterns (by chrimaho)        * <code>01926a1</code>: Refactor test data setup to use shared utility functions</li> <li>Standardise test data access by importing shared utility functions</li> <li>Remove duplicate data generation logic from test setup file</li> <li>Simplify test base class to use direct references to shared data</li> <li>Improve maintainability and consistency of test data across the project (by chrimaho)        * <code>bc95f0f</code>: Add utility functions and datasets for synthetic and real data</li> <li>Enable generation of random, sine wave, straight line, and fractional Gaussian noise datasets via dedicated utility functions for reproducible testing and experimentation</li> <li>Introduce caching for data generators using <code>@lru_cache</code> to improve performance and ensure consistent outputs</li> <li>Add direct access to classic real-world datasets by loading airline passenger and macroeconomic data as pandas objects</li> <li>Standardise data access patterns for downstream code and unit tests by exposing curated data objects</li> <li>Improve code clarity by listing public exports and grouping related logic (by chrimaho)        * <code>bafa581</code>: Standardise return value documentation for functions in the <code>correlation</code> module</li> <li>Align function docstrings to consistently document return types and conditional outputs</li> <li>Improve clarity on optional outputs based on input parameters for core statistical test functions</li> <li>Enhance maintainability and user comprehension of function behaviour (by chrimaho)        * <code>6b19d4b</code>: Standardise docstring formatting and improve references</li> <li>Replace <code>???</code> admonition syntax with <code>!!!</code> for credits and adjust headings to follow a consistent style</li> <li>Add hyperlinks to referenced resources for clarity and ease of access</li> <li>Update reference block styles to better distinguish between credits and citations</li> <li>Clarify return type documentation by removing redundant text and consolidating explanations (by chrimaho)        * <code>fa557b8</code>: Refactor and standardise entropy docstrings and return types</li> <li>Refactor and condense docstrings for all entropy-related functions to improve clarity, consistency, and maintainability</li> <li>Standardise parameter descriptions and sections, ensuring consistent formatting and mathematical notation using LaTeX</li> <li>Add or update examples and notes for better user guidance, using realistic data and reproducible snippets</li> <li>Remove redundant or overly verbose explanations and outdated links, and clarify equations in dedicated sections</li> <li>Standardise all entropy function return types to explicitly cast outputs to <code>float</code> for consistency</li> <li>Update references to external libraries and methods, and ensure proper credit is given to the original sources</li> <li>Adopt imperative mood and Australian English spelling throughout documentation (by chrimaho)        * <code>f913bb3</code>: Standardise docstring param and return sections</li> <li>Replace non-standard <code>???+ itemized</code> markers with standard <code>Params:</code> and <code>Returns:</code> sections in function docstrings</li> <li>Improve clarity and consistency of documentation to align with typical Python docstring conventions</li> <li>Facilitate better integration with documentation tools and enhance readability for maintainers (by chrimaho)        * <code>1cc3d1e</code>: Refactor and clarify statistical docstrings for correlation tests</li> <li>Reorganise and clarify docstring explanations for autocorrelation, partial autocorrelation, cross-correlation, Ljung-Box, Lagrange Multiplier, and Breusch-Godfrey tests</li> <li>Move mathematical definitions and formulas into dedicated <code>equation</code> dropdowns for better readability and accessibility</li> <li>Standardise notation (e.g. use <code>$nlags$</code>, <code>$n_{obs}$</code>, and proper LaTeX formatting) for consistency across documentation</li> <li>Replace raw Markdown examples with <code>example</code> dropdowns and improve usage clarity</li> <li>Fix minor typos and wording for Australian English, e.g. 'standardise', 'summarise', and ensure use of imperative mood in documentation</li> <li>Enhance explanation of test parameters, especially default values and deprecation warnings, for improved user guidance</li> <li>Remove redundant or duplicated information and modernise docstring structure with collapsible sections for details, examples, and references</li> <li>Credit the <code>statsmodels</code> library more consistently and visibly in each section (by chrimaho)        * <code>313e185</code>: Expand and standardise stationarity test coverage</li> <li>Add tests for invalid algorithm handling in <code>stationarity()</code> function to ensure ValueError is raised as expected</li> <li>Standardise test results from statistical libraries by converting outputs to native Python types for consistency and easier assertions</li> <li>Improve branch coverage in <code>is_stationary()</code> function, including logic for boolean and non-numeric p-value handling</li> <li>Explicitly cover all algorithm branches in stationarity dispatcher for comprehensive test coverage</li> <li>Replace <code>pmdarima</code>'s <code>PPTest().should_diff()</code> usage with direct <code>arch.unitroot.PhillipsPerron()</code> calls for result consistency across libraries</li> <li>Enhance readability and maintainability by grouping related imports and adding whitespace for class separation (by chrimaho)        * <code>35aa684</code>: Increase test coverage for normality checks</li> <li>Add tests to cover branch scenarios in <code>is_normal()</code> function where result has <code>statistic</code> and <code>pvalue</code> attributes, or is a simple float</li> <li>Use <code>unittest.mock.patch()</code> to simulate different return types from <code>normality()</code> function</li> <li>Ensure more robust handling and validation of diverse outputs for improved reliability (by chrimaho)        * <code>c0ecca4</code>: Add summary tables of key statistical tests to docs</li> <li>Improve discoverability by listing major correlation, normality, and regularity tests in markdown tables</li> <li>Standardise documentation with clear algorithm names, abbreviations, import scripts, and links for each method</li> <li>Aid users in identifying and accessing relevant statistical tools quickly for time series analysis (by chrimaho)        * <code>fe7c283</code>: Standardise documentation references and configuration keys</li> <li>Remove redundant <code>src.</code> prefix from module references for consistency across documentation</li> <li>Update configuration blocks to use <code>extra:</code> root key, aligning with documentation standards</li> <li>Improve clarity and maintainability for future documentation updates (by chrimaho)        * <code>dcdbd26</code>: Add missing <code>numpy</code> import to the <code>stationarity</code> module (by chrimaho)        * <code>c3093aa</code>: Refactor stationarity result handling and typing</li> <li>Improve clarity and robustness of the <code>is_stationary()</code> function by explicitly casting result types and handling tuple returns safely</li> <li>Standardise type checking and assignment for statistics and p-values, supporting multiple possible types</li> <li>Cast return dictionary to expected types, ensuring outputs are consistently formatted and type-safe</li> <li>Enhance maintainability and reduce ambiguity in interpreting <code>stationarity()</code> function results (by chrimaho)        * <code>1b27b60</code>: Introduce internal helper to standardise function calls</li> <li>Introduce <code>_call()</code> function to centralise keyword argument handling when invoking test functions</li> <li>Improve maintainability and reduce code duplication by standardising how stationarity test functions are called</li> <li>Lay groundwork for future changes that may require consistent invocation logic (by chrimaho)        * <code>e26a2e7</code>: Expand supported types for algorithm keyword arguments</li> <li>Allow <code>stationarity()</code> function and <code>is_stationary()</code> function to accept <code>ArrayLike</code> as a valid type for keyword arguments</li> <li>Enable passing complex argument types directly to underlying algorithms, improving flexibility for users specifying parameters</li> <li>Standardise type annotations in docstrings to match updated function signatures (by chrimaho)        * <code>335aaa1</code>: Expand keyword argument types to support array-like inputs</li> <li>Allow pass array-like types as values for <code>**kwargs</code> in the <code>correlation()</code> function overloads</li> <li>Enable greater flexibility for algorithms requiring non-scalar keyword arguments</li> <li>Improve API consistency and support for advanced use cases (by chrimaho)        * <code>985bd06</code>: Add usage notes and credit to docstring for <code>rur()</code> function</li> <li>Clarify that p-values are interpolated and out-of-range statistics generate a warning</li> <li>Document that missing values are not handled</li> <li>acknowledge the <code>statsmodels</code> library as the basis for the implementation (by chrimaho)        * <code>546a1c9</code>: Fix typo (by chrimaho)        * <code>2df720e</code>: Standardise output types and add explicit casting in tests</li> <li>Ensure all test functions return standardised output formats by explicitly casting results</li> <li>Improve type safety and compatibility across statistical test functions</li> <li>Remove inline usage examples from docstrings to declutter documentation</li> <li>Adjust lag handling logic for the Phillips-Perron test to avoid invalid lag values</li> <li>Enhance maintainability and clarity for future extension or integration (by chrimaho)        * <code>d293fa1</code>: Relocate and standardise example blocks in docstrings</li> <li>Move all example blocks to consistent positions at the end of each function docstring for improved documentation clarity</li> <li>Use uniform markdown formatting for all example code, ensuring consistent presentation across tests including <code>kpss()</code>, <code>rur()</code>, <code>za()</code>, <code>pp()</code>, <code>ers()</code>, and <code>vr()</code></li> <li>Add missing \"See Also\" section to the Zivot-Andrews test to align with other stationarity tests and aid discoverability</li> <li>Improve documentation structure to aid user navigation and future maintenance (by chrimaho)        * <code>4fd8c98</code>: Add missing <code>Credit</code> section to the docstring for the <code>adf()</code> function (by chrimaho)        * <code>714073e</code>: Standardise type annotations and return values in stationarity algorithms</li> <li>Align type annotations for parameters and return types to be more explicit and consistent, using domain-specific type aliases where appropriate</li> <li>Update docstrings to accurately reflect parameter types and improve clarity on optional arguments</li> <li>Adjust return signatures of <code>kpss()</code>, <code>rur()</code>, <code>pp()</code>, <code>za()</code>, and <code>ers()</code> functions for consistency with value structure and expected outputs</li> <li>Clarify the use of <code>store</code> parameter and its effect on returned result tuples</li> <li>Reduce ambiguity and potential confusion for users and static analysis tools, improving maintainability and type safety throughout the statistical test API (by chrimaho)        * <code>fe5eae8</code>: Refactor and standardise docstrings for consistency</li> <li>Improve consistency and clarity in docstring formatting across all stationarity test functions</li> <li>Replace scattered \"Notes\", \"Credits\", and \"Examples\" sections with standardised locations and Markdown formatting</li> <li>Move or consolidate important notes into <code>??? note \"Notes\"</code> blocks for better visibility</li> <li>Relocate all credit attributions to the end of docstrings in a uniform <code>!!! success \"Credit\"</code> block</li> <li>Remove redundant or duplicate content and streamline explanations, focusing on essential mathematical and usage details</li> <li>Enhance maintainability by ensuring all test documentation sections follow a clear, predictable structure (by chrimaho)        * <code>e613d7c</code>: Standardise docstring admonitions for clarity</li> <li>Replace all occurrences of <code>!!! summary</code> with <code>!!! note</code> to improve consistency in documentation style</li> <li>Change <code>???+ info</code> to <code>???+ abstract</code> for detailed sections to better reflect their purpose</li> <li>Update <code>!!! example</code> to <code>???+ example</code> for improved formatting and alignment with documentation tools</li> <li>Amend <code>??? info</code> to <code>??? note</code> to standardise note sections across docstrings</li> <li>Improve readability and maintainability of docstring formatting for future contributors (by chrimaho)        * <code>9ec60ac</code>: Refactor normality result handling for robustness</li> <li>Improve robustness of result extraction by using <code>getattr()</code> and type casting to handle diverse result types from statistical tests</li> <li>Ensure consistent float conversion and type safety for <code>statistic</code> and <code>p_value</code> fields</li> <li>Enhance code clarity around Anderson-Darling test result parsing by separating tuple unpacking and array conversions</li> <li>Reduce risk of runtime errors from unexpected result formats, standardise output structure, and improve maintainability (by chrimaho)        * <code>c2fdd32</code>: Add calculation section to docstring configuration</li> <li>Introduce a <code>calculation</code> section with <code>equation</code> admonition and unique order in the docstring structure</li> <li>Standardise documentation by enabling explicit space for calculation or formula details</li> <li>Improve clarity by shifting the order of existing sections to accommodate <code>calculation</code> (by chrimaho)        * <code>afabea1</code>: Expand pylint <code>disable</code> list for improved code maintainability</li> <li>Add <code>R0911</code>, <code>R0914</code> to the pylint disable list for warnings related to too many return statements and too many local variables</li> <li>Support more flexible code patterns without triggering style violations</li> <li>Facilitate maintainability by reducing unnecessary linting noise and focusing on critical issues (by chrimaho)        * <code>27aca4e</code>: Fix various typos</li> <li>Standardise test naming and refactor variable names</li> <li>Replace inconsistent test method names to use <code>test_stationarity_za_*</code> format for clarity and consistency</li> <li>Update variable names from <code>indx</code> to <code>index</code> for readability and alignment with standard conventions</li> <li>Change import statement to use <code>tests.setup</code> for improved test structure organisation</li> <li>Improve code maintainability and ease future test extensions (by chrimaho)        * <code>1735768</code>: Standardise typing and signatures for keyword arguments</li> <li>Replace generic <code>Any</code> keyword argument types with explicit <code>Union[float, int, str, bool, None]</code> to improve static type checking and code clarity</li> <li>Update function return type annotations for accuracy, clarifying output types for consumers and future maintainers</li> <li>Remove unused <code>Any</code> imports to clean up namespace and avoid confusion</li> <li>Apply <code>cast(int, delay)</code> instead of <code>cast(Any, delay)</code> in <code>permutation_entropy()</code> function to enforce type correctness</li> <li>Use <code>object</code> instead of <code>Any</code> in <code>generate_error_message()</code> function parameters to further restrict accepted types</li> <li>Improve docstrings to accurately reflect updated parameter and return types, supporting clearer auto-generated documentation (by chrimaho)        * <code>f90d46b</code>: Standardise and update stationarity test documentation</li> <li>Update statistical test links to point to relevant documentation pages for improved navigation</li> <li>Standardise stationarity test implementation indicators from <code>\ud83d\udd32</code> to <code>\u2705</code> in the README to reflect completed Python support</li> <li>Revise progress table to show full implementation and unit-test coverage for stationarity tests</li> <li>Clarify test status and improve transparency of documentation for users (by chrimaho)        * <code>826dd68</code>: Update stationarity docs to clarify test coverage</li> <li>Standardise terminology and references for time series stationarity</li> <li>Reference authoritative external sources for definitions and further reading</li> <li>Refine library comparison table to focus on <code>statsmodels</code> and <code>arch</code> for numerical reliability</li> <li>Clarify rationale for selecting <code>statsmodels</code> and <code>arch</code> packages, highlighting their comprehensive suite of tests</li> <li>Update source module references to point to correct stationarity modules (by chrimaho)        * <code>49eb822</code>: Standardise hypothesis logic and update PP test implementation</li> <li>Clarify and align null hypothesis handling for stationarity tests, ensuring consistent interpretation of p-values across algorithms</li> <li>Replace <code>pmdarima</code> PP test implementation with <code>arch</code> library version, refactor <code>pp()</code> function signature and return values</li> <li>Improve documentation with explicit references, trend and test type options for PP test, and fix docstring examples</li> <li>Update VR, ERS, RUR test hypotheses in docstrings for accuracy</li> <li>Enhance stationarity determination logic to robustly support all test types Relates to improved consistency and accuracy in statistical testing APIs (by chrimaho)        * <code>17dd78a</code>: Add documentation for time-series stationarity tests</li> <li>Provide an overview of key stationarity and seasonality tests for time-series data, comparing major Python libraries and their relevant algorithms</li> <li>List supported statistical tests with import instructions and direct links to documentation</li> <li>Reference implementation modules and usage for both algorithmic and test layers</li> <li>Facilitate easier selection and understanding of statistical tools for assessing time-series suitability (by chrimaho)        * <code>5acad0c</code>: Add comprehensive unit tests for stationarity algorithms</li> <li>Expand test coverage for statistical stationarity tests by introducing new test classes and cases</li> <li>Standardise testing by comparing custom implementations to third-party libraries, verifying consistency of results</li> <li>Include tests for major time series stationarity checks such as ADF, KPSS, PP, ERS, ZA, RUR, and VR</li> <li>Use selective assertions and rounding to address minor numerical discrepancies between implementations</li> <li>Improve reliability of statistical test code and facilitate future refactoring or algorithm enhancements (by chrimaho)        * <code>0a1feb9</code>: add comprehensive docstrings for stationarity test functions</li> <li>improve clarity and usability by providing detailed docstrings for <code>stationarity()</code> and <code>is_stationary()</code> functions</li> <li>include algorithm options, parameter descriptions, return values, and example usage in the documentation</li> <li>guide users on the interpretation of test results and clarify statistical assumptions (by chrimaho)        * <code>a1da846</code>: add module header and summary docstring for clarity</li> <li>provide an overview section and description to improve documentation</li> <li>include a summary docstring for the module to explain its purpose</li> <li>enhance code readability and maintainability by standardising file headers (by chrimaho)        * <code>e497a9c</code>: Add stationarity test wrappers with unified interface</li> <li>Provide <code>stationarity()</code> function to standardise access to multiple stationarity tests via a single interface</li> <li>Implement <code>is_stationary()</code> function to streamline stationarity assessment using consistent logic across algorithms</li> <li>Simplify selection of test algorithms with flexible naming and error handling</li> <li>Improve code maintainability and readability by centralising algorithm dispatch and result interpretation (by chrimaho)        * <code>68fbec0</code>: Add comprehensive docstrings for all stationarity test functions</li> <li>Improve clarity and usability by introducing detailed docstrings for each statistical test function</li> <li>Include mathematical formulations, parameter descriptions, expected return values, and practical code examples</li> <li>Standardise documentation style with markdown formatting and KaTeX math for equations</li> <li>Credit original libraries and add references for further reading to facilitate user understanding</li> <li>Enhance discoverability with \"See Also\" sections linking related tests (by chrimaho)        * <code>1eba0d9</code>: Add module overview and detailed documentation for stationarity algorithms</li> <li>Introduce a comprehensive module header describing the purpose and scope of stationarity tests in time series analysis</li> <li>Add a detailed summary discussing the statistical rationale and importance of stationarity tests, including ADF, KPSS, PP, ERS, and VR</li> <li>Reference useful external resources for further reading to aid understanding</li> <li>Improve code readability and maintainability by clarifying context and intent for future contributors (by chrimaho)        * <code>40d087b</code>: Add stationarity test wrappers with type validation</li> <li>Provide standardised function interfaces for multiple stationarity tests</li> <li>Use type validation via <code>@typechecked</code> decorator for input safety</li> <li>Expose consistent argument options and overloads for flexible usage</li> <li>Centralise imports and constants to simplify maintenance and readability</li> <li>Improve usability by mirroring third-party library signatures (by chrimaho)        * <code>0c6c10b</code>: Add stationarity test scaffolding to codebase</li> <li>Prepare groundwork for future stationarity feature implementation</li> <li>Standardise documentation and navigation by including stationarity references</li> <li>Enable future unit testing via placeholder test files</li> <li>Facilitate expansion of statistical algorithms with initial stationarity structure (by chrimaho)</li> </ul> </li> </ul>"},{"location":"usage/changelog/#overview_1","title":"\ud83d\ude80 Overview","text":"<p>Introduce the <code>stationarity</code> module, completing the third major component of the statistical test suite. This release provides a unified interface for seven core unit-root and stationarity tests, expanding the library's capability to assess time-series properties. Alongside the new module, this release implements a comprehensive overhaul of documentation standardisation, introduces new synthetic data generators, and achieves a milestone of 100% global code coverage through enhanced unit tests and doctest validation logic.</p>"},{"location":"usage/changelog/#implementation-details_1","title":"\ud83d\udee0\ufe0f Implementation details","text":""},{"location":"usage/changelog/#stationarity-module-implementation","title":"Stationarity module implementation","text":"<ul> <li>Introduce the <code>src/ts_stat_tests/algorithms/stationarity.py</code> module, implementing seven essential stationarity tests:<ul> <li><code>adf()</code>: Augmented Dickey-Fuller test.</li> <li><code>kpss()</code>: Kwiatkowski-Phillips-Schmidt-Shin test.</li> <li><code>pp()</code>: Phillips-Perron test (utilising <code>arch</code> for improved reliability).</li> <li><code>za()</code>: Zivot-Andrews test for structural breaks.</li> <li><code>ers()</code>: Elliott-Rothenberg-Stock GLS test.</li> <li><code>vr()</code>: Variance Ratio test.</li> <li><code>rur()</code>: Range Unit Root test.</li> </ul> </li> <li>Implement a unified dispatcher in <code>src/ts_stat_tests/tests/stationarity.py</code>:<ul> <li><code>stationarity()</code>: Provide a consistent interface to execute any supported stationarity test via string aliases.</li> <li><code>is_stationary()</code>: Return a standardised dictionary containing the Boolean result, test statistic, and interpreted p-value.</li> </ul> </li> <li>Standardise all algorithm outputs to native Python types (<code>float</code>, <code>int</code>, <code>dict</code>, <code>tuple</code>), ensuring consistent behaviour across the API.</li> </ul>"},{"location":"usage/changelog/#data-utility-enhancements","title":"Data utility enhancements","text":"<ul> <li>Expand the <code>src/ts_stat_tests/utils/data.py</code> module with new synthetic data generators:<ul> <li><code>get_normal_curve()</code>: Generate data following a normal distribution.</li> <li><code>get_trend_data()</code>: Generate data with deterministic linear trends.</li> <li><code>get_uniform_data()</code>: Generate data following a uniform distribution.</li> </ul> </li> <li>Expose these as global variables (<code>data_normal</code>, <code>data_trend</code>, <code>data_uniform</code>) to facilitate reproducible examples and testing.</li> <li>Refactor all docstring examples across the library (correlation, normality, regularity, stationarity) to use these standardised data imports.</li> </ul>"},{"location":"usage/changelog/#documentation-standardisation","title":"Documentation standardisation","text":"<ul> <li>Standardise all function docstrings to follow strict Google-style formatting and DFC compliance.</li> <li>Move mathematical formulations into dedicated <code>equation</code> dropdowns using KaTeX notation for improved readability.</li> <li>Update terminology to Australian English (e.g., <code>standardise</code>, <code>summarise</code>, <code>recognise</code>) and adopt the imperative mood across all descriptions.</li> <li>Enhance the API reference by adding <code>docs/code/stationarity.md</code> and updating the implementation progress tables in <code>README.md</code> and <code>docs/index.md</code>.</li> </ul>"},{"location":"usage/changelog/#checklist_1","title":"\u2705 Checklist","text":"<ul> <li> Implement seven core stationarity algorithms in the <code>algorithms</code> layer.</li> <li> Develop a unified stationarity dispatcher and Boolean check in the <code>tests</code> layer.</li> <li> Achieve 100% global code coverage across the <code>ts_stat_tests</code> package.</li> <li> Standardise all docstrings and mathematical equations across the entire library.</li> <li> Expand data utilities with new synthetic generators and integrate them into examples.</li> <li> Implement robust cross-platform doctest validation for floating-point precision.</li> </ul>"},{"location":"usage/changelog/#testing-and-quality-assurance","title":"\ud83e\uddea Testing and quality assurance","text":"<ul> <li>Achieve 100% code coverage by adding comprehensive unit tests for the stationarity module and addressing edge cases in <code>utils/errors.py</code>.</li> <li>Implement a custom doctest runner in <code>src/utils/scripts.py</code> to enable targeted validation of documentation examples.</li> <li>Standardise numeric outputs in doctests using explicit string formatting to ensure stability across different operating systems and Python versions.</li> <li>Pass all strict <code>pylint</code> (10/10), <code>pyright</code> (strict), and <code>complexipy</code> checks.</li> </ul>"},{"location":"usage/changelog/#pull-requests_1","title":"\ud83d\udcaa Pull requests","text":""},{"location":"usage/changelog/#27-implement-stationarity-module","title":"27: Implement stationarity module","text":""},{"location":"usage/changelog/#v031-add-normality-module","title":"v0.3.1 - Add Normality Module","text":"<p><code>v0.3.1</code> <code>2026-01-08</code> data-science-extensions/ts-stat-tests/releases/v0.3.1</p> Release Notes Updates <ul> <li><code>90cd570</code>: Fix grammar error     (by chrimaho)        * <code>b708fe1</code>: Remove documentation     Duplicate dictionary key \"entropy\" in the return type documentation. The second entry on line 341 should be removed as it's identical to the one on line 340.     (by chrimaho)        * <code>4f554e5</code>: Incomplete documentation     The text \"because REASONS\" is a placeholder that should be replaced with actual reasoning for why the AntroPy package was selected.     (by chrimaho)        * <code>1427c5e</code>: Document additional entropy algorithms in regularity tests<ul> <li>Expand documentation to include <code>permutation_entropy()</code> and <code>svd_entropy()</code> functions as supported algorithms</li> <li>Clarify algorithm selection options and aliases for improved user guidance</li> <li>Ensure consistency across docstrings and abstract sections for easier reference (by chrimaho)        * <code>1782d7b</code>: Update progress docs for completed regularity and normality</li> <li>Mark completed <code>antropy</code> regularity algorithms and add <code>svd_entropy()</code> function to documentation</li> <li>Mark all normality algorithms as implemented and tested</li> <li>Reflect full test and unit test coverage for correlation, regularity, and normality modules in progress tables</li> <li>Improve accuracy of documentation for current feature and test status (by chrimaho)        * <code>d006b1c</code>: Add documentation for normality tests and algorithms</li> <li>Provide an overview of normality testing in time-series analysis, including rationale and references</li> <li>Detail the use of <code>scipy.stats</code> and <code>statsmodels</code> libraries for statistical tests</li> <li>Link to relevant source modules for implementation details</li> <li>Outline available tests and algorithms via code documentation structure</li> <li>Help users understand when and why to perform normality checks on residuals (by chrimaho)        * <code>4584b3f</code>: Add unit test for fallback branch in normality check</li> <li>Improve test coverage by introducing a scenario that triggers the fallback path in the <code>is_normal()</code> function</li> <li>Ensure behaviour when the <code>normality()</code> function returns an object that is neither a tuple/list nor has a <code>pvalue</code> attribute</li> <li>Confirm that the <code>is_normal()</code> function handles unexpected return types gracefully and returns expected results (by chrimaho)        * <code>ec8d327</code>: Introduce unified normality test interface and refactor docs</li> <li>Add <code>normality()</code> and <code>is_normal()</code> functions to standardise access to multiple normality test algorithms</li> <li>Refactor and expand documentation for all normality algorithms, improving clarity and consistency of usage examples and equations</li> <li>Replace scattered summary/info/example blocks with a unified doc structure and imperative notes, using Australian English spelling</li> <li>Update and extend test coverage for new interfaces, ensuring comprehensive behaviour for all supported normality tests</li> <li>Improve parameter handling and error messaging for invalid algorithm selection using <code>generate_error_message()</code></li> <li>Align with latest <code>scipy</code> and <code>statsmodels</code> result object conventions for type safety and compatibility (by chrimaho)        * <code>ad67e9d</code>: Add detailed docstrings for normality test functions</li> <li>Improve documentation by adding comprehensive docstrings to all normality test functions</li> <li>Include summaries, parameter descriptions, return types, example usages, equations, references, notes, and related function links</li> <li>Standardise documentation style and formatting for clarity and consistency</li> <li>Enhance usability for end users by providing practical guidance and mathematical context, referencing the relevant statistical literature and library sources (by chrimaho)        * <code>25c4823</code>: Add module-level summary and structured docstrings</li> <li>Provide a clear overview and description for the normality testing algorithms</li> <li>Improve future maintainability by standardising documentation structure</li> <li>Clarify module purpose for statistical analysis and forecasting workflows</li> <li>Facilitate easier onboarding for new contributors (by chrimaho)        * <code>8f21f94</code>: Add statistical normality test algorithm implementations</li> <li>Provide comprehensive implementations for assessing data normality via <code>jb()</code>, <code>ob()</code>, <code>sw()</code>, <code>dp()</code>, and <code>ad()</code> functions.</li> <li>Include detailed docstrings, equations, practical examples, and references to support correct use and interpretation.</li> <li>Standardise parameter types, outputs, and documentation for consistent usage across different normality tests.</li> <li>Facilitate robust statistical validation in time series forecasting workflows by exposing multiple established tests. (by chrimaho)        * <code>a4cfa2a</code>: Add normality test documentation and stubs</li> <li>Create initial documentation for normality tests</li> <li>Add normality test file stubs for future implementation</li> <li>Update navigation to include normality section</li> <li>Prepare codebase for standardisation of normality test approach (by chrimaho)        * <code>c0e61c5</code>: Fix docs reference bugs (by chrimaho)        * <code>1083496</code>: Add regularity documentation</li> <li>Introduce comprehensive overview and rationale for regularity testing using entropy-based algorithms</li> <li>Standardise documentation with references, examples, and usage guidelines for <code>ts_stat_tests</code> modules</li> <li>Clarify differences between approximate entropy and sample entropy in forecasting context</li> <li>Link to further resources on time-series analysis methodology and data quality</li> <li>Detail regularity algorithms and tests, including filtering logic for code navigation (by chrimaho)</li> </ul> </li> </ul>"},{"location":"usage/changelog/#overview_2","title":"\ud83d\ude80 Overview","text":"<p>Introduce the <code>normality</code> module, providing a suite of statistical tests to assess the distribution of time series data. Significantly expand the documentation and test coverage for the <code>regularity</code> and <code>normality</code> modules, update the implementation progress, and improve the clarity and accuracy of references across the project. Notably, introduce new documentation pages, update the <code>README.md</code> and progress tables to reflect completed work, and add a comprehensive test suite for normality algorithms.</p>"},{"location":"usage/changelog/#implementation-details_2","title":"\ud83d\udee0\ufe0f Implementation details","text":""},{"location":"usage/changelog/#normality-module-implementation","title":"Normality module implementation","text":"<ul> <li>Introduce the <code>src/ts_stat_tests/algorithms/normality.py</code> module, implementing five core normality tests: <code>jb()</code>, <code>ob()</code>, <code>sw()</code>, <code>dp()</code>, and <code>ad()</code>.</li> <li>Implement a unified interface in <code>src/ts_stat_tests/tests/normality.py</code> with the following standardised functions:<ul> <li><code>normality()</code>: Act as a dispatcher to run a selected normality test.</li> <li><code>is_normal()</code>: Return a standardised dictionary containing the test result (<code>True</code>/<code>False</code>), statistic, and p-value.</li> </ul> </li> <li>Support various <code>nan_policy</code> options (\"propagate\", \"raise\", \"omit\") across algorithms to handle missing data gracefully.</li> <li>Provide detailed mathematical documentation, including LaTeX equations and bibliographic references, in the docstrings of all normality test functions.</li> </ul>"},{"location":"usage/changelog/#regularity-module-enhancements","title":"Regularity module enhancements","text":"<ul> <li>Expand the <code>src/ts_stat_tests/algorithms/regularity.py</code> module by exposing additional entropy measures: <code>permutation_entropy()</code> and <code>svd_entropy()</code>.</li> <li>Update the <code>entropy()</code>, <code>regularity()</code>, and <code>is_regular()</code> functions in <code>src/ts_stat_tests/tests/regularity.py</code> to support these new algorithms via intuitive string aliases.</li> <li>Standardise parameter handling and improve error messaging using <code>generate_error_message()</code>.</li> <li>Enhance docstrings with comprehensive examples, mathematical context, and internal cross-references.</li> </ul>"},{"location":"usage/changelog/#documentation-and-progress-tracking","title":"Documentation and progress tracking","text":"<ul> <li>Create new documentation pages to provide detailed guidance:<ul> <li><code>docs/code/normality.md</code>: Provide an overview of normality testing, rationale, algorithm details, and API references.</li> <li><code>docs/code/regularity.md</code>: Detail entropy-based regularity testing and provide a comprehensive API guide.</li> </ul> </li> <li>Update <code>docs/code/index.md</code> and <code>README.md</code> progress tables to mark the <code>normality</code> and <code>regularity</code> modules as 100% complete.</li> <li>Refactor <code>docs/code/correlation.md</code> to correct and clarify module references and formatting.</li> <li>Register the <code>normality</code> section in <code>mkdocs.yml</code> to improve site navigation and information architecture.</li> </ul>"},{"location":"usage/changelog/#testing-and-quality-assurance_1","title":"Testing and quality assurance","text":"<ul> <li>Add a comprehensive unit test suite in <code>src/tests/test_normality.py</code>, covering all implemented normality algorithms and edge cases.</li> <li>Implement a specific mock-based scenario to trigger the fallback path in <code>is_normal()</code>, ensuring 100% code coverage for the normality module.</li> <li>Adhere to Australian English spelling (e.g. <code>standardise</code>, <code>optimise</code>, <code>recognise</code>) and imperative coding style throughout the codebase.</li> <li>Ensure all code changes pass linting and type checks to maintain high standards for reliability, usability, and transparency.</li> </ul>"},{"location":"usage/changelog/#checklist_2","title":"\u2705 Checklist","text":"<ul> <li> Implement core normality test algorithms in <code>src/ts_stat_tests/algorithms/normality.py</code>.</li> <li> Create unified normality test interfaces in <code>src/ts_stat_tests/tests/normality.py</code>.</li> <li> Add comprehensive unit tests in <code>src/tests/test_normality.py</code> and achieve 100% coverage.</li> <li> Expand regularity algorithms and update convenience wrappers.</li> <li> Generate detailed documentation for both normality and regularity modules.</li> <li> Update project-wide progress tables in <code>README.md</code> and <code>docs/code/index.md</code>.</li> <li> Standardise spelling to Australian English and ensure consistent formatting.</li> </ul>"},{"location":"usage/changelog/#pull-requests_2","title":"\ud83d\udcaa Pull Requests","text":"<ul> <li>Implement normality module and enhance regularity documentation by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/26</li> </ul> <p>Full Changelog: https://github.com/data-science-extensions/ts-stat-tests/compare/v0.2.0...v0.3.0</p>"},{"location":"usage/changelog/#v020-add-regularity-algorithms","title":"v0.2.0 - Add Regularity Algorithms","text":"<p><code>v0.2.0</code> <code>2026-01-06</code> data-science-extensions/ts-stat-tests/releases/v0.2.0</p> Release Notes Updates <ul> <li><code>df1172b</code>: Fix f-string syntax in documentation     The example code contains syntax errors with mismatched quotes. Lines 665, 670, and other locations use straight double quotes inside the f-string which will cause a Python syntax error. The method parameter value should use single quotes or escaped double quotes.     (by chrimaho)        * <code>388713e</code>: Fix documentation typo     The examples show calls to <code>\"sample_entropy\"</code>, <code>\"approx_entropy\"</code>, and <code>\"spectral_entropy\"</code> functions, but the surrounding documentation indicates these should be calls to <code>\"is_regular()\"</code> with different algorithm parameters. The function names in the examples should be corrected to match the function being documented.     (by chrimaho)        * <code>49ecb5a</code>: Fix documentation consistency     There's an inconsistency in the documentation examples. The function calls in the examples use <code>\"approx_entropy\"</code>, <code>\"sample_entropy\"</code>, and <code>\"spectral_entropy\"</code> directly, but the surrounding text indicates these should be calls to the <code>\"regularity()\"</code> function with different algorithm parameters. The examples should match the function being documented.     (by chrimaho)        * <code>579c51d</code>: Fix redundant docstring comment     The docstring contains duplicate quote marks in the <code>\"Summary\"</code> admonition. It should be either <code>!!! note \"Summary\"</code> or just <code>!!! note Summary</code> without the extra quotes.     (by chrimaho)        * <code>9b125fb</code>: Fix another duplication     The documentation lists <code>\"approx\"</code> twice in line 337 which appears to be a typo. It should likely list the valid string options as: <code>[\"app\", \"approx\"]</code> based on the implementation in the entropy function.     (by chrimaho)        * <code>b8461aa</code>: Fix duplication     The documentation lists <code>\"approx\"</code> twice in line 99 which appears to be a typo. It should likely list the valid string options as: <code>[\"app\", \"approx\"]</code> based on the implementation in the options dictionary on line 181.     (by chrimaho)        * <code>90ce8c2</code>: Update src/ts_stat_tests/algorithms/regularity.py     The example output shows a raw array representation <code>\"array([0.9426, 0.9383, 0.9411, 0.9375])\"</code> instead of formatted output. Since this is in an f-string print statement, the output format is inconsistent with the other examples which show formatted floats. Consider using <code>.tolist()</code> or formatting the array elements consistently.     (by chrimaho)        * <code>43954ef</code>: Remove redundant docs comment     The docstring contains duplicate quote marks in the <code>\"Summary\"</code> admonition. It should be either <code>!!! note \"Summary\"</code> or just <code>!!! note Summary</code> without the extra quotes. It should not have duplicate <code>\"Summary\"</code> sections.     (by chrimaho)        * <code>81defae</code>: Remove duplicates     The documentation lists <code>\"approx\"</code> twice in line 227 which appears to be a typo. It should likely list the valid string options as: <code>[\"app\", \"approx\"]</code> based on the implementation in the entropy function.     (by chrimaho)        * <code>b2c8d65</code>: Fix <code>numpy</code> cast process     The cast to <code>np.ndarray</code> on line 429 may be incorrect if the input <code>x</code> is actually an <code>ArrayLike</code> that is not a <code>numpy</code> array (e.g., a <code>list</code> or <code>pd.Series</code>). The <code>np.std()</code> function already accepts <code>ArrayLike</code> inputs, so this cast is both unnecessary and potentially misleading. Consider removing the <code>cast()</code> or verifying that <code>x</code> has been converted to a <code>numpy</code> array first.     (by chrimaho)        * <code>a344cca</code>: Fix <code>numpy</code> version constraint     The <code>numpy</code> version constraint <code>\"numpy&lt;2.4\"</code> lacks a lower bound, which could allow installation of very old <code>numpy</code> versions (including 1.x) that may not be compatible with the codebase. Consider adding a lower bound like <code>\"numpy&gt;=2.0.0,&lt;2.4\"</code> to ensure compatibility, matching the override-dependencies specification on line 45.     (by chrimaho)        * <code>541a996</code>: Expand test coverage for utils and entropy logic<ul> <li>Include unit tests for the <code>load_airline()</code> function to verify data type validation and error handling.</li> <li>Add coverage for the <code>is_almost_equal()</code> and <code>assert_almost_equal()</code> utility functions to testing parameter validation and failure messages.</li> <li>Minimise gaps in coverage for the <code>svd_entropy()</code> and <code>entropy()</code> functions by testing direct calls and algorithm selections.</li> <li>Verify tolerance logic in the <code>is_regular()</code> function for <code>None</code> and string-based input values.</li> <li>Bring test coverage up to 100% (by chrimaho)        * <code>63ebd14</code>: Add permutation and SVD entropy options</li> <li>Expose the <code>permutation_entropy()</code> and <code>svd_entropy()</code> functions within the regularity testing module.</li> <li>Extend the <code>entropy()</code> function logic to include support for permutation and SVD algorithm types.</li> <li>Update the options mapping to recognise new aliases and standardise selection of these entropy measures. (by chrimaho)        * <code>369a321</code>: Optimise file discovery performance</li> <li>Use the <code>find</code> system command within the <code>get_all_files()</code> function to accelerate directory traversal.</li> <li>Implement a fallback mechanism to the <code>.glob()</code> method on the <code>Path()</code> class if the system command is unavailable.</li> <li>Prune <code>.venv</code> and hidden directories in the search path to minimise processing time.</li> <li>Standardise the output by applying the <code>sorted()</code> function to the list of discovered file paths.</li> <li>Add a docstring to the <code>get_all_files()</code> function to document the dual-method execution logic. (by chrimaho)        * <code>4dc7cad</code>: Update the Pylint configuration to suppress <code>R0801</code> duplicate-code check</li> <li>Ignore duplicate-code warnings to reduce linting noise</li> <li>Allow code repetition where refactoring for deduplication is not desirable (by chrimaho)        * <code>5f6b9e5</code>: Refine documentation and update test baselines</li> <li>Update baseline numerical values for <code>approx_entropy()</code>, <code>sample_entropy()</code>, <code>permutation_entropy()</code>, and <code>spectral_entropy()</code> functions in the test suite to match revised calculations.</li> <li>Standardise the type hint for the <code>metric</code> parameter in <code>entropy()</code>, <code>regularity()</code>, and <code>is_regular()</code> functions to use <code>VALID_KDTREE_METRIC_OPTIONS</code>.</li> <li>Refine docstrings by removing redundant admonition titles in <code>svd_entropy()</code>, <code>entropy()</code>, <code>regularity()</code>, and <code>is_regular()</code> functions. (by chrimaho)        * <code>89ce566</code>: Improve regularity test type safety and documentation</li> <li>Standardise docstring admonition blocks to use consistent <code>note</code> and <code>abstract</code> labels.</li> <li>Add a comprehensive docstring for the <code>svd_entropy()</code> function including parameters and return types.</li> <li>Enhance type safety by using the <code>cast()</code> function and specific type aliases for <code>metric</code> and <code>method</code> arguments.</li> <li>Refactor logic in the <code>entropy()</code> function to replace nested branches with early returns.</li> <li>Update parameter documentation for <code>tolerance</code> and <code>metric</code> in the <code>approx_entropy()</code> and <code>sample_entropy()</code> functions.</li> <li>Ensure the <code>is_regular()</code> function returns predictable types through explicit dictionary value casting.</li> <li>Add module-level headers and summary documentation to the regularity test utility source code. (by chrimaho)        * <code>3e13d54</code>: Restrict NumPy version for Numba compatibility</li> <li>Limit <code>numpy</code> version to less than <code>2.4</code> to ensure compatibility with Numba</li> <li>Synchronise the version cap across project dependencies and <code>uv</code> overrides (by chrimaho)        * <code>c0d065d</code>: Add unit tests for regularity and entropy metrics</li> <li>Implement the <code>TestRegularity()</code> class to provide comprehensive unit testing for regularity and entropy algorithms.</li> <li>Add test cases for the <code>approx_entropy()</code>, <code>sample_entropy()</code>, <code>permutation_entropy()</code>, and <code>spectral_entropy()</code> functions across multiple data types including noise, sine waves, and linear trends.</li> <li>Verify that the <code>is_regular()</code> function validates return keys and value types while handling invalid algorithm parameters.</li> <li>Validate numerical accuracy for various scenarios such as multidimensional arrays and different distance metrics used within the algorithms.</li> <li>Utilise the <code>assert_almost_equal()</code> function to ensure precise verification of calculated entropy values. (by chrimaho)        * <code>f71614b</code>: Standardise documentation and fix docstring typos</li> <li>Update code block labels to <code>pycon</code> to support interactive example rendering</li> <li>Fix a typographical error in the <code>import</code> statement for the <code>spectral_entropy()</code> function</li> <li>Standardise spacing in arithmetic expressions within the <code>sample_entropy()</code> and <code>permutation_entropy()</code> functions</li> <li>Improve docstring layout by adding vertical spacing before example blocks in the <code>approx_entropy()</code> and <code>spectral_entropy()</code> functions (by chrimaho)        * <code>700a063</code>: Add regularity and entropy statistical tests</li> <li>Introduce <code>entropy()</code> function as a unified interface for several entropy calculation algorithms</li> <li>Add <code>regularity()</code> function to provide a pass-through wrapper for assessing time series regularity</li> <li>Implement <code>is_regular()</code> function to determine if a dataset is regular by comparing entropy against a threshold</li> <li>Apply <code>typechecked</code> decorator to ensure robust parameter validation for all new functions</li> <li>Include helper logic to calculate a default <code>tolerance</code> based on the standard deviation of the input data</li> <li>Support <code>approx_entropy()</code>, <code>sample_entropy()</code>, and <code>spectral_entropy()</code> functions via a central entry point</li> <li>Provide internal parameter validation to normalise algorithm selection strings (by chrimaho)        * <code>50b5405</code>: Add regularity test file</li> <li>Initialise the <code>test_regularity.py</code> file to provide a structure for upcoming test cases (by chrimaho)        * <code>f33cbf7</code>: Add documentation for regularity functions</li> <li>Provide extensive docstrings for <code>approx_entropy()</code>, <code>sample_entropy()</code>, <code>permutation_entropy()</code>, and <code>spectral_entropy()</code> functions.</li> <li>Include mathematical formulations such as \\(ApEn(m, r, N) = \\phi_m(r) - \\phi_{m+1}(r)\\) to describe regularity logic.</li> <li>Add practical examples demonstrating how to apply entropy functions to various datasets.</li> <li>Document parameter configurations such as the <code>order</code> and <code>metric</code> arguments for the <code>sample_entropy()</code> function.</li> <li>Detail the steps required to normalise results within the <code>permutation_entropy()</code> and <code>spectral_entropy()</code> functions.</li> <li>Incorporate academic references and credit the <code>AntroPy</code> library for the underlying implementations.ts (by chrimaho)        * <code>759edd6</code>: Refine typing and expand entropy functions</li> <li>Introduce <code>Literal</code> type hints to restrict valid options for <code>metric</code> and <code>method</code> parameters.</li> <li>Add <code>tolerance</code> parameter to <code>approx_entropy()</code> and <code>sample_entropy()</code> functions.</li> <li>Standardise parameter definitions in <code>approx_entropy()</code>, <code>sample_entropy()</code>, and <code>spectral_entropy()</code> functions using <code>VALID_KDTREE_METRIC_OPTIONS</code> and <code>VALID_SPECTRAL_ENTROPY_METHOD_OPTIONS</code> constants. (by chrimaho)        * <code>5c48940</code>: Add <code>svd_entropy()</code> function for regularity testing</li> <li>Include <code>svd_entropy</code> in the <code>__all__</code> list to expose the new algorithm.</li> <li>Implement the <code>svd_entropy()</code> function as a type-checked wrapper for the <code>antropy</code> implementation to provide Singular Value Decomposition entropy calculations. (by chrimaho)        * <code>a51404c</code>: Add documentation header and module docstring</li> <li>Add a descriptive header to the regularity module to clarify its purpose.</li> <li>Include a module-level docstring that summarises functionality for computing regularity measures. (by chrimaho)        * <code>1796788</code>: Add regularity entropy algorithms</li> <li>Implement <code>approx_entropy()</code>, <code>sample_entropy()</code>, <code>permutation_entropy()</code>, and <code>spectral_entropy()</code> functions</li> <li>Utilise the <code>antropy</code> library for underlying entropy calculations</li> <li>Apply the <code>typechecked</code> decorator to ensure parameter type safety</li> <li>Define the <code>__all__</code> list to expose the public API of the module (by chrimaho)        * <code>ffb08f7</code>: Add regularity module and documentation</li> <li>Provide regularity statistical tests within the algorithms library</li> <li>Ensure code quality with a new test suite</li> <li>Initialise the documentation and update the site navigation (by chrimaho)        * <code>dd00a96</code>: Drop Python 3.9 support from CD workflow</li> <li>Remove <code>3.9</code> from the environment matrix to align with modern support standards.</li> <li>Optimise the deployment pipeline by focusing on more recent releases. (by chrimaho)</li> </ul> </li> </ul>"},{"location":"usage/changelog/#overview_3","title":"\ud83d\ude80 Overview","text":"<p>This release introduces a suite of regularity algorithms for time series analysis, including several entropy-based measures. It also features significant performance optimisations for project maintenance utilities and ensures the package achieves 100% unit test coverage. These enhancements stabilise the codebase and provide a robust foundation for assessing the complexity and regularity of time series data.</p>"},{"location":"usage/changelog/#implementation-details_3","title":"\ud83d\udee0\ufe0f Implementation details","text":""},{"location":"usage/changelog/#regularity-and-entropy-algorithms","title":"Regularity and entropy algorithms","text":"<ul> <li>Implement the <code>entropy()</code> function as a unified dispatcher for multiple entropy calculation algorithms, supporting Approximate, Sample, Permutation, Spectral, and SVD entropy.</li> <li>Introduce the <code>regularity()</code> function to provide a standardised wrapper for assessing time series regularity.</li> <li>Add the <code>is_regular()</code> function to determine if a time series meets a defined regularity threshold.</li> <li>Provide comprehensive wrappers for the <code>approx_entropy()</code>, <code>sample_entropy()</code>, <code>permutation_entropy()</code>, <code>spectral_entropy()</code>, and <code>svd_entropy()</code> functions.</li> <li>Utilise the <code>antropy</code> library for underlying calculations and apply the <code>@typechecked</code> decorator for rigorous runtime parameter validation.</li> <li>Integrate mathematical documentation within docstrings to describe the logic behind regularity measures.</li> </ul>"},{"location":"usage/changelog/#performance-and-maintenance-utilities","title":"Performance and maintenance utilities","text":"<ul> <li>Refactor the <code>get_all_files()</code> function in the <code>scripts.py</code> module to use system-level <code>git ls-files</code> and <code>find</code> commands.</li> <li>Optimise directory traversal for project maintenance tasks, reducing execution time from 41 seconds to near-instantaneous.</li> <li>Update the <code>Pylint</code> configuration to ignore <code>R0801</code> (duplicate code) warnings, facilitating specialised test case implementation.</li> <li>Fix documentation build failures by resolving <code>mkdocs-material</code> extension inconsistencies in the development environment.</li> </ul>"},{"location":"usage/changelog/#quality-assurance-and-environment","title":"Quality assurance and environment","text":"<ul> <li>Achieve 100% unit test coverage across the library by adding comprehensive test cases for regularity algorithms, data loading, and error handling.</li> <li>Enhance the <code>assert_almost_equal()</code> and <code>is_almost_equal()</code> utility functions to support precise verification of statistical results.</li> <li>Update the <code>load_airline()</code> function to include stricter data type validation and error reporting.</li> <li>Restrict the <code>numpy</code> version to <code>&lt; 2.4</code> in <code>pyproject.toml</code> to maintain compatibility with <code>Numba()</code> and <code>antropy</code>.</li> <li>Drop support for Python 3.9 in the CI/CD workflows to align with modern Python lifecycle standards.</li> </ul>"},{"location":"usage/changelog/#checklist_3","title":"\u2705 Checklist","text":"<ul> <li> Implement core regularity algorithms and entropy measures.</li> <li> Optimise file discovery performance in maintenance scripts.</li> <li> Reach 100% unit test coverage across all modules.</li> <li> Update package dependencies and Python support.</li> <li> Standardise documentation and verify build success.</li> </ul>"},{"location":"usage/changelog/#pull-requests_3","title":"\ud83d\udcaa Pull Requests","text":"<ul> <li>Implement regularity algorithms by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/25</li> </ul> <p>Full Changelog: https://github.com/data-science-extensions/ts-stat-tests/compare/v0.1.0...v0.2.0</p>"},{"location":"usage/changelog/#v010-initial-release-of-time-series-statistical-tests","title":"v0.1.0 - Initial release of Time Series Statistical Tests","text":"<p><code>v0.1.0</code> <code>2026-01-05</code> data-science-extensions/ts-stat-tests/releases/v0.1.0</p> Release Notes Updates <ul> <li><code>085a4c0</code>: Re-enable key components in the <code>cd</code> workflow, ready for first deployment     (by chrimaho)        * <code>a722504</code>: Ensure coverage reports are staged even when directory paths are ignored.<ul> <li>Use the <code>--force</code> flag in the <code>git_add_coverage_report()</code> function.</li> <li>Bypass <code>.gitignore</code> restrictions to guarantee documentation updates. (by chrimaho)        * <code>85f68e6</code>: Hide some lines in <code>cd</code> to check workflow during release (by chrimaho)        * <code>16c9288</code>: Standardise type hints and remove dependencies</li> <li>Replace custom type hints with native Python equivalents for the <code>__all__</code> variable</li> <li>Remove the unnecessary import from the <code>toolbox_python.collection_types</code> module</li> <li>Update the <code>replace()</code> function to use the <code>re.Match()</code> class directly (by chrimaho)        * <code>8e76143</code>: Fix missing justifications in docs (by chrimaho)        * <code>a3085ff</code>: Fix typos (by chrimaho)        * <code>aef58db</code>: Fix typo (by chrimaho)        * <code>90c953b</code>: Fix incorrect percentages (by chrimaho)        * <code>15202ad</code>: Re-enable <code>mkdocs</code> checks (by chrimaho)        * <code>728c24a</code>: Add overview of module progress (by chrimaho)        * <code>02d36d1</code>: Add <code>correlation</code> module docs page (by chrimaho)        * <code>60c7127</code>: Fix some typos on the <code>correlation</code> module docs (by chrimaho)        * <code>fe9003d</code>: Add docs landing page and home page structures (by chrimaho)        * <code>37fab51</code>: Add custom hooks for docs to utilise (by chrimaho)        * <code>83dd55f</code>: Initial commit of package icons (by chrimaho)        * <code>8c3f0cf</code>: Initial commit of package stylesheets (by chrimaho)        * <code>950c1fb</code>: Fill in README file (by chrimaho)        * <code>e2c62bd</code>: Initial commit of standard package docs (by chrimaho)        * <code>1f1a9d6</code>: Initial commit of <code>mkdocs</code> config file (by chrimaho)        * <code>c7839a4</code>: Set UTF-8 encoding for Python I/O in CI</li> <li>Define the <code>PYTHONIOENCODING</code> environment variable to ensure consistent character handling across CI environments (by chrimaho)        * <code>cc368ff</code>: Fix typo (by chrimaho)        * <code>0b2d590</code>: Standardise variable naming convention</li> <li>Rename the repository variable to lower-case to follow standard naming conventions for local instances within the <code>main()</code> function.</li> <li>Update references to the variable when calling the <code>.get_releases()</code> method and <code>.get_commits()</code> method.</li> <li>The variable name <code>REPO</code> is defined in uppercase following constant naming conventions, but it's not a constant - it's a dynamically created repository object. Consider using lowercase <code>repo</code> to follow Python naming conventions where uppercase names are reserved for constants. (by chrimaho)        * <code>ed5404d</code>: Initialise the output file before starting the main process</li> <li>Call the <code>prepare_output_file()</code> function to ensure the destination is ready for writing.</li> <li>Set up the file environment prior to opening the GitHub and file context managers. (by chrimaho)        * <code>6060627</code>: Define constant for short SHA length</li> <li>Introduce <code>SHORT_SHA_LENGTH</code> constant to replace hardcoded magic numbers</li> <li>Update <code>add_commit_info()</code> function to use defined constant</li> <li>Standardise representation of short commit identifiers for improved maintainability (by chrimaho)        * <code>aa243b7</code>: FixPotential bug If <code>commit.author</code> is <code>None</code>, the code constructs an incomplete markdown link <code>[]()</code> with empty values. This would result in broken links in the changelog. Consider providing a default fallback text like <code>\"Unknown\"</code> for the author name and omitting the link altogether when the author is unavailable. (by chrimaho)        * <code>d1e0800</code>: Refactor correlation tests to use pytest</li> <li>Replace <code>.assertRaises()</code> method with <code>raises()</code> function to standardise test assertions</li> <li>Update <code>.test_is_correlated()</code> method to expect <code>NotImplementedError()</code> class from <code>is_correlated()</code> function (by chrimaho)        * <code>01d3544</code>: Create script to automate changelog generation</li> <li>Introduce the <code>src/utils/changelog.py</code> script to automate the creation of <code>CHANGELOG.md</code> by retrieving data via the GitHub API.</li> <li>Implement the <code>prepare_output_file()</code> function to ensure the document is recreated from scratch on each run.</li> <li>Define the <code>add_page_styling()</code> function to embed CSS for better navigation in Markdown viewers.</li> <li>Utilise the <code>main()</code> function to fetch releases via the <code>.get_releases()</code> method and filter out irrelevant commits to maintain a clean history.</li> <li>Use the <code>Github()</code> class to authenticate and manage repository interactions.</li> <li>Standardise the output format for release notes and commit details to improve readability. (by chrimaho)        * <code>702d902</code>: Add continuous delivery workflow for releases</li> <li>Introduce <code>cd.yml</code> to automate the release cycle triggered by published GitHub releases.</li> <li>Orchestrate a multi-stage pipeline including testing, building, PyPI deployment, and documentation generation.</li> <li>Utilise <code>uv</code> to synchronise dependencies and manage the build process for improved performance.</li> <li>Execute the <code>git_update_version_cli()</code> function to handle versioning during the build stage.</li> <li>Verify package integrity using an installation matrix across multiple operating systems and Python versions.</li> <li>Automate changelog generation via the <code>changelog.py</code> script and push updates to the repository.</li> <li>Upload distribution assets to GitHub releases and coverage data to Codecov. (by chrimaho)        * <code>3e014f6</code>: Increase CI job parallelism</li> <li>Increase the <code>max-parallel</code> limit to 30 to allow for more concurrent jobs</li> <li>Optimise the CI pipeline performance by utilising more available runners for the matrix build (by chrimaho)        * <code>cd1f7ad</code>: Add CI workflow for automated code validation</li> <li>Implement GitHub Actions to automate validation on push and pull request events.</li> <li>Configure a check job for non-main branches to facilitate early issue detection.</li> <li>Define a matrix strategy to ensure cross-platform compatibility across various OS types.</li> <li>Support Python versions 3.9 through 3.14 to maintain broad environment stability.</li> <li>Utilise <code>uv</code> for efficient dependency management and execution of the validation script. (by chrimaho)        * <code>1f68794</code>: Add Dependabot for GitHub Actions</li> <li>Initialise the configuration file to manage <code>github-actions</code> dependencies.</li> <li>Schedule weekly updates to ensure actions remain current.</li> <li>Assign a default reviewer and label to simplify the pull request review process. (by chrimaho)        * <code>c059c7c</code>: Raise error for unimplemented correlation test</li> <li>Replace the <code>None</code> return value in the <code>is_correlated()</code> function with a <code>NotImplementedError</code> to explicitly signal that this logic is a placeholder and has not yet been implemented. (by chrimaho)        * <code>470fbfa</code>: Clarify type ignore reasons</li> <li>Document rationale for <code># type: ignore</code> on calls to the <code>acorr_lm()</code> function and <code>acorr_breusch_godfrey()</code> function</li> <li>Note that <code>statsmodels</code> type hints are incomplete or incompatible with internal <code>RegressionResults()</code> class types (by chrimaho)        * <code>8ea4c9d</code>: Standardise equality check utility functions</li> <li>Set default value for <code>places</code> parameter in <code>is_almost_equal()</code> function and <code>assert_almost_equal()</code> function overloads</li> <li>Refactor <code>is_almost_equal()</code> function to prioritise argument validation over value equality checks</li> <li>Standardise type annotations for the <code>params</code> dictionary in <code>assert_almost_equal()</code> function</li> <li>Simplify logic for error messages in <code>assert_almost_equal()</code> function by defaulting to precision-based comparison (by chrimaho)        * <code>00a6029</code>: Initialise parent classes in test suites</li> <li>Call the <code>.setUpClass()</code> and <code>.tearDownClass()</code> methods of the superclass in the <code>BaseTester()</code> class</li> <li>Implement the <code>.setUp()</code>, <code>.tearDown()</code>, and <code>.tearDownClass()</code> methods in the <code>TestCorrelation()</code> class to ensure proper lifecycle management (by chrimaho)        * <code>3b018b4</code>: Remove redundant lines in config file (by chrimaho)        * <code>58163f0</code>: Update documentation headers and module summaries</li> <li>Add header comments to the <code>setup.py</code> script explaining its role in unit tests.</li> <li>Standardise documentation in the <code>errors.py</code> module regarding error generation and data equality checks. (by chrimaho)        * <code>9c7206a</code>: Optimise data generation and loading via caching</li> <li>Apply the <code>@lru_cache</code> decorator to the <code>get_random_generator()</code> function and various <code>data_*()</code> functions to reduce redundant test overhead.</li> <li>Remove the global <code>seed()</code> function call to favour the use of independent random generators.</li> <li>Enhance the <code>load_airline()</code> function with result caching to improve data retrieval performance. (by chrimaho)        * <code>891edd2</code>: Refactor correlation logic and remove redundant code</li> <li>Remove commented-out code from the <code>correlation()</code> function to clean up the source.</li> <li>Update the <code>is_correlated()</code> function to return <code>None</code> instead of raising a <code>NotImplementedError</code> class to provide a neutral placeholder. (by chrimaho)        * <code>0d078f3</code>: Add implementation error to the <code>is_correlated</code> function The function <code>is_correlated</code> is defined but returns <code>None</code> and serves no purpose. It's marked as a placeholder in the docstring, but placeholder functions should either be removed or raise <code>NotImplementedError</code> to indicate they're not yet implemented. Returning <code>None</code> with no implementation can lead to confusion. (by chrimaho)        * <code>335045d</code>: Refine correlation APIs and improve type safety</li> <li>Introduce <code>@overload()</code> function signatures for <code>acf()</code>, <code>pacf()</code>, <code>ccf()</code>, <code>lm()</code>, <code>bglm()</code>, and <code>correlation()</code> functions to improve static analysis and developer experience.</li> <li>Standardise parameter validation using <code>Literal()</code> class aliases for algorithm options within the <code>acf()</code> and <code>pacf()</code> functions.</li> <li>Update the <code>ccf()</code> function to support <code>nlags</code> and <code>alpha</code> parameters for better consistency with other correlation tools.</li> <li>Enforce keyword-only arguments for the <code>store</code> parameter in <code>lm()</code> and <code>bglm()</code> functions and the <code>alpha</code> and <code>qstat</code> parameters in the <code>acf()</code> function.</li> <li>Refine the <code>correlation()</code> function to act as a unified interface with specific return type hints based on the provided algorithm string.</li> <li>Ensure the <code>lb()</code> function return type is correctly annotated as a <code>pd.DataFrame()</code> class. (by chrimaho)        * <code>6176186</code>: Validate data type in <code>load_airline()</code></li> <li>Ensure the <code>load_airline()</code> function returns a <code>pd.Series()</code> class by introducing a type check</li> <li>Raise a <code>TypeError()</code> class if the result of the <code>.squeeze()</code> method is not a series to prevent errors when initialising the <code>pd.PeriodIndex()</code> class</li> <li>Resolve PyRight errors (by chrimaho)        * <code>d668fa7</code>: Organise and clean up check function calls</li> <li>Remove the redundant commented out <code>check_mypy()</code> function call from the <code>check()</code> function</li> <li>Move and disable the <code>check_mkdocs()</code> function call to execute after the <code>check_build()</code> function call within the <code>check()</code> function (by chrimaho)        * <code>b9a9c1a</code>: Add <code>pyright</code> for static type analysis</li> <li>Include <code>pyright</code> in the development dependencies</li> <li>Define the <code>check_pyright()</code> function to analyse code types</li> <li>Integrate the <code>check_pyright()</code> function into the validation <code>check()</code> function (by chrimaho)        * <code>d7b0669</code>: Ignore complexipy cache directory</li> <li>Prevent <code>.complexipy_cache/*</code> from being tracked in the repository (by chrimaho)        * <code>15b4f17</code>: Expand correlation tests and organise imports</li> <li>Import <code>correlation()</code> and <code>is_correlated()</code> functions to expand test coverage for the correlation module.</li> <li>Add <code>.test_correlation_ccf_raises()</code> and <code>.test_correlation_bglm()</code> methods to validate algorithm error handling and behaviour.</li> <li>Simplify the <code>.setUpClass()</code> method by removing redundant type annotations from class attributes and local variables.</li> <li>Reformat import statements to improve code structure and maintainability. (by chrimaho)        * <code>f61bbf8</code>: Update type hints to use abstract base classes</li> <li>Replace <code>Dict()</code> and <code>List()</code> with <code>Mapping()</code> and <code>Collection()</code> classes in the <code>generate_error_message()</code> function to support broader input types</li> <li>Standardise the use of the built-in <code>dict()</code> class within the <code>assert_almost_equal()</code> function</li> <li>Simplify complex <code>Union</code> type annotations to improve code maintainability (by chrimaho)        * <code>c77a089</code>: Introduce correlation test dispatcher</li> <li>Implement the <code>correlation()</code> function to provide a unified interface for various statistical correlation algorithms.</li> <li>Map string aliases to internal implementations such as the <code>_acf()</code>, <code>_pacf()</code>, and <code>_ccf()</code> functions.</li> <li>Utilise the <code>generate_error_message()</code> function to handle unsupported algorithm selections with descriptive feedback.</li> <li>Ensure the <code>y</code> parameter is provided when the <code>ccf</code> algorithm is requested via the <code>correlation()</code> function.</li> <li>Add a placeholder <code>is_correlated()</code> function to define the intended package structure. (by chrimaho)        * <code>47ff459</code>: Refine type annotations for correlation functions</li> <li>Standardise return type signatures for the <code>lm()</code> function and the <code>bglm()</code> function.</li> <li>Introduce <code>overload()</code> definitions for the <code>lm()</code> function to improve static analysis and type safety.</li> <li>Replace <code>np.float64</code>, <code>np.ndarray</code>, and complex <code>Union</code> types with specific <code>float</code> tuples to ensure consistency. (by chrimaho)        * <code>4f783a0</code>: Add error handling and float comparison utilities</li> <li>Add <code>generate_error_message()</code> function to standardise error reporting for invalid parameter options.</li> <li>Implement <code>is_almost_equal()</code> function to provide flexible float comparison using either decimal places or a specific delta.</li> <li>Include <code>assert_almost_equal()</code> function to facilitate testing by raising descriptive <code>AssertionError</code> messages when float values deviate beyond tolerances. (by chrimaho)        * <code>aa0c816</code>: Disable invalid-name linting rule</li> <li>Disable the <code>C0103</code> rule to standardise the project's naming convention exceptions. (by chrimaho)        * <code>8b1424e</code>: Standardise docstrings and improve type hinting</li> <li>Define <code>VALID_PACF_METHODS</code> constant to centralise allowed methods for the <code>pacf()</code> function.</li> <li>Update docstring callouts to use <code>!!! note</code> and <code>???+ abstract</code> admonitions for improved documentation consistency.</li> <li>Refine parameter type descriptions in docstrings to use <code>ArrayLike</code> and <code>Optional</code> types for the <code>acf()</code>, <code>pacf()</code>, <code>ccf()</code>, <code>lb()</code>, and <code>lm()</code> functions.</li> <li>Reformat code examples to use <code>pycon</code> syntax and ensure correct line continuation for better rendering.</li> <li>Clean up <code>!!! deprecation</code> blocks and standardise internal formatting. (by chrimaho)        * <code>02fd5d2</code>: Document and standardise data utility module</li> <li>Add module-level docstrings and structural headers to improve navigation</li> <li>Enhance the <code>load_airline()</code> function with comprehensive documentation and academic references</li> <li>Remove unused <code>numpy</code> import to tidy the <code>imports</code> section</li> <li>Standardise the file layout using consistent section markers (by chrimaho)        * <code>f6824ad</code>: Suppress specific Pylint linting warnings</li> <li>Introduce configuration to relax linting constraints</li> <li>Minimise noise by disabling checks for line length and file length</li> <li>Disable warnings for argument counts and redefined built-ins (by chrimaho)        * <code>716db6d</code>: Expand and standardise docstring sections</li> <li>Add <code>credit</code>, <code>references</code>, <code>see also</code>, and <code>deprecation</code> sections to the configuration</li> <li>Standardise the internal key order for all section definitions to maintain consistency</li> <li>Align configuration properties to improve visual clarity and maintenance (by chrimaho)        * <code>48abfe1</code>: Update package name and add script documentation</li> <li>Add a header comment block to the utility script to document usage and purpose.</li> <li>Update the <code>PACKAGE_NAME</code> constant to reflect the project identity. (by chrimaho)        * <code>9ba6915</code>: Introduce a centralised scripts module</li> <li>Provide a centralised suite of automation utilities for project maintenance.</li> <li>Implement a <code>run_command()</code> function to handle shell execution and argument expansion.</li> <li>Include a <code>lint()</code> function to standardise code formatting across the repository.</li> <li>Define a <code>check()</code> function to aggregate quality assurance tests and build verification.</li> <li>Automate git workflows for versioning, tagging, and documentation deployment.</li> <li>Expose utilities via a CLI entry point to simplify development and CI/CD pipelines. (by chrimaho)        * <code>c6021f8</code>: Introduce pre-commit hooks for code quality</li> <li>Standardise repository-wide formatting and linting rules</li> <li>Automate file-level sanitisation for whitespace and line endings</li> <li>Enforce Python style consistency using <code>black</code>, <code>isort</code>, and <code>pyupgrade</code></li> <li>Prevent direct commits to the <code>main</code> branch via <code>no-commit-to-branch</code></li> <li>Validate configuration files with <code>check-json</code>, <code>check-toml</code>, and <code>check-yaml</code></li> <li>Integrate <code>uv</code> lockfile and synchronisation checks to maintain environment integrity</li> <li>Sanitise documentation and check spelling with <code>blacken-docs</code> and <code>codespell</code></li> <li>Implement a local <code>ty-check</code> hook for type safety (by chrimaho)        * <code>ccc7aed</code>: Add unit tests for the <code>correlation</code> module</li> <li>Update <code>llvmlite</code> and <code>numba</code> packages to stable versions in <code>pyproject.toml</code></li> <li>Add <code>stochastic</code> package to the test dependencies</li> <li>Introduce <code>BaseTester()</code> class to standardise test data setup via the <code>.setUpClass()</code> method</li> <li>Implement <code>load_airline()</code> function to retrieve sample datasets for time series analysis</li> <li>Add unit tests for correlation functions including <code>acf()</code> and <code>pacf()</code></li> <li>Configure <code>uv</code> dependency overrides to maintain <code>numpy</code> package version consistency (by chrimaho)        * <code>f18bc49</code>: Fix the PACF formula clarity The PACF formula is incorrectly formatted and unclear. The notation <code>\"Corr(Y_t, Y_{t-k} / Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1})\"</code> uses division where conditional notation should be used. It should be <code>\"Corr(Y_t, Y_{t-k} | Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1})\"</code> with a vertical bar (<code>|</code>) to denote conditioning, not a division symbol. (by chrimaho)        * <code>9de39de</code>: Correct ACF formula in <code>acf()</code> function docstring</li> <li>Update the mathematical expression in the <code>acf()</code> function docstring to correctly show square roots in the denominator.</li> <li>Add the simplified version of the ACF formula for stationary time series to the <code>acf()</code> function documentation. (by chrimaho)        * <code>984fb1d</code>: Fix typo issue Inconsistent terminology: The documentation states \"Ljung-Box and Box-Pierce statistic differ\" but should use \"statistics\" (plural) since there are two statistics being discussed. (by chrimaho)        * <code>04dbd4b</code>: Fix <code>p-value</code> formatting issue (by chrimaho)        * <code>9631202</code>: Fix self-referential documentation issue The text \"See <code>q_stat</code> for more information\" is circular and unhelpful since this IS the documentation for the <code>qstat</code> parameter. This reference should either be removed or point to relevant external documentation or the Returns section. (by chrimaho)        * <code>044f8a0</code>: Fix typo (by chrimaho)        * <code>cbf0fb7</code>: Fix duplicate word (by chrimaho)        * <code>feba34c</code>: Standardise formatting of <code>p-value</code> in docs (by chrimaho)        * <code>ac283f5</code>: Correct the formula provided in the <code>lm()</code> function documentation</li> <li>Clarify the relationship between the LM test, Engle's ARCH test, and the Breusch-Godfrey test within the <code>lm()</code> function</li> <li>Refine the mathematical definition of the test statistic to focus on the auxiliary regression <code>$R^2$</code>, the number of observations, and degrees of freedom</li> <li>Provide a clearer step-by-step procedure for fitting the time series model and running the auxiliary regression to obtain the test statistic</li> <li>Improve the explanation of the null hypothesis and the resulting asymptotic chi-squared distribution of the LM statistic</li> <li>Standardise document formatting by using KaTeX for math equations and back-ticks for parameters like <code>resid</code> and <code>nlags</code> (by chrimaho)        * <code>1d32293</code>: Add correlation algorithms for time series analysis</li> <li>Introduce <code>acf()</code> function, <code>pacf()</code> function, and <code>ccf()</code> function to compute temporal dependencies.</li> <li>Implement <code>lb()</code> function, <code>lm()</code> function, and <code>bglm()</code> function for residual autocorrelation testing.</li> <li>Utilise <code>statsmodels</code> library to ensure robust and standardised statistical results.</li> <li>Include detailed docstrings with mathematical formulas and usage examples.</li> <li>Apply <code>@typechecked</code> decorator to ensure runtime type safety for all algorithm inputs. (by chrimaho)        * <code>fa3e3fd</code>: Remove duplicate function declaration (by chrimaho)        * <code>94efa8b</code>: Initialise project structure and configuration</li> <li>Define project metadata and dependencies within <code>pyproject.toml</code> using <code>uv_build</code>.</li> <li>Configure linting, formatting, and testing tools to standardise development.</li> <li>Implement <code>strip_ansi_codes()</code> function in <code>setup.py</code> to facilitate environment-agnostic CLI output verification.</li> <li>Add test naming helper functions like <code>name_func_flat_list()</code> function to <code>setup.py</code>.</li> <li>Configure <code>sys.path</code> in the <code>tests</code> <code>__init__.py</code> file to ensure local module resolution.</li> <li>Extend <code>.gitignore</code> to exclude <code>uv.lock</code>, <code>.vscode/</code>, and other local environment directories. (by chrimaho)        * <code>b963f05</code>: Initial commit (by chrimaho)</li> </ul> </li> </ul>"},{"location":"usage/changelog/#overview_4","title":"\ud83d\ude80 Overview","text":"<p>This is the initial release of <code>ts-stat-tests</code>, a Python package dedicated to providing a unified and standardised interface for time series statistical testing. This release establishes the foundational project infrastructure, implements a comprehensive suite of correlation algorithms, and provides extensive documentation and automated CI/CD pipelines. It aims to bridge the gap between R and Python for time series analysis by offering a single, robust library for standard time series statistical tests.</p>"},{"location":"usage/changelog/#implementation-details_4","title":"\ud83d\udee0\ufe0f Implementation details","text":""},{"location":"usage/changelog/#core-correlation-algorithms","title":"Core correlation algorithms","text":"<ul> <li>Introduce the <code>acf()</code> function, <code>pacf()</code> function, and <code>ccf()</code> function to estimate autocorrelation, partial autocorrelation, and cross-correlation functions.</li> <li>Implement residual diagnostic tests including the <code>lb()</code> function for Ljung-Box testing, the <code>lm()</code> function for Lagrange Multiplier tests, and the <code>bglm()</code> function for Breusch-Godfrey tests.</li> <li>Provide a unified <code>correlation()</code> dispatcher function to simplify algorithm selection via string aliases.</li> <li>Utilise the <code>statsmodels</code> library as the underlying engine to ensure robust and standardised statistical results.</li> <li>Include detailed docstrings for all algorithms, featuring mathematical definitions, parameter details, and practical usage examples.</li> </ul>"},{"location":"usage/changelog/#project-infrastructure-and-automation","title":"Project infrastructure and automation","text":"<ul> <li>Initialise the project structure using <code>uv</code> for efficient dependency management and build orchestration.</li> <li>Implement a centralised <code>scripts.py</code> module to provide a suite of automation utilities for linting, checking, and maintenance.</li> <li>Configure pre-commit hooks to enforce code quality, standardise formatting, and prevent direct commits to the <code>main</code> branch.</li> <li>Establish GitHub Actions workflows for continuous integration (<code>ci.yml</code>) and continuous delivery (<code>cd.yml</code>) to automate testing and releases.</li> <li>Integrate Dependabot to automate updates for GitHub Actions dependencies on a weekly schedule.</li> </ul>"},{"location":"usage/changelog/#documentation-and-presentation","title":"Documentation and presentation","text":"<ul> <li>Launch a comprehensive documentation site using <code>mkdocs</code> with the <code>material</code> theme and custom stylesheets.</li> <li>Expand the <code>README.md</code> with project badges, a motivation section, and a detailed feature implementation table.</li> <li>Implement custom documentation hooks in <code>shortcodes.py</code> to support dynamic content and enhanced rendering.</li> <li>Add placeholder pages and \"To Do\" notes for <code>CHANGELOG.md</code> and <code>CONTRIBUTING.md</code> to guide future documentation efforts.</li> </ul>"},{"location":"usage/changelog/#type-safety-and-quality-assurance","title":"Type safety and quality assurance","text":"<ul> <li>Enforce runtime type safety by applying the <code>@typechecked</code> decorator to all core algorithm functions.</li> <li>Integrate <code>pyright</code> for rigorous static type analysis and resolve complex type-hinting issues.</li> <li>Implement utility functions for error reporting and float comparison, including <code>generate_error_message()</code>, <code>is_almost_equal()</code>, and <code>assert_almost_equal()</code>.</li> <li>Achieve 100% unit test coverage for the correlation module to ensure reliability and correctness.</li> </ul>"},{"location":"usage/changelog/#checklist_4","title":"\u2705 Checklist","text":"<ul> <li> Establish foundational project structure and configuration.</li> <li> Implement core correlation algorithms and diagnostic tests.</li> <li> Create a centralised automation and maintenance script.</li> <li> Launch a comprehensive documentation site and expand <code>README.md</code>.</li> <li> Configure CI/CD pipelines and pre-commit hooks.</li> <li> Enforce runtime and static type safety across the package.</li> <li> Achieve high unit test coverage for the initial release.</li> </ul>"},{"location":"usage/changelog/#pull-requests_4","title":"\ud83d\udcaa Pull Requests","text":"<ul> <li>Initialise project structure and configuration by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/15</li> <li>Add correlation algorithms for time series analysis by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/16</li> <li>Set up unit tests and enhance development infrastructure by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/17</li> <li>Establish CI/CD pipelines and automate release workflows by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/18</li> <li>Bump actions/checkout from 5 to 6 by @dependabot[bot] in https://github.com/data-science-extensions/ts-stat-tests/pull/19</li> <li>Bump actions/setup-python from 5 to 6 by @dependabot[bot] in https://github.com/data-science-extensions/ts-stat-tests/pull/23</li> <li>Bump astral-sh/setup-uv from 6 to 7 by @dependabot[bot] in https://github.com/data-science-extensions/ts-stat-tests/pull/22</li> <li>Bump actions/upload-artifact from 4 to 6 by @dependabot[bot] in https://github.com/data-science-extensions/ts-stat-tests/pull/21</li> <li>Bump actions/download-artifact from 5 to 7 by @dependabot[bot] in https://github.com/data-science-extensions/ts-stat-tests/pull/20</li> <li>Enhance project documentation and presentation by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/24</li> </ul>"},{"location":"usage/changelog/#new-contributors_1","title":"\ud83c\udd95 New Contributors","text":"<ul> <li>@chrimaho made their first contribution in https://github.com/data-science-extensions/ts-stat-tests/pull/15</li> <li>@dependabot[bot] made their first contribution in https://github.com/data-science-extensions/ts-stat-tests/pull/19</li> </ul> <p>Full Changelog: https://github.com/data-science-extensions/ts-stat-tests/commits/v0.1.0</p>"},{"location":"usage/contributing/","title":"Contributing","text":"<p>To Do</p> <p>Add docs for contributing to this package.</p>"},{"location":"usage/overview/","title":"Overview","text":"<code>ts-stat-tests</code>"},{"location":"usage/overview/#motivation","title":"Motivation","text":"<p>Time Series Analysis has been around for a long time, especially for doing Statistical Testing. Some Python packages are going a long way to make this even easier than it has ever been before. Such as <code>sktime</code> and <code>pycaret</code> and <code>pmdarima</code> and <code>statsmodels</code>.</p> <p>There are some typical Statistical Tests which are accessible in these Python (Normality, Stationarity, Correlation, etc). However, there are still some statistical tests which are not yet ported over to Python, but which have been written in R and are quite stable.</p> <p>Moreover, there is no one single library package for doing time-series statistical tests in Python.</p> <p>That's exactly what this package aims to achieve.</p> <p>A single package for doing all the standard time-series statistical tests.</p>"},{"location":"usage/overview/#tests","title":"Tests","text":"<p>Full credit goes to the packages listed in this table.</p> Type Name Source Package Source Language Implemented Correlation Auto-Correlation function (ACF) <code>statsmodels</code> Python \u2705 Correlation Partial Auto-Correlation function (PACF) <code>statsmodels</code> Python \u2705 Correlation Cross-Correlation function (CCF) <code>statsmodels</code> Python \u2705 Correlation Ljung-Box test of autocorrelation in residuals (LB) <code>statsmodels</code> Python \u2705 Correlation Lagrange Multiplier tests for autocorrelation (LM) <code>statsmodels</code> Python \u2705 Correlation Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation (BGLM) <code>statsmodels</code> Python \u2705 Regularity Approximate Entropy <code>antropy</code> python \u2705 Regularity Sample Entropy <code>antropy</code> python \u2705 Regularity Permutation Entropy <code>antropy</code> python \u2705 Regularity Spectral Entropy <code>antropy</code> python \u2705 Regularity SVD Entropy <code>antropy</code> python \u2705 Seasonality QS <code>seastests</code> R \u2705 Seasonality Osborn-Chui-Smith-Birchenhall test of seasonality (OCSB) <code>pmdarima</code> Python \u2705 Seasonality Canova-Hansen test for seasonal differences (CH) <code>pmdarima</code> Python \u2705 Seasonality Seasonal Strength <code>tsfeatures</code> Python \u2705 Seasonality Trend Strength <code>tsfeatures</code> Python \u2705 Seasonality Spikiness <code>tsfeatures</code> Python \u2705 Stability Stability <code>tsfeatures</code> Python \ud83d\udd32 Stability Lumpiness <code>tsfeatures</code> Python \ud83d\udd32 Stationarity Augmented Dickey-Fuller test for stationarity (ADF) <code>statsmodels</code> Python \u2705 Stationarity Kwiatkowski-Phillips-Schmidt-Shin test for stationarity (KPSS) <code>statsmodels</code> Python \u2705 Stationarity Range unit-root test for stationarity (RUR) <code>statsmodels</code> Python \u2705 Stationarity Zivot-Andrews structural-break unit-root test (ZA) <code>statsmodels</code> Python \u2705 Stationarity Phillips-Peron test for stationarity (PP) <code>arch</code> Python \u2705 Stationarity Elliott-Rothenberg-Stock (ERS) de-trended Dickey-Fuller test <code>arch</code> Python \u2705 Stationarity Variance Ratio (VR) test for a random walk <code>arch</code> Python \u2705 Normality Jarque-Bera test of normality (JB) <code>statsmodels</code> Python \u2705 Normality Omnibus test for normality (OB) <code>statsmodels</code> Python \u2705 Normality Shapiro-Wilk test for normality (SW) <code>scipy</code> Python \u2705 Normality D'Agostino &amp; Pearson's test for normality <code>scipy</code> Python \u2705 Normality Anderson-Darling test for normality <code>scipy</code> Python \u2705 Linearity Harvey Collier test for linearity (HC) <code>statsmodels</code> Python \ud83d\udd32 Linearity Lagrange Multiplier test for linearity (LM) <code>statsmodels</code> Python \ud83d\udd32 Linearity Rainbow test for linearity (RB) <code>statsmodels</code> Python \ud83d\udd32 Linearity Ramsey's RESET test for neglected nonlinearity (RR) <code>statsmodels</code> Python \ud83d\udd32 Heteroscedasticity Engle's Test for Autoregressive Conditional Heteroscedasticity (ARCH) <code>statsmodels</code> Python \ud83d\udd32 Heteroscedasticity Breusch-Pagan Lagrange Multiplier test for heteroscedasticity (BPL) <code>statsmodels</code> Python \ud83d\udd32 Heteroscedasticity Goldfeld-Quandt test for homoskedasticity (GQ) <code>statsmodels</code> Python \ud83d\udd32 Heteroscedasticity White's Lagrange Multiplier Test for Heteroscedasticity (WLM) <code>statsmodels</code> Python \ud83d\udd32"},{"location":"usage/overview/#known-limitations","title":"Known limitations","text":"<ul> <li>These listed tests is not exhaustive, and there is probably some more that could be added. Therefore, we encourage you to raise issues or pull requests to add more statistical tests to this suite.</li> <li>This package does not re-invent any of these tests. It merely calls the underlying packages, and calls the functions which are already written elsewhere.</li> </ul>"}]}