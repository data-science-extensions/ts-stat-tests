{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"code/","title":"Details about the tests","text":""},{"location":"code/#intro","title":"Intro","text":"<p>TL;DR</p> <p>There are a number of other really good libraries which implements these tests individually:</p> <ul> <li><code>pmdarima</code></li> <li><code>statsmodels</code></li> <li><code>arch</code></li> <li><code>tsfeatures</code></li> <li><code>antropy</code></li> <li><code>scipy</code></li> </ul> <p>These packages all implement the statistical tests in a slightly different way.  However, no one library contains all of the required tests, all in one place.</p>"},{"location":"code/#implementation-progress","title":"Implementation Progress","text":"module algorithms tests unit-tests Correlation <p>6/6   =  100%</p> <p>1/2  =  50%</p> <p>15/16   =  94%</p> Regularity <p>0/4   =   0%</p> <p>0/3  =   0%</p> <p>0/30    =   0%</p> Seasonality <p>0/6   =   0%</p> <p>0/2  =   0%</p> <p>0/10    =   0%</p> Stability <p>0/2   =   0%</p> <p>0/2  =   0%</p> <p>0/4     =   0%</p> Stationarity <p>0/7   =   0%</p> <p>0/2  =   0%</p> <p>0/44    =   0%</p> Normality <p>0/5   =   0%</p> <p>0/2  =   0%</p> <p>0/5     =   0%</p> Linearity <p>0/4   =   0%</p> <p>0/2  =   0%</p> <p>0/0     =   0%</p> Heteroscedasticity <p>0/4   =   0%</p> <p>0/2  =   0%</p> <p>0/0     =   0%</p> Overall <p>6/38  =  16%</p> <p>1/17 =   6%</p> <p>15/106 =  14%</p>"},{"location":"code/#tests","title":"Tests","text":"<p>Details</p> <p>Legend:</p> icon description \u2705 Already implemented in this package \ud83d\udd32 To be developed and implemented \u274e Will not be implemented as it is covered by a function from a different package Test InfoPython Import category algorithm library:test Correlation Auto-Correlation function (ACF) \u2705<code>statsmodels</code>:<code>acf()</code>\u274e<code>pmdarima</code>:<code>acf()</code> Correlation Partial Auto-Correlation function (PACF) \u2705<code>statsmodels</code>:<code>pacf()</code>\u274e<code>pmdarima</code>:<code>pacf()</code> Correlation Cross-Correlation function (CCF) \u2705<code>statsmodels</code>:<code>ccf()</code> Correlation Ljung-Box test of autocorrelation in residuals (LB) \u2705<code>statsmodels</code>:<code>acorr_ljungbox()</code> Correlation Lagrange Multiplier tests for autocorrelation (LM) \u2705<code>statsmodels</code>:<code>acorr_lm()</code> Correlation Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation (BGLM) \u2705<code>statsmodels</code>:<code>acorr_breusch_godfrey()</code> Regularity Approximate Entropy \u2705<code>antropy</code>:<code>app_entropy()</code> Regularity Sample Entropy \u2705<code>antropy</code>:<code>sample_entropy()</code> Regularity Permutation Entropy \u2705<code>antropy</code>:<code>perm_entropy()</code> Regularity Spectral Entropy \u2705<code>antropy</code>:<code>spectral_entropy()</code> Seasonality QS \u2705<code>seastests</code>:<code>qs()</code> Seasonality Osborn-Chui-Smith-Birchenhall test of seasonality (OCSB) \u2705<code>pmdarima</code>:<code>OCSBTest()</code> Seasonality Canova-Hansen test for seasonal differences (CH) \u2705<code>pmdarima</code>:<code>CHTest()</code> Seasonality Seasonal Strength \u2705<code>tsfeatures</code>:<code>stl_features()</code> Seasonality Trend Strength \u2705<code>tsfeatures</code>:<code>stl_features()</code> Seasonality Spikiness \u2705<code>tsfeatures</code>:<code>stl_features()</code> Stability Stability \u2705<code>tsfeatures</code>:<code>stability()</code> Stability Lumpiness \u2705<code>tsfeatures</code>:<code>lumpiness()</code> Stationarity Augmented Dickey-Fuller test for stationarity (ADF) \u2705<code>statsmodels</code>:<code>adfuller()</code>\u274e<code>pmdarima</code>:<code>ADFTest()</code>\u274e<code>arch</code>:<code>ADF()</code> Stationarity Kwiatkowski-Phillips-Schmidt-Shin test for stationarity (KPSS) \u2705<code>statsmodels</code>:<code>kpss()</code>\u274e<code>pmdarima</code>:<code>KPSSTest()</code>\u274e<code>arch</code>:<code>KPSS()</code> Stationarity Range unit-root test for stationarity (RUR) \u2705<code>statsmodels</code>:<code>range_unit_root_test()</code> Stationarity Zivot-Andrews structural-break unit-root test (ZA) \u2705<code>statsmodels</code>:<code>zivot_andrews()</code>\u274e<code>arch</code>:<code>ZivotAndrews()</code> Stationarity Phillips-Peron test for stationarity (PP) \u2705<code>pmdarima</code>:<code>PPTest()</code>\u274e<code>arch</code>:<code>PhillipsPerron()</code> Stationarity Elliott-Rothenberg-Stock (ERS) de-trended Dickey-Fuller test \u2705<code>arch</code>:<code>DFGLS()</code> Stationarity Variance Ratio (VR) test for a random walk \u2705<code>arch</code>:<code>VarianceRatio()</code> Normality Jarque-Bera test of normality (JB) \u2705<code>statsmodels</code>:<code>jarque_bera()</code> Normality Omnibus test for normality (OB) \u2705<code>statsmodels</code>:<code>omni_normtest()</code> Normality Shapiro-Wilk test for normality (SW) \u2705<code>scipy</code>:<code>shapiro()</code> Normality D'Agostino &amp; Pearson's test for normality (DP) \u2705<code>scipy</code>:<code>normaltest()</code> Normality Anderson-Darling test for normality (AD) \u2705<code>scipy</code>:<code>anderson()</code> Linearity Harvey Collier test for linearity (HC) \ud83d\udd32<code>statsmodels</code>:<code>linear_harvey_collier()</code> Linearity Lagrange Multiplier test for linearity (LM) \ud83d\udd32<code>statsmodels</code>:<code>linear_lm()</code> Linearity Rainbow test for linearity (RB) \ud83d\udd32<code>statsmodels</code>:<code>linear_rainbow()</code> Linearity Ramsey's RESET test for neglected nonlinearity (RR) \ud83d\udd32<code>statsmodels</code>:<code>linear_reset()</code> Heteroscedasticity Engle's Test for Autoregressive Conditional Heteroscedasticity (ARCH) \ud83d\udd32<code>statsmodels</code>:<code>het_arch()</code> Heteroscedasticity Breusch-Pagan Lagrange Multiplier test for heteroscedasticity (BPL) \ud83d\udd32<code>statsmodels</code>:<code>het_breuschpagan()</code> Heteroscedasticity Goldfeld-Quandt test for homoskedasticity (GQ) \ud83d\udd32<code>statsmodels</code>:<code>het_goldfeldquandt()</code> Heteroscedasticity White's Lagrange Multiplier Test for Heteroscedasticity (WLM) \ud83d\udd32<code>statsmodels</code>:<code>het_white()</code> Covariance ... test library:import ADF pmdarima: <code>from pmdarima.arima import ADFTest</code>statsmodels: <code>from statsmodels.tsa.stattools import adfuller</code>arch: <code>from arch.unitroot import ADF</code> KPSS pmdarima: <code>from pmdarima.arima import KPSSTest</code>statsmodels: <code>from statsmodels.tsa.stattools import kpss</code>arch: <code>from arch.unitroot import KPSS</code> PP pmdarima: <code>from pmdarima.arima import PPTest</code>arch: <code>from arch.unitroot import PhillipsPerron</code> RUR pmdarima: <code>from statsmodels.tsa.stattools import range_unit_root_test</code> ZA pmdarima: <code>from statsmodels.tsa.stattools import zivot_andrews</code> arch: <code>from arch.unitroot import ZivotAndrews</code> OCSB pmdarima: <code>from pmdarima.arima import OCSBTest</code> CH pmdarima: <code>from pmdarima.arima import CHTest</code> ACF pmdarima: <code>from pmdarima.utils import acf</code>statsmodels: <code>from statsmodels.tsa.stattools import acf</code> PACF pmdarima: <code>from pmdarima.utils import pacf</code>statsmodels: <code>from statsmodels.tsa.stattools import pacf</code> CCF statsmodels: <code>from statsmodels.tsa.stattools import ccf</code> ALB statsmodels: <code>from statsmodels.stats.diagnostic import acorr_ljungbox</code> ALM statsmodels: <code>from statsmodels.stats.diagnostic import acorr_lm</code> ABG statsmodels: <code>from statsmodels.stats.diagnostic import acorr_breusch_godfrey</code> JB statsmodels: <code>from statsmodels.stats.stattools import jarque_bera</code> OB statsmodels: <code>from statsmodels.stats.stattools import omni_normtest</code> HC statsmodels: <code>from statsmodels.stats.diagnostic import linear_harvey_collier</code> LM statsmodels: <code>from statsmodels.stats.diagnostic import linear_lm</code> RB statsmodels: <code>from statsmodels.stats.diagnostic import linear_rainbow</code> RR statsmodels: <code>from statsmodels.stats.diagnostic import linear_reset</code> ARCH statsmodels: <code>from statsmodels.stats.diagnostic import het_arch</code> BPL statsmodels: <code>from statsmodels.stats.diagnostic import het_breuschpagan</code> GQ statsmodels: <code>from statsmodels.stats.diagnostic import het_goldfeldquandt</code> WLM statsmodels: <code>from statsmodels.stats.diagnostic import het_white</code>"},{"location":"code/correlation/","title":"Test the <code>correlation</code> of a given Time-Series Dataset","text":""},{"location":"code/correlation/#introduction","title":"Introduction","text":"<p>Summary</p> <p>As stated by Anais Dotis-Georgiou:</p> <p>The term autocorrelation refers to the degree of similarity between A) a given time series, and B) a lagged version of itself, over C) successive time intervals. In other words, autocorrelation is intended to measure the relationship between a variable's present value and any past values that you may have access to.</p> <p>Therefore, a time series autocorrelation attempts to measure the current values of a variable against the historical data of that variable. It ultimately plots one series over the other, and determines the degree of similarity between the two.</p> <p>For the sake of comparison, autocorrelation is essentially the exact same process that you would go through when calculating the correlation between two different sets of time series values on your own. The major difference here is that autocorrelation uses the same time series two times: once in its original values, and then again once a few different time periods have occurred.</p> <p>Autocorrelation is also known as serial correlation, time series correlation and lagged correlation. Regardless of how it's being used, autocorrelation is an ideal method for uncovering trends and patterns in time series data that would have otherwise gone undiscovered.</p> <p> For more info, see: InfluxData: Autocorrelation in Time Series Data.</p> <p>Info</p> <p>An important test to do on Time-Series data is to measure it's level of Auto-Correlation (McMurry &amp; Politis, 2010; Hyndman, nd.(b)). While 'correlation' refers to how two variables change based on the other's value, 'auto-correlation' is how a variable changes based on it's own value over time (the phrase \"auto\" refers to \"self\"). For the Auto-Correlation Function, it uses a '<code>lag</code>' function. For example, a lag value of <code>0</code> is 100% correlated, which is logical, because that is it's own value; whereas a lag value of <code>1</code> or greater, the level of auto-correlation decreases as it gets further away from <code>lag0</code>.</p> <p>For well-structured time-series data sets, it would be expected to see a conical-shaped Auto-Correlation plot. If it were not a well-structured time-series data set, then this Auto-Correlation plot would look more like white noise, and there would not be any logical shape. The blue dotted lines are included as a reference point for determining if any of the observations are significantly different from zero.</p> <p>Moreover, analysis of the data's Auto-Correlation (ACF) should be combined with analysis of its Partial Auto-Correlation (PACF). While the ACF is the \"direct\" relationship between an observation and it's relevant lag observation, the PACF removes the \"indirect\" relationship between these observations. Effectively, the Partial Auto-Correlation between <code>lag1</code> and <code>lag5</code> is the \"actual\" correlation between these two observations, after removing the influence that <code>lag2</code>, <code>lag3</code>, and <code>lag4</code> has on <code>lag5</code>.</p> <p>What this means is that the Partial Auto-Correlation plot would have a very high value at <code>lag0</code>, which will drop very quickly at <code>lag1</code>, and should remain below the blue reference lines for the remainder of the Correlogram. The observations of <code>lag&gt;0</code> should resemble white noise data points. If it does not resemble white noise, and there is a distinct pattern occurring, then the data is not suitable for time-series forecasting.</p> <p> For more info, see: Time Series Analysis in Python: A Comprehensive Guide with Examples.</p> <p>Source Library</p> <p>The <code>statsmodels</code> package was chosen because it provides mature, well-tested implementations of core time-series tools (such as ACF, PACF, and correlograms), integrates seamlessly with NumPy and pandas data structures, and offers a comprehensive suite of statistical tests that align closely with the methods demonstrated in this project.</p> <p>Source Module</p> <p>All of the source code can be found within this modules:</p> <ul> <li><code>src.ts_stat_tests.algorithms.correlation</code>.</li> <li><code>src.ts_stat_tests.tests.correlation</code>.</li> </ul>"},{"location":"code/correlation/#correlation-tests","title":"Correlation Tests","text":""},{"location":"code/correlation/#ts_stat_tests.tests.correlation","title":"ts_stat_tests.tests.correlation","text":"<p>Summary</p> <p>This module contains tests for the correlation functions defined in the <code>ts_stat_tests.algorithms.correlation</code> module.</p>"},{"location":"code/correlation/#ts_stat_tests.tests.correlation.correlation","title":"correlation","text":"<pre><code>correlation(\n    x: ArrayLike,\n    algorithm: Literal[\"acf\", \"auto\", \"ac\"],\n    **kwargs: Any\n) -&gt; Union[np.ndarray, tuple[np.ndarray, ...]]\n</code></pre><pre><code>correlation(\n    x: ArrayLike1D,\n    algorithm: Literal[\"pacf\", \"partial\", \"pc\"],\n    **kwargs: Any\n) -&gt; Union[np.ndarray, tuple[np.ndarray, ...]]\n</code></pre><pre><code>correlation(\n    x: ArrayLike,\n    algorithm: Literal[\n        \"ccf\", \"cross\", \"cross-correlation\", \"cc\"\n    ],\n    **kwargs: Any\n) -&gt; Union[np.ndarray, tuple[np.ndarray, ...]]\n</code></pre><pre><code>correlation(\n    x: ArrayLike,\n    algorithm: Literal[\n        \"lb\",\n        \"alb\",\n        \"acorr_ljungbox\",\n        \"acor_lb\",\n        \"a_lb\",\n        \"ljungbox\",\n    ],\n    **kwargs: Any\n) -&gt; pd.DataFrame\n</code></pre><pre><code>correlation(\n    x: ArrayLike,\n    algorithm: Literal[\"lm\", \"alm\", \"acorr_lm\", \"a_lm\"],\n    **kwargs: Any\n) -&gt; Union[\n    tuple[float, float, float, float],\n    tuple[float, float, float, float, ResultsStore],\n]\n</code></pre><pre><code>correlation(\n    x: Union[RegressionResults, RegressionResultsWrapper],\n    algorithm: Literal[\"bglm\", \"breusch_godfrey\", \"bg\"],\n    **kwargs: Any\n) -&gt; Union[\n    tuple[float, float, float, float],\n    tuple[float, float, float, float, ResultsStore],\n]\n</code></pre> <pre><code>correlation(\n    x: Union[\n        ArrayLike,\n        ArrayLike1D,\n        RegressionResults,\n        RegressionResultsWrapper,\n    ],\n    algorithm: str = \"acf\",\n    **kwargs: Any\n) -&gt; Any\n</code></pre> <p>Summary</p> <p>A unified interface for various correlation tests.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[ArrayLike, ArrayLike1D, RegressionResults, RegressionResultsWrapper]</code> <p>The input time series data or regression results.</p> required <code>algorithm</code> <code>str</code> <p>The correlation algorithm to use. Options include: - \"acf\", \"auto\", \"ac\": Autocorrelation Function - \"pacf\", \"partial\", \"pc\": Partial Autocorrelation Function - \"ccf\", \"cross\", \"cross-correlation\", \"cc\": Cross-Correlation Function - \"lb\", \"alb\", \"acorr_ljungbox\", \"acor_lb\", \"a_lb\", \"ljungbox\": Ljung-Box Test - \"lm\", \"alm\", \"acorr_lm\", \"a_lm\": Lagrange Multiplier Test - \"bglm\", \"breusch_godfrey\", \"bg\": Breusch-Godfrey Test</p> <code>'acf'</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments specific to the chosen algorithm.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>The result of the specified correlation test.</p> Source code in <code>src/ts_stat_tests/tests/correlation.py</code> <pre><code>@typechecked\ndef correlation(\n    x: Union[ArrayLike, ArrayLike1D, RegressionResults, RegressionResultsWrapper],\n    algorithm: str = \"acf\",\n    **kwargs: Any,\n) -&gt; Any:\n    \"\"\"\n    !!! note \"Summary\"\n        A unified interface for various correlation tests.\n\n    Params:\n        x (Union[ArrayLike, ArrayLike1D, RegressionResults, RegressionResultsWrapper]):\n            The input time series data or regression results.\n        algorithm (str):\n            The correlation algorithm to use. Options include:\n            - \"acf\", \"auto\", \"ac\": Autocorrelation Function\n            - \"pacf\", \"partial\", \"pc\": Partial Autocorrelation Function\n            - \"ccf\", \"cross\", \"cross-correlation\", \"cc\": Cross-Correlation Function\n            - \"lb\", \"alb\", \"acorr_ljungbox\", \"acor_lb\", \"a_lb\", \"ljungbox\": Ljung-Box Test\n            - \"lm\", \"alm\", \"acorr_lm\", \"a_lm\": Lagrange Multiplier Test\n            - \"bglm\", \"breusch_godfrey\", \"bg\": Breusch-Godfrey Test\n        kwargs (Any):\n            Additional keyword arguments specific to the chosen algorithm.\n\n    Returns:\n        (Any):\n            The result of the specified correlation test.\n    \"\"\"\n\n    options: dict[str, tuple[str, ...]] = {\n        \"acf\": (\"acf\", \"auto\", \"ac\"),\n        \"pacf\": (\"pacf\", \"partial\", \"pc\"),\n        \"ccf\": (\"ccf\", \"cross\", \"cross-correlation\", \"cc\"),\n        \"lb\": (\"alb\", \"acorr_ljungbox\", \"acor_lb\", \"a_lb\", \"lb\", \"ljungbox\"),\n        \"lm\": (\"alm\", \"acorr_lm\", \"a_lm\", \"lm\"),\n        \"bglm\": (\"bglm\", \"breusch_godfrey\", \"bg\"),\n    }\n\n    if algorithm in options[\"acf\"]:\n        return _acf(x=x, **kwargs)  # type: ignore\n\n    if algorithm in options[\"pacf\"]:\n        return _pacf(x=x, **kwargs)  # type: ignore\n\n    if algorithm in options[\"lb\"]:\n        return _lb(x=x, **kwargs)  # type: ignore\n\n    if algorithm in options[\"lm\"]:\n        return _lm(resid=x, **kwargs)  # type: ignore\n\n    if algorithm in options[\"ccf\"]:\n        if \"y\" not in kwargs or kwargs[\"y\"] is None:\n            raise ValueError(\"The 'ccf' algorithm requires a 'y' parameter.\")\n        return _ccf(x=x, **kwargs)  # type: ignore\n\n    if algorithm in options[\"bglm\"]:\n        return _bglm(res=x, **kwargs)  # type: ignore\n\n    raise ValueError(\n        generate_error_message(\n            parameter_name=\"algorithm\",\n            value_parsed=algorithm,\n            options=options,\n        )\n    )\n</code></pre>"},{"location":"code/correlation/#ts_stat_tests.tests.correlation.is_correlated","title":"is_correlated","text":"<pre><code>is_correlated() -&gt; None\n</code></pre> <p>Summary</p> <p>A placeholder function for checking if a time series is correlated.</p> Source code in <code>src/ts_stat_tests/tests/correlation.py</code> <pre><code>@typechecked\ndef is_correlated() -&gt; None:\n    \"\"\"\n    !!! note \"Summary\"\n        A placeholder function for checking if a time series is correlated.\n    \"\"\"\n    raise NotImplementedError(\"is_correlated is a placeholder and has not been implemented yet.\")\n</code></pre>"},{"location":"code/correlation/#correlation-algorithms","title":"Correlation Algorithms","text":""},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation","title":"ts_stat_tests.algorithms.correlation","text":"<p>Summary</p> <p>The correlation algorithms module provides functions to compute correlation measures for time series data, including the autocorrelation function (ACF), partial autocorrelation function (PACF), and cross-correlation function (CCF). These measures help identify relationships and dependencies between time series variables, which are essential for time series analysis and forecasting.</p> <p>This module leverages the <code>statsmodels</code> library to implement these correlation measures, ensuring robust and efficient computations. The functions are designed to handle various input scenarios and provide options for customization, such as specifying the number of lags, confidence intervals, and handling missing data.</p> <p>By using these correlation algorithms, users can gain insights into the temporal dependencies within their time series data, aiding in model selection and improving forecasting accuracy.</p>"},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation.acf","title":"acf","text":"<pre><code>acf(\n    x: ArrayLike,\n    adjusted: bool = False,\n    nlags: Optional[int] = None,\n    fft: bool = True,\n    bartlett_confint: bool = True,\n    missing: VALID_ACF_MISSING_OPTIONS = \"none\",\n    *,\n    qstat: Literal[False] = False,\n    alpha: None = None\n) -&gt; np.ndarray\n</code></pre><pre><code>acf(\n    x: ArrayLike,\n    adjusted: bool = False,\n    nlags: Optional[int] = None,\n    fft: bool = True,\n    bartlett_confint: bool = True,\n    missing: VALID_ACF_MISSING_OPTIONS = \"none\",\n    *,\n    qstat: Literal[False] = False,\n    alpha: float\n) -&gt; tuple[np.ndarray, np.ndarray]\n</code></pre><pre><code>acf(\n    x: ArrayLike,\n    adjusted: bool = False,\n    nlags: Optional[int] = None,\n    fft: bool = True,\n    bartlett_confint: bool = True,\n    missing: VALID_ACF_MISSING_OPTIONS = \"none\",\n    *,\n    qstat: Literal[True],\n    alpha: None = None\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]\n</code></pre><pre><code>acf(\n    x: ArrayLike,\n    adjusted: bool = False,\n    nlags: Optional[int] = None,\n    fft: bool = True,\n    bartlett_confint: bool = True,\n    missing: VALID_ACF_MISSING_OPTIONS = \"none\",\n    *,\n    qstat: Literal[True],\n    alpha: float\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n</code></pre> <pre><code>acf(\n    x: ArrayLike,\n    adjusted: bool = False,\n    nlags: Optional[int] = None,\n    fft: bool = True,\n    bartlett_confint: bool = True,\n    missing: VALID_ACF_MISSING_OPTIONS = \"none\",\n    *,\n    qstat: bool = False,\n    alpha: Optional[float] = None\n) -&gt; Union[\n    np.ndarray,\n    tuple[np.ndarray, np.ndarray],\n    tuple[np.ndarray, np.ndarray, np.ndarray],\n    tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n]\n</code></pre> <p>Summary</p> <p>The autocorrelation function (ACF) is a statistical tool used to study the correlation between a time series and its lagged values. In time series forecasting, the ACF is used to identify patterns and relationships between values in a time series at different lags, which can then be used to make predictions about future values.</p> <p>This function will implement the <code>acf()</code> function from the <code>statsmodels</code> library.</p> Details <p>The acf at lag <code>0</code> (ie., <code>1</code>) is returned.</p> <p>For very long time series it is recommended to use <code>fft</code> convolution instead. When <code>fft</code> is <code>False</code> uses a simple, direct estimator of the autocovariances that only computes the first \\(nlag + 1\\) values. This can be much faster when the time series is long and only a small number of autocovariances are needed.</p> <p>If <code>adjusted</code> is <code>True</code>, the denominator for the autocovariance is adjusted for the loss of data.</p> <p>The ACF measures the correlation between a time series and its lagged values at different lags. The correlation is calculated as the ratio of the covariance between the series and its lagged values to the product of their standard deviations. The ACF is typically plotted as a graph, with the lag on the <code>x</code>-axis and the correlation coefficient on the <code>y</code>-axis.</p> <p>The ACF at lag \\(k\\) is defined as:</p> \\[ ACF(k) = \\frac{ Cov(Y_t, Y_{t-k}) } { Var(Y_t) \\times Var(Y_{t-k}) } \\] <p>where:</p> <ul> <li>\\(Y_t\\) and \\(Y_{t-k}\\) are the values of the time series at time \\(t\\) and time \\(t-k\\), respectively,</li> <li>\\(Cov(Y_t, Y_{t-k})\\) is the covariance between the two values, and</li> <li>\\(Var(Y_t)\\) and \\(Var(Y_{t-k})\\) are the variances of the two values.</li> </ul> <pre><code>ACF(k) = Cov(Y_t, Y_{t-k}) / (sqrt(Var(Y_t)) * sqrt(Var(Y_{t-k})))\n</code></pre> <p>For a stationary series, this simplifies to:</p> <pre><code>ACF(k) = Cov(Y_t, Y_{t-k}) / Var(Y_t)\n</code></pre> <p>If the ACF shows a strong positive correlation at lag \\(k\\), this means that values in the time series at time \\(t\\) and time \\(t-k\\) are strongly related. This can be useful in forecasting, as it suggests that past values can be used to predict future values. If the ACF shows a strong negative correlation at lag \\(k\\), this means that values at time \\(t\\) and time \\(t-k\\) are strongly inversely related, which can also be useful in forecasting.</p> <p>The ACF can be used to identify the order of an autoregressive (AR) model, which is a type of model used in time series forecasting. The order of an AR model is the number of lags that are used to predict future values. The ACF can also be used to diagnose the presence of seasonality in a time series.</p> <p>Overall, the autocorrelation function is a valuable tool in time series forecasting, as it helps to identify patterns and relationships between values in a time series that can be used to make predictions about future values.</p> <p>The ACF can be calculated using the <code>acf()</code> function in the <code>statsmodels</code> package in Python. The function takes a time series array as input and returns an array of autocorrelation coefficients at different lags. The significance of the autocorrelation coefficients can be tested using the Ljung-Box test, which tests the null hypothesis that the autocorrelation coefficients are zero up to a certain lag. The Ljung-Box test can be performed using the <code>acorr_ljungbox()</code> function in the <code>statsmodels</code> package. If the p-value of the test is less than a certain significance level (e.g. \\(0.05\\)), then there is evidence of significant autocorrelation in the time series up to the specified lag.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The time series data.</p> required <code>adjusted</code> <code>bool</code> <p>If <code>True</code>, then denominators for auto-covariance are \\(n-k\\), otherwise \\(n\\). Defaults to <code>False</code>.</p> <code>False</code> <code>nlags</code> <code>Optional[int]</code> <p>Number of lags to return autocorrelation for. If not provided, uses \\(\\min(10 \\times \\text{log10}(nobs),nobs-1)\\) (calculated with: <code>min(int(10 * np.log10(nobs)), nobs - 1)</code>). The returned value includes \\(lag 0\\) (ie., \\(1\\)) so size of the acf vector is \\((nlags + 1,)\\). Defaults to <code>None</code>.</p> <code>None</code> <code>qstat</code> <code>bool</code> <p>If <code>True</code>, also returns the Ljung-Box \\(q\\) statistic and corresponding p-values for each autocorrelation coefficient; see the Returns section for details. Defaults to <code>False</code>.</p> <code>False</code> <code>fft</code> <code>bool</code> <p>If <code>True</code>, computes the ACF via FFT. Defaults to <code>True</code>.</p> <code>True</code> <code>alpha</code> <code>Optional[float]</code> <p>If a number is given, the confidence intervals for the given level are returned. For instance if <code>alpha=0.05</code>, a \\(95\\%\\) confidence intervals are returned where the standard deviation is computed according to Bartlett\"s formula. Defaults to <code>None</code>.</p> <code>None</code> <code>bartlett_confint</code> <code>bool</code> <p>Confidence intervals for ACF values are generally placed at 2 standard errors around \\(r_k\\). The formula used for standard error depends upon the situation. If the autocorrelations are being used to test for randomness of residuals as part of the ARIMA routine, the standard errors are determined assuming the residuals are white noise. The approximate formula for any lag is that standard error of each \\(r_k = \\frac{1}{\\sqrt{N}}\\). See section 9.4 of [2] for more details on the \\(\\frac{1}{\\sqrt{N}}\\) result. For more elementary discussion, see section 5.3.2 in [3]. For the ACF of raw data, the standard error at a lag \\(k\\) is found as if the right model was an \\(\\text{MA}(k-1)\\). This allows the possible interpretation that if all autocorrelations past a certain lag are within the limits, the model might be an \\(\\text{MA}\\) of order defined by the last significant autocorrelation. In this case, a moving average model is assumed for the data and the standard errors for the confidence intervals should be generated using Bartlett's formula. For more details on Bartlett formula result, see section 7.2 in [2]. Defaults to <code>True</code>.</p> <code>True</code> <code>missing</code> <code>VALID_ACF_MISSING_OPTIONS</code> <p>A string in <code>[\"none\", \"raise\", \"conservative\", \"drop\"]</code> specifying how the <code>NaN</code>'s are to be treated.</p> <ul> <li><code>\"none\"</code> performs no checks.</li> <li><code>\"raise\"</code> raises an exception if NaN values are found.</li> <li><code>\"drop\"</code> removes the missing observations and then estimates the autocovariances treating the non-missing as contiguous.</li> <li><code>\"conservative\"</code> computes the autocovariance using nan-ops so that nans are removed when computing the mean and cross-products that are used to estimate the autocovariance.</li> </ul> <p>When using <code>\"conservative\"</code>, \\(n\\) is set to the number of non-missing observations. Defaults to <code>\"none\"</code>.</p> <code>'none'</code> <p>Returns:</p> Name Type Description <code>acf</code> <code>ndarray</code> <p>The autocorrelation function for lags <code>0, 1, ..., nlags</code>. Shape <code>(nlags+1,)</code>.</p> <code>confint</code> <code>Optional[ndarray]</code> <p>Confidence intervals for the ACF at lags <code>0, 1, ..., nlags</code>. Shape <code>(nlags + 1, 2)</code>. Returned if <code>alpha</code> is not <code>None</code>.</p> <code>qstat</code> <code>Optional[ndarray]</code> <p>The Ljung-Box Q-Statistic for lags <code>1, 2, ..., nlags</code> (excludes lag zero). Returned if <code>qstat</code> is <code>True</code>.</p> <code>pvalues</code> <code>Optional[ndarray]</code> <p>The p-values associated with the Q-statistics for lags <code>1, 2, ..., nlags</code> (excludes lag zero). Returned if <code>qstat</code> is <code>True</code>.</p> <p>Credit</p> <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> <p>Examples</p> Test ACF without FFT<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from statsmodels.datasets import macrodata\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import acf\n&gt;&gt;&gt; data = macrodata.load_pandas()\n&gt;&gt;&gt; x = data.data[\"realgdp\"]\n&gt;&gt;&gt; res_acf, res_confint, res_qstat, res_pvalues = acf(\n...     x, nlags=40, qstat=True, alpha=0.05, fft=False\n... )\n&gt;&gt;&gt; pprint(res_acf[1:11])\narray([0.94804734, 0.87557484, 0.80668116, 0.75262542, 0.71376997,\n       0.6817336 , 0.66290439, 0.65561048, 0.67094833, 0.70271992])\n&gt;&gt;&gt; pprint(res_confint[1:11])\narray([[0.78471701, 1.11137767],\n       [0.60238868, 1.14876099],\n       [0.46677939, 1.14658292],\n       [0.36500159, 1.14024925],\n       [0.28894752, 1.13859242],\n       [0.22604068, 1.13742653],\n       [0.18077091, 1.14503787],\n       [0.14974636, 1.16147461],\n       [0.1429036 , 1.19899305],\n       [0.15240228, 1.25303756]])\n&gt;&gt;&gt; pprint(res_qstat[:10])\narray([132.14153858, 245.64616028, 342.67482586, 427.73868355,\n       504.79657041, 575.6018536 , 643.03859337, 709.48449817,\n       779.59123116, 857.06863862])\n&gt;&gt;&gt; pprint(res_pvalues[:10])\narray([1.39323140e-030, 4.55631819e-054, 5.75108846e-074, 2.81773062e-091,\n       7.36019524e-107, 4.26400770e-121, 1.30546283e-134, 6.49627091e-148,\n       5.24937010e-162, 1.10078935e-177])\n</code></pre> Test ACF with FFT<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import acf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_acf, res_confint, res_qstat, res_pvalues = acf(\n...     data, nlags=40, qstat=True, alpha=0.05, fft=True\n... )\n&gt;&gt;&gt; pprint(res_acf[1:11])\narray([0.94804734, 0.87557484, 0.80668116, 0.75262542, 0.71376997,\n       0.6817336 , 0.66290439, 0.65561048, 0.67094833, 0.70271992])\n&gt;&gt;&gt; pprint(res_confint[1:11])\narray([[0.78471701, 1.11137767],\n       [0.60238868, 1.14876099],\n       [0.46677939, 1.14658292],\n       [0.36500159, 1.14024925],\n       [0.28894752, 1.13859242],\n       [0.22604068, 1.13742653],\n       [0.18077091, 1.14503787],\n       [0.14974636, 1.16147461],\n       [0.1429036 , 1.19899305],\n       [0.15240228, 1.25303756]])\n&gt;&gt;&gt; pprint(res_qstat[:10])\narray([132.14153858, 245.64616028, 342.67482586, 427.73868355,\n       504.79657041, 575.6018536 , 643.03859337, 709.48449817,\n       779.59123116, 857.06863862])\n&gt;&gt;&gt; pprint(res_pvalues[:10])\narray([1.39323140e-030, 4.55631819e-054, 5.75108846e-074, 2.81773062e-091,\n       7.36019524e-107, 4.26400770e-121, 1.30546283e-134, 6.49627091e-148,\n       5.24937010e-162, 1.10078935e-177])\n</code></pre> References <ol> <li>Parzen, E., 1963. On spectral analysis with missing observations and amplitude modulation. Sankhya: The Indian Journal of Statistics, Series A, pp.383-392.</li> <li>Brockwell and Davis, 1987. Time Series Theory and Methods.</li> <li>Brockwell and Davis, 2010. Introduction to Time Series and Forecasting, 2nd edition.</li> </ol> See Also <ul> <li><code>statsmodels.tsa.stattools.acf</code>: Estimate the autocorrelation function.</li> <li><code>statsmodels.tsa.stattools.pacf</code>: Partial autocorrelation estimation.</li> <li><code>statsmodels.tsa.stattools.ccf</code>: The cross-correlation function.</li> <li><code>ts_stat_tests.algorithms.correlation.acf</code>: Estimate the autocorrelation function</li> <li><code>ts_stat_tests.algorithms.correlation.pacf</code>: Partial autocorrelation estimate.</li> <li><code>ts_stat_tests.algorithms.correlation.ccf</code>: The cross-correlation function.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/correlation.py</code> <pre><code>@typechecked\ndef acf(\n    x: ArrayLike,\n    adjusted: bool = False,\n    nlags: Optional[int] = None,\n    fft: bool = True,\n    bartlett_confint: bool = True,\n    missing: VALID_ACF_MISSING_OPTIONS = \"none\",\n    *,\n    qstat: bool = False,\n    alpha: Optional[float] = None,\n) -&gt; Union[\n    np.ndarray,\n    tuple[np.ndarray, np.ndarray],\n    tuple[np.ndarray, np.ndarray, np.ndarray],\n    tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray],\n]:\n    r\"\"\"\n    !!! note \"Summary\"\n\n        The autocorrelation function (ACF) is a statistical tool used to study the correlation between a time series and its lagged values. In time series forecasting, the ACF is used to identify patterns and relationships between values in a time series at different lags, which can then be used to make predictions about future values.\n\n        This function will implement the [`acf()`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.acf.html) function from the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ???+ abstract \"Details\"\n\n        The acf at lag `0` (ie., `1`) is returned.\n\n        For very long time series it is recommended to use `fft` convolution instead. When `fft` is `False` uses a simple, direct estimator of the autocovariances that only computes the first $nlag + 1$ values. This can be much faster when the time series is long and only a small number of autocovariances are needed.\n\n        If `adjusted` is `True`, the denominator for the autocovariance is adjusted for the loss of data.\n\n        The ACF measures the correlation between a time series and its lagged values at different lags. The correlation is calculated as the ratio of the covariance between the series and its lagged values to the product of their standard deviations. The ACF is typically plotted as a graph, with the lag on the `x`-axis and the correlation coefficient on the `y`-axis.\n\n        The ACF at lag $k$ is defined as:\n\n        $$\n        ACF(k) = \\frac{ Cov(Y_t, Y_{t-k}) } { Var(Y_t) \\times Var(Y_{t-k}) }\n        $$\n\n        where:\n\n        - $Y_t$ and $Y_{t-k}$ are the values of the time series at time $t$ and time $t-k$, respectively,\n        - $Cov(Y_t, Y_{t-k})$ is the covariance between the two values, and\n        - $Var(Y_t)$ and $Var(Y_{t-k})$ are the variances of the two values.\n\n        ```\n        ACF(k) = Cov(Y_t, Y_{t-k}) / (sqrt(Var(Y_t)) * sqrt(Var(Y_{t-k})))\n        ```\n\n        For a stationary series, this simplifies to:\n\n        ```\n        ACF(k) = Cov(Y_t, Y_{t-k}) / Var(Y_t)\n        ```\n\n        If the ACF shows a strong positive correlation at lag $k$, this means that values in the time series at time $t$ and time $t-k$ are strongly related. This can be useful in forecasting, as it suggests that past values can be used to predict future values. If the ACF shows a strong negative correlation at lag $k$, this means that values at time $t$ and time $t-k$ are strongly inversely related, which can also be useful in forecasting.\n\n        The ACF can be used to identify the order of an autoregressive (AR) model, which is a type of model used in time series forecasting. The order of an AR model is the number of lags that are used to predict future values. The ACF can also be used to diagnose the presence of seasonality in a time series.\n\n        Overall, the autocorrelation function is a valuable tool in time series forecasting, as it helps to identify patterns and relationships between values in a time series that can be used to make predictions about future values.\n\n        The ACF can be calculated using the `acf()` function in the `statsmodels` package in Python. The function takes a time series array as input and returns an array of autocorrelation coefficients at different lags. The significance of the autocorrelation coefficients can be tested using the Ljung-Box test, which tests the null hypothesis that the autocorrelation coefficients are zero up to a certain lag. The Ljung-Box test can be performed using the `acorr_ljungbox()` function in the `statsmodels` package. If the p-value of the test is less than a certain significance level (e.g. $0.05$), then there is evidence of significant autocorrelation in the time series up to the specified lag.\n\n    Params:\n        x (ArrayLike):\n            The time series data.\n        adjusted (bool, optional):\n            If `True`, then denominators for auto-covariance are $n-k$, otherwise $n$.&lt;br&gt;\n            Defaults to `False`.\n        nlags (Optional[int], optional):\n            Number of lags to return autocorrelation for. If not provided, uses $\\min(10 \\times \\text{log10}(nobs),nobs-1)$ (calculated with: `min(int(10 * np.log10(nobs)), nobs - 1)`). The returned value includes $lag 0$ (ie., $1$) so size of the acf vector is $(nlags + 1,)$.&lt;br&gt;\n            Defaults to `None`.\n        qstat (bool, optional):\n            If `True`, also returns the Ljung-Box $q$ statistic and corresponding p-values for each autocorrelation coefficient; see the *Returns* section for details.&lt;br&gt;\n            Defaults to `False`.\n        fft (bool, optional):\n            If `True`, computes the ACF via FFT.&lt;br&gt;\n            Defaults to `True`.\n        alpha (Optional[float], optional):\n            If a number is given, the confidence intervals for the given level are returned. For instance if `alpha=0.05`, a $95\\%$ confidence intervals are returned where the standard deviation is computed according to Bartlett\"s formula.&lt;br&gt;\n            Defaults to `None`.\n        bartlett_confint (bool, optional):\n            Confidence intervals for ACF values are generally placed at 2 standard errors around $r_k$. The formula used for standard error depends upon the situation. If the autocorrelations are being used to test for randomness of residuals as part of the ARIMA routine, the standard errors are determined assuming the residuals are white noise. The approximate formula for any lag is that standard error of each $r_k = \\frac{1}{\\sqrt{N}}$. See section 9.4 of [2] for more details on the $\\frac{1}{\\sqrt{N}}$ result. For more elementary discussion, see section 5.3.2 in [3]. For the ACF of raw data, the standard error at a lag $k$ is found as if the right model was an $\\text{MA}(k-1)$. This allows the possible interpretation that if all autocorrelations past a certain lag are within the limits, the model might be an $\\text{MA}$ of order defined by the last significant autocorrelation. In this case, a moving average model is assumed for the data and the standard errors for the confidence intervals should be generated using Bartlett's formula. For more details on Bartlett formula result, see section 7.2 in [2].&lt;br&gt;\n            Defaults to `True`.\n        missing (VALID_ACF_MISSING_OPTIONS, optional):\n            A string in `[\"none\", \"raise\", \"conservative\", \"drop\"]` specifying how the `NaN`'s are to be treated.\n\n            - `\"none\"` performs no checks.\n            - `\"raise\"` raises an exception if NaN values are found.\n            - `\"drop\"` removes the missing observations and then estimates the autocovariances treating the non-missing as contiguous.\n            - `\"conservative\"` computes the autocovariance using nan-ops so that nans are removed when computing the mean and cross-products that are used to estimate the autocovariance.\n\n            When using `\"conservative\"`, $n$ is set to the number of non-missing observations.&lt;br&gt;\n            Defaults to `\"none\"`.\n\n    Returns:\n        acf (np.ndarray):\n            The autocorrelation function for lags `0, 1, ..., nlags`.&lt;br&gt;\n            Shape `(nlags+1,)`.\n        confint (Optional[np.ndarray]):\n            Confidence intervals for the ACF at lags `0, 1, ..., nlags`.&lt;br&gt;\n            Shape `(nlags + 1, 2)`.&lt;br&gt;\n            Returned if `alpha` is not `None`.\n        qstat (Optional[np.ndarray]):\n            The Ljung-Box Q-Statistic for lags `1, 2, ..., nlags` (excludes lag zero).&lt;br&gt;\n            Returned if `qstat` is `True`.\n        pvalues (Optional[np.ndarray]):\n            The p-values associated with the Q-statistics for lags `1, 2, ..., nlags` (excludes lag zero).&lt;br&gt;\n            Returned if `qstat` is `True`.\n\n    !!! success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/) library.\n\n    !!! example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test ACF without FFT\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from statsmodels.datasets import macrodata\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import acf\n        &gt;&gt;&gt; data = macrodata.load_pandas()\n        &gt;&gt;&gt; x = data.data[\"realgdp\"]\n        &gt;&gt;&gt; res_acf, res_confint, res_qstat, res_pvalues = acf(\n        ...     x, nlags=40, qstat=True, alpha=0.05, fft=False\n        ... )\n        &gt;&gt;&gt; pprint(res_acf[1:11])\n        array([0.94804734, 0.87557484, 0.80668116, 0.75262542, 0.71376997,\n               0.6817336 , 0.66290439, 0.65561048, 0.67094833, 0.70271992])\n        &gt;&gt;&gt; pprint(res_confint[1:11])\n        array([[0.78471701, 1.11137767],\n               [0.60238868, 1.14876099],\n               [0.46677939, 1.14658292],\n               [0.36500159, 1.14024925],\n               [0.28894752, 1.13859242],\n               [0.22604068, 1.13742653],\n               [0.18077091, 1.14503787],\n               [0.14974636, 1.16147461],\n               [0.1429036 , 1.19899305],\n               [0.15240228, 1.25303756]])\n        &gt;&gt;&gt; pprint(res_qstat[:10])\n        array([132.14153858, 245.64616028, 342.67482586, 427.73868355,\n               504.79657041, 575.6018536 , 643.03859337, 709.48449817,\n               779.59123116, 857.06863862])\n        &gt;&gt;&gt; pprint(res_pvalues[:10])\n        array([1.39323140e-030, 4.55631819e-054, 5.75108846e-074, 2.81773062e-091,\n               7.36019524e-107, 4.26400770e-121, 1.30546283e-134, 6.49627091e-148,\n               5.24937010e-162, 1.10078935e-177])\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test ACF with FFT\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import acf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_acf, res_confint, res_qstat, res_pvalues = acf(\n        ...     data, nlags=40, qstat=True, alpha=0.05, fft=True\n        ... )\n        &gt;&gt;&gt; pprint(res_acf[1:11])\n        array([0.94804734, 0.87557484, 0.80668116, 0.75262542, 0.71376997,\n               0.6817336 , 0.66290439, 0.65561048, 0.67094833, 0.70271992])\n        &gt;&gt;&gt; pprint(res_confint[1:11])\n        array([[0.78471701, 1.11137767],\n               [0.60238868, 1.14876099],\n               [0.46677939, 1.14658292],\n               [0.36500159, 1.14024925],\n               [0.28894752, 1.13859242],\n               [0.22604068, 1.13742653],\n               [0.18077091, 1.14503787],\n               [0.14974636, 1.16147461],\n               [0.1429036 , 1.19899305],\n               [0.15240228, 1.25303756]])\n        &gt;&gt;&gt; pprint(res_qstat[:10])\n        array([132.14153858, 245.64616028, 342.67482586, 427.73868355,\n               504.79657041, 575.6018536 , 643.03859337, 709.48449817,\n               779.59123116, 857.06863862])\n        &gt;&gt;&gt; pprint(res_pvalues[:10])\n        array([1.39323140e-030, 4.55631819e-054, 5.75108846e-074, 2.81773062e-091,\n               7.36019524e-107, 4.26400770e-121, 1.30546283e-134, 6.49627091e-148,\n               5.24937010e-162, 1.10078935e-177])\n        ```\n\n    ??? question \"References\"\n        1. Parzen, E., 1963. On spectral analysis with missing observations and amplitude modulation. Sankhya: The Indian Journal of Statistics, Series A, pp.383-392.\n        1. Brockwell and Davis, 1987. Time Series Theory and Methods.\n        1. Brockwell and Davis, 2010. Introduction to Time Series and Forecasting, 2nd edition.\n\n    ??? tip \"See Also\"\n        - [`statsmodels.tsa.stattools.acf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.acf.html): Estimate the autocorrelation function.\n        - [`statsmodels.tsa.stattools.pacf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf.html): Partial autocorrelation estimation.\n        - [`statsmodels.tsa.stattools.ccf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.ccf.html): The cross-correlation function.\n        - [`ts_stat_tests.algorithms.correlation.acf`][ts_stat_tests.algorithms.correlation.acf]: Estimate the autocorrelation function\n        - [`ts_stat_tests.algorithms.correlation.pacf`][ts_stat_tests.algorithms.correlation.pacf]: Partial autocorrelation estimate.\n        - [`ts_stat_tests.algorithms.correlation.ccf`][ts_stat_tests.algorithms.correlation.ccf]: The cross-correlation function.\n    \"\"\"\n    return st_acf(\n        x=x,\n        adjusted=adjusted,\n        nlags=nlags,\n        qstat=qstat,\n        fft=fft,\n        alpha=alpha,\n        bartlett_confint=bartlett_confint,\n        missing=missing,\n    )\n</code></pre>"},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation.pacf","title":"pacf","text":"<pre><code>pacf(\n    x: ArrayLike1D,\n    nlags: Optional[int] = None,\n    method: VALID_PACF_METHOD_OPTIONS = \"ywadjusted\",\n    *,\n    alpha: None = None\n) -&gt; np.ndarray\n</code></pre><pre><code>pacf(\n    x: ArrayLike1D,\n    nlags: Optional[int] = None,\n    method: VALID_PACF_METHOD_OPTIONS = \"ywadjusted\",\n    *,\n    alpha: float\n) -&gt; tuple[np.ndarray, np.ndarray]\n</code></pre> <pre><code>pacf(\n    x: ArrayLike1D,\n    nlags: Optional[int] = None,\n    method: VALID_PACF_METHOD_OPTIONS = \"ywadjusted\",\n    *,\n    alpha: Optional[float] = None\n) -&gt; Union[np.ndarray, tuple[np.ndarray, np.ndarray]]\n</code></pre> <p>Summary</p> <p>The partial autocorrelation function (PACF) is a statistical tool used in time series forecasting to identify the direct relationship between two variables, controlling for the effect of the other variables in the time series. In other words, the PACF measures the correlation between a time series and its lagged values, while controlling for the effects of other intermediate lags.</p> <p>This function will implement the <code>pacf()</code> function from the <code>statsmodels</code> library.</p> Details <p>Based on simulation evidence across a range of low-order ARMA models, the best methods based on root MSE are Yule-Walker (MLW), Levinson-Durbin (MLE) and Burg, respectively. The estimators with the lowest bias included these three in addition to OLS and OLS-adjusted. Yule-Walker (adjusted) and Levinson-Durbin (adjusted) performed consistently worse than the other options.</p> <p>The PACF is a plot of the correlation between a time series and its lagged values, controlling for the effect of other lags. The PACF is useful for identifying the order of an autoregressive (AR) model, which is a type of model used in time series forecasting. The order of an AR model is the number of lags that are used to predict future values.</p> <p>The PACF at lag \\(k\\) is defined as:</p> \\[ PACF(k) = Corr \\left( Y_t, Y_{t-k} \\mid Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1} \\right) \\] <p>where:</p> <ul> <li>\\(Y_t\\) and \\(Y_{t-k}\\) are the values of the time series at time \\(t\\) and time \\(t-k\\), respectively, and</li> <li>\\(Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1}\\) are the values of the time series at intervening lags.</li> <li>\\(Corr()\\) denotes the correlation coefficient between two variables.</li> </ul> <pre><code>PACF(k) = Corr(Y_t, Y_{t-k} | Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1})\n</code></pre> <p>The PACF is calculated using the Yule-Walker equations, which are a set of linear equations that describe the relationship between a time series and its lagged values. The PACF is calculated as the difference between the correlation coefficient at lag \\(k\\) and the correlation coefficient at lag \\(k-1\\), controlling for the effects of intermediate lags.</p> <p>The PACF is typically plotted as a graph, with the lag on the <code>x</code>-axis and the correlation coefficient on the <code>y</code>-axis. If the PACF shows a strong positive correlation at lag \\(k\\), this means that values in the time series at time \\(t\\) and time \\(t-k\\) are strongly related, after controlling for the effects of intermediate lags. This suggests that past values can be used to predict future values using an AR model with an order of \\(k\\).</p> <p>Overall, the partial autocorrelation function is a valuable tool in time series forecasting, as it helps to identify the order of an autoregressive model and to control for the effects of intermediate lags. By identifying the direct relationship between two variables, the PACF can help to improve the accuracy of time series forecasting models.</p> <p>The PACF can be calculated using the pacf() function in the statsmodels package in Python. The function takes a time series array as input and returns an array of partial autocorrelation coefficients at different lags. The significance of the partial autocorrelation coefficients can be tested using the same Ljung-Box test as for the ACF. If the p-value of the test is less than a certain significance level (e.g. \\(0.05\\)), then there is evidence of significant partial autocorrelation in the time series up to the specified lag.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike1D</code> <p>Observations of time series for which pacf is calculated.</p> required <code>nlags</code> <code>Optional[int]</code> <p>Number of lags to return autocorrelation for. If not provided, uses \\(min(10 \\times log10(nobs) , (\\frac{nobs}{2}-1))\\) (calculated with: <code>min(int(10*np.log10(nobs)), nobs // 2 - 1)</code>). The returned value includes lag <code>0</code> (ie., <code>1</code>) so size of the pacf vector is \\((nlags + 1,)\\). Defaults to <code>None</code>.</p> <code>None</code> <code>method</code> <code>VALID_PACF_METHOD_OPTIONS</code> <p>Specifies which method for the calculations to use.</p> <ul> <li><code>\"yw\"</code> or <code>\"ywadjusted\"</code>: Yule-Walker with sample-size adjustment in denominator for acovf. Default.</li> <li><code>\"ywm\"</code> or <code>\"ywmle\"</code>: Yule-Walker without adjustment.</li> <li><code>\"ols\"</code>: regression of time series on lags of it and on constant.</li> <li><code>\"ols-inefficient\"</code>: regression of time series on lags using a single common sample to estimate all pacf coefficients.</li> <li><code>\"ols-adjusted\"</code>: regression of time series on lags with a bias adjustment.</li> <li><code>\"ld\"</code> or <code>\"ldadjusted\"</code>: Levinson-Durbin recursion with bias correction.</li> <li><code>\"ldb\"</code> or <code>\"ldbiased\"</code>: Levinson-Durbin recursion without bias correction.</li> </ul> <p>Defaults to <code>\"ywadjusted\"</code>.</p> <code>'ywadjusted'</code> <code>alpha</code> <code>Optional[float]</code> <p>If a number is given, the confidence intervals for the given level are returned. For instance if <code>alpha=.05</code>, \\(95\\%\\) confidence intervals are returned where the standard deviation is computed according to \\(\\frac{1}{\\sqrt{len(x)}}\\). Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>pacf</code> <code>ndarray</code> <p>The partial autocorrelations for lags <code>0, 1, ..., nlags</code>. Shape <code>(nlags+1,)</code>.</p> <code>confint</code> <code>Optional[ndarray]</code> <p>Confidence intervals for the PACF at lags <code>0, 1, ..., nlags</code>. Shape <code>(nlags + 1, 2)</code>. Returned if <code>alpha</code> is not <code>None</code>.</p> <p>Credit</p> <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> <p>Examples</p> Test PACF using Yule-Walker method with sample-size adjustment<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, alpha=0.05)\n&gt;&gt;&gt; pprint(res_acf[1:11])\narray([ 0.95467704, -0.26527732,  0.05546955,  0.10885622,  0.08112579,\n        0.00412541,  0.15616955,  0.10370833,  0.28878144,  0.20691805])\n&gt;&gt;&gt; pprint(res_confint[1:11])\narray([[ 0.79134671,  1.11800737],\n       [-0.42860765, -0.10194698],\n       [-0.10786078,  0.21879988],\n       [-0.05447412,  0.27218655],\n       [-0.08220455,  0.24445612],\n       [-0.15920493,  0.16745574],\n       [-0.00716078,  0.31949988],\n       [-0.059622  ,  0.26703866],\n       [ 0.12545111,  0.45211177],\n       [ 0.04358772,  0.37024838]])\n</code></pre> Test PACF using Yule-Walker method without adjustment<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ywm\", alpha=0.05)\n&gt;&gt;&gt; pprint(res_acf[1:11])\narray([ 0.94804734, -0.22942187,  0.03814778,  0.09378544,  0.0736067 ,\n        0.0077276 ,  0.12559713,  0.08995134,  0.23248854,  0.16605126])\n&gt;&gt;&gt; pprint(res_confint[1:11])\narray([[ 0.78471701,  1.11137767],\n       [-0.39275221, -0.06609154],\n       [-0.12518255,  0.20147811],\n       [-0.06954489,  0.25711577],\n       [-0.08972363,  0.23693703],\n       [-0.15560273,  0.17105793],\n       [-0.0377332 ,  0.28892746],\n       [-0.07337899,  0.25328168],\n       [ 0.06915821,  0.39581887],\n       [ 0.00272093,  0.32938159]])\n</code></pre> Test PACF using regression of time series<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ols\", alpha=0.05)\n&gt;&gt;&gt; pprint(res_acf[1:11])\narray([ 0.95893198, -0.32983096,  0.2018249 ,  0.14500798,  0.25848232,\n       -0.02690283,  0.20433019,  0.15607896,  0.56860841,  0.29256358])\n&gt;&gt;&gt; pprint(res_confint[1:11])\narray([[ 0.79560165,  1.12226231],\n       [-0.49316129, -0.16650062],\n       [ 0.03849457,  0.36515523],\n       [-0.01832235,  0.30833831],\n       [ 0.09515198,  0.42181265],\n       [-0.19023316,  0.1364275 ],\n       [ 0.04099986,  0.36766053],\n       [-0.00725137,  0.31940929],\n       [ 0.40527808,  0.73193874],\n       [ 0.12923325,  0.45589391]])\n</code></pre> Test PACF using regression of time series on lags with inefficient optimisation<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ols-inefficient\", alpha=0.05)\n&gt;&gt;&gt; pprint(res_acf[1:11])\narray([ 0.94692978, -0.3540491 ,  0.18292698,  0.12813227,  0.23647898,\n       -0.04596983,  0.19748537,  0.12877966,  0.55357665,  0.22081591])\n&gt;&gt;&gt; pprint(res_confint[1:11])\narray([[ 0.78359944,  1.11026011],\n       [-0.51737943, -0.19071876],\n       [ 0.01959665,  0.34625731],\n       [-0.03519806,  0.2914626 ],\n       [ 0.07314865,  0.39980932],\n       [-0.20930016,  0.1173605 ],\n       [ 0.03415504,  0.3608157 ],\n       [-0.03455067,  0.29211   ],\n       [ 0.39024632,  0.71690698],\n       [ 0.05748558,  0.38414625]])\n</code></pre> Test PACF using regression of time series on lags with a bias adjustment<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ols-adjusted\", alpha=0.05)\n&gt;&gt;&gt; pprint(res_acf[1:11])\narray([ 0.9656378 , -0.33447646,  0.20611905,  0.14915107,  0.26778024,\n       -0.02807252,  0.21477042,  0.16526008,  0.60651564,  0.31439668])\n&gt;&gt;&gt; pprint(res_confint[1:11])\narray([[ 0.80230746,  1.12896813],\n       [-0.49780679, -0.17114613],\n       [ 0.04278872,  0.36944938],\n       [-0.01417926,  0.3124814 ],\n       [ 0.10444991,  0.43111057],\n       [-0.19140285,  0.13525782],\n       [ 0.05144009,  0.37810076],\n       [ 0.00192974,  0.32859041],\n       [ 0.4431853 ,  0.76984597],\n       [ 0.15106635,  0.47772701]])\n</code></pre> Test PACF using Levinson-Durbin recursion with bias correction<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ldadjusted\", alpha=0.05)\n&gt;&gt;&gt; pprint(res_acf[1:11])\narray([ 0.95467704, -0.26527732,  0.05546955,  0.10885622,  0.08112579,\n        0.00412541,  0.15616955,  0.10370833,  0.28878144,  0.20691805])\n&gt;&gt;&gt; pprint(res_confint[1:11])\narray([[ 0.79134671,  1.11800737],\n       [-0.42860765, -0.10194698],\n       [-0.10786078,  0.21879988],\n       [-0.05447412,  0.27218655],\n       [-0.08220455,  0.24445612],\n       [-0.15920493,  0.16745574],\n       [-0.00716078,  0.31949988],\n       [-0.059622  ,  0.26703866],\n       [ 0.12545111,  0.45211177],\n       [ 0.04358772,  0.37024838]])\n</code></pre> Test PACF using Levinson-Durbin recursion without bias correction<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ldbiased\", alpha=0.05)\n&gt;&gt;&gt; pprint(res_acf[1:11])\narray([ 0.94804734, -0.22942187,  0.03814778,  0.09378544,  0.0736067 ,\n        0.0077276 ,  0.12559713,  0.08995134,  0.23248854,  0.16605126])\n&gt;&gt;&gt; pprint(res_confint[1:11])\narray([[ 0.78471701,  1.11137767],\n       [-0.39275221, -0.06609154],\n       [-0.12518255,  0.20147811],\n       [-0.06954489,  0.25711577],\n       [-0.08972363,  0.23693703],\n       [-0.15560273,  0.17105793],\n       [-0.0377332 ,  0.28892746],\n       [-0.07337899,  0.25328168],\n       [ 0.06915821,  0.39581887],\n       [ 0.00272093,  0.32938159]])\n</code></pre> References <ol> <li>Box, G. E., Jenkins, G. M., Reinsel, G. C., &amp; Ljung, G. M. (2015). Time series analysis: forecasting and control. John Wiley &amp; Sons, p. 66.</li> <li>Brockwell, P.J. and Davis, R.A., 2016. Introduction to time series and forecasting. Springer.</li> </ol> See Also <ul> <li><code>statsmodels.tsa.stattools.acf</code>: Estimate the autocorrelation function.</li> <li><code>statsmodels.tsa.stattools.pacf</code>: Partial autocorrelation estimation.</li> <li><code>statsmodels.tsa.stattools.ccf</code>: The cross-correlation function.</li> <li><code>statsmodels.tsa.stattools.pacf_yw</code>: Partial autocorrelation estimation using Yule-Walker.</li> <li><code>statsmodels.tsa.stattools.pacf_ols</code>: Partial autocorrelation estimation using OLS.</li> <li><code>statsmodels.tsa.stattools.pacf_burg</code>: Partial autocorrelation estimation using Burg's method.</li> <li><code>ts_stat_tests.algorithms.correlation.acf</code>: Estimate the autocorrelation function</li> <li><code>ts_stat_tests.algorithms.correlation.pacf</code>: Partial autocorrelation estimate.</li> <li><code>ts_stat_tests.algorithms.correlation.ccf</code>: The cross-correlation function.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/correlation.py</code> <pre><code>@typechecked\ndef pacf(\n    x: ArrayLike1D,\n    nlags: Optional[int] = None,\n    method: VALID_PACF_METHOD_OPTIONS = \"ywadjusted\",\n    *,\n    alpha: Optional[float] = None,\n) -&gt; Union[np.ndarray, tuple[np.ndarray, np.ndarray]]:\n    r\"\"\"\n    !!! note \"Summary\"\n\n        The partial autocorrelation function (PACF) is a statistical tool used in time series forecasting to identify the direct relationship between two variables, controlling for the effect of the other variables in the time series. In other words, the PACF measures the correlation between a time series and its lagged values, while controlling for the effects of other intermediate lags.\n\n        This function will implement the [`pacf()`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf.html) function from the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ???+ abstract \"Details\"\n\n        Based on simulation evidence across a range of low-order ARMA models, the best methods based on root MSE are Yule-Walker (MLW), Levinson-Durbin (MLE) and Burg, respectively. The estimators with the lowest bias included these three in addition to OLS and OLS-adjusted. Yule-Walker (adjusted) and Levinson-Durbin (adjusted) performed consistently worse than the other options.\n\n        The PACF is a plot of the correlation between a time series and its lagged values, controlling for the effect of other lags. The PACF is useful for identifying the order of an autoregressive (AR) model, which is a type of model used in time series forecasting. The order of an AR model is the number of lags that are used to predict future values.\n\n        The PACF at lag $k$ is defined as:\n\n        $$\n        PACF(k) = Corr \\left( Y_t, Y_{t-k} \\mid Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1} \\right)\n        $$\n\n        where:\n\n        - $Y_t$ and $Y_{t-k}$ are the values of the time series at time $t$ and time $t-k$, respectively, and\n        - $Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1}$ are the values of the time series at intervening lags.\n        - $Corr()$ denotes the correlation coefficient between two variables.\n\n        ```\n        PACF(k) = Corr(Y_t, Y_{t-k} | Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1})\n        ```\n\n        The PACF is calculated using the Yule-Walker equations, which are a set of linear equations that describe the relationship between a time series and its lagged values. The PACF is calculated as the difference between the correlation coefficient at lag $k$ and the correlation coefficient at lag $k-1$, controlling for the effects of intermediate lags.\n\n        The PACF is typically plotted as a graph, with the lag on the `x`-axis and the correlation coefficient on the `y`-axis. If the PACF shows a strong positive correlation at lag $k$, this means that values in the time series at time $t$ and time $t-k$ are strongly related, after controlling for the effects of intermediate lags. This suggests that past values can be used to predict future values using an AR model with an order of $k$.\n\n        Overall, the partial autocorrelation function is a valuable tool in time series forecasting, as it helps to identify the order of an autoregressive model and to control for the effects of intermediate lags. By identifying the direct relationship between two variables, the PACF can help to improve the accuracy of time series forecasting models.\n\n        The PACF can be calculated using the pacf() function in the statsmodels package in Python. The function takes a time series array as input and returns an array of partial autocorrelation coefficients at different lags. The significance of the partial autocorrelation coefficients can be tested using the same Ljung-Box test as for the ACF. If the p-value of the test is less than a certain significance level (e.g. $0.05$), then there is evidence of significant partial autocorrelation in the time series up to the specified lag.\n\n    Params:\n        x (ArrayLike1D):\n            Observations of time series for which pacf is calculated.\n        nlags (Optional[int], optional):\n            Number of lags to return autocorrelation for. If not provided, uses $min(10 \\times log10(nobs) , (\\frac{nobs}{2}-1))$ (calculated with: `min(int(10*np.log10(nobs)), nobs // 2 - 1)`). The returned value includes lag `0` (ie., `1`) so size of the pacf vector is $(nlags + 1,)$.&lt;br&gt;\n            Defaults to `None`.\n        method (VALID_PACF_METHOD_OPTIONS, optional):\n            Specifies which method for the calculations to use.\n\n            - `\"yw\"` or `\"ywadjusted\"`: Yule-Walker with sample-size adjustment in denominator for acovf. Default.\n            - `\"ywm\"` or `\"ywmle\"`: Yule-Walker without adjustment.\n            - `\"ols\"`: regression of time series on lags of it and on constant.\n            - `\"ols-inefficient\"`: regression of time series on lags using a single common sample to estimate all pacf coefficients.\n            - `\"ols-adjusted\"`: regression of time series on lags with a bias adjustment.\n            - `\"ld\"` or `\"ldadjusted\"`: Levinson-Durbin recursion with bias correction.\n            - `\"ldb\"` or `\"ldbiased\"`: Levinson-Durbin recursion without bias correction.&lt;br&gt;\n\n            Defaults to `\"ywadjusted\"`.\n        alpha (Optional[float], optional):\n            If a number is given, the confidence intervals for the given level are returned. For instance if `alpha=.05`, $95\\%$ confidence intervals are returned where the standard deviation is computed according to $\\frac{1}{\\sqrt{len(x)}}$.&lt;br&gt;\n            Defaults to `None`.\n\n    Returns:\n        pacf (np.ndarray):\n            The partial autocorrelations for lags `0, 1, ..., nlags`.&lt;br&gt;\n            Shape `(nlags+1,)`.\n        confint (Optional[np.ndarray]):\n            Confidence intervals for the PACF at lags `0, 1, ..., nlags`.&lt;br&gt;\n            Shape `(nlags + 1, 2)`.&lt;br&gt;\n            Returned if `alpha` is not `None`.\n\n    !!! success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/) library.\n\n    !!! example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test PACF using Yule-Walker method with sample-size adjustment\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, alpha=0.05)\n        &gt;&gt;&gt; pprint(res_acf[1:11])\n        array([ 0.95467704, -0.26527732,  0.05546955,  0.10885622,  0.08112579,\n                0.00412541,  0.15616955,  0.10370833,  0.28878144,  0.20691805])\n        &gt;&gt;&gt; pprint(res_confint[1:11])\n        array([[ 0.79134671,  1.11800737],\n               [-0.42860765, -0.10194698],\n               [-0.10786078,  0.21879988],\n               [-0.05447412,  0.27218655],\n               [-0.08220455,  0.24445612],\n               [-0.15920493,  0.16745574],\n               [-0.00716078,  0.31949988],\n               [-0.059622  ,  0.26703866],\n               [ 0.12545111,  0.45211177],\n               [ 0.04358772,  0.37024838]])\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test PACF using Yule-Walker method without adjustment\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ywm\", alpha=0.05)\n        &gt;&gt;&gt; pprint(res_acf[1:11])\n        array([ 0.94804734, -0.22942187,  0.03814778,  0.09378544,  0.0736067 ,\n                0.0077276 ,  0.12559713,  0.08995134,  0.23248854,  0.16605126])\n        &gt;&gt;&gt; pprint(res_confint[1:11])\n        array([[ 0.78471701,  1.11137767],\n               [-0.39275221, -0.06609154],\n               [-0.12518255,  0.20147811],\n               [-0.06954489,  0.25711577],\n               [-0.08972363,  0.23693703],\n               [-0.15560273,  0.17105793],\n               [-0.0377332 ,  0.28892746],\n               [-0.07337899,  0.25328168],\n               [ 0.06915821,  0.39581887],\n               [ 0.00272093,  0.32938159]])\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test PACF using regression of time series\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ols\", alpha=0.05)\n        &gt;&gt;&gt; pprint(res_acf[1:11])\n        array([ 0.95893198, -0.32983096,  0.2018249 ,  0.14500798,  0.25848232,\n               -0.02690283,  0.20433019,  0.15607896,  0.56860841,  0.29256358])\n        &gt;&gt;&gt; pprint(res_confint[1:11])\n        array([[ 0.79560165,  1.12226231],\n               [-0.49316129, -0.16650062],\n               [ 0.03849457,  0.36515523],\n               [-0.01832235,  0.30833831],\n               [ 0.09515198,  0.42181265],\n               [-0.19023316,  0.1364275 ],\n               [ 0.04099986,  0.36766053],\n               [-0.00725137,  0.31940929],\n               [ 0.40527808,  0.73193874],\n               [ 0.12923325,  0.45589391]])\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test PACF using regression of time series on lags with inefficient optimisation\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ols-inefficient\", alpha=0.05)\n        &gt;&gt;&gt; pprint(res_acf[1:11])\n        array([ 0.94692978, -0.3540491 ,  0.18292698,  0.12813227,  0.23647898,\n               -0.04596983,  0.19748537,  0.12877966,  0.55357665,  0.22081591])\n        &gt;&gt;&gt; pprint(res_confint[1:11])\n        array([[ 0.78359944,  1.11026011],\n               [-0.51737943, -0.19071876],\n               [ 0.01959665,  0.34625731],\n               [-0.03519806,  0.2914626 ],\n               [ 0.07314865,  0.39980932],\n               [-0.20930016,  0.1173605 ],\n               [ 0.03415504,  0.3608157 ],\n               [-0.03455067,  0.29211   ],\n               [ 0.39024632,  0.71690698],\n               [ 0.05748558,  0.38414625]])\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test PACF using regression of time series on lags with a bias adjustment\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ols-adjusted\", alpha=0.05)\n        &gt;&gt;&gt; pprint(res_acf[1:11])\n        array([ 0.9656378 , -0.33447646,  0.20611905,  0.14915107,  0.26778024,\n               -0.02807252,  0.21477042,  0.16526008,  0.60651564,  0.31439668])\n        &gt;&gt;&gt; pprint(res_confint[1:11])\n        array([[ 0.80230746,  1.12896813],\n               [-0.49780679, -0.17114613],\n               [ 0.04278872,  0.36944938],\n               [-0.01417926,  0.3124814 ],\n               [ 0.10444991,  0.43111057],\n               [-0.19140285,  0.13525782],\n               [ 0.05144009,  0.37810076],\n               [ 0.00192974,  0.32859041],\n               [ 0.4431853 ,  0.76984597],\n               [ 0.15106635,  0.47772701]])\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test PACF using Levinson-Durbin recursion with bias correction\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ldadjusted\", alpha=0.05)\n        &gt;&gt;&gt; pprint(res_acf[1:11])\n        array([ 0.95467704, -0.26527732,  0.05546955,  0.10885622,  0.08112579,\n                0.00412541,  0.15616955,  0.10370833,  0.28878144,  0.20691805])\n        &gt;&gt;&gt; pprint(res_confint[1:11])\n        array([[ 0.79134671,  1.11800737],\n               [-0.42860765, -0.10194698],\n               [-0.10786078,  0.21879988],\n               [-0.05447412,  0.27218655],\n               [-0.08220455,  0.24445612],\n               [-0.15920493,  0.16745574],\n               [-0.00716078,  0.31949988],\n               [-0.059622  ,  0.26703866],\n               [ 0.12545111,  0.45211177],\n               [ 0.04358772,  0.37024838]])\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test PACF using Levinson-Durbin recursion without bias correction\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import pacf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_acf, res_confint = pacf(data, nlags=40, method=\"ldbiased\", alpha=0.05)\n        &gt;&gt;&gt; pprint(res_acf[1:11])\n        array([ 0.94804734, -0.22942187,  0.03814778,  0.09378544,  0.0736067 ,\n                0.0077276 ,  0.12559713,  0.08995134,  0.23248854,  0.16605126])\n        &gt;&gt;&gt; pprint(res_confint[1:11])\n        array([[ 0.78471701,  1.11137767],\n               [-0.39275221, -0.06609154],\n               [-0.12518255,  0.20147811],\n               [-0.06954489,  0.25711577],\n               [-0.08972363,  0.23693703],\n               [-0.15560273,  0.17105793],\n               [-0.0377332 ,  0.28892746],\n               [-0.07337899,  0.25328168],\n               [ 0.06915821,  0.39581887],\n               [ 0.00272093,  0.32938159]])\n        ```\n\n    ??? question \"References\"\n        1. Box, G. E., Jenkins, G. M., Reinsel, G. C., &amp; Ljung, G. M. (2015). Time series analysis: forecasting and control. John Wiley &amp; Sons, p. 66.\n        1. Brockwell, P.J. and Davis, R.A., 2016. Introduction to time series and forecasting. Springer.\n\n    ??? tip \"See Also\"\n        - [`statsmodels.tsa.stattools.acf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.acf.html): Estimate the autocorrelation function.\n        - [`statsmodels.tsa.stattools.pacf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf.html): Partial autocorrelation estimation.\n        - [`statsmodels.tsa.stattools.ccf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.ccf.html): The cross-correlation function.\n        - [`statsmodels.tsa.stattools.pacf_yw`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf_yw.html): Partial autocorrelation estimation using Yule-Walker.\n        - [`statsmodels.tsa.stattools.pacf_ols`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf_ols.html): Partial autocorrelation estimation using OLS.\n        - [`statsmodels.tsa.stattools.pacf_burg`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf_burg.html): Partial autocorrelation estimation using Burg's method.\n        - [`ts_stat_tests.algorithms.correlation.acf`][ts_stat_tests.algorithms.correlation.acf]: Estimate the autocorrelation function\n        - [`ts_stat_tests.algorithms.correlation.pacf`][ts_stat_tests.algorithms.correlation.pacf]: Partial autocorrelation estimate.\n        - [`ts_stat_tests.algorithms.correlation.ccf`][ts_stat_tests.algorithms.correlation.ccf]: The cross-correlation function.\n    \"\"\"\n    return st_pacf(\n        x=x,\n        nlags=nlags,\n        method=method,\n        alpha=alpha,\n    )\n</code></pre>"},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation.ccf","title":"ccf","text":"<pre><code>ccf(\n    x: ArrayLike,\n    y: ArrayLike,\n    adjusted: bool = True,\n    fft: bool = True,\n    *,\n    nlags: Optional[int] = None,\n    alpha: None = None\n) -&gt; np.ndarray\n</code></pre><pre><code>ccf(\n    x: ArrayLike,\n    y: ArrayLike,\n    adjusted: bool = True,\n    fft: bool = True,\n    *,\n    nlags: Optional[int] = None,\n    alpha: float\n) -&gt; tuple[np.ndarray, np.ndarray]\n</code></pre> <pre><code>ccf(\n    x: ArrayLike,\n    y: ArrayLike,\n    adjusted: bool = True,\n    fft: bool = True,\n    *,\n    nlags: Optional[int] = None,\n    alpha: Optional[float] = None\n) -&gt; Union[np.ndarray, tuple[np.ndarray, np.ndarray]]\n</code></pre> <p>Summary</p> <p>The cross-correlation function (CCF) is a statistical tool used in time series forecasting to measure the correlation between two time series at different lags. It is used to study the relationship between two time series, and can help to identify lead-lag relationships and causal effects.</p> <p>This function will implement the <code>ccf()</code> function from the <code>statsmodels</code> library.</p> Details <p>If <code>adjusted</code> is <code>True</code>, the denominator for the autocovariance is adjusted.</p> <p>The CCF measures the correlation between two time series at different lags. It is calculated as the ratio of the covariance between the two series at lag k to the product of their standard deviations. The CCF is typically plotted as a graph, with the lag on the <code>x</code>-axis and the correlation coefficient on the <code>y</code>-axis.</p> <p>The CCF at lag k is defined as:</p> \\[ CCF(k) = Corr(X_t, Y_{t-k}) \\] <p>where:</p> <ul> <li>\\(X_t\\) and \\(Y_{t-k}\\) are the values of the two time series at time \\(t\\) and time \\(t-k\\), respectively.</li> <li>\\(Corr()\\) denotes the correlation coefficient between two variables.</li> </ul> <pre><code>CCF(k) = Corr(X_t, Y_{t-k})\n</code></pre> <p>If the CCF shows a strong positive correlation at lag \\(k\\), this means that changes in one time series at time \\(t\\) are strongly related to changes in the other time series at time \\(t-k\\). This suggests a lead-lag relationship between the two time series, where changes in one series lead changes in the other series by a certain number of periods. The CCF can be used to estimate the time lag between the two time series.</p> <p>The CCF can also help to identify causal relationships between two time series. If the CCF shows a strong positive correlation at lag \\(k\\), and the lag is consistent with a causal relationship between the two time series, this suggests that changes in one time series are causing changes in the other time series.</p> <p>Overall, the cross-correlation function is a valuable tool in time series forecasting, as it helps to study the relationship between two time series and to identify lead-lag relationships and causal effects. By identifying the relationship between two time series, the CCF can help to improve the accuracy of time series forecasting models.</p> <p>The CCF can be calculated using the <code>ccf()</code> function in the <code>statsmodels</code> package in Python. The function takes two time series arrays as input and returns an array of cross-correlation coefficients at different lags. The significance of the cross-correlation coefficients can be tested using a similar test to the Ljung-Box test, such as the Box-Pierce test or the Breusch-Godfrey test. These tests can be performed using the <code>boxpierce()</code> and <code>lm()</code> functions in the <code>statsmodels</code> package, respectively. If the p-value of the test is less than a certain significance level (e.g. \\(0.05\\)), then there is evidence of significant cross-correlation between the two time series at the specified lag.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The time series data to use in the calculation.</p> required <code>y</code> <code>ArrayLike</code> <p>The time series data to use in the calculation.</p> required <code>adjusted</code> <code>bool</code> <p>If <code>True</code>, then denominators for cross-correlation is \\(n-k\\), otherwise \\(n\\). Defaults to <code>True</code>.</p> <code>True</code> <code>fft</code> <code>bool</code> <p>If <code>True</code>, use FFT convolution. This method should be preferred for long time series. Defaults to <code>True</code>.</p> <code>True</code> <code>nlags</code> <code>Optional[int]</code> <p>Number of lags to return cross-correlations for. If not provided, the number of lags equals len(x). Defaults to <code>None</code>.</p> <code>None</code> <code>alpha</code> <code>Optional[float]</code> <p>If a number is given, the confidence intervals for the given level are returned. For instance if <code>alpha=.05</code>, 95% confidence intervals are returned where the standard deviation is computed according to <code>1/sqrt(len(x))</code>. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>ccf</code> <code>ndarray</code> <p>The cross-correlation function of <code>x</code> and <code>y</code>.</p> <p>Credit</p> <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> <p>Examples</p> Test CCF without FFT<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import ccf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_ccf = ccf(data, data + 1, adjusted=True, fft=False)\n&gt;&gt;&gt; pprint(res_ccf[1:11])\narray([0.95467704, 0.88790688, 0.82384458, 0.774129  , 0.73944515,\n       0.71137419, 0.69677541, 0.69417581, 0.71567822, 0.75516171])\n</code></pre> Test CCF with FFT<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import ccf\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_ccf = ccf(data, data + 1, adjusted=True, fft=True)\n&gt;&gt;&gt; pprint(res_ccf[1:11])\narray([0.95467704, 0.88790688, 0.82384458, 0.774129  , 0.73944515,\n       0.71137419, 0.69677541, 0.69417581, 0.71567822, 0.75516171])\n</code></pre> See Also <ul> <li><code>statsmodels.tsa.stattools.acf</code>: Estimate the autocorrelation function.</li> <li><code>statsmodels.tsa.stattools.pacf</code>: Partial autocorrelation estimation.</li> <li><code>statsmodels.tsa.stattools.ccf</code>: The cross-correlation function.</li> <li><code>ts_stat_tests.algorithms.correlation.acf</code>: Estimate the autocorrelation function</li> <li><code>ts_stat_tests.algorithms.correlation.pacf</code>: Partial autocorrelation estimate.</li> <li><code>ts_stat_tests.algorithms.correlation.ccf</code>: The cross-correlation function.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/correlation.py</code> <pre><code>@typechecked\ndef ccf(\n    x: ArrayLike,\n    y: ArrayLike,\n    adjusted: bool = True,\n    fft: bool = True,\n    *,\n    nlags: Optional[int] = None,\n    alpha: Optional[float] = None,\n) -&gt; Union[np.ndarray, tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    !!! note \"Summary\"\n\n        The cross-correlation function (CCF) is a statistical tool used in time series forecasting to measure the correlation between two time series at different lags. It is used to study the relationship between two time series, and can help to identify lead-lag relationships and causal effects.\n\n        This function will implement the [`ccf()`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.ccf.html) function from the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ???+ abstract \"Details\"\n\n        If `adjusted` is `True`, the denominator for the autocovariance is adjusted.\n\n        The CCF measures the correlation between two time series at different lags. It is calculated as the ratio of the covariance between the two series at lag k to the product of their standard deviations. The CCF is typically plotted as a graph, with the lag on the `x`-axis and the correlation coefficient on the `y`-axis.\n\n        The CCF at lag k is defined as:\n\n        $$\n        CCF(k) = Corr(X_t, Y_{t-k})\n        $$\n\n        where:\n\n        - $X_t$ and $Y_{t-k}$ are the values of the two time series at time $t$ and time $t-k$, respectively.\n        - $Corr()$ denotes the correlation coefficient between two variables.\n\n        ```\n        CCF(k) = Corr(X_t, Y_{t-k})\n        ```\n\n        If the CCF shows a strong positive correlation at lag $k$, this means that changes in one time series at time $t$ are strongly related to changes in the other time series at time $t-k$. This suggests a lead-lag relationship between the two time series, where changes in one series lead changes in the other series by a certain number of periods. The CCF can be used to estimate the time lag between the two time series.\n\n        The CCF can also help to identify causal relationships between two time series. If the CCF shows a strong positive correlation at lag $k$, and the lag is consistent with a causal relationship between the two time series, this suggests that changes in one time series are causing changes in the other time series.\n\n        Overall, the cross-correlation function is a valuable tool in time series forecasting, as it helps to study the relationship between two time series and to identify lead-lag relationships and causal effects. By identifying the relationship between two time series, the CCF can help to improve the accuracy of time series forecasting models.\n\n        The CCF can be calculated using the `ccf()` function in the `statsmodels` package in Python. The function takes two time series arrays as input and returns an array of cross-correlation coefficients at different lags. The significance of the cross-correlation coefficients can be tested using a similar test to the Ljung-Box test, such as the Box-Pierce test or the Breusch-Godfrey test. These tests can be performed using the `boxpierce()` and `lm()` functions in the `statsmodels` package, respectively. If the p-value of the test is less than a certain significance level (e.g. $0.05$), then there is evidence of significant cross-correlation between the two time series at the specified lag.\n\n    Params:\n        x (ArrayLike):\n            The time series data to use in the calculation.\n        y (ArrayLike):\n            The time series data to use in the calculation.\n        adjusted (bool, optional):\n            If `True`, then denominators for cross-correlation is $n-k$, otherwise $n$.&lt;br&gt;\n            Defaults to `True`.\n        fft (bool, optional):\n            If `True`, use FFT convolution. This method should be preferred for long time series.&lt;br&gt;\n            Defaults to `True`.\n        nlags (Optional[int], optional):\n            Number of lags to return cross-correlations for. If not provided, the number of lags equals len(x).\n            Defaults to `None`.\n        alpha (Optional[float], optional):\n            If a number is given, the confidence intervals for the given level are returned. For instance if `alpha=.05`, 95% confidence intervals are returned where the standard deviation is computed according to `1/sqrt(len(x))`.\n            Defaults to `None`.\n\n    Returns:\n        ccf (np.ndarray):\n            The cross-correlation function of `x` and `y`.\n\n    !!! success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/) library.\n\n    !!! example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test CCF without FFT\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import ccf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_ccf = ccf(data, data + 1, adjusted=True, fft=False)\n        &gt;&gt;&gt; pprint(res_ccf[1:11])\n        array([0.95467704, 0.88790688, 0.82384458, 0.774129  , 0.73944515,\n               0.71137419, 0.69677541, 0.69417581, 0.71567822, 0.75516171])\n        ```\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test CCF with FFT\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import ccf\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_ccf = ccf(data, data + 1, adjusted=True, fft=True)\n        &gt;&gt;&gt; pprint(res_ccf[1:11])\n        array([0.95467704, 0.88790688, 0.82384458, 0.774129  , 0.73944515,\n               0.71137419, 0.69677541, 0.69417581, 0.71567822, 0.75516171])\n        ```\n\n    ??? tip \"See Also\"\n        - [`statsmodels.tsa.stattools.acf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.acf.html): Estimate the autocorrelation function.\n        - [`statsmodels.tsa.stattools.pacf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.pacf.html): Partial autocorrelation estimation.\n        - [`statsmodels.tsa.stattools.ccf`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.ccf.html): The cross-correlation function.\n        - [`ts_stat_tests.algorithms.correlation.acf`][ts_stat_tests.algorithms.correlation.acf]: Estimate the autocorrelation function\n        - [`ts_stat_tests.algorithms.correlation.pacf`][ts_stat_tests.algorithms.correlation.pacf]: Partial autocorrelation estimate.\n        - [`ts_stat_tests.algorithms.correlation.ccf`][ts_stat_tests.algorithms.correlation.ccf]: The cross-correlation function.\n    \"\"\"\n    return st_ccf(\n        x=x,\n        y=y,\n        adjusted=adjusted,\n        fft=fft,\n        nlags=nlags,\n        alpha=alpha,\n    )\n</code></pre>"},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation.lb","title":"lb","text":"<pre><code>lb(\n    x: ArrayLike,\n    lags: Optional[Union[int, ArrayLike]] = None,\n    boxpierce: bool = False,\n    model_df: int = 0,\n    period: Optional[int] = None,\n    return_df: bool = True,\n    auto_lag: bool = False,\n) -&gt; pd.DataFrame\n</code></pre> <p>Summary</p> <p>The Ljung-Box test is a statistical test used in time series forecasting to test for the presence of autocorrelation in the residuals of a model. The test is based on the autocorrelation function (ACF) of the residuals, and can be used to assess the adequacy of a time series model and to identify areas for improvement.</p> <p>This function will implement the <code>acorr_ljungbox()</code> function from the <code>statsmodels</code> library.</p> Details <p>The Ljung-Box and Box-Pierce statistics differ in how they scale the autocorrelation function; the Ljung-Box test has better finite-sample properties.</p> <p>The test statistic is calculated as:</p> \\[ Q(m) = n(n+2) \\times \\sum_{k=1}^m \\left( \\frac{ r_k^2 }{ n-k } \\right) \\] <p>where:</p> <ul> <li>\\(n\\) is the sample size,</li> <li>\\(m\\) is the maximum lag being tested,</li> <li>\\(r_k\\) is the sample autocorrelation at lag \\(k\\), and</li> <li>\\(\\sum\\) (\\(sum\\)) denotes the sum over \\(k\\) from \\(1\\) to \\(m\\).</li> </ul> <pre><code>Q(m) = n(n+2) * Sum(r_k^2 / (n-k))\n</code></pre> <p>Under the null hypothesis, the test statistic follows a chi-squared distribution with degrees of freedom equal to \\(m-p\\), where \\(p\\) is the number of parameters estimated in fitting the time series model.</p> <p>The Ljung-Box test is performed by calculating the autocorrelation function (ACF) of the residuals from a time series model, and then comparing the ACF values to the expected values under the null hypothesis of no autocorrelation. The test statistic is calculated as the sum of the squared autocorrelations up to a given lag, and is compared to a chi-squared distribution with degrees of freedom equal to the number of lags tested.</p> <p>If the test statistic is greater than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is rejected, indicating that there is evidence of autocorrelation in the residuals. This suggests that the time series model is inadequate, and that additional terms may need to be added to the model to account for the remaining autocorrelation.</p> <p>If the test statistic is less than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is not rejected, indicating that there is no evidence of autocorrelation in the residuals. This suggests that the time series model is adequate, and that no further improvements are needed.</p> <p>Overall, the Ljung-Box test is a valuable tool in time series forecasting, as it helps to assess the adequacy of a time series model and to identify areas for improvement. By testing for autocorrelation in the residuals, the test helps to ensure that the model is accurately capturing the underlying patterns in the time series data.</p> <p>The Ljung-Box test can be calculated using the <code>acorr_ljungbox()</code> function in the <code>statsmodels</code> package in Python. The function takes a time series array and the maximum lag \\(m\\) as input, and returns an array of Q-statistics and associated p-values for each lag up to \\(m\\). If the p-value of the test is less than a certain significance level (e.g. \\(0.05\\)), then there is evidence of significant autocorrelation in the time series up to the specified lag.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ArrayLike</code> <p>The data series. The data is demeaned before the test statistic is computed.</p> required <code>lags</code> <code>Optional[Union[int, ArrayLike]]</code> <p>If lags is an integer (<code>int</code>) then this is taken to be the largest lag that is included, the test result is reported for all smaller lag length. If lags is a list or array, then all lags are included up to the largest lag in the list, however only the tests for the lags in the list are reported. If lags is <code>None</code>, then the default maxlag is currently \\(\\min(\\frac{nobs}{2}-2,40)\\) (calculated with: <code>min(nobs // 2 - 2, 40)</code>). The default number of <code>lags</code> changes if <code>period</code> is set.</p> <p>Deprecation</p> <p>After <code>statsmodels</code> version <code>0.12</code>, this will calculation change from</p> \\[ \\min(\\frac{nobs}{2}-2,40) \\] <p>to</p> \\[ \\min(10,\\frac{nobs}{5}) \\] <p>Defaults to <code>None</code>.</p> <code>None</code> <code>boxpierce</code> <code>bool</code> <p>If <code>True</code>, then additional to the results of the Ljung-Box test also the Box-Pierce test results are returned. Defaults to <code>False</code>.</p> <code>False</code> <code>model_df</code> <code>int</code> <p>Number of degrees of freedom consumed by the model. In an ARMA model, this value is usually \\(p+q\\) where \\(p\\) is the AR order and \\(q\\) is the MA order. This value is subtracted from the degrees-of-freedom used in the test so that the adjusted dof for the statistics are \\(lags - model_df\\). If \\(lags - model_df &lt;= 0\\), then <code>NaN</code> is returned. Defaults to <code>0</code>.</p> <code>0</code> <code>period</code> <code>Optional[int]</code> <p>The period of a Seasonal time series. Used to compute the max lag for seasonal data which uses \\(\\min(2 \\times period, \\frac{nobs}{5})\\) (calculated with: <code>min(2*period,nobs//5)</code>) if set. If <code>None</code>, then the default rule is used to set the number of lags. When set, must be \\(&gt;= 2\\). Defaults to <code>None</code>.</p> <code>None</code> <code>return_df</code> <code>bool</code> <p>Flag indicating whether to return the result as a single DataFrame with columns <code>lb_stat</code>, <code>lb_pvalue</code>, and optionally <code>bp_stat</code> and <code>bp_pvalue</code>. Set to <code>True</code> to return the DataFrame or <code>False</code> to continue returning the \\(2-4\\) output. If <code>None</code> (the default), a warning is raised.</p> <p>Deprecation</p> <p>After <code>statsmodels</code> version <code>0.12</code>, this will become the only return method.</p> <p>Defaults to <code>True</code>.</p> <code>True</code> <code>auto_lag</code> <code>bool</code> <p>Flag indicating whether to automatically determine the optimal lag length based on threshold of maximum correlation value. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>lbvalue</code> <code>Union[float, ndarray]</code> <p>The Ljung-Box test statistic.</p> <code>pvalue</code> <code>Union[float, ndarray]</code> <p>The p-value based on chi-square distribution. The p-value is computed as \\(1-\\text{cdf}(lbvalue,dof)\\) where \\(dof\\) is \\(lag - model\\_df\\) (calculated with: <code>1.0 - chi2.cdf(lbvalue, dof)</code>). If \\(lag - model\\_df &lt;= 0\\), then <code>NaN</code> is returned for the <code>pvalue</code>.</p> <code>bpvalue</code> <code>Optional[Union[float, ndarray]]</code> <p>The test statistic for Box-Pierce test.</p> <code>bppvalue</code> <code>Optional[Union[float, ndarray]]</code> <p>The p-value based for Box-Pierce test on chi-square distribution. The p-value is computed as \\(1-\\text{cdf}(bpvalue,dof)\\) where \\(dof\\) is \\(lag - model_df\\) (calculated with: <code>1.0 - chi2.cdf(bpvalue, dof)</code>). If \\(lag - model_df &lt;= 0\\), then <code>NaN</code> is returned for the <code>pvalue</code>.</p> <p>Credit</p> <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> <p>Examples</p> Python<pre><code>&gt;&gt;&gt; import statsmodels.api as sm\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import lb\n&gt;&gt;&gt; data = sm.datasets.sunspots.load_pandas().data\n&gt;&gt;&gt; res = sm.tsa.ARIMA(data[\"SUNACTIVITY\"], order=(1, 0, 1)).fit()\n&gt;&gt;&gt; lb(res.resid, lags=[10], return_df=True)\n    lb_stat     lb_pvalue\n10  214.106992  1.827374e-40\n</code></pre> References <ul> <li>Green, W. \"Econometric Analysis,\" 5th ed., Pearson, 2003.</li> <li>J. Carlos Escanciano, Ignacio N. Lobato \"An automatic Portmanteau test for serial correlation\"., Volume 151, 2009.</li> </ul> See Also <ul> <li><code>statsmodels.regression.linear_model.OLS.fit</code>:</li> <li><code>statsmodels.regression.linear_model.RegressionResults</code>: The output results of a linear regression model.</li> <li><code>statsmodels.stats.diagnostic.acorr_ljungbox</code>: Ljung-Box test for serial correlation.</li> <li><code>statsmodels.stats.diagnostic.acorr_lm</code>: Lagrange Multiplier tests for autocorrelation.</li> <li><code>statsmodels.stats.diagnostic.acorr_breusch_godfrey</code>: Breusch-Godfrey test for serial correlation.</li> <li><code>ts_stat_tests.algorithms.correlation.lb</code>: Ljung-Box test of autocorrelation in residuals.</li> <li><code>ts_stat_tests.algorithms.correlation.lm</code>: Lagrange Multiplier tests for autocorrelation.</li> <li><code>ts_stat_tests.algorithms.correlation.bglm</code>: Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/correlation.py</code> <pre><code>@typechecked\ndef lb(\n    x: ArrayLike,\n    lags: Optional[Union[int, ArrayLike]] = None,\n    boxpierce: bool = False,\n    model_df: int = 0,\n    period: Optional[int] = None,\n    return_df: bool = True,\n    auto_lag: bool = False,\n) -&gt; pd.DataFrame:\n    r\"\"\"\n    !!! note \"Summary\"\n\n        The Ljung-Box test is a statistical test used in time series forecasting to test for the presence of autocorrelation in the residuals of a model. The test is based on the autocorrelation function (ACF) of the residuals, and can be used to assess the adequacy of a time series model and to identify areas for improvement.\n\n        This function will implement the [`acorr_ljungbox()`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_ljungbox.html) function from the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ???+ abstract \"Details\"\n\n        The Ljung-Box and Box-Pierce statistics differ in how they scale the autocorrelation function; the Ljung-Box test has better finite-sample properties.\n\n        The test statistic is calculated as:\n\n        $$\n        Q(m) = n(n+2) \\times \\sum_{k=1}^m \\left( \\frac{ r_k^2 }{ n-k } \\right)\n        $$\n\n        where:\n\n        - $n$ is the sample size,\n        - $m$ is the maximum lag being tested,\n        - $r_k$ is the sample autocorrelation at lag $k$, and\n        - $\\sum$ ($sum$) denotes the sum over $k$ from $1$ to $m$.\n\n        ```\n        Q(m) = n(n+2) * Sum(r_k^2 / (n-k))\n        ```\n\n        Under the null hypothesis, the test statistic follows a chi-squared distribution with degrees of freedom equal to $m-p$, where $p$ is the number of parameters estimated in fitting the time series model.\n\n        The Ljung-Box test is performed by calculating the autocorrelation function (ACF) of the residuals from a time series model, and then comparing the ACF values to the expected values under the null hypothesis of no autocorrelation. The test statistic is calculated as the sum of the squared autocorrelations up to a given lag, and is compared to a chi-squared distribution with degrees of freedom equal to the number of lags tested.\n\n        If the test statistic is greater than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is rejected, indicating that there is evidence of autocorrelation in the residuals. This suggests that the time series model is inadequate, and that additional terms may need to be added to the model to account for the remaining autocorrelation.\n\n        If the test statistic is less than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is not rejected, indicating that there is no evidence of autocorrelation in the residuals. This suggests that the time series model is adequate, and that no further improvements are needed.\n\n        Overall, the Ljung-Box test is a valuable tool in time series forecasting, as it helps to assess the adequacy of a time series model and to identify areas for improvement. By testing for autocorrelation in the residuals, the test helps to ensure that the model is accurately capturing the underlying patterns in the time series data.\n\n        The Ljung-Box test can be calculated using the `acorr_ljungbox()` function in the `statsmodels` package in Python. The function takes a time series array and the maximum lag $m$ as input, and returns an array of Q-statistics and associated p-values for each lag up to $m$. If the p-value of the test is less than a certain significance level (e.g. $0.05$), then there is evidence of significant autocorrelation in the time series up to the specified lag.\n\n    Params:\n        x (ArrayLike):\n            The data series. The data is demeaned before the test statistic is computed.\n        lags (Optional[Union[int, ArrayLike]], optional):\n            If lags is an integer (`int`) then this is taken to be the largest lag that is included, the test result is reported for all smaller lag length. If lags is a list or array, then all lags are included up to the largest lag in the list, however only the tests for the lags in the list are reported. If lags is `None`, then the default maxlag is currently $\\min(\\frac{nobs}{2}-2,40)$ (calculated with: `min(nobs // 2 - 2, 40)`). The default number of `lags` changes if `period` is set.&lt;br&gt;\n            !!! deprecation \"Deprecation\"\n                After `statsmodels` version `0.12`, this will calculation change from\n\n                $$\n                \\min(\\frac{nobs}{2}-2,40)\n                $$\n\n                to\n\n                $$\n                \\min(10,\\frac{nobs}{5})\n                $$\n            Defaults to `None`.\n        boxpierce (bool, optional):\n            If `True`, then additional to the results of the Ljung-Box test also the Box-Pierce test results are returned.&lt;br&gt;\n            Defaults to `False`.\n        model_df (int, optional):\n            Number of degrees of freedom consumed by the model. In an ARMA model, this value is usually $p+q$ where $p$ is the AR order and $q$ is the MA order. This value is subtracted from the degrees-of-freedom used in the test so that the adjusted dof for the statistics are $lags - model_df$. If $lags - model_df &lt;= 0$, then `NaN` is returned.&lt;br&gt;\n            Defaults to `0`.\n        period (Optional[int], optional):\n            The period of a Seasonal time series. Used to compute the max lag for seasonal data which uses $\\min(2 \\times period, \\frac{nobs}{5})$ (calculated with: `min(2*period,nobs//5)`) if set. If `None`, then the default rule is used to set the number of lags. When set, must be $&gt;= 2$.&lt;br&gt;\n            Defaults to `None`.\n        return_df (bool, optional):\n            Flag indicating whether to return the result as a single DataFrame with columns `lb_stat`, `lb_pvalue`, and optionally `bp_stat` and `bp_pvalue`. Set to `True` to return the DataFrame or `False` to continue returning the $2-4$ output. If `None` (the default), a warning is raised.\n            !!! deprecation \"Deprecation\"\n                After `statsmodels` version `0.12`, this will become the only return method.\n            Defaults to `True`.\n        auto_lag (bool, optional):\n            Flag indicating whether to automatically determine the optimal lag length based on threshold of maximum correlation value.&lt;br&gt;\n            Defaults to `False`.\n\n    Returns:\n        lbvalue (Union[float, np.ndarray]):\n            The Ljung-Box test statistic.\n        pvalue (Union[float, np.ndarray]):\n            The p-value based on chi-square distribution. The p-value is computed as $1-\\text{cdf}(lbvalue,dof)$ where $dof$ is $lag - model\\_df$ (calculated with: `1.0 - chi2.cdf(lbvalue, dof)`). If $lag - model\\_df &lt;= 0$, then `NaN` is returned for the `pvalue`.\n        bpvalue (Optional[Union[float, np.ndarray]]):\n            The test statistic for Box-Pierce test.\n        bppvalue (Optional[Union[float, np.ndarray]]):\n            The p-value based for Box-Pierce test on chi-square distribution. The p-value is computed as $1-\\text{cdf}(bpvalue,dof)$ where $dof$ is $lag - model_df$ (calculated with: `1.0 - chi2.cdf(bpvalue, dof)`). If $lag - model_df &lt;= 0$, then `NaN` is returned for the `pvalue`.\n\n    !!! success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/) library.\n\n    !!! example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Python\"}\n        &gt;&gt;&gt; import statsmodels.api as sm\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import lb\n        &gt;&gt;&gt; data = sm.datasets.sunspots.load_pandas().data\n        &gt;&gt;&gt; res = sm.tsa.ARIMA(data[\"SUNACTIVITY\"], order=(1, 0, 1)).fit()\n        &gt;&gt;&gt; lb(res.resid, lags=[10], return_df=True)\n            lb_stat     lb_pvalue\n        10  214.106992  1.827374e-40\n        ```\n\n    ??? question \"References\"\n        - Green, W. \"Econometric Analysis,\" 5th ed., Pearson, 2003.\n        - J. Carlos Escanciano, Ignacio N. Lobato \"An automatic Portmanteau test for serial correlation\"., Volume 151, 2009.\n\n    ??? tip \"See Also\"\n        - [`statsmodels.regression.linear_model.OLS.fit`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.fit.html):\n        - [`statsmodels.regression.linear_model.RegressionResults`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.RegressionResults.html): The output results of a linear regression model.\n        - [`statsmodels.stats.diagnostic.acorr_ljungbox`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_ljungbox.html): Ljung-Box test for serial correlation.\n        - [`statsmodels.stats.diagnostic.acorr_lm`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_lm.html): Lagrange Multiplier tests for autocorrelation.\n        - [`statsmodels.stats.diagnostic.acorr_breusch_godfrey`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_breusch_godfrey.html): Breusch-Godfrey test for serial correlation.\n        - [`ts_stat_tests.algorithms.correlation.lb`][ts_stat_tests.algorithms.correlation.lb]: Ljung-Box test of autocorrelation in residuals.\n        - [`ts_stat_tests.algorithms.correlation.lm`][ts_stat_tests.algorithms.correlation.lm]: Lagrange Multiplier tests for autocorrelation.\n        - [`ts_stat_tests.algorithms.correlation.bglm`][ts_stat_tests.algorithms.correlation.bglm]: Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation.\n    \"\"\"\n    return acorr_ljungbox(\n        x=x,\n        lags=lags,\n        boxpierce=boxpierce,\n        model_df=model_df,\n        period=period,\n        return_df=return_df,\n        auto_lag=auto_lag,\n    )\n</code></pre>"},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation.lm","title":"lm","text":"<pre><code>lm(\n    resid: ArrayLike,\n    nlags: Optional[int] = None,\n    *,\n    store: Literal[False] = False,\n    period: Optional[int] = None,\n    ddof: int = 0,\n    cov_type: Literal[\"nonrobust\"] = \"nonrobust\",\n    cov_kwargs: Optional[dict] = None\n) -&gt; tuple[float, float, float, float]\n</code></pre><pre><code>lm(\n    resid: ArrayLike,\n    nlags: Optional[int] = None,\n    *,\n    store: Literal[True],\n    period: Optional[int] = None,\n    ddof: int = 0,\n    cov_type: Literal[\"nonrobust\"] = \"nonrobust\",\n    cov_kwargs: Optional[dict] = None\n) -&gt; tuple[\n    np.ndarray, np.ndarray, float, float, ResultsStore\n]\n</code></pre><pre><code>lm(\n    resid: ArrayLike,\n    nlags: Optional[int] = None,\n    *,\n    store: Literal[False] = False,\n    period: Optional[int] = None,\n    ddof: int = 0,\n    cov_type: VALID_LM_COV_TYPE_OPTIONS,\n    cov_kwargs: Optional[dict] = None\n) -&gt; tuple[np.ndarray, np.ndarray, float, float]\n</code></pre><pre><code>lm(\n    resid: ArrayLike,\n    nlags: Optional[int] = None,\n    *,\n    store: Literal[True],\n    period: Optional[int] = None,\n    ddof: int = 0,\n    cov_type: VALID_LM_COV_TYPE_OPTIONS,\n    cov_kwargs: Optional[dict] = None\n) -&gt; tuple[float, float, float, float, ResultsStore]\n</code></pre> <pre><code>lm(\n    resid: ArrayLike,\n    nlags: Optional[int] = None,\n    *,\n    store: bool = False,\n    period: Optional[int] = None,\n    ddof: int = 0,\n    cov_type: Union[\n        Literal[\"nonrobust\"], VALID_LM_COV_TYPE_OPTIONS\n    ] = \"nonrobust\",\n    cov_kwargs: Optional[dict] = None\n) -&gt; Union[\n    tuple[np.ndarray, np.ndarray, float, float],\n    tuple[\n        np.ndarray, np.ndarray, float, float, ResultsStore\n    ],\n    tuple[float, float, float, float],\n    tuple[float, float, float, float, ResultsStore],\n]\n</code></pre> <p>Summary</p> <p>The Lagrange Multiplier (LM) test is a statistical test used in time series forecasting to test for the presence of autocorrelation in a model. The test is based on the residual sum of squares (RSS) of a time series model, and can be used to assess the adequacy of the model and to identify areas for improvement.</p> <p>This function will implement the <code>acorr_lm()</code> function from the <code>statsmodels</code> library.</p> Details <p>This is a generic Lagrange Multiplier (LM) test for autocorrelation. It returns Engle's ARCH test if <code>resid</code> is the squared residual array. The Breusch-Godfrey test is a variation on this LM test with additional exogenous variables in the auxiliary regression.</p> <p>The LM test statistic is computed as</p> \\[ LM = (n_{obs} - ddof) \\times R^2, \\] <pre><code>LM = (n_obs - ddof) * R^2\n</code></pre> <p>where \\(R^2\\) is the coefficient of determination from the auxiliary regression of the residuals on their own <code>nlags</code> lags (and any additional regressors included in the model), \\(n_{obs}\\) is the number of observations, and \\(ddof\\) is the model degrees of freedom lost due to parameter estimation.</p> <p>&lt;!-- Previous algorithm included below</p> \\[ LM = n \\times (n+2) \\times \\sum_{k=1}^m \\left( \\frac { r_k^2 }{ n-k } \\right) - 2 \\times (n-1) \\times (n-2) \\times \\sum_{k=1}^m \\left( r_k \\times \\frac { r_{k+1} }{ n-k } \\right) \\] <p>where:</p> <ul> <li>\\(n\\) is the sample size,</li> <li>\\(m\\) is the maximum lag being tested,</li> <li>\\(r_k\\) is the sample autocorrelation at lag \\(k\\), and</li> <li>\\(\\sum\\) (\\(sum\\)) denotes the sum over \\(k\\) from \\(1\\) to \\(m\\).</li> </ul> <pre><code>LM = n * (n+2) * Sum(r_k^2 / (n-k)) - 2 * (n-1) * (n-2) * Sum(r_k * r_(k+1) / (n-k))\n</code></pre> <p>--&gt;</p> <p>In practice, the LM test proceeds by:</p> <ul> <li>Fitting a time series model to the data and obtaining the residuals.</li> <li>Running an auxiliary regression of these residuals on their past <code>nlags</code> values (and any relevant exogenous variables).</li> <li>Computing the LM statistic as \\((n_{obs} - ddof) \\times R^2\\) from this auxiliary regression.</li> </ul> <p>Under the null hypothesis that the autocorrelations up to the specified lag are zero (no serial correlation in the residuals), the LM statistic is asymptotically distributed as a chi-squared random variable with degrees of freedom equal to the number of lagged residual terms included in the auxiliary regression (i.e. the number of lags being tested, adjusted for any restrictions implied by the model).</p> <p>If the test statistic is greater than the critical value from the chi-squared distribution (or equivalently, if the p-value is less than a chosen significance level such as \\(0.05\\)), then the null hypothesis of no autocorrelation is rejected, indicating that there is evidence of autocorrelation in the residuals. This suggests that the time series model may be inadequate and that additional terms may need to be added to the model to account for the remaining autocorrelation.</p> <p>If the test statistic is less than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is not rejected, indicating that there is no evidence of autocorrelation in the residuals. This suggests that the time series model is adequate and that no further improvements are needed with respect to serial correlation.</p> <p>The LM test is a generalization of the Durbin-Watson test, which is a simpler test that only tests for first-order autocorrelation. The LM test can be used to test for higher-order autocorrelation and is more powerful than the Durbin-Watson test.</p> <p>Overall, the Lagrange Multiplier test is a valuable tool in time series forecasting, as it helps to assess the adequacy of a time series model and to identify areas for improvement. By testing for autocorrelation in the residuals, the test helps to ensure that the model is accurately capturing the underlying patterns in the time series data.</p> <p>The LM test can be calculated using the <code>acorr_lm()</code> function in the <code>statsmodels</code> package in Python. The function takes a time series array and the maximum lag <code>m</code> as input, and returns the LM test statistic and associated p-value. If the p-value of the test is less than a certain significance level (e.g. \\(0.05\\)), then there is evidence of significant autocorrelation in the time series up to the specified lag.</p> <p>Parameters:</p> Name Type Description Default <code>resid</code> <code>ArrayLike</code> <p>Time series to test.</p> required <code>nlags</code> <code>Optional[int]</code> <p>Highest lag to use. Defaults to <code>None</code>.</p> <p>Deprecation</p> <p>The behavior of this parameter will change after <code>statsmodels</code> version <code>0.12</code>.</p> <code>None</code> <code>store</code> <code>bool</code> <p>If <code>True</code> then the intermediate results are also returned. Defaults to <code>False</code>.</p> <code>False</code> <code>period</code> <code>Optional[int]</code> <p>The period of a Seasonal time series. Used to compute the max lag for seasonal data which uses \\(\\min(2 \\times period, \\frac{nobs}{5})\\) (calculated with: <code>min(2*period,nobs//5)</code>) if set. If <code>None</code>, then the default rule is used to set the number of lags. When set, must be \\(&gt;=\\) <code>2</code>. Defaults to <code>None</code>.</p> <code>None</code> <code>ddof</code> <code>int</code> <p>The number of degrees of freedom consumed by the model used to produce resid Defaults to <code>0</code>.</p> <code>0</code> <code>cov_type</code> <code>Union[Literal['nonrobust'], VALID_LM_COV_TYPE_OPTIONS]</code> <p>Covariance type. The default is <code>\"nonrobust\"</code> which uses the classic OLS covariance estimator. Specify one of <code>\"HC0\"</code>, <code>\"HC1\"</code>, <code>\"HC2\"</code>, <code>\"HC3\"</code> to use White's covariance estimator. All covariance types supported by <code>OLS.fit</code> are accepted. Defaults to <code>\"nonrobust\"</code>.</p> <code>'nonrobust'</code> <code>cov_kwargs</code> <code>Optional[dict]</code> <p>Dictionary of covariance options passed to <code>OLS.fit</code>. See <code>OLS.fit</code> for more details. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>lm</code> <code>float</code> <p>Lagrange multiplier test statistic.</p> <code>lmpval</code> <code>float</code> <p>The <code>p-value</code> for Lagrange multiplier test.</p> <code>fval</code> <code>float</code> <p>The <code>f-statistic</code> of the F test, alternative version of the same test based on F test for the parameter restriction.</p> <code>fpval</code> <code>float</code> <p>The <code>p-value</code> of the F test.</p> <code>res_store</code> <code>Optional[ResultsStore]</code> <p>Intermediate results. Only returned if <code>store=True</code>.</p> <p>Credit</p> <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> <p>Examples</p> Test Lagrange Multiplier for autocorrelation<pre><code>&gt;&gt;&gt; from pprint import pprint\n&gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import lm\n&gt;&gt;&gt; data = load_airline()\n&gt;&gt;&gt; res_lm, res_lmpval, res_fval, res_fpval = lm(data)\n&gt;&gt;&gt; pprint(res_lm)\n128.09655717844828\n&gt;&gt;&gt; pprint(res_lmpval)\n1.1416848684314836e-22\n&gt;&gt;&gt; pprint(res_fval)\n266.89301496118736\n&gt;&gt;&gt; pprint(res_fpval)\n2.36205831339912e-78\n</code></pre> See Also <ul> <li><code>statsmodels.regression.linear_model.OLS.fit</code>: Fit a linear model.</li> <li><code>statsmodels.regression.linear_model.RegressionResults</code>: The output results of a linear regression model.</li> <li><code>statsmodels.stats.diagnostic.het_arch</code>: Conditional heteroskedasticity testing.</li> <li><code>statsmodels.stats.diagnostic.acorr_ljungbox</code>: Ljung-Box test for serial correlation.</li> <li><code>statsmodels.stats.diagnostic.acorr_lm</code>: Lagrange Multiplier tests for autocorrelation.</li> <li><code>statsmodels.stats.diagnostic.acorr_breusch_godfrey</code>: Breusch-Godfrey test for serial correlation.</li> <li><code>ts_stat_tests.algorithms.correlation.lb</code>: Ljung-Box test of autocorrelation in residuals.</li> <li><code>ts_stat_tests.algorithms.correlation.lm</code>: Lagrange Multiplier tests for autocorrelation.</li> <li><code>ts_stat_tests.algorithms.correlation.bglm</code>: Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/correlation.py</code> <pre><code>@typechecked\ndef lm(\n    resid: ArrayLike,\n    nlags: Optional[int] = None,\n    *,\n    store: bool = False,\n    period: Optional[int] = None,\n    ddof: int = 0,\n    cov_type: Union[Literal[\"nonrobust\"], VALID_LM_COV_TYPE_OPTIONS] = \"nonrobust\",\n    cov_kwargs: Optional[dict] = None,\n) -&gt; Union[\n    tuple[np.ndarray, np.ndarray, float, float],\n    tuple[np.ndarray, np.ndarray, float, float, ResultsStore],\n    tuple[float, float, float, float],\n    tuple[float, float, float, float, ResultsStore],\n]:\n    \"\"\"\n    !!! note \"Summary\"\n\n        The Lagrange Multiplier (LM) test is a statistical test used in time series forecasting to test for the presence of autocorrelation in a model. The test is based on the residual sum of squares (RSS) of a time series model, and can be used to assess the adequacy of the model and to identify areas for improvement.\n\n        This function will implement the [`acorr_lm()`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_lm.html) function from the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ???+ abstract \"Details\"\n\n        This is a generic Lagrange Multiplier (LM) test for autocorrelation. It returns Engle's ARCH test if `resid` is the squared residual array. The Breusch-Godfrey test is a variation on this LM test with additional exogenous variables in the auxiliary regression.\n\n        The LM test statistic is computed as\n\n        $$\n        LM = (n_{obs} - ddof) \\\\times R^2,\n        $$\n\n        ```\n        LM = (n_obs - ddof) * R^2\n        ```\n\n        where $R^2$ is the coefficient of determination from the **auxiliary regression** of the residuals on their own `nlags` lags (and any additional regressors included in the model), $n_{obs}$ is the number of observations, and $ddof$ is the model degrees of freedom lost due to parameter estimation.\n\n        &lt;!-- Previous algorithm included below\n\n        $$\n        LM = n \\\\times (n+2) \\\\times \\\\sum_{k=1}^m \\\\left( \\\\frac { r_k^2 }{ n-k } \\\\right) - 2 \\\\times (n-1) \\\\times (n-2) \\\\times \\\\sum_{k=1}^m \\\\left( r_k \\\\times \\\\frac { r_{k+1} }{ n-k } \\\\right)\n        $$\n\n        where:\n\n        - $n$ is the sample size,\n        - $m$ is the maximum lag being tested,\n        - $r_k$ is the sample autocorrelation at lag $k$, and\n        - $\\\\sum$ ($sum$) denotes the sum over $k$ from $1$ to $m$.\n\n        ```\n        LM = n * (n+2) * Sum(r_k^2 / (n-k)) - 2 * (n-1) * (n-2) * Sum(r_k * r_(k+1) / (n-k))\n        ```\n\n        --&gt;\n\n        In practice, the LM test proceeds by:\n\n        - Fitting a time series model to the data and obtaining the residuals.\n        - Running an auxiliary regression of these residuals on their past `nlags` values (and any relevant exogenous variables).\n        - Computing the LM statistic as $(n_{obs} - ddof) \\\\times R^2$ from this auxiliary regression.\n\n        Under the null hypothesis that the autocorrelations up to the specified lag are zero (no serial correlation in the residuals), the LM statistic is asymptotically distributed as a chi-squared random variable with degrees of freedom equal to the number of lagged residual terms included in the auxiliary regression (i.e. the number of lags being tested, adjusted for any restrictions implied by the model).\n\n        If the test statistic is greater than the critical value from the chi-squared distribution (or equivalently, if the p-value is less than a chosen significance level such as $0.05$), then the null hypothesis of no autocorrelation is rejected, indicating that there is evidence of autocorrelation in the residuals. This suggests that the time series model may be inadequate and that additional terms may need to be added to the model to account for the remaining autocorrelation.\n\n        If the test statistic is less than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is not rejected, indicating that there is no evidence of autocorrelation in the residuals. This suggests that the time series model is adequate and that no further improvements are needed with respect to serial correlation.\n\n        The LM test is a generalization of the Durbin-Watson test, which is a simpler test that only tests for first-order autocorrelation. The LM test can be used to test for higher-order autocorrelation and is more powerful than the Durbin-Watson test.\n\n        Overall, the Lagrange Multiplier test is a valuable tool in time series forecasting, as it helps to assess the adequacy of a time series model and to identify areas for improvement. By testing for autocorrelation in the residuals, the test helps to ensure that the model is accurately capturing the underlying patterns in the time series data.\n\n        The LM test can be calculated using the `acorr_lm()` function in the `statsmodels` package in Python. The function takes a time series array and the maximum lag `m` as input, and returns the LM test statistic and associated p-value. If the p-value of the test is less than a certain significance level (e.g. $0.05$), then there is evidence of significant autocorrelation in the time series up to the specified lag.\n\n    Params:\n        resid (ArrayLike):\n            Time series to test.\n        nlags (Optional[int], optional):\n            Highest lag to use.&lt;br&gt;\n            Defaults to `None`.\n            !!! deprecation \"Deprecation\"\n                The behavior of this parameter will change after `statsmodels` version `0.12`.\n        store (bool, optional):\n            If `True` then the intermediate results are also returned.&lt;br&gt;\n            Defaults to `False`.\n        period (Optional[int], optional):\n            The period of a Seasonal time series. Used to compute the max lag for seasonal data which uses $\\\\min(2 \\\\times period, \\\\frac{nobs}{5})$ (calculated with: `min(2*period,nobs//5)`) if set. If `None`, then the default rule is used to set the number of lags. When set, must be $&gt;=$ `2`.&lt;br&gt;\n            Defaults to `None`.\n        ddof (int, optional):\n            The number of degrees of freedom consumed by the model used to produce resid&lt;br&gt;\n            Defaults to `0`.\n        cov_type (Union[Literal[\"nonrobust\"], VALID_LM_COV_TYPE_OPTIONS], optional):\n            Covariance type. The default is `\"nonrobust\"` which uses the classic OLS covariance estimator. Specify one of `\"HC0\"`, `\"HC1\"`, `\"HC2\"`, `\"HC3\"` to use White's covariance estimator. All covariance types supported by `OLS.fit` are accepted.&lt;br&gt;\n            Defaults to `\"nonrobust\"`.\n        cov_kwargs (Optional[dict], optional):\n            Dictionary of covariance options passed to `OLS.fit`. See [`OLS.fit`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.fit.html) for more details.&lt;br&gt;\n            Defaults to `None`.\n\n    Returns:\n        lm (float):\n            Lagrange multiplier test statistic.\n        lmpval (float):\n            The `p-value` for Lagrange multiplier test.\n        fval (float):\n            The `f-statistic` of the F test, alternative version of the same test based on F test for the parameter restriction.\n        fpval (float):\n            The `p-value` of the F test.\n        res_store (Optional[ResultsStore]):\n            Intermediate results. Only returned if `store=True`.\n\n    !!! success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/) library.\n\n    !!! example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test Lagrange Multiplier for autocorrelation\"}\n        &gt;&gt;&gt; from pprint import pprint\n        &gt;&gt;&gt; from ts_stat_tests.utils.data import load_airline\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import lm\n        &gt;&gt;&gt; data = load_airline()\n        &gt;&gt;&gt; res_lm, res_lmpval, res_fval, res_fpval = lm(data)\n        &gt;&gt;&gt; pprint(res_lm)\n        128.09655717844828\n        &gt;&gt;&gt; pprint(res_lmpval)\n        1.1416848684314836e-22\n        &gt;&gt;&gt; pprint(res_fval)\n        266.89301496118736\n        &gt;&gt;&gt; pprint(res_fpval)\n        2.36205831339912e-78\n        ```\n\n    ??? tip \"See Also\"\n        - [`statsmodels.regression.linear_model.OLS.fit`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.fit.html): Fit a linear model.\n        - [`statsmodels.regression.linear_model.RegressionResults`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.RegressionResults.html): The output results of a linear regression model.\n        - [`statsmodels.stats.diagnostic.het_arch`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.het_arch.html#statsmodels.stats.diagnostic.het_arch): Conditional heteroskedasticity testing.\n        - [`statsmodels.stats.diagnostic.acorr_ljungbox`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_ljungbox.html): Ljung-Box test for serial correlation.\n        - [`statsmodels.stats.diagnostic.acorr_lm`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_lm.html): Lagrange Multiplier tests for autocorrelation.\n        - [`statsmodels.stats.diagnostic.acorr_breusch_godfrey`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_breusch_godfrey.html): Breusch-Godfrey test for serial correlation.\n        - [`ts_stat_tests.algorithms.correlation.lb`][ts_stat_tests.algorithms.correlation.lb]: Ljung-Box test of autocorrelation in residuals.\n        - [`ts_stat_tests.algorithms.correlation.lm`][ts_stat_tests.algorithms.correlation.lm]: Lagrange Multiplier tests for autocorrelation.\n        - [`ts_stat_tests.algorithms.correlation.bglm`][ts_stat_tests.algorithms.correlation.bglm]: Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation.\n    \"\"\"\n    return acorr_lm(  # type: ignore  # statsmodels' acorr_lm has incomplete type hints for these arguments\n        resid=resid,\n        nlags=nlags,\n        store=store,\n        period=period,\n        ddof=ddof,\n        cov_type=cov_type,\n        cov_kwargs=cov_kwargs,\n    )\n</code></pre>"},{"location":"code/correlation/#ts_stat_tests.algorithms.correlation.bglm","title":"bglm","text":"<pre><code>bglm(\n    res: Union[RegressionResults, RegressionResultsWrapper],\n    nlags: Optional[int] = None,\n    *,\n    store: Literal[False] = False\n) -&gt; tuple[float, float, float, float]\n</code></pre><pre><code>bglm(\n    res: Union[RegressionResults, RegressionResultsWrapper],\n    nlags: Optional[int] = None,\n    *,\n    store: Literal[True]\n) -&gt; tuple[float, float, float, float, ResultsStore]\n</code></pre> <pre><code>bglm(\n    res: Union[RegressionResults, RegressionResultsWrapper],\n    nlags: Optional[int] = None,\n    *,\n    store: bool = False\n) -&gt; Union[\n    tuple[float, float, float, float],\n    tuple[float, float, float, float, ResultsStore],\n]\n</code></pre> <p>Summary</p> <p>The Breusch-Godfrey Lagrange Multiplier (BGLM) test is a statistical test used in time series forecasting to test for the presence of autocorrelation in the residuals of a model. The test is a generalization of the LM test and can be used to test for autocorrelation up to a specified order.</p> <p>This function will implement the <code>acorr_breusch_godfrey()</code> function from the <code>statsmodels</code> library.</p> Details <p>BG adds lags of residual to exog in the design matrix for the auxiliary regression with residuals as endog. See Greene (2002), section 12.7.1.</p> <p>The BGLM test is performed by first fitting a time series model to the data and then obtaining the residuals from the model. The residuals are then used to estimate the autocorrelation function (ACF) up to a specified order, typically using the Box-Pierce or Ljung-Box tests. The estimated ACF values are then used to construct the BGLM test statistic, which is compared to a chi-squared distribution with degrees of freedom equal to the number of lags tested.</p> <p>The BGLM test statistic is calculated as:</p> \\[ BGLM = n \\times R^2 \\] <p>where:</p> <ul> <li>\\(n\\) is the sample size and</li> <li>\\(R^2\\) is the coefficient of determination from a regression of the residuals on the lagged values of the residuals and the lagged values of the predictor variable.</li> </ul> <pre><code>BGLM = n * R^2\n</code></pre> <p>Under the null hypothesis that there is no autocorrelation in the residuals of the regression model, the BGLM test statistic follows a chi-squared distribution with degrees of freedom equal to the number of lags included in the model.</p> <p>If the test statistic is greater than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is rejected, indicating that there is evidence of autocorrelation in the residuals. This suggests that the time series model is inadequate, and that additional terms may need to be added to the model to account for the remaining autocorrelation.</p> <p>If the test statistic is less than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is not rejected, indicating that there is no evidence of autocorrelation in the residuals. This suggests that the time series model is adequate, and that no further improvements are needed.</p> <p>Overall, the Breusch-Godfrey Lagrange Multiplier test is a valuable tool in time series forecasting, as it helps to assess the adequacy of a time series model and to identify areas for improvement. By testing for autocorrelation in the residuals, the test helps to ensure that the model is accurately capturing the underlying patterns in the time series data. The test is also useful for determining the appropriate order of an autoregressive integrated moving average (ARIMA) model.</p> <p>The BGLM test can be calculated using the <code>acorr_breusch_godfrey()</code> function in the <code>statsmodels</code> package in Python. The function takes a fitted regression model and the maximum number of lags to include in the test as input, and returns the BGLM test statistic and associated p-value. If the p-value of the test is less than a certain significance level (e.g. \\(0.05\\)), then there is evidence of significant autocorrelation in the residuals of the regression model up to the specified lag.</p> <p>Parameters:</p> Name Type Description Default <code>res</code> <code>Union[RegressionResults, RegressionResultsWrapper]</code> <p>Estimation results for which the residuals are tested for serial correlation.</p> required <code>nlags</code> <code>Optional[int]</code> <p>Number of lags to include in the auxiliary regression. (<code>nlags</code> is highest lag). Defaults to <code>None</code>.</p> <code>None</code> <code>store</code> <code>bool</code> <p>If <code>store</code> is <code>True</code>, then an additional class instance that contains intermediate results is returned. Defaults to <code>False</code>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>lm</code> <code>float</code> <p>Lagrange multiplier test statistic.</p> <code>lmpval</code> <code>float</code> <p>The <code>p-value</code> for Lagrange multiplier test.</p> <code>fval</code> <code>float</code> <p>The value of the <code>f-statistic</code> for F test, alternative version of the same test based on F test for the parameter restriction.</p> <code>fpval</code> <code>float</code> <p>The <code>p-value</code> of the F test.</p> <code>res_store</code> <code>Optional[ResultsStore]</code> <p>A class instance that holds intermediate results. Only returned if <code>store=True</code>.</p> <p>Credit</p> <ul> <li>All credit goes to the <code>statsmodels</code> library.</li> </ul> <p>Examples</p> Test for Breusch-Godfrey Lagrange Multiplier in residual autocorrelation<pre><code>&gt;&gt;&gt; from statsmodels import api as sm\n&gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import bglm\n&gt;&gt;&gt; y = sm.datasets.longley.load_pandas().endog\n&gt;&gt;&gt; X = sm.datasets.longley.load_pandas().exog\n&gt;&gt;&gt; X = sm.add_constant(X)\n&gt;&gt;&gt; res_lm, res_lmpval, res_fval, res_fpval = bglm(sm.OLS(y, X).fit())\n&gt;&gt;&gt; print(res_lm)\n5.1409448555268185\n&gt;&gt;&gt; print(res_lmpval)\n0.16176265367835008\n&gt;&gt;&gt; print(res_fval)\n0.9468493873718188\n&gt;&gt;&gt; print(res_fpval)\n0.4751521243357578\n</code></pre> References <ol> <li>Greene, W. H. Econometric Analysis. New Jersey. Prentice Hall; 5th edition. (2002).</li> </ol> See Also <ul> <li><code>statsmodels.regression.linear_model.OLS.fit</code>: Fit a linear model.</li> <li><code>statsmodels.regression.linear_model.RegressionResults</code>: The output results of a linear regression model.</li> <li><code>statsmodels.stats.diagnostic.het_arch</code>: Conditional heteroskedasticity testing.</li> <li><code>statsmodels.stats.diagnostic.acorr_ljungbox</code>: Ljung-Box test for serial correlation.</li> <li><code>statsmodels.stats.diagnostic.acorr_lm</code>: Lagrange Multiplier tests for autocorrelation.</li> <li><code>statsmodels.stats.diagnostic.acorr_breusch_godfrey</code>: Breusch-Godfrey test for serial correlation.</li> <li><code>ts_stat_tests.algorithms.correlation.lb</code>: Ljung-Box test of autocorrelation in residuals.</li> <li><code>ts_stat_tests.algorithms.correlation.lm</code>: Lagrange Multiplier tests for autocorrelation.</li> <li><code>ts_stat_tests.algorithms.correlation.bglm</code>: Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation.</li> </ul> Source code in <code>src/ts_stat_tests/algorithms/correlation.py</code> <pre><code>@typechecked\ndef bglm(\n    res: Union[RegressionResults, RegressionResultsWrapper],\n    nlags: Optional[int] = None,\n    *,\n    store: bool = False,\n) -&gt; Union[\n    tuple[float, float, float, float],\n    tuple[float, float, float, float, ResultsStore],\n]:\n    \"\"\"\n    !!! note \"Summary\"\n\n        The Breusch-Godfrey Lagrange Multiplier (BGLM) test is a statistical test used in time series forecasting to test for the presence of autocorrelation in the residuals of a model. The test is a generalization of the LM test and can be used to test for autocorrelation up to a specified order.\n\n        This function will implement the [`acorr_breusch_godfrey()`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_breusch_godfrey.html) function from the [`statsmodels`](https://www.statsmodels.org) library.\n\n    ???+ abstract \"Details\"\n\n        BG adds lags of residual to exog in the design matrix for the auxiliary regression with residuals as endog. See Greene (2002), section 12.7.1.\n\n        The BGLM test is performed by first fitting a time series model to the data and then obtaining the residuals from the model. The residuals are then used to estimate the autocorrelation function (ACF) up to a specified order, typically using the Box-Pierce or Ljung-Box tests. The estimated ACF values are then used to construct the BGLM test statistic, which is compared to a chi-squared distribution with degrees of freedom equal to the number of lags tested.\n\n        The BGLM test statistic is calculated as:\n\n        $$\n        BGLM = n \\\\times R^2\n        $$\n\n        where:\n\n        - $n$ is the sample size and\n        - $R^2$ is the coefficient of determination from a regression of the residuals on the lagged values of the residuals and the lagged values of the predictor variable.\n\n        ```\n        BGLM = n * R^2\n        ```\n\n        Under the null hypothesis that there is no autocorrelation in the residuals of the regression model, the BGLM test statistic follows a chi-squared distribution with degrees of freedom equal to the number of lags included in the model.\n\n        If the test statistic is greater than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is rejected, indicating that there is evidence of autocorrelation in the residuals. This suggests that the time series model is inadequate, and that additional terms may need to be added to the model to account for the remaining autocorrelation.\n\n        If the test statistic is less than the critical value from the chi-squared distribution, then the null hypothesis of no autocorrelation is not rejected, indicating that there is no evidence of autocorrelation in the residuals. This suggests that the time series model is adequate, and that no further improvements are needed.\n\n        Overall, the Breusch-Godfrey Lagrange Multiplier test is a valuable tool in time series forecasting, as it helps to assess the adequacy of a time series model and to identify areas for improvement. By testing for autocorrelation in the residuals, the test helps to ensure that the model is accurately capturing the underlying patterns in the time series data. The test is also useful for determining the appropriate order of an autoregressive integrated moving average (ARIMA) model.\n\n        The BGLM test can be calculated using the `acorr_breusch_godfrey()` function in the `statsmodels` package in Python. The function takes a fitted regression model and the maximum number of lags to include in the test as input, and returns the BGLM test statistic and associated p-value. If the p-value of the test is less than a certain significance level (e.g. $0.05$), then there is evidence of significant autocorrelation in the residuals of the regression model up to the specified lag.\n\n    Params:\n        res (Union[RegressionResults, RegressionResultsWrapper]):\n            Estimation results for which the residuals are tested for serial correlation.\n        nlags (Optional[int], optional):\n            Number of lags to include in the auxiliary regression. (`nlags` is highest lag).&lt;br&gt;\n            Defaults to `None`.\n        store (bool, optional):\n            If `store` is `True`, then an additional class instance that contains intermediate results is returned.&lt;br&gt;\n            Defaults to `False`.\n\n    Returns:\n        lm (float):\n            Lagrange multiplier test statistic.\n        lmpval (float):\n            The `p-value` for Lagrange multiplier test.\n        fval (float):\n            The value of the `f-statistic` for F test, alternative version of the same test based on F test for the parameter restriction.\n        fpval (float):\n            The `p-value` of the F test.\n        res_store (Optional[ResultsStore]):\n            A class instance that holds intermediate results. Only returned if `store=True`.\n\n    !!! success \"Credit\"\n        - All credit goes to the [`statsmodels`](https://www.statsmodels.org/) library.\n\n    !!! example \"Examples\"\n\n        ```pycon {.py .python linenums=\"1\" title=\"Test for Breusch-Godfrey Lagrange Multiplier in residual autocorrelation\"}\n        &gt;&gt;&gt; from statsmodels import api as sm\n        &gt;&gt;&gt; from ts_stat_tests.algorithms.correlation import bglm\n        &gt;&gt;&gt; y = sm.datasets.longley.load_pandas().endog\n        &gt;&gt;&gt; X = sm.datasets.longley.load_pandas().exog\n        &gt;&gt;&gt; X = sm.add_constant(X)\n        &gt;&gt;&gt; res_lm, res_lmpval, res_fval, res_fpval = bglm(sm.OLS(y, X).fit())\n        &gt;&gt;&gt; print(res_lm)\n        5.1409448555268185\n        &gt;&gt;&gt; print(res_lmpval)\n        0.16176265367835008\n        &gt;&gt;&gt; print(res_fval)\n        0.9468493873718188\n        &gt;&gt;&gt; print(res_fpval)\n        0.4751521243357578\n        ```\n\n    ??? question \"References\"\n        1. Greene, W. H. Econometric Analysis. New Jersey. Prentice Hall; 5th edition. (2002).\n\n    ??? tip \"See Also\"\n        - [`statsmodels.regression.linear_model.OLS.fit`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.OLS.fit.html): Fit a linear model.\n        - [`statsmodels.regression.linear_model.RegressionResults`](https://www.statsmodels.org/stable/generated/statsmodels.regression.linear_model.RegressionResults.html): The output results of a linear regression model.\n        - [`statsmodels.stats.diagnostic.het_arch`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.het_arch.html#statsmodels.stats.diagnostic.het_arch): Conditional heteroskedasticity testing.\n        - [`statsmodels.stats.diagnostic.acorr_ljungbox`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_ljungbox.html): Ljung-Box test for serial correlation.\n        - [`statsmodels.stats.diagnostic.acorr_lm`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_lm.html): Lagrange Multiplier tests for autocorrelation.\n        - [`statsmodels.stats.diagnostic.acorr_breusch_godfrey`](https://www.statsmodels.org/stable/generated/statsmodels.stats.diagnostic.acorr_breusch_godfrey.html): Breusch-Godfrey test for serial correlation.\n        - [`ts_stat_tests.algorithms.correlation.lb`][ts_stat_tests.algorithms.correlation.lb]: Ljung-Box test of autocorrelation in residuals.\n        - [`ts_stat_tests.algorithms.correlation.lm`][ts_stat_tests.algorithms.correlation.lm]: Lagrange Multiplier tests for autocorrelation.\n        - [`ts_stat_tests.algorithms.correlation.bglm`][ts_stat_tests.algorithms.correlation.bglm]: Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation.\n    \"\"\"\n    return acorr_breusch_godfrey(  # type: ignore  # statsmodels typing for acorr_breusch_godfrey is incomplete/incompatible with our RegressionResults types\n        res=res,\n        nlags=nlags,\n        store=store,\n    )\n</code></pre>"},{"location":"usage/changelog/","title":"Change Log","text":"<p>v0.1.0</p>"},{"location":"usage/changelog/#v010-initial-release-of-time-series-statistical-tests","title":"v0.1.0 - Initial release of Time Series Statistical Tests","text":"<p><code>v0.1.0</code> <code>2026-01-05</code> data-science-extensions/ts-stat-tests/releases/v0.1.0</p> Release Notes Updates <ul> <li><code>085a4c0</code>: Re-enable key components in the <code>cd</code> workflow, ready for first deployment     (by chrimaho)        * <code>a722504</code>: Ensure coverage reports are staged even when directory paths are ignored.<ul> <li>Use the <code>--force</code> flag in the <code>git_add_coverage_report()</code> function.</li> <li>Bypass <code>.gitignore</code> restrictions to guarantee documentation updates. (by chrimaho)        * <code>85f68e6</code>: Hide some lines in <code>cd</code> to check workflow during release (by chrimaho)        * <code>16c9288</code>: Standardise type hints and remove dependencies</li> <li>Replace custom type hints with native Python equivalents for the <code>__all__</code> variable</li> <li>Remove the unnecessary import from the <code>toolbox_python.collection_types</code> module</li> <li>Update the <code>replace()</code> function to use the <code>re.Match()</code> class directly (by chrimaho)        * <code>8e76143</code>: Fix missing justifications in docs (by chrimaho)        * <code>a3085ff</code>: Fix typos (by chrimaho)        * <code>aef58db</code>: Fix typo (by chrimaho)        * <code>90c953b</code>: Fix incorrect percentages (by chrimaho)        * <code>15202ad</code>: Re-enable <code>mkdocs</code> checks (by chrimaho)        * <code>728c24a</code>: Add overview of module progress (by chrimaho)        * <code>02d36d1</code>: Add <code>correlation</code> module docs page (by chrimaho)        * <code>60c7127</code>: Fix some typos on the <code>correlation</code> module docs (by chrimaho)        * <code>fe9003d</code>: Add docs landing page and home page structures (by chrimaho)        * <code>37fab51</code>: Add custom hooks for docs to utilise (by chrimaho)        * <code>83dd55f</code>: Initial commit of package icons (by chrimaho)        * <code>8c3f0cf</code>: Initial commit of package stylesheets (by chrimaho)        * <code>950c1fb</code>: Fill in README file (by chrimaho)        * <code>e2c62bd</code>: Initial commit of standard package docs (by chrimaho)        * <code>1f1a9d6</code>: Initial commit of <code>mkdocs</code> config file (by chrimaho)        * <code>c7839a4</code>: Set UTF-8 encoding for Python I/O in CI</li> <li>Define the <code>PYTHONIOENCODING</code> environment variable to ensure consistent character handling across CI environments (by chrimaho)        * <code>cc368ff</code>: Fix typo (by chrimaho)        * <code>0b2d590</code>: Standardise variable naming convention</li> <li>Rename the repository variable to lower-case to follow standard naming conventions for local instances within the <code>main()</code> function.</li> <li>Update references to the variable when calling the <code>.get_releases()</code> method and <code>.get_commits()</code> method.</li> <li>The variable name <code>REPO</code> is defined in uppercase following constant naming conventions, but it's not a constant - it's a dynamically created repository object. Consider using lowercase <code>repo</code> to follow Python naming conventions where uppercase names are reserved for constants. (by chrimaho)        * <code>ed5404d</code>: Initialise the output file before starting the main process</li> <li>Call the <code>prepare_output_file()</code> function to ensure the destination is ready for writing.</li> <li>Set up the file environment prior to opening the GitHub and file context managers. (by chrimaho)        * <code>6060627</code>: Define constant for short SHA length</li> <li>Introduce <code>SHORT_SHA_LENGTH</code> constant to replace hardcoded magic numbers</li> <li>Update <code>add_commit_info()</code> function to use defined constant</li> <li>Standardise representation of short commit identifiers for improved maintainability (by chrimaho)        * <code>aa243b7</code>: FixPotential bug If <code>commit.author</code> is <code>None</code>, the code constructs an incomplete markdown link <code>[]()</code> with empty values. This would result in broken links in the changelog. Consider providing a default fallback text like <code>\"Unknown\"</code> for the author name and omitting the link altogether when the author is unavailable. (by chrimaho)        * <code>d1e0800</code>: Refactor correlation tests to use pytest</li> <li>Replace <code>.assertRaises()</code> method with <code>raises()</code> function to standardise test assertions</li> <li>Update <code>.test_is_correlated()</code> method to expect <code>NotImplementedError()</code> class from <code>is_correlated()</code> function (by chrimaho)        * <code>01d3544</code>: Create script to automate changelog generation</li> <li>Introduce the <code>src/utils/changelog.py</code> script to automate the creation of <code>CHANGELOG.md</code> by retrieving data via the GitHub API.</li> <li>Implement the <code>prepare_output_file()</code> function to ensure the document is recreated from scratch on each run.</li> <li>Define the <code>add_page_styling()</code> function to embed CSS for better navigation in Markdown viewers.</li> <li>Utilise the <code>main()</code> function to fetch releases via the <code>.get_releases()</code> method and filter out irrelevant commits to maintain a clean history.</li> <li>Use the <code>Github()</code> class to authenticate and manage repository interactions.</li> <li>Standardise the output format for release notes and commit details to improve readability. (by chrimaho)        * <code>702d902</code>: Add continuous delivery workflow for releases</li> <li>Introduce <code>cd.yml</code> to automate the release cycle triggered by published GitHub releases.</li> <li>Orchestrate a multi-stage pipeline including testing, building, PyPI deployment, and documentation generation.</li> <li>Utilise <code>uv</code> to synchronise dependencies and manage the build process for improved performance.</li> <li>Execute the <code>git_update_version_cli()</code> function to handle versioning during the build stage.</li> <li>Verify package integrity using an installation matrix across multiple operating systems and Python versions.</li> <li>Automate changelog generation via the <code>changelog.py</code> script and push updates to the repository.</li> <li>Upload distribution assets to GitHub releases and coverage data to Codecov. (by chrimaho)        * <code>3e014f6</code>: Increase CI job parallelism</li> <li>Increase the <code>max-parallel</code> limit to 30 to allow for more concurrent jobs</li> <li>Optimise the CI pipeline performance by utilising more available runners for the matrix build (by chrimaho)        * <code>cd1f7ad</code>: Add CI workflow for automated code validation</li> <li>Implement GitHub Actions to automate validation on push and pull request events.</li> <li>Configure a check job for non-main branches to facilitate early issue detection.</li> <li>Define a matrix strategy to ensure cross-platform compatibility across various OS types.</li> <li>Support Python versions 3.9 through 3.14 to maintain broad environment stability.</li> <li>Utilise <code>uv</code> for efficient dependency management and execution of the validation script. (by chrimaho)        * <code>1f68794</code>: Add Dependabot for GitHub Actions</li> <li>Initialise the configuration file to manage <code>github-actions</code> dependencies.</li> <li>Schedule weekly updates to ensure actions remain current.</li> <li>Assign a default reviewer and label to simplify the pull request review process. (by chrimaho)        * <code>c059c7c</code>: Raise error for unimplemented correlation test</li> <li>Replace the <code>None</code> return value in the <code>is_correlated()</code> function with a <code>NotImplementedError</code> to explicitly signal that this logic is a placeholder and has not yet been implemented. (by chrimaho)        * <code>470fbfa</code>: Clarify type ignore reasons</li> <li>Document rationale for <code># type: ignore</code> on calls to the <code>acorr_lm()</code> function and <code>acorr_breusch_godfrey()</code> function</li> <li>Note that <code>statsmodels</code> type hints are incomplete or incompatible with internal <code>RegressionResults()</code> class types (by chrimaho)        * <code>8ea4c9d</code>: Standardise equality check utility functions</li> <li>Set default value for <code>places</code> parameter in <code>is_almost_equal()</code> function and <code>assert_almost_equal()</code> function overloads</li> <li>Refactor <code>is_almost_equal()</code> function to prioritise argument validation over value equality checks</li> <li>Standardise type annotations for the <code>params</code> dictionary in <code>assert_almost_equal()</code> function</li> <li>Simplify logic for error messages in <code>assert_almost_equal()</code> function by defaulting to precision-based comparison (by chrimaho)        * <code>00a6029</code>: Initialise parent classes in test suites</li> <li>Call the <code>.setUpClass()</code> and <code>.tearDownClass()</code> methods of the superclass in the <code>BaseTester()</code> class</li> <li>Implement the <code>.setUp()</code>, <code>.tearDown()</code>, and <code>.tearDownClass()</code> methods in the <code>TestCorrelation()</code> class to ensure proper lifecycle management (by chrimaho)        * <code>3b018b4</code>: Remove redundant lines in config file (by chrimaho)        * <code>58163f0</code>: Update documentation headers and module summaries</li> <li>Add header comments to the <code>setup.py</code> script explaining its role in unit tests.</li> <li>Standardise documentation in the <code>errors.py</code> module regarding error generation and data equality checks. (by chrimaho)        * <code>9c7206a</code>: Optimise data generation and loading via caching</li> <li>Apply the <code>@lru_cache</code> decorator to the <code>get_random_generator()</code> function and various <code>data_*()</code> functions to reduce redundant test overhead.</li> <li>Remove the global <code>seed()</code> function call to favour the use of independent random generators.</li> <li>Enhance the <code>load_airline()</code> function with result caching to improve data retrieval performance. (by chrimaho)        * <code>891edd2</code>: Refactor correlation logic and remove redundant code</li> <li>Remove commented-out code from the <code>correlation()</code> function to clean up the source.</li> <li>Update the <code>is_correlated()</code> function to return <code>None</code> instead of raising a <code>NotImplementedError</code> class to provide a neutral placeholder. (by chrimaho)        * <code>0d078f3</code>: Add implementation error to the <code>is_correlated</code> function The function <code>is_correlated</code> is defined but returns <code>None</code> and serves no purpose. It's marked as a placeholder in the docstring, but placeholder functions should either be removed or raise <code>NotImplementedError</code> to indicate they're not yet implemented. Returning <code>None</code> with no implementation can lead to confusion. (by chrimaho)        * <code>335045d</code>: Refine correlation APIs and improve type safety</li> <li>Introduce <code>@overload()</code> function signatures for <code>acf()</code>, <code>pacf()</code>, <code>ccf()</code>, <code>lm()</code>, <code>bglm()</code>, and <code>correlation()</code> functions to improve static analysis and developer experience.</li> <li>Standardise parameter validation using <code>Literal()</code> class aliases for algorithm options within the <code>acf()</code> and <code>pacf()</code> functions.</li> <li>Update the <code>ccf()</code> function to support <code>nlags</code> and <code>alpha</code> parameters for better consistency with other correlation tools.</li> <li>Enforce keyword-only arguments for the <code>store</code> parameter in <code>lm()</code> and <code>bglm()</code> functions and the <code>alpha</code> and <code>qstat</code> parameters in the <code>acf()</code> function.</li> <li>Refine the <code>correlation()</code> function to act as a unified interface with specific return type hints based on the provided algorithm string.</li> <li>Ensure the <code>lb()</code> function return type is correctly annotated as a <code>pd.DataFrame()</code> class. (by chrimaho)        * <code>6176186</code>: Validate data type in <code>load_airline()</code></li> <li>Ensure the <code>load_airline()</code> function returns a <code>pd.Series()</code> class by introducing a type check</li> <li>Raise a <code>TypeError()</code> class if the result of the <code>.squeeze()</code> method is not a series to prevent errors when initialising the <code>pd.PeriodIndex()</code> class</li> <li>Resolve PyRight errors (by chrimaho)        * <code>d668fa7</code>: Organise and clean up check function calls</li> <li>Remove the redundant commented out <code>check_mypy()</code> function call from the <code>check()</code> function</li> <li>Move and disable the <code>check_mkdocs()</code> function call to execute after the <code>check_build()</code> function call within the <code>check()</code> function (by chrimaho)        * <code>b9a9c1a</code>: Add <code>pyright</code> for static type analysis</li> <li>Include <code>pyright</code> in the development dependencies</li> <li>Define the <code>check_pyright()</code> function to analyse code types</li> <li>Integrate the <code>check_pyright()</code> function into the validation <code>check()</code> function (by chrimaho)        * <code>d7b0669</code>: Ignore complexipy cache directory</li> <li>Prevent <code>.complexipy_cache/*</code> from being tracked in the repository (by chrimaho)        * <code>15b4f17</code>: Expand correlation tests and organise imports</li> <li>Import <code>correlation()</code> and <code>is_correlated()</code> functions to expand test coverage for the correlation module.</li> <li>Add <code>.test_correlation_ccf_raises()</code> and <code>.test_correlation_bglm()</code> methods to validate algorithm error handling and behaviour.</li> <li>Simplify the <code>.setUpClass()</code> method by removing redundant type annotations from class attributes and local variables.</li> <li>Reformat import statements to improve code structure and maintainability. (by chrimaho)        * <code>f61bbf8</code>: Update type hints to use abstract base classes</li> <li>Replace <code>Dict()</code> and <code>List()</code> with <code>Mapping()</code> and <code>Collection()</code> classes in the <code>generate_error_message()</code> function to support broader input types</li> <li>Standardise the use of the built-in <code>dict()</code> class within the <code>assert_almost_equal()</code> function</li> <li>Simplify complex <code>Union</code> type annotations to improve code maintainability (by chrimaho)        * <code>c77a089</code>: Introduce correlation test dispatcher</li> <li>Implement the <code>correlation()</code> function to provide a unified interface for various statistical correlation algorithms.</li> <li>Map string aliases to internal implementations such as the <code>_acf()</code>, <code>_pacf()</code>, and <code>_ccf()</code> functions.</li> <li>Utilise the <code>generate_error_message()</code> function to handle unsupported algorithm selections with descriptive feedback.</li> <li>Ensure the <code>y</code> parameter is provided when the <code>ccf</code> algorithm is requested via the <code>correlation()</code> function.</li> <li>Add a placeholder <code>is_correlated()</code> function to define the intended package structure. (by chrimaho)        * <code>47ff459</code>: Refine type annotations for correlation functions</li> <li>Standardise return type signatures for the <code>lm()</code> function and the <code>bglm()</code> function.</li> <li>Introduce <code>overload()</code> definitions for the <code>lm()</code> function to improve static analysis and type safety.</li> <li>Replace <code>np.float64</code>, <code>np.ndarray</code>, and complex <code>Union</code> types with specific <code>float</code> tuples to ensure consistency. (by chrimaho)        * <code>4f783a0</code>: Add error handling and float comparison utilities</li> <li>Add <code>generate_error_message()</code> function to standardise error reporting for invalid parameter options.</li> <li>Implement <code>is_almost_equal()</code> function to provide flexible float comparison using either decimal places or a specific delta.</li> <li>Include <code>assert_almost_equal()</code> function to facilitate testing by raising descriptive <code>AssertionError</code> messages when float values deviate beyond tolerances. (by chrimaho)        * <code>aa0c816</code>: Disable invalid-name linting rule</li> <li>Disable the <code>C0103</code> rule to standardise the project's naming convention exceptions. (by chrimaho)        * <code>8b1424e</code>: Standardise docstrings and improve type hinting</li> <li>Define <code>VALID_PACF_METHODS</code> constant to centralise allowed methods for the <code>pacf()</code> function.</li> <li>Update docstring callouts to use <code>!!! note</code> and <code>???+ abstract</code> admonitions for improved documentation consistency.</li> <li>Refine parameter type descriptions in docstrings to use <code>ArrayLike</code> and <code>Optional</code> types for the <code>acf()</code>, <code>pacf()</code>, <code>ccf()</code>, <code>lb()</code>, and <code>lm()</code> functions.</li> <li>Reformat code examples to use <code>pycon</code> syntax and ensure correct line continuation for better rendering.</li> <li>Clean up <code>!!! deprecation</code> blocks and standardise internal formatting. (by chrimaho)        * <code>02fd5d2</code>: Document and standardise data utility module</li> <li>Add module-level docstrings and structural headers to improve navigation</li> <li>Enhance the <code>load_airline()</code> function with comprehensive documentation and academic references</li> <li>Remove unused <code>numpy</code> import to tidy the <code>imports</code> section</li> <li>Standardise the file layout using consistent section markers (by chrimaho)        * <code>f6824ad</code>: Suppress specific Pylint linting warnings</li> <li>Introduce configuration to relax linting constraints</li> <li>Minimise noise by disabling checks for line length and file length</li> <li>Disable warnings for argument counts and redefined built-ins (by chrimaho)        * <code>716db6d</code>: Expand and standardise docstring sections</li> <li>Add <code>credit</code>, <code>references</code>, <code>see also</code>, and <code>deprecation</code> sections to the configuration</li> <li>Standardise the internal key order for all section definitions to maintain consistency</li> <li>Align configuration properties to improve visual clarity and maintenance (by chrimaho)        * <code>48abfe1</code>: Update package name and add script documentation</li> <li>Add a header comment block to the utility script to document usage and purpose.</li> <li>Update the <code>PACKAGE_NAME</code> constant to reflect the project identity. (by chrimaho)        * <code>9ba6915</code>: Introduce a centralised scripts module</li> <li>Provide a centralised suite of automation utilities for project maintenance.</li> <li>Implement a <code>run_command()</code> function to handle shell execution and argument expansion.</li> <li>Include a <code>lint()</code> function to standardise code formatting across the repository.</li> <li>Define a <code>check()</code> function to aggregate quality assurance tests and build verification.</li> <li>Automate git workflows for versioning, tagging, and documentation deployment.</li> <li>Expose utilities via a CLI entry point to simplify development and CI/CD pipelines. (by chrimaho)        * <code>c6021f8</code>: Introduce pre-commit hooks for code quality</li> <li>Standardise repository-wide formatting and linting rules</li> <li>Automate file-level sanitisation for whitespace and line endings</li> <li>Enforce Python style consistency using <code>black</code>, <code>isort</code>, and <code>pyupgrade</code></li> <li>Prevent direct commits to the <code>main</code> branch via <code>no-commit-to-branch</code></li> <li>Validate configuration files with <code>check-json</code>, <code>check-toml</code>, and <code>check-yaml</code></li> <li>Integrate <code>uv</code> lockfile and synchronisation checks to maintain environment integrity</li> <li>Sanitise documentation and check spelling with <code>blacken-docs</code> and <code>codespell</code></li> <li>Implement a local <code>ty-check</code> hook for type safety (by chrimaho)        * <code>ccc7aed</code>: Add unit tests for the <code>correlation</code> module</li> <li>Update <code>llvmlite</code> and <code>numba</code> packages to stable versions in <code>pyproject.toml</code></li> <li>Add <code>stochastic</code> package to the test dependencies</li> <li>Introduce <code>BaseTester()</code> class to standardise test data setup via the <code>.setUpClass()</code> method</li> <li>Implement <code>load_airline()</code> function to retrieve sample datasets for time series analysis</li> <li>Add unit tests for correlation functions including <code>acf()</code> and <code>pacf()</code></li> <li>Configure <code>uv</code> dependency overrides to maintain <code>numpy</code> package version consistency (by chrimaho)        * <code>f18bc49</code>: Fix the PACF formula clarity The PACF formula is incorrectly formatted and unclear. The notation <code>\"Corr(Y_t, Y_{t-k} / Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1})\"</code> uses division where conditional notation should be used. It should be <code>\"Corr(Y_t, Y_{t-k} | Y_{t-1}, Y_{t-2}, ..., Y_{t-k+1})\"</code> with a vertical bar (<code>|</code>) to denote conditioning, not a division symbol. (by chrimaho)        * <code>9de39de</code>: Correct ACF formula in <code>acf()</code> function docstring</li> <li>Update the mathematical expression in the <code>acf()</code> function docstring to correctly show square roots in the denominator.</li> <li>Add the simplified version of the ACF formula for stationary time series to the <code>acf()</code> function documentation. (by chrimaho)        * <code>984fb1d</code>: Fix typo issue Inconsistent terminology: The documentation states \"Ljung-Box and Box-Pierce statistic differ\" but should use \"statistics\" (plural) since there are two statistics being discussed. (by chrimaho)        * <code>04dbd4b</code>: Fix <code>p-value</code> formatting issue (by chrimaho)        * <code>9631202</code>: Fix self-referential documentation issue The text \"See <code>q_stat</code> for more information\" is circular and unhelpful since this IS the documentation for the <code>qstat</code> parameter. This reference should either be removed or point to relevant external documentation or the Returns section. (by chrimaho)        * <code>044f8a0</code>: Fix typo (by chrimaho)        * <code>cbf0fb7</code>: Fix duplicate word (by chrimaho)        * <code>feba34c</code>: Standardise formatting of <code>p-value</code> in docs (by chrimaho)        * <code>ac283f5</code>: Correct the formula provided in the <code>lm()</code> function documentation</li> <li>Clarify the relationship between the LM test, Engle's ARCH test, and the Breusch-Godfrey test within the <code>lm()</code> function</li> <li>Refine the mathematical definition of the test statistic to focus on the auxiliary regression <code>$R^2$</code>, the number of observations, and degrees of freedom</li> <li>Provide a clearer step-by-step procedure for fitting the time series model and running the auxiliary regression to obtain the test statistic</li> <li>Improve the explanation of the null hypothesis and the resulting asymptotic chi-squared distribution of the LM statistic</li> <li>Standardise document formatting by using KaTeX for math equations and back-ticks for parameters like <code>resid</code> and <code>nlags</code> (by chrimaho)        * <code>1d32293</code>: Add correlation algorithms for time series analysis</li> <li>Introduce <code>acf()</code> function, <code>pacf()</code> function, and <code>ccf()</code> function to compute temporal dependencies.</li> <li>Implement <code>lb()</code> function, <code>lm()</code> function, and <code>bglm()</code> function for residual autocorrelation testing.</li> <li>Utilise <code>statsmodels</code> library to ensure robust and standardised statistical results.</li> <li>Include detailed docstrings with mathematical formulas and usage examples.</li> <li>Apply <code>@typechecked</code> decorator to ensure runtime type safety for all algorithm inputs. (by chrimaho)        * <code>fa3e3fd</code>: Remove duplicate function declaration (by chrimaho)        * <code>94efa8b</code>: Initialise project structure and configuration</li> <li>Define project metadata and dependencies within <code>pyproject.toml</code> using <code>uv_build</code>.</li> <li>Configure linting, formatting, and testing tools to standardise development.</li> <li>Implement <code>strip_ansi_codes()</code> function in <code>setup.py</code> to facilitate environment-agnostic CLI output verification.</li> <li>Add test naming helper functions like <code>name_func_flat_list()</code> function to <code>setup.py</code>.</li> <li>Configure <code>sys.path</code> in the <code>tests</code> <code>__init__.py</code> file to ensure local module resolution.</li> <li>Extend <code>.gitignore</code> to exclude <code>uv.lock</code>, <code>.vscode/</code>, and other local environment directories. (by chrimaho)        * <code>b963f05</code>: Initial commit (by chrimaho)</li> </ul> </li> </ul>"},{"location":"usage/changelog/#overview","title":"\ud83d\ude80 Overview","text":"<p>This is the initial release of <code>ts-stat-tests</code>, a Python package dedicated to providing a unified and standardised interface for time series statistical testing. This release establishes the foundational project infrastructure, implements a comprehensive suite of correlation algorithms, and provides extensive documentation and automated CI/CD pipelines. It aims to bridge the gap between R and Python for time series analysis by offering a single, robust library for standard time series statistical tests.</p>"},{"location":"usage/changelog/#implementation-details","title":"\ud83d\udee0\ufe0f Implementation details","text":""},{"location":"usage/changelog/#core-correlation-algorithms","title":"Core correlation algorithms","text":"<ul> <li>Introduce the <code>acf()</code> function, <code>pacf()</code> function, and <code>ccf()</code> function to estimate autocorrelation, partial autocorrelation, and cross-correlation functions.</li> <li>Implement residual diagnostic tests including the <code>lb()</code> function for Ljung-Box testing, the <code>lm()</code> function for Lagrange Multiplier tests, and the <code>bglm()</code> function for Breusch-Godfrey tests.</li> <li>Provide a unified <code>correlation()</code> dispatcher function to simplify algorithm selection via string aliases.</li> <li>Utilise the <code>statsmodels</code> library as the underlying engine to ensure robust and standardised statistical results.</li> <li>Include detailed docstrings for all algorithms, featuring mathematical definitions, parameter details, and practical usage examples.</li> </ul>"},{"location":"usage/changelog/#project-infrastructure-and-automation","title":"Project infrastructure and automation","text":"<ul> <li>Initialise the project structure using <code>uv</code> for efficient dependency management and build orchestration.</li> <li>Implement a centralised <code>scripts.py</code> module to provide a suite of automation utilities for linting, checking, and maintenance.</li> <li>Configure pre-commit hooks to enforce code quality, standardise formatting, and prevent direct commits to the <code>main</code> branch.</li> <li>Establish GitHub Actions workflows for continuous integration (<code>ci.yml</code>) and continuous delivery (<code>cd.yml</code>) to automate testing and releases.</li> <li>Integrate Dependabot to automate updates for GitHub Actions dependencies on a weekly schedule.</li> </ul>"},{"location":"usage/changelog/#documentation-and-presentation","title":"Documentation and presentation","text":"<ul> <li>Launch a comprehensive documentation site using <code>mkdocs</code> with the <code>material</code> theme and custom stylesheets.</li> <li>Expand the <code>README.md</code> with project badges, a motivation section, and a detailed feature implementation table.</li> <li>Implement custom documentation hooks in <code>shortcodes.py</code> to support dynamic content and enhanced rendering.</li> <li>Add placeholder pages and \"To Do\" notes for <code>CHANGELOG.md</code> and <code>CONTRIBUTING.md</code> to guide future documentation efforts.</li> </ul>"},{"location":"usage/changelog/#type-safety-and-quality-assurance","title":"Type safety and quality assurance","text":"<ul> <li>Enforce runtime type safety by applying the <code>@typechecked</code> decorator to all core algorithm functions.</li> <li>Integrate <code>pyright</code> for rigorous static type analysis and resolve complex type-hinting issues.</li> <li>Implement utility functions for error reporting and float comparison, including <code>generate_error_message()</code>, <code>is_almost_equal()</code>, and <code>assert_almost_equal()</code>.</li> <li>Achieve 100% unit test coverage for the correlation module to ensure reliability and correctness.</li> </ul>"},{"location":"usage/changelog/#checklist","title":"\u2705 Checklist","text":"<ul> <li> Establish foundational project structure and configuration.</li> <li> Implement core correlation algorithms and diagnostic tests.</li> <li> Create a centralised automation and maintenance script.</li> <li> Launch a comprehensive documentation site and expand <code>README.md</code>.</li> <li> Configure CI/CD pipelines and pre-commit hooks.</li> <li> Enforce runtime and static type safety across the package.</li> <li> Achieve high unit test coverage for the initial release.</li> </ul>"},{"location":"usage/changelog/#pull-requests","title":"\ud83d\udcaa Pull Requests","text":"<ul> <li>Initialise project structure and configuration by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/15</li> <li>Add correlation algorithms for time series analysis by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/16</li> <li>Set up unit tests and enhance development infrastructure by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/17</li> <li>Establish CI/CD pipelines and automate release workflows by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/18</li> <li>Bump actions/checkout from 5 to 6 by @dependabot[bot] in https://github.com/data-science-extensions/ts-stat-tests/pull/19</li> <li>Bump actions/setup-python from 5 to 6 by @dependabot[bot] in https://github.com/data-science-extensions/ts-stat-tests/pull/23</li> <li>Bump astral-sh/setup-uv from 6 to 7 by @dependabot[bot] in https://github.com/data-science-extensions/ts-stat-tests/pull/22</li> <li>Bump actions/upload-artifact from 4 to 6 by @dependabot[bot] in https://github.com/data-science-extensions/ts-stat-tests/pull/21</li> <li>Bump actions/download-artifact from 5 to 7 by @dependabot[bot] in https://github.com/data-science-extensions/ts-stat-tests/pull/20</li> <li>Enhance project documentation and presentation by @chrimaho in https://github.com/data-science-extensions/ts-stat-tests/pull/24</li> </ul>"},{"location":"usage/changelog/#new-contributors","title":"\ud83c\udd95 New Contributors","text":"<ul> <li>@chrimaho made their first contribution in https://github.com/data-science-extensions/ts-stat-tests/pull/15</li> <li>@dependabot[bot] made their first contribution in https://github.com/data-science-extensions/ts-stat-tests/pull/19</li> </ul> <p>Full Changelog: https://github.com/data-science-extensions/ts-stat-tests/commits/v0.1.0</p>"},{"location":"usage/contributing/","title":"Contributing","text":"<p>To Do</p> <p>Add docs for contributing to this package.</p>"},{"location":"usage/overview/","title":"Overview","text":"# Time Series Statistical Tests  ### `ts-stat-tests`  [![PyPI version](https://img.shields.io/pypi/v/ts-stat-tests?label=version&amp;logo=git&amp;color=blue)](https://pypi.org/project/ts-stat-tests/) [![Released](https://img.shields.io/github/release-date/data-science-extensions/ts-stat-tests?label=released&amp;color=blue&amp;logo=google-calendar&amp;logoColor=FF7143)](https://pypi.org/project/ts-stat-tests/#history) [![Python](https://img.shields.io/pypi/pyversions/ts-stat-tests.svg?style=flat&amp;logo=python&amp;logoColor=FFDE50&amp;color=blue)](https://pypi.org/project/ts-stat-tests/) [![OS](https://img.shields.io/static/v1?label=os&amp;message=ubuntu+|+macos+|+windows&amp;color=blue&amp;logo=ubuntu&amp;logoColor=green)](https://pypi.org/project/ts-stat-tests/) [![Build Tests](https://img.shields.io/github/actions/workflow/status/data-science-extensions/ts-stat-tests/ci-build-package.yml?logo=github&amp;logoColor=white&amp;label=build+tests)](https://github.com/data-science-extensions/ts-stat-tests/actions/workflows/ci-build-package.yml) [![MyPy Tests](https://img.shields.io/github/actions/workflow/status/data-science-extensions/ts-stat-tests/ci-mypy-tests.yml?logo=github&amp;logoColor=white&amp;label=mypy+tests)](https://github.com/data-science-extensions/ts-stat-tests/actions/workflows/ci-mypy-tests.yml) [![Unit Tests](https://img.shields.io/github/actions/workflow/status/data-science-extensions/ts-stat-tests/ci-unit-tests.yml?logo=github&amp;logoColor=white&amp;label=unit+tests)](https://github.com/data-science-extensions/ts-stat-tests/actions/workflows/ci-unit-tests.yml) [![codecov](https://codecov.io/gh/data-science-extensions/ts-stat-tests/branch/main/graph/badge.svg)](https://codecov.io/gh/data-science-extensions/ts-stat-tests) [![Deploy Docs](https://img.shields.io/github/actions/workflow/status/data-science-extensions/ts-stat-tests/cd-deploy-docs.yml?logo=github&amp;logoColor=white&amp;label=deploy+docs)](https://github.com/data-science-extensions/ts-stat-tests/actions/workflows/cd-deploy-docs.yml) [![Publish Package](https://img.shields.io/github/actions/workflow/status/data-science-extensions/ts-stat-tests/cd-publish-package.yml?logo=github&amp;logoColor=white&amp;label=publish+package)](https://github.com/data-science-extensions/ts-stat-tests/actions/workflows/ci-publish-package.yml) [![CodeQL](https://github.com/data-science-extensions/ts-stat-tests/actions/workflows/github-code-scanning/codeql/badge.svg?branch=main&amp;label=code+ql)](https://github.com/data-science-extensions/ts-stat-tests/actions/workflows/github-code-scanning/codeql) [![License][badge-license]](https://github.com/data-science-extensions/ts-stat-tests/blob/master/LICENSE) [![Downloads][badge-downloads]](https://piptrends.com/package/ts-stat-tests) [![Code Style][badge-style]](https://github.com/psf/black) [![Contributions](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/data-science-extensions/ts-stat-tests/issues)"},{"location":"usage/overview/#motivation","title":"Motivation","text":"<p>Time Series Analysis has been around for a long time, especially for doing Statistical Testing. Some Python packages are going a long way to make this even easier than it has ever been before. Such as <code>sktime</code> and <code>pycaret</code> and <code>pmdarima</code> and <code>statsmodels</code>.</p> <p>There are some typical Statistical Tests which are accessible in these Python (QS, Normality, Stability, etc). However, there are still some statistical tests which are not yet ported over to Python, but which have been written in R and are quite stable.</p> <p>Moreover, there is no one single library package for doing time-series statistical tests in Python.</p> <p>That's exactly what this package aims to achieve.</p> <p>A single package for doing all the standard time-series statistical tests.</p>"},{"location":"usage/overview/#tests","title":"Tests","text":"<p>Full credit goes to the packages listed in this table.</p> Type Name Source Package Source Language Implemented Correlation Auto-Correlation function (ACF) <code>statsmodels</code> Python \u2705 Correlation Partial Auto-Correlation function (PACF) <code>statsmodels</code> Python \u2705 Correlation Cross-Correlation function (CCF) <code>statsmodels</code> Python \u2705 Correlation Ljung-Box test of autocorrelation in residuals (LB) <code>statsmodels</code> Python \u2705 Correlation Lagrange Multiplier tests for autocorrelation (LM) <code>statsmodels</code> Python \u2705 Correlation Breusch-Godfrey Lagrange Multiplier tests for residual autocorrelation (BGLM) <code>statsmodels</code> Python \u2705 Regularity Approximate Entropy <code>antropy</code> python \ud83d\udd32 Regularity Sample Entropy <code>antropy</code> python \ud83d\udd32 Regularity Permutation Entropy <code>antropy</code> python \ud83d\udd32 Regularity Spectral Entropy <code>antropy</code> python \ud83d\udd32 Seasonality QS <code>seastests</code> R \ud83d\udd32 Seasonality Osborn-Chui-Smith-Birchenhall test of seasonality (OCSB) <code>pmdarima</code> Python \ud83d\udd32 Seasonality Canova-Hansen test for seasonal differences (CH) <code>pmdarima</code> Python \ud83d\udd32 Seasonality Seasonal Strength <code>tsfeatures</code> Python \ud83d\udd32 Seasonality Trend Strength <code>tsfeatures</code> Python \ud83d\udd32 Seasonality Spikiness <code>tsfeatures</code> Python \ud83d\udd32 Stability Stability <code>tsfeatures</code> Python \ud83d\udd32 Stability Lumpiness <code>tsfeatures</code> Python \ud83d\udd32 Stationarity Augmented Dickey-Fuller test for stationarity (ADF) <code>statsmodels</code> Python \ud83d\udd32 Stationarity Kwiatkowski-Phillips-Schmidt-Shin test for stationarity (KPSS) <code>statsmodels</code> Python \ud83d\udd32 Stationarity Range unit-root test for stationarity (RUR) <code>statsmodels</code> Python \ud83d\udd32 Stationarity Zivot-Andrews structural-break unit-root test (ZA) <code>statsmodels</code> Python \ud83d\udd32 Stationarity Phillips-Peron test for stationarity (PP) <code>pmdarima</code> Python \ud83d\udd32 Stationarity Elliott-Rothenberg-Stock (ERS) de-trended Dickey-Fuller test <code>arch</code> Python \ud83d\udd32 Stationarity Variance Ratio (VR) test for a random walk <code>arch</code> Python \ud83d\udd32 Normality Jarque-Bera test of normality (JB) <code>statsmodels</code> Python \ud83d\udd32 Normality Omnibus test for normality (OB) <code>statsmodels</code> Python \ud83d\udd32 Normality Shapiro-Wilk test for normality (SW) <code>scipy</code> Python \ud83d\udd32 Normality D'Agostino &amp; Pearson's test for normality <code>scipy</code> Python \ud83d\udd32 Normality Anderson-Darling test for normality <code>scipy</code> Python \ud83d\udd32 Linearity Harvey Collier test for linearity (HC) <code>statsmodels</code> Python \ud83d\udd32 Linearity Lagrange Multiplier test for linearity (LM) <code>statsmodels</code> Python \ud83d\udd32 Linearity Rainbow test for linearity (RB) <code>statsmodels</code> Python \ud83d\udd32 Linearity Ramsey's RESET test for neglected nonlinearity (RR) <code>statsmodels</code> Python \ud83d\udd32 Heteroscedasticity Engle's Test for Autoregressive Conditional Heteroscedasticity (ARCH) <code>statsmodels</code> Python \ud83d\udd32 Heteroscedasticity Breusch-Pagan Lagrange Multiplier test for heteroscedasticity (BPL) <code>statsmodels</code> Python \ud83d\udd32 Heteroscedasticity Goldfeld-Quandt test for homoskedasticity (GQ) <code>statsmodels</code> Python \ud83d\udd32 Heteroscedasticity White's Lagrange Multiplier Test for Heteroscedasticity (WLM) <code>statsmodels</code> Python \ud83d\udd32"},{"location":"usage/overview/#known-limitations","title":"Known limitations","text":"<ul> <li>These listed tests is not exhaustive, and there is probably some more that could be added. Therefore, we encourage you to raise issues or pull requests to add more statistical tests to this suite.</li> <li>This package does not re-invent any of these tests. It merely calls the underlying packages, and calls the functions which are already written elsewhere.</li> </ul>"}]}